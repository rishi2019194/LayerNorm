Model: google/vit-base-patch16-224-in21k, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 9
Label counts for Train:
  Label 4: 1600
  Label 1: 1600
  Label 8: 1600
  Label 2: 1600
  Label 7: 1600
  Label 3: 1600
  Label 0: 1600
  Label 6: 1600
  Label 5: 1600
  Label 9: 1600
Label counts for Validation:
  Label 3: 400
  Label 5: 400
  Label 0: 400
  Label 2: 400
  Label 7: 400
  Label 8: 400
  Label 9: 400
  Label 1: 400
  Label 6: 400
  Label 4: 400
Label counts for Test:
  Label 3: 1000
  Label 8: 1000
  Label 0: 1000
  Label 6: 1000
  Label 1: 1000
  Label 9: 1000
  Label 5: 1000
  Label 7: 1000
  Label 4: 1000
  Label 2: 1000
160
Actual labels:  [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]
Label counts for Train:
  Label 4: 1615
  Label 1: 1622
  Label 8: 1616
  Label 2: 1618
  Label 7: 1621
  Label 3: 1613
  Label 0: 1620
  Label 6: 1620
  Label 5: 1615
  Label 9: 1440
16000
(3, 224, 224)
Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:
- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated
- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([10, 768]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Layer: backbone.vit.embeddings.cls_token, Size: torch.Size([1, 1, 768]), req grad: True
Layer: backbone.vit.embeddings.position_embeddings, Size: torch.Size([1, 197, 768]), req grad: True
Layer: backbone.vit.embeddings.patch_embeddings.projection.weight, Size: torch.Size([768, 3, 16, 16]), req grad: True
Layer: backbone.vit.embeddings.patch_embeddings.projection.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.0.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.1.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.2.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.3.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.4.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.5.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.6.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.7.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.8.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.9.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.10.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.attention.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.vit.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.vit.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.vit.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.vit.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.layernorm_before.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.layernorm_before.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.layernorm_after.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.encoder.layer.11.layernorm_after.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.layernorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.vit.layernorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.weight, Size: torch.Size([10, 768]), req grad: True
Layer: backbone.classifier.bias, Size: torch.Size([10]), req grad: True
Testing Loss: 0.5560, Accuracy: 0.9368, Precision: 0.9370, Recall: 0.9368, F1: 0.9367
LM Predictions:  [1, 6, 4, 7, 4, 0, 1, 3, 8, 5, 4, 2, 2, 7, 8, 8, 3, 0, 7, 1, 7, 7, 2, 3, 0, 0, 8, 2, 4, 4, 5, 3, 7, 0, 5, 5, 1, 0, 7, 0, 8, 1, 2, 6, 5, 4, 5, 1, 6, 8, 5, 0, 6, 6, 4, 6, 8, 6, 2, 7, 3, 7, 4, 0, 1, 6, 2, 7, 4, 6, 7, 0, 3, 5, 1, 7, 2, 6, 1, 1, 0, 8, 2, 6, 6, 0, 6, 5, 7, 2, 2, 4, 6, 1, 2, 5, 1, 3, 0, 8, 6, 1, 5, 8, 3, 5, 7, 1, 1, 2, 1, 2, 5, 1, 5, 1, 4, 6, 6, 0, 6, 7, 0, 2, 4, 7, 1, 3, 2, 7, 2, 8, 0, 0, 7, 2, 8, 3, 6, 4, 4, 0, 3, 8, 0, 8, 3, 5, 1, 1, 7, 4, 3, 0, 1, 8, 6, 8, 7, 7]
LM Labels:  [1, 6, 4, 7, 4, 0, 1, 3, 8, 5, 4, 2, 2, 7, 8, 8, 3, 0, 7, 1, 7, 7, 2, 3, 0, 0, 8, 2, 4, 4, 5, 3, 7, 0, 5, 5, 1, 0, 7, 0, 8, 1, 2, 6, 5, 4, 5, 1, 6, 8, 5, 0, 6, 6, 4, 6, 8, 6, 2, 7, 3, 7, 4, 0, 1, 6, 2, 7, 4, 6, 7, 0, 3, 5, 1, 7, 2, 6, 1, 1, 0, 8, 2, 6, 6, 0, 6, 5, 7, 2, 2, 4, 6, 1, 2, 5, 1, 3, 0, 8, 6, 1, 5, 8, 3, 5, 7, 1, 1, 2, 1, 2, 5, 1, 5, 1, 4, 6, 6, 0, 6, 7, 0, 2, 4, 7, 1, 3, 2, 7, 2, 8, 0, 0, 7, 2, 8, 3, 6, 4, 4, 0, 3, 8, 0, 8, 3, 5, 1, 1, 7, 4, 3, 0, 1, 8, 6, 8, 7, 7]
LM Loss: 0.0000, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Attention LayerNorm grads:  {11: 7.419910019734743e-08, 10: 9.121941246803544e-08, 9: 1.2855925035637483e-07, 8: 1.4725732455644902e-07, 7: 1.663894266812349e-07, 6: 1.8640088228494278e-07, 5: 2.2947175182252977e-07, 4: 2.713395872433466e-07, 3: 2.941302170711424e-07, 2: 3.8999181128929195e-07, 1: 4.515329123933043e-07, 0: 5.44241345323826e-07}
Output LayerNorm grads:  {11: 7.28415061601595e-09, 10: 1.8840488280602585e-08, 9: 3.9391839123936734e-08, 8: 7.154894632321884e-08, 7: 9.473519924085849e-08, 6: 1.2257953585503856e-07, 5: 1.488654532977307e-07, 4: 1.8952798086502298e-07, 3: 2.3028258056001505e-07, 2: 2.784305195291381e-07, 1: 3.892538131822221e-07, 0: 5.104312208459305e-07}

Attention LayerNorm grads:  {11: 0.0030546949710696936, 10: 0.0043723201379179955, 9: 0.01092719379812479, 8: 0.016824418678879738, 7: 0.02140633761882782, 6: 0.027180254459381104, 5: 0.03965757414698601, 4: 0.049900151789188385, 3: 0.055165763944387436, 2: 0.07459993660449982, 1: 0.08827690780162811, 0: 0.1042976900935173}
Output LayerNorm grads:  {11: 0.00018483636085875332, 10: 0.0007987347198650241, 9: 0.002385188126936555, 8: 0.005796059500426054, 7: 0.010231101885437965, 6: 0.015001281164586544, 5: 0.02147280052304268, 4: 0.03093661181628704, 3: 0.04046976938843727, 2: 0.05091862753033638, 1: 0.07362040877342224, 0: 0.10064999759197235}
