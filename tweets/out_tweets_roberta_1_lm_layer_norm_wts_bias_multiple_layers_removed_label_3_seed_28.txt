---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1651, Accuracy: 0.5553, Precision: 0.2716, Recall: 0.2770, F1: 0.2672
Validation Loss: 0.8231, Accuracy: 0.7074, Precision: 0.3373, Recall: 0.3836, F1: 0.3584
Testing Loss: 0.8056, Accuracy: 0.7314, Precision: 0.3501, Recall: 0.4062, F1: 0.3722
LM Predictions:  [5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 2, 5, 5, 2, 4, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9370, Accuracy: 0.2143, Precision: 0.1048, Recall: 0.1714, F1: 0.1187
Epoch 2/70
Train Loss: 0.7521, Accuracy: 0.7456, Precision: 0.4471, Recall: 0.4272, F1: 0.4181
Validation Loss: 0.6966, Accuracy: 0.7585, Precision: 0.4365, Recall: 0.4672, F1: 0.4459
Testing Loss: 0.6864, Accuracy: 0.7819, Precision: 0.4774, Recall: 0.5041, F1: 0.4882
LM Predictions:  [3, 3, 3, 3, 5, 2, 5, 5, 3, 3, 3, 3, 5, 4, 4, 3, 3, 3, 3, 3, 2, 3, 5, 2, 4, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9985, Accuracy: 0.1429, Precision: 0.1722, Recall: 0.1048, F1: 0.1301
Epoch 3/70
Train Loss: 0.6243, Accuracy: 0.7841, Precision: 0.4969, Recall: 0.5050, F1: 0.4954
Validation Loss: 0.6292, Accuracy: 0.7699, Precision: 0.4811, Recall: 0.5166, F1: 0.4750
Testing Loss: 0.6343, Accuracy: 0.7580, Precision: 0.4587, Recall: 0.5074, F1: 0.4464
LM Predictions:  [1, 1, 1, 1, 1, 2, 1, 5, 1, 1, 1, 1, 1, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8960, Accuracy: 0.2500, Precision: 0.4348, Recall: 0.2971, F1: 0.2108
Epoch 4/70
Train Loss: 0.5481, Accuracy: 0.8181, Precision: 0.5337, Recall: 0.5388, F1: 0.5309
Validation Loss: 0.5848, Accuracy: 0.8097, Precision: 0.4800, Recall: 0.5622, F1: 0.5043
Testing Loss: 0.6030, Accuracy: 0.8218, Precision: 0.4970, Recall: 0.5521, F1: 0.5179
LM Predictions:  [3, 3, 3, 3, 3, 2, 3, 5, 3, 3, 3, 3, 3, 4, 4, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4718, Accuracy: 0.1071, Precision: 0.1667, Recall: 0.0810, F1: 0.1082
Epoch 5/70
Train Loss: 0.5026, Accuracy: 0.8310, Precision: 0.6373, Recall: 0.5728, F1: 0.5638
Validation Loss: 0.6220, Accuracy: 0.7955, Precision: 0.6486, Recall: 0.5047, F1: 0.5205
Testing Loss: 0.6231, Accuracy: 0.8165, Precision: 0.5641, Recall: 0.5250, F1: 0.5316
LM Predictions:  [1, 3, 3, 3, 5, 2, 4, 5, 5, 3, 3, 3, 2, 4, 4, 5, 3, 1, 1, 4, 2, 2, 5, 2, 4, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9154, Accuracy: 0.2857, Precision: 0.2492, Recall: 0.2274, F1: 0.2370
Epoch 6/70
Train Loss: 0.4463, Accuracy: 0.8555, Precision: 0.5875, Recall: 0.6026, F1: 0.5905
Validation Loss: 0.5533, Accuracy: 0.8210, Precision: 0.5715, Recall: 0.5575, F1: 0.5573
Testing Loss: 0.6047, Accuracy: 0.8032, Precision: 0.5786, Recall: 0.5533, F1: 0.5331
LM Predictions:  [1, 3, 5, 5, 5, 2, 4, 5, 1, 5, 3, 5, 2, 4, 4, 5, 1, 3, 1, 1, 3, 5, 5, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7631, Accuracy: 0.3571, Precision: 0.3167, Recall: 0.2929, F1: 0.2922
Epoch 7/70
Train Loss: 0.4184, Accuracy: 0.8628, Precision: 0.6994, Recall: 0.6246, F1: 0.6312
Validation Loss: 0.4980, Accuracy: 0.8295, Precision: 0.5840, Recall: 0.5787, F1: 0.5798
Testing Loss: 0.5179, Accuracy: 0.8484, Precision: 0.6068, Recall: 0.6140, F1: 0.6082
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 3, 2, 1, 4, 5, 3, 3, 3, 1, 2, 2, 5, 3, 1, 5, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0385, Accuracy: 0.2857, Precision: 0.4056, Recall: 0.2274, F1: 0.2868
Epoch 8/70
Train Loss: 0.3780, Accuracy: 0.8726, Precision: 0.6992, Recall: 0.6479, F1: 0.6395
Validation Loss: 0.5741, Accuracy: 0.8153, Precision: 0.5891, Recall: 0.5960, F1: 0.5618
Testing Loss: 0.6049, Accuracy: 0.8351, Precision: 0.5941, Recall: 0.6150, F1: 0.6009
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 3, 2, 1, 4, 5, 3, 3, 3, 3, 3, 2, 1, 3, 3, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7719, Accuracy: 0.2857, Precision: 0.5694, Recall: 0.2190, F1: 0.3084
Epoch 9/70
Train Loss: 0.3272, Accuracy: 0.8912, Precision: 0.7144, Recall: 0.7009, F1: 0.7033
Validation Loss: 0.5947, Accuracy: 0.8239, Precision: 0.6454, Recall: 0.5774, F1: 0.5805
Testing Loss: 0.5638, Accuracy: 0.8404, Precision: 0.6508, Recall: 0.6271, F1: 0.6373
LM Predictions:  [3, 3, 3, 3, 0, 2, 4, 5, 0, 3, 3, 0, 2, 2, 4, 5, 3, 3, 3, 3, 3, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1829, Accuracy: 0.3929, Precision: 0.5952, Recall: 0.3179, F1: 0.3942
Epoch 10/70
Train Loss: 0.3186, Accuracy: 0.9027, Precision: 0.7450, Recall: 0.7293, F1: 0.7302
Validation Loss: 0.5592, Accuracy: 0.8352, Precision: 0.6470, Recall: 0.6370, F1: 0.6362
Testing Loss: 0.5459, Accuracy: 0.8484, Precision: 0.6141, Recall: 0.6338, F1: 0.6201
LM Predictions:  [3, 3, 3, 3, 1, 2, 3, 5, 3, 3, 3, 3, 2, 2, 3, 5, 3, 3, 1, 1, 5, 2, 5, 3, 5, 3, 0, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3109, Accuracy: 0.2857, Precision: 0.4444, Recall: 0.2357, F1: 0.2883
Epoch 11/70
Train Loss: 0.2899, Accuracy: 0.9143, Precision: 0.7695, Recall: 0.7593, F1: 0.7617
Validation Loss: 0.6457, Accuracy: 0.8125, Precision: 0.6586, Recall: 0.5830, F1: 0.5962
Testing Loss: 0.6069, Accuracy: 0.8218, Precision: 0.6586, Recall: 0.6078, F1: 0.6208
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 3, 2, 3, 3, 5, 3, 3, 3, 3, 2, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8836, Accuracy: 0.3214, Precision: 0.6786, Recall: 0.2512, F1: 0.3360
Epoch 12/70
Train Loss: 0.2562, Accuracy: 0.9213, Precision: 0.8105, Recall: 0.7965, F1: 0.8032
Validation Loss: 0.5106, Accuracy: 0.8580, Precision: 0.6585, Recall: 0.6358, F1: 0.6448
Testing Loss: 0.5219, Accuracy: 0.8457, Precision: 0.6208, Recall: 0.6326, F1: 0.6224
LM Predictions:  [2, 3, 3, 3, 1, 2, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7722, Accuracy: 0.4286, Precision: 0.5486, Recall: 0.3405, F1: 0.4016
Epoch 13/70
Train Loss: 0.2351, Accuracy: 0.9290, Precision: 0.8054, Recall: 0.8106, F1: 0.8074
Validation Loss: 0.6107, Accuracy: 0.8494, Precision: 0.6536, Recall: 0.6280, F1: 0.6321
Testing Loss: 0.6059, Accuracy: 0.8537, Precision: 0.6340, Recall: 0.6263, F1: 0.6292
LM Predictions:  [3, 3, 3, 3, 1, 2, 4, 5, 3, 3, 3, 0, 2, 2, 3, 5, 3, 3, 1, 1, 0, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1436, Accuracy: 0.4286, Precision: 0.6556, Recall: 0.3679, F1: 0.4266
Epoch 14/70
Train Loss: 0.2105, Accuracy: 0.9353, Precision: 0.8233, Recall: 0.8387, F1: 0.8301
Validation Loss: 0.5387, Accuracy: 0.8580, Precision: 0.6779, Recall: 0.6869, F1: 0.6803
Testing Loss: 0.6110, Accuracy: 0.8457, Precision: 0.6178, Recall: 0.6326, F1: 0.6220
LM Predictions:  [3, 3, 3, 2, 1, 4, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 1, 1, 1, 5, 2, 1, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9918, Accuracy: 0.4643, Precision: 0.6778, Recall: 0.3833, F1: 0.4572
Epoch 15/70
Train Loss: 0.1924, Accuracy: 0.9426, Precision: 0.8612, Recall: 0.8533, F1: 0.8563
Validation Loss: 0.6631, Accuracy: 0.8409, Precision: 0.6739, Recall: 0.6368, F1: 0.6513
Testing Loss: 0.6448, Accuracy: 0.8431, Precision: 0.6349, Recall: 0.6236, F1: 0.6229
LM Predictions:  [1, 3, 3, 2, 1, 4, 4, 5, 3, 3, 3, 0, 2, 2, 4, 5, 3, 1, 1, 5, 5, 2, 1, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5510, Accuracy: 0.5357, Precision: 0.6778, Recall: 0.4405, F1: 0.5181
Epoch 16/70
Train Loss: 0.1834, Accuracy: 0.9489, Precision: 0.8708, Recall: 0.8798, F1: 0.8741
Validation Loss: 0.5922, Accuracy: 0.8466, Precision: 0.6298, Recall: 0.6598, F1: 0.6423
Testing Loss: 0.5786, Accuracy: 0.8590, Precision: 0.6847, Recall: 0.6801, F1: 0.6679
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 2, 4, 5, 3, 3, 3, 2, 5, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6010, Accuracy: 0.4643, Precision: 0.7063, Recall: 0.3833, F1: 0.4710
Epoch 17/70
Train Loss: 0.1703, Accuracy: 0.9538, Precision: 0.8656, Recall: 0.8755, F1: 0.8695
Validation Loss: 0.6327, Accuracy: 0.8182, Precision: 0.6475, Recall: 0.6223, F1: 0.6333
Testing Loss: 0.6246, Accuracy: 0.8617, Precision: 0.6871, Recall: 0.6682, F1: 0.6744
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 3, 3, 3, 0, 2, 2, 4, 5, 3, 3, 5, 2, 5, 2, 3, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5993, Accuracy: 0.4643, Precision: 0.6647, Recall: 0.3750, F1: 0.4611
Epoch 18/70
Train Loss: 0.1503, Accuracy: 0.9591, Precision: 0.8849, Recall: 0.8874, F1: 0.8854
Validation Loss: 0.5944, Accuracy: 0.8608, Precision: 0.6756, Recall: 0.6591, F1: 0.6658
Testing Loss: 0.7208, Accuracy: 0.8457, Precision: 0.6272, Recall: 0.6396, F1: 0.6188
LM Predictions:  [1, 3, 3, 2, 1, 2, 4, 5, 3, 3, 3, 0, 2, 2, 4, 5, 3, 3, 1, 1, 5, 2, 1, 3, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8572, Accuracy: 0.4286, Precision: 0.6458, Recall: 0.3679, F1: 0.4175
Epoch 19/70
Train Loss: 0.1347, Accuracy: 0.9626, Precision: 0.9096, Recall: 0.9055, F1: 0.9065
Validation Loss: 0.5661, Accuracy: 0.8636, Precision: 0.7019, Recall: 0.6621, F1: 0.6793
Testing Loss: 0.7563, Accuracy: 0.8404, Precision: 0.6210, Recall: 0.6162, F1: 0.6131
LM Predictions:  [0, 3, 3, 0, 1, 2, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 2, 5, 2, 1, 2, 3, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3788, Accuracy: 0.5000, Precision: 0.6230, Recall: 0.4071, F1: 0.4810
Epoch 20/70
Train Loss: 0.1277, Accuracy: 0.9654, Precision: 0.8986, Recall: 0.9104, F1: 0.9035
Validation Loss: 0.7126, Accuracy: 0.8210, Precision: 0.6462, Recall: 0.6340, F1: 0.6340
Testing Loss: 0.7507, Accuracy: 0.8245, Precision: 0.6411, Recall: 0.6325, F1: 0.6278
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 5, 2, 5, 2, 1, 2, 3, 3, 0, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5959, Accuracy: 0.4643, Precision: 0.6750, Recall: 0.3750, F1: 0.4778
Epoch 21/70
Train Loss: 0.1205, Accuracy: 0.9668, Precision: 0.9186, Recall: 0.9167, F1: 0.9169
Validation Loss: 0.5629, Accuracy: 0.8608, Precision: 0.6759, Recall: 0.6388, F1: 0.6543
Testing Loss: 0.7106, Accuracy: 0.8590, Precision: 0.6481, Recall: 0.6399, F1: 0.6415
LM Predictions:  [3, 3, 3, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 3, 5, 2, 5, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1505, Accuracy: 0.5714, Precision: 0.6819, Recall: 0.4643, F1: 0.5377
Epoch 22/70
Train Loss: 0.1247, Accuracy: 0.9619, Precision: 0.8918, Recall: 0.8977, F1: 0.8935
Validation Loss: 0.7107, Accuracy: 0.8551, Precision: 0.6970, Recall: 0.6124, F1: 0.6450
Testing Loss: 0.8812, Accuracy: 0.8324, Precision: 0.6656, Recall: 0.6348, F1: 0.6467
LM Predictions:  [0, 3, 3, 5, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 1, 5, 2, 3, 2, 3, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4501, Accuracy: 0.5714, Precision: 0.6944, Recall: 0.4643, F1: 0.5530
Epoch 23/70
Train Loss: 0.0960, Accuracy: 0.9762, Precision: 0.9388, Recall: 0.9392, F1: 0.9384
Validation Loss: 0.7046, Accuracy: 0.8466, Precision: 0.6765, Recall: 0.6469, F1: 0.6603
Testing Loss: 0.8788, Accuracy: 0.8378, Precision: 0.6368, Recall: 0.6359, F1: 0.6279
LM Predictions:  [3, 3, 3, 2, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 1, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3481, Accuracy: 0.5714, Precision: 0.7333, Recall: 0.4821, F1: 0.5637
Epoch 24/70
Train Loss: 0.0896, Accuracy: 0.9762, Precision: 0.9346, Recall: 0.9476, F1: 0.9405
Validation Loss: 0.6945, Accuracy: 0.8494, Precision: 0.6949, Recall: 0.6298, F1: 0.6560
Testing Loss: 0.8342, Accuracy: 0.8404, Precision: 0.6287, Recall: 0.6174, F1: 0.6179
LM Predictions:  [3, 3, 3, 0, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 5, 5, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9297, Accuracy: 0.6071, Precision: 0.7444, Recall: 0.5060, F1: 0.6012
Epoch 25/70
Train Loss: 0.0982, Accuracy: 0.9713, Precision: 0.9161, Recall: 0.9279, F1: 0.9217
Validation Loss: 0.6088, Accuracy: 0.8551, Precision: 0.6608, Recall: 0.6561, F1: 0.6479
Testing Loss: 0.7120, Accuracy: 0.8564, Precision: 0.6410, Recall: 0.6548, F1: 0.6460
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 3, 4, 5, 3, 3, 1, 2, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6077, Accuracy: 0.5357, Precision: 0.8000, Recall: 0.4583, F1: 0.5742
Epoch 26/70
Train Loss: 0.0867, Accuracy: 0.9780, Precision: 0.9346, Recall: 0.9422, F1: 0.9379
Validation Loss: 0.6488, Accuracy: 0.8636, Precision: 0.6906, Recall: 0.6634, F1: 0.6726
Testing Loss: 0.7567, Accuracy: 0.8590, Precision: 0.6509, Recall: 0.6493, F1: 0.6477
LM Predictions:  [2, 3, 3, 0, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 2, 5, 2, 3, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6391, Accuracy: 0.5714, Precision: 0.7500, Recall: 0.4643, F1: 0.5689
Epoch 27/70
Train Loss: 0.0894, Accuracy: 0.9745, Precision: 0.9345, Recall: 0.9450, F1: 0.9395
Validation Loss: 0.7451, Accuracy: 0.8438, Precision: 0.6708, Recall: 0.6499, F1: 0.6570
Testing Loss: 0.9303, Accuracy: 0.8431, Precision: 0.6317, Recall: 0.6326, F1: 0.6273
LM Predictions:  [0, 3, 5, 0, 1, 4, 4, 5, 3, 3, 5, 0, 2, 5, 4, 5, 3, 3, 1, 5, 5, 2, 3, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1749, Accuracy: 0.6071, Precision: 0.7262, Recall: 0.4881, F1: 0.5743
Epoch 28/70
Train Loss: 0.0832, Accuracy: 0.9783, Precision: 0.9367, Recall: 0.9401, F1: 0.9381
Validation Loss: 0.8501, Accuracy: 0.8523, Precision: 0.6606, Recall: 0.6522, F1: 0.6505
Testing Loss: 0.9700, Accuracy: 0.8218, Precision: 0.6118, Recall: 0.6263, F1: 0.5940
LM Predictions:  [1, 3, 3, 2, 1, 4, 4, 5, 3, 3, 3, 0, 2, 1, 4, 5, 3, 3, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4609, Accuracy: 0.5714, Precision: 0.7167, Recall: 0.4821, F1: 0.5526
Epoch 29/70
Train Loss: 0.0814, Accuracy: 0.9755, Precision: 0.9286, Recall: 0.9414, F1: 0.9345
Validation Loss: 0.7111, Accuracy: 0.8381, Precision: 0.6356, Recall: 0.6482, F1: 0.6399
Testing Loss: 0.7856, Accuracy: 0.8351, Precision: 0.6219, Recall: 0.6367, F1: 0.6162
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 5, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0995, Accuracy: 0.7143, Precision: 0.8095, Recall: 0.5774, F1: 0.6598
Epoch 30/70
Train Loss: 0.0820, Accuracy: 0.9769, Precision: 0.9376, Recall: 0.9382, F1: 0.9376
Validation Loss: 0.8076, Accuracy: 0.8494, Precision: 0.7112, Recall: 0.6416, F1: 0.6566
Testing Loss: 0.8644, Accuracy: 0.8378, Precision: 0.6682, Recall: 0.6053, F1: 0.6229
LM Predictions:  [0, 3, 3, 0, 1, 4, 4, 5, 2, 3, 3, 0, 2, 0, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9535, Accuracy: 0.6429, Precision: 0.6917, Recall: 0.5393, F1: 0.5999
Epoch 31/70
Train Loss: 0.0843, Accuracy: 0.9787, Precision: 0.9348, Recall: 0.9444, F1: 0.9393
Validation Loss: 0.7330, Accuracy: 0.8466, Precision: 0.6785, Recall: 0.6381, F1: 0.6557
Testing Loss: 0.9569, Accuracy: 0.8271, Precision: 0.6565, Recall: 0.6226, F1: 0.6337
LM Predictions:  [5, 2, 0, 0, 1, 4, 4, 5, 5, 2, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9778, Accuracy: 0.6429, Precision: 0.6190, Recall: 0.5393, F1: 0.5730
Epoch 32/70
Train Loss: 0.0860, Accuracy: 0.9738, Precision: 0.9271, Recall: 0.9392, F1: 0.9327
Validation Loss: 0.7683, Accuracy: 0.8267, Precision: 0.6281, Recall: 0.6488, F1: 0.6338
Testing Loss: 0.9276, Accuracy: 0.8271, Precision: 0.6369, Recall: 0.6777, F1: 0.6397
LM Predictions:  [2, 3, 5, 5, 1, 4, 4, 5, 3, 3, 5, 0, 2, 5, 4, 3, 3, 4, 1, 1, 5, 2, 3, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1820, Accuracy: 0.6786, Precision: 0.7440, Recall: 0.5631, F1: 0.6263
Epoch 33/70
Train Loss: 0.0769, Accuracy: 0.9790, Precision: 0.9345, Recall: 0.9368, F1: 0.9356
Validation Loss: 0.8686, Accuracy: 0.8466, Precision: 0.6665, Recall: 0.6273, F1: 0.6421
Testing Loss: 0.9350, Accuracy: 0.8245, Precision: 0.6794, Recall: 0.6370, F1: 0.6508
LM Predictions:  [3, 3, 0, 5, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 4, 4, 5, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5210, Accuracy: 0.6429, Precision: 0.7206, Recall: 0.5214, F1: 0.5918
Epoch 34/70
Train Loss: 0.0623, Accuracy: 0.9822, Precision: 0.9335, Recall: 0.9600, F1: 0.9458
Validation Loss: 0.7240, Accuracy: 0.8551, Precision: 0.6926, Recall: 0.6151, F1: 0.6444
Testing Loss: 0.9136, Accuracy: 0.8324, Precision: 0.6382, Recall: 0.6050, F1: 0.6169
LM Predictions:  [5, 3, 0, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 4, 4, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8134, Accuracy: 0.7143, Precision: 0.7345, Recall: 0.5786, F1: 0.6373
Epoch 35/70
Train Loss: 0.0587, Accuracy: 0.9850, Precision: 0.9699, Recall: 0.9612, F1: 0.9652
Validation Loss: 0.8874, Accuracy: 0.8267, Precision: 0.6528, Recall: 0.6187, F1: 0.6336
Testing Loss: 0.9517, Accuracy: 0.8245, Precision: 0.6447, Recall: 0.6423, F1: 0.6353
LM Predictions:  [5, 2, 2, 5, 1, 4, 4, 5, 2, 5, 2, 0, 2, 5, 4, 5, 3, 4, 4, 5, 5, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0423, Accuracy: 0.7143, Precision: 0.7028, Recall: 0.5690, F1: 0.5980
Epoch 36/70
Train Loss: 0.0630, Accuracy: 0.9839, Precision: 0.9627, Recall: 0.9483, F1: 0.9551
Validation Loss: 0.8391, Accuracy: 0.8438, Precision: 0.6648, Recall: 0.6205, F1: 0.6354
Testing Loss: 0.8436, Accuracy: 0.8537, Precision: 0.6572, Recall: 0.6481, F1: 0.6519
LM Predictions:  [5, 3, 5, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8583, Accuracy: 0.7857, Precision: 0.7963, Recall: 0.6440, F1: 0.7007
Epoch 37/70
Train Loss: 0.0871, Accuracy: 0.9755, Precision: 0.9431, Recall: 0.9390, F1: 0.9407
Validation Loss: 0.7739, Accuracy: 0.8494, Precision: 0.6657, Recall: 0.6478, F1: 0.6507
Testing Loss: 0.8804, Accuracy: 0.8378, Precision: 0.6250, Recall: 0.6346, F1: 0.6168
LM Predictions:  [2, 2, 2, 2, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7273, Accuracy: 0.7500, Precision: 0.7727, Recall: 0.6107, F1: 0.6548
Epoch 38/70
Train Loss: 0.0597, Accuracy: 0.9878, Precision: 0.9730, Recall: 0.9663, F1: 0.9695
Validation Loss: 0.8241, Accuracy: 0.8551, Precision: 0.6543, Recall: 0.6465, F1: 0.6464
Testing Loss: 0.9122, Accuracy: 0.8457, Precision: 0.6623, Recall: 0.6805, F1: 0.6576
LM Predictions:  [0, 5, 5, 2, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8349, Accuracy: 0.7500, Precision: 0.7222, Recall: 0.6202, F1: 0.6637
Epoch 39/70
Train Loss: 0.0701, Accuracy: 0.9839, Precision: 0.9543, Recall: 0.9524, F1: 0.9533
Validation Loss: 0.7953, Accuracy: 0.8409, Precision: 0.6888, Recall: 0.6236, F1: 0.6464
Testing Loss: 0.9139, Accuracy: 0.8245, Precision: 0.6372, Recall: 0.6271, F1: 0.6211
LM Predictions:  [0, 5, 5, 5, 1, 4, 4, 5, 3, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8238, Accuracy: 0.7500, Precision: 0.7000, Recall: 0.6202, F1: 0.6427
Epoch 40/70
Train Loss: 0.0629, Accuracy: 0.9836, Precision: 0.9514, Recall: 0.9636, F1: 0.9573
Validation Loss: 0.8133, Accuracy: 0.8466, Precision: 0.6915, Recall: 0.6401, F1: 0.6620
Testing Loss: 0.8816, Accuracy: 0.8404, Precision: 0.6629, Recall: 0.6504, F1: 0.6432
LM Predictions:  [2, 2, 2, 2, 1, 4, 4, 5, 2, 2, 2, 0, 2, 5, 4, 5, 3, 1, 1, 5, 5, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7591, Accuracy: 0.7143, Precision: 0.6972, Recall: 0.5774, F1: 0.5930
Epoch 41/70
Train Loss: 0.0475, Accuracy: 0.9888, Precision: 0.9819, Recall: 0.9752, F1: 0.9785
Validation Loss: 0.8934, Accuracy: 0.8523, Precision: 0.7013, Recall: 0.6450, F1: 0.6697
Testing Loss: 0.8875, Accuracy: 0.8457, Precision: 0.6615, Recall: 0.6497, F1: 0.6510
LM Predictions:  [2, 5, 5, 2, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5901, Accuracy: 0.7500, Precision: 0.6984, Recall: 0.6024, F1: 0.6362
Epoch 42/70
Train Loss: 0.0563, Accuracy: 0.9871, Precision: 0.9803, Recall: 0.9740, F1: 0.9770
Validation Loss: 0.8755, Accuracy: 0.8551, Precision: 0.7149, Recall: 0.6527, F1: 0.6793
Testing Loss: 1.0122, Accuracy: 0.8378, Precision: 0.6233, Recall: 0.6092, F1: 0.6109
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5141, Accuracy: 0.8214, Precision: 0.7417, Recall: 0.6679, F1: 0.6892
Epoch 43/70
Train Loss: 0.0376, Accuracy: 0.9906, Precision: 0.9836, Recall: 0.9781, F1: 0.9808
Validation Loss: 0.8326, Accuracy: 0.8381, Precision: 0.6835, Recall: 0.6300, F1: 0.6478
Testing Loss: 0.9127, Accuracy: 0.8298, Precision: 0.6234, Recall: 0.6088, F1: 0.6060
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 2, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5442, Accuracy: 0.8214, Precision: 0.7833, Recall: 0.6679, F1: 0.7071
Epoch 44/70
Train Loss: 0.0312, Accuracy: 0.9920, Precision: 0.9869, Recall: 0.9840, F1: 0.9854
Validation Loss: 0.8335, Accuracy: 0.8381, Precision: 0.6712, Recall: 0.6292, F1: 0.6477
Testing Loss: 0.8849, Accuracy: 0.8245, Precision: 0.6491, Recall: 0.6411, F1: 0.6367
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5228, Accuracy: 0.8214, Precision: 0.7833, Recall: 0.6679, F1: 0.7071
Epoch 45/70
Train Loss: 0.0267, Accuracy: 0.9927, Precision: 0.9757, Recall: 0.9758, F1: 0.9758
Validation Loss: 0.7545, Accuracy: 0.8608, Precision: 0.7054, Recall: 0.6678, F1: 0.6834
Testing Loss: 0.8527, Accuracy: 0.8431, Precision: 0.6436, Recall: 0.6256, F1: 0.6297
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6067, Accuracy: 0.8214, Precision: 0.7833, Recall: 0.6679, F1: 0.7071
Epoch 46/70
Train Loss: 0.0207, Accuracy: 0.9944, Precision: 0.9868, Recall: 0.9843, F1: 0.9855
Validation Loss: 1.0047, Accuracy: 0.8438, Precision: 0.6598, Recall: 0.6470, F1: 0.6469
Testing Loss: 1.0197, Accuracy: 0.8484, Precision: 0.6234, Recall: 0.6062, F1: 0.6125
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5093, Accuracy: 0.8571, Precision: 0.7963, Recall: 0.7012, F1: 0.7388
Epoch 47/70
Train Loss: 0.1301, Accuracy: 0.9549, Precision: 0.8413, Recall: 0.8250, F1: 0.8302
Validation Loss: 0.9024, Accuracy: 0.8580, Precision: 0.7318, Recall: 0.6331, F1: 0.6658
Testing Loss: 1.0526, Accuracy: 0.8271, Precision: 0.6109, Recall: 0.5763, F1: 0.5887
LM Predictions:  [5, 2, 2, 5, 1, 4, 4, 5, 2, 2, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6318, Accuracy: 0.7857, Precision: 0.7123, Recall: 0.6440, F1: 0.6660
Epoch 48/70
Train Loss: 0.0395, Accuracy: 0.9895, Precision: 0.9571, Recall: 0.9677, F1: 0.9621
Validation Loss: 0.8076, Accuracy: 0.8580, Precision: 0.6950, Recall: 0.6760, F1: 0.6837
Testing Loss: 0.8801, Accuracy: 0.8378, Precision: 0.6086, Recall: 0.6076, F1: 0.6027
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 1, 2, 0, 2, 5, 4, 4, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4956, Accuracy: 0.8571, Precision: 0.7630, Recall: 0.7190, F1: 0.7329
Epoch 49/70
Train Loss: 0.0372, Accuracy: 0.9895, Precision: 0.9749, Recall: 0.9734, F1: 0.9740
Validation Loss: 0.8163, Accuracy: 0.8438, Precision: 0.7126, Recall: 0.5788, F1: 0.5873
Testing Loss: 1.1069, Accuracy: 0.8138, Precision: 0.6021, Recall: 0.5513, F1: 0.5455
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5433, Accuracy: 0.8571, Precision: 0.7833, Recall: 0.7012, F1: 0.7303
Epoch 50/70
Train Loss: 0.0474, Accuracy: 0.9871, Precision: 0.9750, Recall: 0.9651, F1: 0.9697
Validation Loss: 0.9632, Accuracy: 0.8466, Precision: 0.6958, Recall: 0.6050, F1: 0.6358
Testing Loss: 1.1080, Accuracy: 0.8218, Precision: 0.6608, Recall: 0.6009, F1: 0.6163
LM Predictions:  [2, 0, 5, 5, 1, 4, 4, 5, 2, 2, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6852, Accuracy: 0.8214, Precision: 0.7524, Recall: 0.6774, F1: 0.7101
Epoch 51/70
Train Loss: 0.0520, Accuracy: 0.9885, Precision: 0.9695, Recall: 0.9638, F1: 0.9666
Validation Loss: 0.8620, Accuracy: 0.8636, Precision: 0.7185, Recall: 0.6657, F1: 0.6857
Testing Loss: 0.9654, Accuracy: 0.8484, Precision: 0.7102, Recall: 0.6746, F1: 0.6846
LM Predictions:  [5, 2, 5, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7514, Accuracy: 0.7857, Precision: 0.7685, Recall: 0.6440, F1: 0.6900
Epoch 52/70
Train Loss: 0.0401, Accuracy: 0.9906, Precision: 0.9777, Recall: 0.9796, F1: 0.9785
Validation Loss: 0.8625, Accuracy: 0.8494, Precision: 0.6614, Recall: 0.6244, F1: 0.6407
Testing Loss: 1.0924, Accuracy: 0.8324, Precision: 0.6652, Recall: 0.6423, F1: 0.6417
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 2, 0, 0, 2, 5, 4, 5, 3, 5, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9922, Accuracy: 0.7857, Precision: 0.7130, Recall: 0.6345, F1: 0.6581
Epoch 53/70
Train Loss: 0.0410, Accuracy: 0.9916, Precision: 0.9787, Recall: 0.9755, F1: 0.9770
Validation Loss: 0.8386, Accuracy: 0.8551, Precision: 0.6818, Recall: 0.6389, F1: 0.6377
Testing Loss: 0.9353, Accuracy: 0.8484, Precision: 0.6395, Recall: 0.6047, F1: 0.6158
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7293, Accuracy: 0.7857, Precision: 0.7470, Recall: 0.6262, F1: 0.6688
Epoch 54/70
Train Loss: 0.0475, Accuracy: 0.9871, Precision: 0.9703, Recall: 0.9695, F1: 0.9699
Validation Loss: 0.7948, Accuracy: 0.8608, Precision: 0.6804, Recall: 0.6620, F1: 0.6606
Testing Loss: 0.9327, Accuracy: 0.8564, Precision: 0.6948, Recall: 0.6883, F1: 0.6898
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6440, Accuracy: 0.8571, Precision: 0.7917, Recall: 0.6917, F1: 0.7271
Epoch 55/70
Train Loss: 0.0241, Accuracy: 0.9955, Precision: 0.9840, Recall: 0.9862, F1: 0.9851
Validation Loss: 0.9188, Accuracy: 0.8608, Precision: 0.7003, Recall: 0.6448, F1: 0.6641
Testing Loss: 1.0377, Accuracy: 0.8351, Precision: 0.6595, Recall: 0.6357, F1: 0.6357
LM Predictions:  [2, 0, 0, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5112, Accuracy: 0.8214, Precision: 0.7361, Recall: 0.6774, F1: 0.7021
Epoch 56/70
Train Loss: 0.0290, Accuracy: 0.9948, Precision: 0.9907, Recall: 0.9873, F1: 0.9889
Validation Loss: 0.8384, Accuracy: 0.8523, Precision: 0.6833, Recall: 0.6572, F1: 0.6657
Testing Loss: 1.0439, Accuracy: 0.8378, Precision: 0.6340, Recall: 0.6448, F1: 0.6298
LM Predictions:  [2, 0, 1, 0, 1, 4, 4, 5, 2, 5, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7352, Accuracy: 0.7857, Precision: 0.6833, Recall: 0.6536, F1: 0.6625
Epoch 57/70
Train Loss: 0.0301, Accuracy: 0.9923, Precision: 0.9754, Recall: 0.9811, F1: 0.9783
Validation Loss: 0.9548, Accuracy: 0.8551, Precision: 0.6911, Recall: 0.6529, F1: 0.6702
Testing Loss: 1.0174, Accuracy: 0.8511, Precision: 0.6763, Recall: 0.6723, F1: 0.6729
LM Predictions:  [2, 0, 3, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6355, Accuracy: 0.8571, Precision: 0.7917, Recall: 0.7012, F1: 0.7418
Epoch 58/70
Train Loss: 0.0681, Accuracy: 0.9839, Precision: 0.9556, Recall: 0.9648, F1: 0.9598
Validation Loss: 0.8678, Accuracy: 0.8580, Precision: 0.6829, Recall: 0.6360, F1: 0.6554
Testing Loss: 1.0319, Accuracy: 0.8537, Precision: 0.6311, Recall: 0.6408, F1: 0.6330
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4513, Accuracy: 0.8571, Precision: 0.7833, Recall: 0.6917, F1: 0.7199
Epoch 59/70
Train Loss: 0.0261, Accuracy: 0.9941, Precision: 0.9863, Recall: 0.9849, F1: 0.9855
Validation Loss: 1.0127, Accuracy: 0.8409, Precision: 0.7033, Recall: 0.5895, F1: 0.6191
Testing Loss: 1.1116, Accuracy: 0.8351, Precision: 0.6340, Recall: 0.5993, F1: 0.5956
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3929, Accuracy: 0.8929, Precision: 0.7708, Recall: 0.7250, F1: 0.7435
Epoch 60/70
Train Loss: 0.0271, Accuracy: 0.9941, Precision: 0.9812, Recall: 0.9792, F1: 0.9801
Validation Loss: 0.9912, Accuracy: 0.8608, Precision: 0.6820, Recall: 0.6678, F1: 0.6732
Testing Loss: 1.1365, Accuracy: 0.8404, Precision: 0.6185, Recall: 0.6301, F1: 0.6106
LM Predictions:  [2, 2, 3, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6549, Accuracy: 0.8214, Precision: 0.7679, Recall: 0.6679, F1: 0.7077
Epoch 61/70
Train Loss: 0.0378, Accuracy: 0.9916, Precision: 0.9755, Recall: 0.9751, F1: 0.9752
Validation Loss: 0.9773, Accuracy: 0.8494, Precision: 0.7003, Recall: 0.6683, F1: 0.6815
Testing Loss: 1.0933, Accuracy: 0.8378, Precision: 0.6448, Recall: 0.6505, F1: 0.6415
LM Predictions:  [2, 5, 3, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6946, Accuracy: 0.8214, Precision: 0.7708, Recall: 0.6679, F1: 0.7075
Epoch 62/70
Train Loss: 0.0276, Accuracy: 0.9948, Precision: 0.9813, Recall: 0.9837, F1: 0.9825
Validation Loss: 1.0572, Accuracy: 0.8438, Precision: 0.7216, Recall: 0.5977, F1: 0.6404
Testing Loss: 1.1818, Accuracy: 0.8298, Precision: 0.6404, Recall: 0.5947, F1: 0.6101
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4717, Accuracy: 0.8571, Precision: 0.7708, Recall: 0.6917, F1: 0.7204
Epoch 63/70
Train Loss: 0.0458, Accuracy: 0.9909, Precision: 0.9738, Recall: 0.9814, F1: 0.9774
Validation Loss: 0.9683, Accuracy: 0.8636, Precision: 0.7350, Recall: 0.6637, F1: 0.6878
Testing Loss: 1.1547, Accuracy: 0.8404, Precision: 0.6664, Recall: 0.6357, F1: 0.6382
LM Predictions:  [2, 0, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7582, Accuracy: 0.8571, Precision: 0.7546, Recall: 0.7012, F1: 0.7210
Epoch 64/70
Train Loss: 0.0209, Accuracy: 0.9962, Precision: 0.9927, Recall: 0.9880, F1: 0.9903
Validation Loss: 1.0188, Accuracy: 0.8324, Precision: 0.6915, Recall: 0.6963, F1: 0.6748
Testing Loss: 1.1643, Accuracy: 0.8191, Precision: 0.6517, Recall: 0.6749, F1: 0.6442
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 2, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5932, Accuracy: 0.8929, Precision: 0.7708, Recall: 0.7250, F1: 0.7435
Epoch 65/70
Train Loss: 0.0200, Accuracy: 0.9965, Precision: 0.9885, Recall: 0.9879, F1: 0.9882
Validation Loss: 1.0645, Accuracy: 0.8381, Precision: 0.6775, Recall: 0.6635, F1: 0.6679
Testing Loss: 1.1817, Accuracy: 0.8351, Precision: 0.6445, Recall: 0.6571, F1: 0.6437
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 2, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4500, Accuracy: 0.8929, Precision: 0.7708, Recall: 0.7250, F1: 0.7435
Epoch 66/70
Train Loss: 0.0351, Accuracy: 0.9916, Precision: 0.9703, Recall: 0.9648, F1: 0.9672
Validation Loss: 0.8668, Accuracy: 0.8636, Precision: 0.7390, Recall: 0.7197, F1: 0.7180
Testing Loss: 0.9550, Accuracy: 0.8431, Precision: 0.6353, Recall: 0.6022, F1: 0.6149
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 2, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6408, Accuracy: 0.7857, Precision: 0.7292, Recall: 0.6345, F1: 0.6573
Epoch 67/70
Train Loss: 0.0482, Accuracy: 0.9885, Precision: 0.9732, Recall: 0.9609, F1: 0.9666
Validation Loss: 0.7729, Accuracy: 0.8608, Precision: 0.7089, Recall: 0.6778, F1: 0.6896
Testing Loss: 0.9400, Accuracy: 0.8484, Precision: 0.6550, Recall: 0.6264, F1: 0.6359
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 3, 5, 5, 2, 1, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8412, Accuracy: 0.7500, Precision: 0.7153, Recall: 0.5929, F1: 0.6275
Epoch 68/70
Train Loss: 0.0169, Accuracy: 0.9965, Precision: 0.9929, Recall: 0.9847, F1: 0.9885
Validation Loss: 0.9506, Accuracy: 0.8580, Precision: 0.7004, Recall: 0.6543, F1: 0.6748
Testing Loss: 1.0321, Accuracy: 0.8457, Precision: 0.6413, Recall: 0.6355, F1: 0.6367
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 1, 1, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5650, Accuracy: 0.8571, Precision: 0.7619, Recall: 0.7095, F1: 0.7149
Epoch 69/70
Train Loss: 0.0419, Accuracy: 0.9927, Precision: 0.9837, Recall: 0.9793, F1: 0.9813
Validation Loss: 0.9776, Accuracy: 0.8494, Precision: 0.6767, Recall: 0.6560, F1: 0.6658
Testing Loss: 1.0842, Accuracy: 0.8537, Precision: 0.6939, Recall: 0.6809, F1: 0.6866
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 3, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2736, Accuracy: 0.7500, Precision: 0.7917, Recall: 0.6202, F1: 0.6909
Epoch 70/70
Train Loss: 0.0425, Accuracy: 0.9930, Precision: 0.9787, Recall: 0.9824, F1: 0.9805
Validation Loss: 0.9632, Accuracy: 0.8580, Precision: 0.6925, Recall: 0.6618, F1: 0.6729
Testing Loss: 1.1903, Accuracy: 0.8245, Precision: 0.6117, Recall: 0.6104, F1: 0.5966
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4959, Accuracy: 0.8571, Precision: 0.7500, Recall: 0.6917, F1: 0.7093
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.9840, Accuracy: 0.6445, Precision: 0.3141, Recall: 0.3328, F1: 0.3202
Validation Loss: 0.5569, Accuracy: 0.7898, Precision: 0.3868, Recall: 0.4436, F1: 0.4095
Testing Loss: 0.5969, Accuracy: 0.7952, Precision: 0.3859, Recall: 0.4539, F1: 0.4108
LM Predictions:  [5, 5, 2, 2, 5, 2, 5, 5, 5, 5, 5, 5, 2, 5, 2, 2, 5, 5, 1, 5, 5, 5, 5, 2, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7917, Accuracy: 0.3571, Precision: 0.3444, Recall: 0.3071, F1: 0.2600
Epoch 2/70
Train Loss: 0.5411, Accuracy: 0.8310, Precision: 0.5767, Recall: 0.5609, F1: 0.5639
Validation Loss: 0.5367, Accuracy: 0.8210, Precision: 0.6000, Recall: 0.5765, F1: 0.5806
Testing Loss: 0.5489, Accuracy: 0.8484, Precision: 0.6239, Recall: 0.6416, F1: 0.6149
LM Predictions:  [3, 3, 3, 3, 1, 2, 1, 5, 1, 3, 3, 1, 2, 2, 3, 1, 1, 3, 1, 3, 3, 3, 1, 2, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4205, Accuracy: 0.1786, Precision: 0.2750, Recall: 0.1548, F1: 0.1528
Epoch 3/70
Train Loss: 0.4449, Accuracy: 0.8632, Precision: 0.6104, Recall: 0.6424, F1: 0.6253
Validation Loss: 0.4965, Accuracy: 0.8267, Precision: 0.6918, Recall: 0.5091, F1: 0.5198
Testing Loss: 0.5177, Accuracy: 0.8191, Precision: 0.5822, Recall: 0.5238, F1: 0.5148
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 5, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8766, Accuracy: 0.2500, Precision: 0.1000, Recall: 0.2000, F1: 0.1138
Epoch 4/70
Train Loss: 0.3299, Accuracy: 0.8996, Precision: 0.6729, Recall: 0.6955, F1: 0.6838
Validation Loss: 0.3903, Accuracy: 0.8665, Precision: 0.6703, Recall: 0.6658, F1: 0.6630
Testing Loss: 0.4425, Accuracy: 0.8777, Precision: 0.6753, Recall: 0.6822, F1: 0.6767
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5174, Accuracy: 0.2143, Precision: 0.6111, Recall: 0.1976, F1: 0.2738
Epoch 5/70
Train Loss: 0.2550, Accuracy: 0.9209, Precision: 0.7740, Recall: 0.7517, F1: 0.7530
Validation Loss: 0.4069, Accuracy: 0.8693, Precision: 0.7021, Recall: 0.6588, F1: 0.6772
Testing Loss: 0.5047, Accuracy: 0.8617, Precision: 0.6554, Recall: 0.6563, F1: 0.6454
LM Predictions:  [3, 3, 3, 0, 1, 4, 4, 5, 5, 3, 3, 3, 2, 5, 3, 3, 1, 3, 1, 3, 3, 3, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8323, Accuracy: 0.3571, Precision: 0.5028, Recall: 0.3107, F1: 0.3639
Epoch 6/70
Train Loss: 0.2072, Accuracy: 0.9367, Precision: 0.8170, Recall: 0.7988, F1: 0.8053
Validation Loss: 0.5006, Accuracy: 0.8494, Precision: 0.7387, Recall: 0.7270, F1: 0.6970
Testing Loss: 0.4794, Accuracy: 0.8590, Precision: 0.6592, Recall: 0.6362, F1: 0.6431
LM Predictions:  [3, 5, 3, 2, 1, 2, 4, 5, 3, 3, 5, 3, 2, 3, 4, 3, 3, 3, 1, 1, 3, 3, 0, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3942, Accuracy: 0.3571, Precision: 0.6270, Recall: 0.3024, F1: 0.3841
Epoch 7/70
Train Loss: 0.1634, Accuracy: 0.9517, Precision: 0.8720, Recall: 0.8719, F1: 0.8703
Validation Loss: 0.4260, Accuracy: 0.8722, Precision: 0.7249, Recall: 0.7307, F1: 0.7111
Testing Loss: 0.5949, Accuracy: 0.8564, Precision: 0.6635, Recall: 0.6867, F1: 0.6606
LM Predictions:  [3, 3, 3, 0, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 3, 1, 3, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0782, Accuracy: 0.6071, Precision: 0.7250, Recall: 0.5155, F1: 0.5896
Epoch 8/70
Train Loss: 0.1205, Accuracy: 0.9657, Precision: 0.8970, Recall: 0.9096, F1: 0.9030
Validation Loss: 0.4553, Accuracy: 0.8778, Precision: 0.7150, Recall: 0.6594, F1: 0.6747
Testing Loss: 0.5632, Accuracy: 0.8644, Precision: 0.6494, Recall: 0.6297, F1: 0.6299
LM Predictions:  [2, 0, 2, 3, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 2, 3, 3, 1, 5, 5, 2, 2, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9460, Accuracy: 0.7143, Precision: 0.7963, Recall: 0.5869, F1: 0.6599
Epoch 9/70
Train Loss: 0.1148, Accuracy: 0.9633, Precision: 0.9155, Recall: 0.9168, F1: 0.9158
Validation Loss: 0.4443, Accuracy: 0.8722, Precision: 0.7302, Recall: 0.7174, F1: 0.7166
Testing Loss: 0.5246, Accuracy: 0.8723, Precision: 0.6575, Recall: 0.6789, F1: 0.6607
LM Predictions:  [3, 0, 3, 5, 1, 4, 4, 5, 5, 3, 0, 0, 2, 5, 4, 5, 3, 3, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7542, Accuracy: 0.7143, Precision: 0.7345, Recall: 0.5964, F1: 0.6474
Epoch 10/70
Train Loss: 0.0878, Accuracy: 0.9748, Precision: 0.9253, Recall: 0.9220, F1: 0.9232
Validation Loss: 0.5043, Accuracy: 0.8551, Precision: 0.6602, Recall: 0.6936, F1: 0.6743
Testing Loss: 0.6236, Accuracy: 0.8564, Precision: 0.6158, Recall: 0.6527, F1: 0.6247
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 3, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6189, Accuracy: 0.7857, Precision: 0.7639, Recall: 0.6345, F1: 0.6417
Epoch 11/70
Train Loss: 0.0774, Accuracy: 0.9780, Precision: 0.9344, Recall: 0.9336, F1: 0.9338
Validation Loss: 0.5178, Accuracy: 0.8693, Precision: 0.7404, Recall: 0.7091, F1: 0.7206
Testing Loss: 0.6544, Accuracy: 0.8644, Precision: 0.6545, Recall: 0.6568, F1: 0.6484
LM Predictions:  [2, 0, 2, 0, 1, 4, 4, 5, 2, 1, 4, 0, 2, 2, 4, 5, 4, 4, 1, 5, 5, 2, 2, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4106, Accuracy: 0.8571, Precision: 0.8822, Recall: 0.8743, F1: 0.8623
Epoch 12/70
Train Loss: 0.0856, Accuracy: 0.9773, Precision: 0.9324, Recall: 0.9408, F1: 0.9365
Validation Loss: 0.5809, Accuracy: 0.8580, Precision: 0.7113, Recall: 0.6344, F1: 0.6594
Testing Loss: 0.6361, Accuracy: 0.8590, Precision: 0.6495, Recall: 0.6199, F1: 0.6212
LM Predictions:  [2, 5, 2, 4, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3559, Accuracy: 0.8929, Precision: 0.7817, Recall: 0.7345, F1: 0.7520
Epoch 13/70
Train Loss: 0.0555, Accuracy: 0.9850, Precision: 0.9611, Recall: 0.9685, F1: 0.9647
Validation Loss: 0.6135, Accuracy: 0.8693, Precision: 0.7346, Recall: 0.7320, F1: 0.7230
Testing Loss: 0.7397, Accuracy: 0.8431, Precision: 0.6502, Recall: 0.6330, F1: 0.6378
LM Predictions:  [2, 0, 2, 0, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1129, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 14/70
Train Loss: 0.0524, Accuracy: 0.9853, Precision: 0.9686, Recall: 0.9708, F1: 0.9696
Validation Loss: 0.6491, Accuracy: 0.8722, Precision: 0.7550, Recall: 0.7680, F1: 0.7428
Testing Loss: 0.7288, Accuracy: 0.8484, Precision: 0.6156, Recall: 0.6174, F1: 0.5965
LM Predictions:  [2, 0, 2, 1, 1, 4, 4, 5, 3, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2475, Accuracy: 0.8929, Precision: 0.7722, Recall: 0.7619, F1: 0.7591
Epoch 15/70
Train Loss: 0.0356, Accuracy: 0.9895, Precision: 0.9668, Recall: 0.9662, F1: 0.9664
Validation Loss: 0.7601, Accuracy: 0.8608, Precision: 0.7455, Recall: 0.7776, F1: 0.7346
Testing Loss: 0.7642, Accuracy: 0.8457, Precision: 0.6156, Recall: 0.6273, F1: 0.6132
LM Predictions:  [3, 0, 2, 4, 1, 4, 4, 5, 2, 1, 4, 0, 2, 5, 4, 5, 4, 4, 1, 5, 3, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4338, Accuracy: 0.8571, Precision: 0.7857, Recall: 0.7381, F1: 0.7473
Epoch 16/70
Train Loss: 0.0571, Accuracy: 0.9836, Precision: 0.9677, Recall: 0.9698, F1: 0.9688
Validation Loss: 0.6215, Accuracy: 0.8750, Precision: 0.7407, Recall: 0.7328, F1: 0.7255
Testing Loss: 0.6285, Accuracy: 0.8750, Precision: 0.6578, Recall: 0.6687, F1: 0.6565
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1268, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 17/70
Train Loss: 0.0335, Accuracy: 0.9909, Precision: 0.9719, Recall: 0.9788, F1: 0.9753
Validation Loss: 0.6784, Accuracy: 0.8722, Precision: 0.7638, Recall: 0.6967, F1: 0.7234
Testing Loss: 0.7902, Accuracy: 0.8670, Precision: 0.6553, Recall: 0.6367, F1: 0.6389
LM Predictions:  [2, 0, 2, 4, 1, 4, 4, 5, 2, 4, 4, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4580, Accuracy: 0.8571, Precision: 0.9000, Recall: 0.8529, F1: 0.8564
Epoch 18/70
Train Loss: 0.0247, Accuracy: 0.9941, Precision: 0.9807, Recall: 0.9764, F1: 0.9786
Validation Loss: 0.6736, Accuracy: 0.8778, Precision: 0.7423, Recall: 0.7117, F1: 0.7225
Testing Loss: 0.7548, Accuracy: 0.8777, Precision: 0.6725, Recall: 0.6675, F1: 0.6681
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2300, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 19/70
Train Loss: 0.0205, Accuracy: 0.9948, Precision: 0.9896, Recall: 0.9910, F1: 0.9903
Validation Loss: 0.7324, Accuracy: 0.8636, Precision: 0.6937, Recall: 0.6495, F1: 0.6578
Testing Loss: 0.8662, Accuracy: 0.8644, Precision: 0.6572, Recall: 0.6441, F1: 0.6256
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 5, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1124, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9714, F1: 0.9713
Epoch 20/70
Train Loss: 0.0409, Accuracy: 0.9895, Precision: 0.9683, Recall: 0.9645, F1: 0.9663
Validation Loss: 0.7808, Accuracy: 0.8608, Precision: 0.7258, Recall: 0.7172, F1: 0.7135
Testing Loss: 0.8364, Accuracy: 0.8484, Precision: 0.6410, Recall: 0.6491, F1: 0.6228
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1166, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 21/70
Train Loss: 0.0474, Accuracy: 0.9881, Precision: 0.9743, Recall: 0.9836, F1: 0.9789
Validation Loss: 0.6696, Accuracy: 0.8580, Precision: 0.6781, Recall: 0.6941, F1: 0.6838
Testing Loss: 0.7416, Accuracy: 0.8644, Precision: 0.6527, Recall: 0.6934, F1: 0.6679
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1771, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 22/70
Train Loss: 0.0293, Accuracy: 0.9934, Precision: 0.9812, Recall: 0.9818, F1: 0.9815
Validation Loss: 0.6484, Accuracy: 0.8750, Precision: 0.7319, Recall: 0.8024, F1: 0.7406
Testing Loss: 0.7038, Accuracy: 0.8750, Precision: 0.6781, Recall: 0.7014, F1: 0.6814
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0169, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 23/70
Train Loss: 0.0577, Accuracy: 0.9860, Precision: 0.9679, Recall: 0.9657, F1: 0.9668
Validation Loss: 0.6569, Accuracy: 0.8665, Precision: 0.7262, Recall: 0.7123, F1: 0.7065
Testing Loss: 0.6798, Accuracy: 0.8697, Precision: 0.6658, Recall: 0.6675, F1: 0.6558
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 4, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1104, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 24/70
Train Loss: 0.0421, Accuracy: 0.9881, Precision: 0.9703, Recall: 0.9769, F1: 0.9734
Validation Loss: 0.7563, Accuracy: 0.8693, Precision: 0.7722, Recall: 0.7477, F1: 0.7308
Testing Loss: 0.7655, Accuracy: 0.8697, Precision: 0.6815, Recall: 0.6375, F1: 0.6500
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0044, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0439, Accuracy: 0.9899, Precision: 0.9749, Recall: 0.9726, F1: 0.9738
Validation Loss: 0.7024, Accuracy: 0.8722, Precision: 0.6951, Recall: 0.6697, F1: 0.6804
Testing Loss: 0.6795, Accuracy: 0.8803, Precision: 0.6857, Recall: 0.6580, F1: 0.6680
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1112, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 26/70
Train Loss: 0.0111, Accuracy: 0.9965, Precision: 0.9934, Recall: 0.9894, F1: 0.9914
Validation Loss: 0.7197, Accuracy: 0.8750, Precision: 0.7359, Recall: 0.7376, F1: 0.7287
Testing Loss: 0.8051, Accuracy: 0.8803, Precision: 0.6724, Recall: 0.6761, F1: 0.6676
LM Predictions:  [2, 4, 2, 4, 1, 4, 4, 5, 2, 1, 4, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3785, Accuracy: 0.8929, Precision: 0.9250, Recall: 0.9029, F1: 0.8983
Epoch 27/70
Train Loss: 0.0213, Accuracy: 0.9948, Precision: 0.9948, Recall: 0.9902, F1: 0.9924
Validation Loss: 0.8311, Accuracy: 0.8494, Precision: 0.6820, Recall: 0.6912, F1: 0.6836
Testing Loss: 0.8893, Accuracy: 0.8617, Precision: 0.6684, Recall: 0.6848, F1: 0.6747
LM Predictions:  [2, 4, 2, 4, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4704, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9314, F1: 0.9291
Epoch 28/70
Train Loss: 0.0251, Accuracy: 0.9941, Precision: 0.9856, Recall: 0.9835, F1: 0.9845
Validation Loss: 0.8087, Accuracy: 0.8636, Precision: 0.7110, Recall: 0.6471, F1: 0.6413
Testing Loss: 0.8477, Accuracy: 0.8697, Precision: 0.6972, Recall: 0.6236, F1: 0.6396
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0205, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0524, Accuracy: 0.9871, Precision: 0.9603, Recall: 0.9590, F1: 0.9596
Validation Loss: 0.6329, Accuracy: 0.8665, Precision: 0.6933, Recall: 0.6832, F1: 0.6865
Testing Loss: 0.6845, Accuracy: 0.8883, Precision: 0.7012, Recall: 0.6847, F1: 0.6906
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2344, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 30/70
Train Loss: 0.0262, Accuracy: 0.9930, Precision: 0.9808, Recall: 0.9832, F1: 0.9820
Validation Loss: 0.6458, Accuracy: 0.8693, Precision: 0.6997, Recall: 0.6956, F1: 0.6966
Testing Loss: 0.7273, Accuracy: 0.8750, Precision: 0.6665, Recall: 0.6679, F1: 0.6662
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0284, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0268, Accuracy: 0.9930, Precision: 0.9865, Recall: 0.9885, F1: 0.9874
Validation Loss: 0.8031, Accuracy: 0.8693, Precision: 0.7050, Recall: 0.6845, F1: 0.6928
Testing Loss: 0.7454, Accuracy: 0.8883, Precision: 0.6783, Recall: 0.6868, F1: 0.6784
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0143, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0125, Accuracy: 0.9979, Precision: 0.9947, Recall: 0.9961, F1: 0.9954
Validation Loss: 0.8320, Accuracy: 0.8722, Precision: 0.7109, Recall: 0.6906, F1: 0.6986
Testing Loss: 0.7993, Accuracy: 0.8856, Precision: 0.6774, Recall: 0.6913, F1: 0.6806
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0103, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0190, Accuracy: 0.9944, Precision: 0.9843, Recall: 0.9909, F1: 0.9876
Validation Loss: 0.8505, Accuracy: 0.8750, Precision: 0.7122, Recall: 0.6929, F1: 0.7018
Testing Loss: 0.7270, Accuracy: 0.8910, Precision: 0.6953, Recall: 0.6687, F1: 0.6787
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 1, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0952, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 34/70
Train Loss: 0.0126, Accuracy: 0.9965, Precision: 0.9885, Recall: 0.9927, F1: 0.9905
Validation Loss: 0.7715, Accuracy: 0.8665, Precision: 0.6809, Recall: 0.6810, F1: 0.6751
Testing Loss: 0.8831, Accuracy: 0.8537, Precision: 0.6330, Recall: 0.6441, F1: 0.6128
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 1, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0776, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 35/70
Train Loss: 0.0147, Accuracy: 0.9958, Precision: 0.9905, Recall: 0.9857, F1: 0.9880
Validation Loss: 0.8276, Accuracy: 0.8722, Precision: 0.7151, Recall: 0.6721, F1: 0.6887
Testing Loss: 0.7335, Accuracy: 0.8936, Precision: 0.7076, Recall: 0.6860, F1: 0.6887
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0205, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0159, Accuracy: 0.9948, Precision: 0.9873, Recall: 0.9868, F1: 0.9871
Validation Loss: 0.8929, Accuracy: 0.8722, Precision: 0.7258, Recall: 0.6700, F1: 0.6937
Testing Loss: 0.9322, Accuracy: 0.8590, Precision: 0.6616, Recall: 0.6293, F1: 0.6421
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0382, Accuracy: 0.9923, Precision: 0.9963, Recall: 0.9945, F1: 0.9954
Validation Loss: 0.8133, Accuracy: 0.8494, Precision: 0.7065, Recall: 0.6669, F1: 0.6838
Testing Loss: 0.8597, Accuracy: 0.8564, Precision: 0.6556, Recall: 0.6408, F1: 0.6436
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0144, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0322, Accuracy: 0.9941, Precision: 0.9778, Recall: 0.9863, F1: 0.9820
Validation Loss: 0.8846, Accuracy: 0.8665, Precision: 0.6988, Recall: 0.6959, F1: 0.6968
Testing Loss: 0.8343, Accuracy: 0.8723, Precision: 0.6691, Recall: 0.6662, F1: 0.6650
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0444, Accuracy: 0.9913, Precision: 0.9847, Recall: 0.9804, F1: 0.9825
Validation Loss: 0.6793, Accuracy: 0.8778, Precision: 0.7064, Recall: 0.7027, F1: 0.7035
Testing Loss: 0.7257, Accuracy: 0.8777, Precision: 0.6719, Recall: 0.6584, F1: 0.6621
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1940, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7583, F1: 0.7799
Epoch 40/70
Train Loss: 0.0192, Accuracy: 0.9955, Precision: 0.9879, Recall: 0.9901, F1: 0.9890
Validation Loss: 0.6736, Accuracy: 0.8750, Precision: 0.7631, Recall: 0.7565, F1: 0.7575
Testing Loss: 0.8316, Accuracy: 0.8564, Precision: 0.6368, Recall: 0.6634, F1: 0.6442
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0394, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 41/70
Train Loss: 0.0184, Accuracy: 0.9951, Precision: 0.9945, Recall: 0.9892, F1: 0.9918
Validation Loss: 0.7848, Accuracy: 0.8722, Precision: 0.7520, Recall: 0.7257, F1: 0.7340
Testing Loss: 0.8110, Accuracy: 0.8697, Precision: 0.6706, Recall: 0.6519, F1: 0.6556
LM Predictions:  [5, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2703, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9714, F1: 0.9713
Epoch 42/70
Train Loss: 0.0312, Accuracy: 0.9941, Precision: 0.9875, Recall: 0.9912, F1: 0.9893
Validation Loss: 0.7210, Accuracy: 0.8778, Precision: 0.8100, Recall: 0.7262, F1: 0.7613
Testing Loss: 0.7660, Accuracy: 0.8644, Precision: 0.6495, Recall: 0.6424, F1: 0.6411
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0105, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0067, Accuracy: 0.9986, Precision: 0.9981, Recall: 0.9950, F1: 0.9965
Validation Loss: 0.7570, Accuracy: 0.8778, Precision: 0.7098, Recall: 0.6707, F1: 0.6866
Testing Loss: 0.8428, Accuracy: 0.8537, Precision: 0.6369, Recall: 0.6448, F1: 0.6328
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0025, Accuracy: 0.9990, Precision: 0.9993, Recall: 0.9995, F1: 0.9994
Validation Loss: 0.8240, Accuracy: 0.8949, Precision: 0.8241, Recall: 0.7973, F1: 0.7973
Testing Loss: 1.0884, Accuracy: 0.8537, Precision: 0.6624, Recall: 0.6444, F1: 0.6465
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0077, Accuracy: 0.9983, Precision: 0.9976, Recall: 0.9964, F1: 0.9970
Validation Loss: 0.8162, Accuracy: 0.8722, Precision: 0.7478, Recall: 0.7114, F1: 0.7197
Testing Loss: 0.9070, Accuracy: 0.8644, Precision: 0.7023, Recall: 0.6310, F1: 0.6514
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 3, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0428, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 46/70
Train Loss: 0.0157, Accuracy: 0.9976, Precision: 0.9959, Recall: 0.9923, F1: 0.9941
Validation Loss: 0.9005, Accuracy: 0.8750, Precision: 0.7850, Recall: 0.7867, F1: 0.7808
Testing Loss: 0.9958, Accuracy: 0.8670, Precision: 0.6405, Recall: 0.6687, F1: 0.6501
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0392, Accuracy: 0.9916, Precision: 0.9665, Recall: 0.9776, F1: 0.9719
Validation Loss: 0.8925, Accuracy: 0.8608, Precision: 0.6751, Recall: 0.6809, F1: 0.6735
Testing Loss: 0.9696, Accuracy: 0.8378, Precision: 0.6117, Recall: 0.6330, F1: 0.5948
LM Predictions:  [2, 0, 3, 5, 1, 4, 4, 5, 3, 1, 5, 0, 2, 1, 4, 5, 4, 4, 1, 5, 5, 3, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4956, Accuracy: 0.8571, Precision: 0.8000, Recall: 0.7381, F1: 0.7565
Epoch 48/70
Train Loss: 0.0555, Accuracy: 0.9878, Precision: 0.9676, Recall: 0.9685, F1: 0.9679
Validation Loss: 0.7486, Accuracy: 0.8636, Precision: 0.7789, Recall: 0.7266, F1: 0.7316
Testing Loss: 0.8011, Accuracy: 0.8723, Precision: 0.8884, Recall: 0.6694, F1: 0.7115
LM Predictions:  [2, 4, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 5, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1290, Accuracy: 0.9286, Precision: 0.9417, Recall: 0.9314, F1: 0.9309
Epoch 49/70
Train Loss: 0.0342, Accuracy: 0.9916, Precision: 0.9839, Recall: 0.9726, F1: 0.9778
Validation Loss: 0.7932, Accuracy: 0.8665, Precision: 0.7126, Recall: 0.6497, F1: 0.6740
Testing Loss: 0.8545, Accuracy: 0.8723, Precision: 0.6688, Recall: 0.6318, F1: 0.6446
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 3, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1517, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 50/70
Train Loss: 0.0142, Accuracy: 0.9972, Precision: 0.9969, Recall: 0.9971, F1: 0.9970
Validation Loss: 0.8181, Accuracy: 0.8722, Precision: 0.7490, Recall: 0.7150, F1: 0.7253
Testing Loss: 1.0437, Accuracy: 0.8723, Precision: 0.6548, Recall: 0.6712, F1: 0.6556
LM Predictions:  [2, 0, 2, 4, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 5, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4534, Accuracy: 0.9286, Precision: 0.9381, Recall: 0.9429, F1: 0.9379
Epoch 51/70
Train Loss: 0.0109, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9992, F1: 0.9985
Validation Loss: 0.7766, Accuracy: 0.8835, Precision: 0.7573, Recall: 0.7907, F1: 0.7621
Testing Loss: 0.9313, Accuracy: 0.8723, Precision: 0.6579, Recall: 0.6666, F1: 0.6575
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0043, Accuracy: 0.9993, Precision: 0.9995, Recall: 0.9995, F1: 0.9995
Validation Loss: 0.7400, Accuracy: 0.8750, Precision: 0.7643, Recall: 0.7741, F1: 0.7613
Testing Loss: 0.9243, Accuracy: 0.8644, Precision: 0.6510, Recall: 0.6358, F1: 0.6381
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0269, Accuracy: 0.9937, Precision: 0.9865, Recall: 0.9870, F1: 0.9867
Validation Loss: 0.8085, Accuracy: 0.8750, Precision: 0.7592, Recall: 0.7875, F1: 0.7489
Testing Loss: 0.9878, Accuracy: 0.8590, Precision: 0.6477, Recall: 0.6482, F1: 0.6282
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0205, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0180, Accuracy: 0.9965, Precision: 0.9952, Recall: 0.9942, F1: 0.9947
Validation Loss: 0.9589, Accuracy: 0.8580, Precision: 0.7191, Recall: 0.7299, F1: 0.7106
Testing Loss: 0.9570, Accuracy: 0.8564, Precision: 0.6360, Recall: 0.6593, F1: 0.6400
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0053, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0334, Accuracy: 0.9927, Precision: 0.9821, Recall: 0.9788, F1: 0.9804
Validation Loss: 0.7533, Accuracy: 0.8693, Precision: 0.7371, Recall: 0.7259, F1: 0.7292
Testing Loss: 0.8255, Accuracy: 0.8750, Precision: 0.6454, Recall: 0.6810, F1: 0.6564
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0167, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0098, Accuracy: 0.9983, Precision: 0.9967, Recall: 0.9965, F1: 0.9966
Validation Loss: 0.8482, Accuracy: 0.8807, Precision: 0.7893, Recall: 0.7184, F1: 0.7466
Testing Loss: 0.8493, Accuracy: 0.8697, Precision: 0.6679, Recall: 0.6408, F1: 0.6493
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0037, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9986, F1: 0.9992
Validation Loss: 0.7549, Accuracy: 0.8835, Precision: 0.8113, Recall: 0.7911, F1: 0.7942
Testing Loss: 0.8676, Accuracy: 0.8750, Precision: 0.6540, Recall: 0.6733, F1: 0.6624
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0076, Accuracy: 0.9990, Precision: 0.9992, Recall: 0.9994, F1: 0.9993
Validation Loss: 0.7639, Accuracy: 0.8864, Precision: 0.7787, Recall: 0.8082, F1: 0.7871
Testing Loss: 0.9930, Accuracy: 0.8644, Precision: 0.6566, Recall: 0.6683, F1: 0.6598
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0187, Accuracy: 0.9962, Precision: 0.9867, Recall: 0.9900, F1: 0.9883
Validation Loss: 0.7923, Accuracy: 0.8750, Precision: 0.7665, Recall: 0.7790, F1: 0.7525
Testing Loss: 0.8309, Accuracy: 0.8750, Precision: 0.6921, Recall: 0.6818, F1: 0.6831
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0045, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0348, Accuracy: 0.9927, Precision: 0.9855, Recall: 0.9820, F1: 0.9837
Validation Loss: 0.6735, Accuracy: 0.8864, Precision: 0.7507, Recall: 0.7699, F1: 0.7588
Testing Loss: 0.7733, Accuracy: 0.8723, Precision: 0.6694, Recall: 0.6744, F1: 0.6676
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0054, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0205, Accuracy: 0.9944, Precision: 0.9888, Recall: 0.9917, F1: 0.9902
Validation Loss: 0.7632, Accuracy: 0.8636, Precision: 0.7213, Recall: 0.6265, F1: 0.6597
Testing Loss: 0.8830, Accuracy: 0.8617, Precision: 0.6678, Recall: 0.6055, F1: 0.6182
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 4, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4658, Accuracy: 0.8929, Precision: 0.9222, Recall: 0.8800, F1: 0.8711
Epoch 62/70
Train Loss: 0.0244, Accuracy: 0.9941, Precision: 0.9853, Recall: 0.9824, F1: 0.9837
Validation Loss: 0.7469, Accuracy: 0.8807, Precision: 0.7256, Recall: 0.6762, F1: 0.6952
Testing Loss: 0.8999, Accuracy: 0.8803, Precision: 0.6851, Recall: 0.6540, F1: 0.6633
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0075, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0098, Accuracy: 0.9979, Precision: 0.9985, Recall: 0.9918, F1: 0.9951
Validation Loss: 0.6809, Accuracy: 0.8835, Precision: 0.7821, Recall: 0.8044, F1: 0.7868
Testing Loss: 0.8751, Accuracy: 0.8723, Precision: 0.6723, Recall: 0.6679, F1: 0.6687
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8081, Accuracy: 0.8750, Precision: 0.7536, Recall: 0.7441, F1: 0.7460
Testing Loss: 0.9238, Accuracy: 0.8803, Precision: 0.6861, Recall: 0.6839, F1: 0.6818
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0138, Accuracy: 0.9965, Precision: 0.9946, Recall: 0.9889, F1: 0.9917
Validation Loss: 0.7412, Accuracy: 0.8551, Precision: 0.6972, Recall: 0.7037, F1: 0.6961
Testing Loss: 0.7883, Accuracy: 0.8697, Precision: 0.6699, Recall: 0.6753, F1: 0.6651
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0212, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0103, Accuracy: 0.9976, Precision: 0.9982, Recall: 0.9987, F1: 0.9985
Validation Loss: 0.8588, Accuracy: 0.8722, Precision: 0.6895, Recall: 0.7106, F1: 0.6977
Testing Loss: 0.8604, Accuracy: 0.8750, Precision: 0.6458, Recall: 0.6724, F1: 0.6565
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0106, Accuracy: 0.9986, Precision: 0.9990, Recall: 0.9964, F1: 0.9977
Validation Loss: 0.7259, Accuracy: 0.8807, Precision: 0.7215, Recall: 0.6841, F1: 0.6996
Testing Loss: 0.8452, Accuracy: 0.8803, Precision: 0.6754, Recall: 0.6703, F1: 0.6709
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0021, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0091, Accuracy: 0.9986, Precision: 0.9988, Recall: 0.9993, F1: 0.9991
Validation Loss: 0.8376, Accuracy: 0.8693, Precision: 0.7103, Recall: 0.7541, F1: 0.7235
Testing Loss: 0.9824, Accuracy: 0.8670, Precision: 0.6794, Recall: 0.7026, F1: 0.6835
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0358, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0339, Accuracy: 0.9934, Precision: 0.9905, Recall: 0.9851, F1: 0.9878
Validation Loss: 0.8225, Accuracy: 0.8750, Precision: 0.7463, Recall: 0.7529, F1: 0.7217
Testing Loss: 0.9677, Accuracy: 0.8484, Precision: 0.6245, Recall: 0.6256, F1: 0.5972
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0334, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0185, Accuracy: 0.9969, Precision: 0.9904, Recall: 0.9921, F1: 0.9912
Validation Loss: 0.8598, Accuracy: 0.8807, Precision: 0.7071, Recall: 0.6970, F1: 0.7015
Testing Loss: 0.8888, Accuracy: 0.8670, Precision: 0.6609, Recall: 0.6622, F1: 0.6613
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 4, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0463, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9500, F1: 0.9532
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.7868, Accuracy: 0.7232, Precision: 0.5196, Recall: 0.4297, F1: 0.4450
Validation Loss: 0.4257, Accuracy: 0.8466, Precision: 0.6684, Recall: 0.6479, F1: 0.6475
Testing Loss: 0.4453, Accuracy: 0.8590, Precision: 0.6473, Recall: 0.6807, F1: 0.6552
LM Predictions:  [3, 3, 3, 5, 5, 5, 5, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 1, 3, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3769, Accuracy: 0.0714, Precision: 0.0476, Recall: 0.0476, F1: 0.0476
Epoch 2/70
Train Loss: 0.4040, Accuracy: 0.8733, Precision: 0.6583, Recall: 0.6785, F1: 0.6671
Validation Loss: 0.3989, Accuracy: 0.8807, Precision: 0.7110, Recall: 0.6654, F1: 0.6837
Testing Loss: 0.4038, Accuracy: 0.8803, Precision: 0.6823, Recall: 0.6723, F1: 0.6753
LM Predictions:  [3, 3, 3, 2, 5, 2, 5, 5, 5, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 1, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2249, Accuracy: 0.1786, Precision: 0.1250, Recall: 0.1190, F1: 0.1192
Epoch 3/70
Train Loss: 0.2619, Accuracy: 0.9195, Precision: 0.8542, Recall: 0.7851, F1: 0.7959
Validation Loss: 0.4354, Accuracy: 0.8665, Precision: 0.6884, Recall: 0.6567, F1: 0.6619
Testing Loss: 0.4150, Accuracy: 0.8777, Precision: 0.7076, Recall: 0.6720, F1: 0.6859
LM Predictions:  [2, 3, 3, 2, 1, 2, 4, 5, 2, 3, 3, 3, 2, 5, 3, 3, 3, 4, 3, 3, 2, 2, 3, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9326, Accuracy: 0.3929, Precision: 0.6111, Recall: 0.2988, F1: 0.3610
Epoch 4/70
Train Loss: 0.1848, Accuracy: 0.9409, Precision: 0.8635, Recall: 0.8510, F1: 0.8545
Validation Loss: 0.4486, Accuracy: 0.8835, Precision: 0.7458, Recall: 0.6617, F1: 0.6934
Testing Loss: 0.4342, Accuracy: 0.8803, Precision: 0.6803, Recall: 0.6613, F1: 0.6643
LM Predictions:  [3, 3, 3, 2, 1, 4, 4, 5, 2, 3, 3, 3, 2, 5, 3, 3, 5, 3, 4, 5, 2, 2, 1, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3996, Accuracy: 0.3929, Precision: 0.3986, Recall: 0.2988, F1: 0.3333
Epoch 5/70
Train Loss: 0.1263, Accuracy: 0.9608, Precision: 0.9063, Recall: 0.8937, F1: 0.8979
Validation Loss: 0.3976, Accuracy: 0.8750, Precision: 0.7320, Recall: 0.6617, F1: 0.6904
Testing Loss: 0.5166, Accuracy: 0.8803, Precision: 0.6763, Recall: 0.6683, F1: 0.6671
LM Predictions:  [2, 3, 2, 3, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 2, 5, 4, 1, 5, 3, 2, 1, 2, 3, 4, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3223, Accuracy: 0.6071, Precision: 0.6819, Recall: 0.4881, F1: 0.5306
Epoch 6/70
Train Loss: 0.0730, Accuracy: 0.9794, Precision: 0.9641, Recall: 0.9513, F1: 0.9570
Validation Loss: 0.5081, Accuracy: 0.8835, Precision: 0.7284, Recall: 0.6858, F1: 0.7042
Testing Loss: 0.5175, Accuracy: 0.8564, Precision: 0.6420, Recall: 0.6427, F1: 0.6378
LM Predictions:  [2, 3, 2, 3, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 4, 4, 1, 5, 3, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9615, Accuracy: 0.7143, Precision: 0.7708, Recall: 0.5869, F1: 0.6240
Epoch 7/70
Train Loss: 0.0692, Accuracy: 0.9797, Precision: 0.9577, Recall: 0.9503, F1: 0.9535
Validation Loss: 0.5823, Accuracy: 0.8722, Precision: 0.7503, Recall: 0.6163, F1: 0.6577
Testing Loss: 0.6676, Accuracy: 0.8484, Precision: 0.6586, Recall: 0.5763, F1: 0.5940
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 2, 0, 0, 2, 5, 2, 5, 4, 4, 1, 5, 2, 2, 3, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6092, Accuracy: 0.7857, Precision: 0.7311, Recall: 0.6440, F1: 0.6706
Epoch 8/70
Train Loss: 0.0594, Accuracy: 0.9811, Precision: 0.9637, Recall: 0.9592, F1: 0.9612
Validation Loss: 0.5689, Accuracy: 0.8665, Precision: 0.7309, Recall: 0.7220, F1: 0.7137
Testing Loss: 0.6324, Accuracy: 0.8484, Precision: 0.6351, Recall: 0.6296, F1: 0.6269
LM Predictions:  [2, 0, 2, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3376, Accuracy: 0.8571, Precision: 0.8125, Recall: 0.7107, F1: 0.7521
Epoch 9/70
Train Loss: 0.0328, Accuracy: 0.9888, Precision: 0.9790, Recall: 0.9713, F1: 0.9750
Validation Loss: 0.5074, Accuracy: 0.8750, Precision: 0.7213, Recall: 0.6488, F1: 0.6776
Testing Loss: 0.6633, Accuracy: 0.8590, Precision: 0.6478, Recall: 0.6267, F1: 0.6339
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2645, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7345, F1: 0.7512
Epoch 10/70
Train Loss: 0.0417, Accuracy: 0.9867, Precision: 0.9743, Recall: 0.9739, F1: 0.9739
Validation Loss: 0.6248, Accuracy: 0.8693, Precision: 0.7423, Recall: 0.6442, F1: 0.6811
Testing Loss: 0.6447, Accuracy: 0.8564, Precision: 0.6379, Recall: 0.6108, F1: 0.6161
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2007, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9214, F1: 0.9227
Epoch 11/70
Train Loss: 0.0456, Accuracy: 0.9836, Precision: 0.9692, Recall: 0.9702, F1: 0.9697
Validation Loss: 0.6286, Accuracy: 0.8665, Precision: 0.7030, Recall: 0.6820, F1: 0.6869
Testing Loss: 0.6902, Accuracy: 0.8511, Precision: 0.6328, Recall: 0.6425, F1: 0.6234
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2034, Accuracy: 0.9286, Precision: 0.9200, Recall: 0.9314, F1: 0.9224
Epoch 12/70
Train Loss: 0.0211, Accuracy: 0.9944, Precision: 0.9913, Recall: 0.9924, F1: 0.9918
Validation Loss: 0.5879, Accuracy: 0.8778, Precision: 0.7147, Recall: 0.6812, F1: 0.6961
Testing Loss: 0.6427, Accuracy: 0.8697, Precision: 0.6650, Recall: 0.6568, F1: 0.6589
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1504, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9214, F1: 0.9227
Epoch 13/70
Train Loss: 0.0435, Accuracy: 0.9895, Precision: 0.9739, Recall: 0.9808, F1: 0.9773
Validation Loss: 0.7484, Accuracy: 0.8494, Precision: 0.6828, Recall: 0.6205, F1: 0.6459
Testing Loss: 0.6868, Accuracy: 0.8617, Precision: 0.6511, Recall: 0.6346, F1: 0.6309
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1932, Accuracy: 0.8929, Precision: 0.8833, Recall: 0.8814, F1: 0.8801
Epoch 14/70
Train Loss: 0.0478, Accuracy: 0.9874, Precision: 0.9676, Recall: 0.9654, F1: 0.9665
Validation Loss: 0.5605, Accuracy: 0.8835, Precision: 0.7195, Recall: 0.7060, F1: 0.7089
Testing Loss: 0.5089, Accuracy: 0.8883, Precision: 0.6778, Recall: 0.7028, F1: 0.6895
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1805, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7762, F1: 0.7872
Epoch 15/70
Train Loss: 0.0185, Accuracy: 0.9958, Precision: 0.9852, Recall: 0.9876, F1: 0.9864
Validation Loss: 0.6427, Accuracy: 0.8835, Precision: 0.7182, Recall: 0.6962, F1: 0.7036
Testing Loss: 0.6496, Accuracy: 0.8803, Precision: 0.6742, Recall: 0.6782, F1: 0.6732
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1176, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9214, F1: 0.9227
Epoch 16/70
Train Loss: 0.0316, Accuracy: 0.9916, Precision: 0.9806, Recall: 0.9808, F1: 0.9807
Validation Loss: 0.7222, Accuracy: 0.8665, Precision: 0.7059, Recall: 0.6787, F1: 0.6907
Testing Loss: 0.6877, Accuracy: 0.8723, Precision: 0.6723, Recall: 0.6589, F1: 0.6632
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1119, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 17/70
Train Loss: 0.0324, Accuracy: 0.9902, Precision: 0.9726, Recall: 0.9780, F1: 0.9751
Validation Loss: 0.7058, Accuracy: 0.8665, Precision: 0.6937, Recall: 0.6693, F1: 0.6791
Testing Loss: 0.7666, Accuracy: 0.8590, Precision: 0.6445, Recall: 0.6312, F1: 0.6277
LM Predictions:  [2, 0, 2, 5, 1, 3, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1451, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 18/70
Train Loss: 0.0328, Accuracy: 0.9920, Precision: 0.9794, Recall: 0.9850, F1: 0.9821
Validation Loss: 0.6719, Accuracy: 0.8551, Precision: 0.6834, Recall: 0.6558, F1: 0.6683
Testing Loss: 0.7663, Accuracy: 0.8617, Precision: 0.6635, Recall: 0.6400, F1: 0.6501
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0659, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 19/70
Train Loss: 0.0103, Accuracy: 0.9979, Precision: 0.9897, Recall: 0.9891, F1: 0.9894
Validation Loss: 0.6860, Accuracy: 0.8608, Precision: 0.7010, Recall: 0.7244, F1: 0.6980
Testing Loss: 0.6614, Accuracy: 0.8697, Precision: 0.6869, Recall: 0.7092, F1: 0.6958
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0429, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 20/70
Train Loss: 0.0064, Accuracy: 0.9990, Precision: 0.9964, Recall: 0.9992, F1: 0.9978
Validation Loss: 0.6443, Accuracy: 0.8778, Precision: 0.7365, Recall: 0.7231, F1: 0.7205
Testing Loss: 0.7399, Accuracy: 0.8803, Precision: 0.6811, Recall: 0.6518, F1: 0.6633
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0217, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0021, Accuracy: 0.9997, Precision: 0.9969, Recall: 0.9997, F1: 0.9983
Validation Loss: 0.6866, Accuracy: 0.8864, Precision: 0.7461, Recall: 0.7347, F1: 0.7332
Testing Loss: 0.7726, Accuracy: 0.8670, Precision: 0.6601, Recall: 0.6272, F1: 0.6402
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0403, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 22/70
Train Loss: 0.0118, Accuracy: 0.9986, Precision: 0.9990, Recall: 0.9978, F1: 0.9984
Validation Loss: 0.8811, Accuracy: 0.8409, Precision: 0.7067, Recall: 0.6983, F1: 0.6665
Testing Loss: 1.0719, Accuracy: 0.8165, Precision: 0.6188, Recall: 0.6067, F1: 0.5737
LM Predictions:  [2, 0, 2, 1, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1813, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9714, F1: 0.9624
Epoch 23/70
Train Loss: 0.0288, Accuracy: 0.9899, Precision: 0.9774, Recall: 0.9761, F1: 0.9767
Validation Loss: 0.6964, Accuracy: 0.8778, Precision: 0.7191, Recall: 0.6789, F1: 0.6787
Testing Loss: 0.7579, Accuracy: 0.8644, Precision: 0.6501, Recall: 0.6473, F1: 0.6465
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0246, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0255, Accuracy: 0.9930, Precision: 0.9866, Recall: 0.9918, F1: 0.9892
Validation Loss: 0.7742, Accuracy: 0.8722, Precision: 0.7293, Recall: 0.6516, F1: 0.6821
Testing Loss: 0.8764, Accuracy: 0.8511, Precision: 0.6764, Recall: 0.6079, F1: 0.6238
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 4, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3302, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9600, F1: 0.9596
Epoch 25/70
Train Loss: 0.0341, Accuracy: 0.9899, Precision: 0.9860, Recall: 0.9836, F1: 0.9848
Validation Loss: 0.6828, Accuracy: 0.8778, Precision: 0.7655, Recall: 0.7841, F1: 0.7459
Testing Loss: 0.6865, Accuracy: 0.8777, Precision: 0.7053, Recall: 0.6703, F1: 0.6853
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0259, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0249, Accuracy: 0.9937, Precision: 0.9864, Recall: 0.9921, F1: 0.9892
Validation Loss: 0.6954, Accuracy: 0.8750, Precision: 0.7894, Recall: 0.7869, F1: 0.7667
Testing Loss: 0.7768, Accuracy: 0.8564, Precision: 0.6549, Recall: 0.6169, F1: 0.6323
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0060, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0153, Accuracy: 0.9965, Precision: 0.9979, Recall: 0.9979, F1: 0.9979
Validation Loss: 0.6480, Accuracy: 0.8807, Precision: 0.7528, Recall: 0.7370, F1: 0.7327
Testing Loss: 0.8097, Accuracy: 0.8644, Precision: 0.6538, Recall: 0.6473, F1: 0.6473
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0042, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0059, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9979, F1: 0.9979
Validation Loss: 0.6638, Accuracy: 0.8864, Precision: 0.7590, Recall: 0.7281, F1: 0.7277
Testing Loss: 0.9329, Accuracy: 0.8537, Precision: 0.6518, Recall: 0.6165, F1: 0.6300
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0056, Accuracy: 0.9976, Precision: 0.9955, Recall: 0.9924, F1: 0.9939
Validation Loss: 0.9544, Accuracy: 0.8409, Precision: 0.7165, Recall: 0.7109, F1: 0.6886
Testing Loss: 0.8774, Accuracy: 0.8457, Precision: 0.6529, Recall: 0.6608, F1: 0.6458
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 0]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1163, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 30/70
Train Loss: 0.0265, Accuracy: 0.9930, Precision: 0.9766, Recall: 0.9789, F1: 0.9777
Validation Loss: 0.9216, Accuracy: 0.8665, Precision: 0.7219, Recall: 0.7168, F1: 0.7073
Testing Loss: 0.8366, Accuracy: 0.8644, Precision: 0.6672, Recall: 0.6498, F1: 0.6551
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0099, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0405, Accuracy: 0.9909, Precision: 0.9820, Recall: 0.9818, F1: 0.9819
Validation Loss: 0.9389, Accuracy: 0.8523, Precision: 0.6907, Recall: 0.6843, F1: 0.6765
Testing Loss: 0.8525, Accuracy: 0.8511, Precision: 0.6495, Recall: 0.6342, F1: 0.6397
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0416, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 32/70
Train Loss: 0.0229, Accuracy: 0.9937, Precision: 0.9873, Recall: 0.9818, F1: 0.9845
Validation Loss: 0.8072, Accuracy: 0.8523, Precision: 0.7007, Recall: 0.7708, F1: 0.7067
Testing Loss: 0.8573, Accuracy: 0.8617, Precision: 0.6602, Recall: 0.6802, F1: 0.6667
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0171, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0252, Accuracy: 0.9923, Precision: 0.9877, Recall: 0.9862, F1: 0.9869
Validation Loss: 0.7131, Accuracy: 0.8580, Precision: 0.7175, Recall: 0.7099, F1: 0.6936
Testing Loss: 0.8108, Accuracy: 0.8723, Precision: 0.7049, Recall: 0.6699, F1: 0.6807
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 3, 1, 5, 0, 2, 5, 4, 3, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0788, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7857, F1: 0.8077
Epoch 34/70
Train Loss: 0.0178, Accuracy: 0.9962, Precision: 0.9886, Recall: 0.9948, F1: 0.9917
Validation Loss: 0.6914, Accuracy: 0.8949, Precision: 0.7789, Recall: 0.7212, F1: 0.7312
Testing Loss: 0.7848, Accuracy: 0.8723, Precision: 0.6779, Recall: 0.6120, F1: 0.6316
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0038, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0315, Accuracy: 0.9916, Precision: 0.9811, Recall: 0.9805, F1: 0.9808
Validation Loss: 0.8791, Accuracy: 0.8523, Precision: 0.7273, Recall: 0.6619, F1: 0.6393
Testing Loss: 0.8165, Accuracy: 0.8697, Precision: 0.6820, Recall: 0.6153, F1: 0.6261
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1232, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 36/70
Train Loss: 0.0125, Accuracy: 0.9969, Precision: 0.9932, Recall: 0.9900, F1: 0.9916
Validation Loss: 0.7899, Accuracy: 0.8778, Precision: 0.7386, Recall: 0.7374, F1: 0.7234
Testing Loss: 0.8303, Accuracy: 0.8723, Precision: 0.6694, Recall: 0.6576, F1: 0.6614
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0116, Accuracy: 0.9976, Precision: 0.9984, Recall: 0.9984, F1: 0.9984
Validation Loss: 0.7078, Accuracy: 0.8551, Precision: 0.7201, Recall: 0.6961, F1: 0.6885
Testing Loss: 0.7688, Accuracy: 0.8670, Precision: 0.6850, Recall: 0.6428, F1: 0.6547
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0133, Accuracy: 0.9979, Precision: 0.9926, Recall: 0.9927, F1: 0.9927
Validation Loss: 0.8569, Accuracy: 0.8580, Precision: 0.7562, Recall: 0.7575, F1: 0.7170
Testing Loss: 0.7979, Accuracy: 0.8537, Precision: 0.6781, Recall: 0.6273, F1: 0.6472
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0141, Accuracy: 0.9969, Precision: 0.9912, Recall: 0.9895, F1: 0.9904
Validation Loss: 0.9365, Accuracy: 0.8580, Precision: 0.7456, Recall: 0.7100, F1: 0.7124
Testing Loss: 0.9343, Accuracy: 0.8670, Precision: 0.6816, Recall: 0.6495, F1: 0.6559
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0239, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0198, Accuracy: 0.9944, Precision: 0.9923, Recall: 0.9910, F1: 0.9917
Validation Loss: 0.7684, Accuracy: 0.8608, Precision: 0.7056, Recall: 0.7147, F1: 0.6949
Testing Loss: 0.7719, Accuracy: 0.8723, Precision: 0.6753, Recall: 0.6633, F1: 0.6683
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0159, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0288, Accuracy: 0.9923, Precision: 0.9735, Recall: 0.9770, F1: 0.9752
Validation Loss: 0.6518, Accuracy: 0.8835, Precision: 0.7256, Recall: 0.6949, F1: 0.7040
Testing Loss: 0.8157, Accuracy: 0.8564, Precision: 0.6445, Recall: 0.6211, F1: 0.6299
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0135, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0046, Accuracy: 0.9986, Precision: 0.9949, Recall: 0.9964, F1: 0.9957
Validation Loss: 0.8112, Accuracy: 0.8778, Precision: 0.7413, Recall: 0.6702, F1: 0.6992
Testing Loss: 0.8484, Accuracy: 0.8590, Precision: 0.6647, Recall: 0.6125, F1: 0.6328
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0476, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 43/70
Train Loss: 0.0146, Accuracy: 0.9958, Precision: 0.9877, Recall: 0.9893, F1: 0.9885
Validation Loss: 0.7464, Accuracy: 0.8693, Precision: 0.7431, Recall: 0.7117, F1: 0.7098
Testing Loss: 0.7498, Accuracy: 0.8750, Precision: 0.7146, Recall: 0.6416, F1: 0.6707
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2614, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 44/70
Train Loss: 0.0076, Accuracy: 0.9969, Precision: 0.9927, Recall: 0.9952, F1: 0.9940
Validation Loss: 0.7624, Accuracy: 0.8892, Precision: 0.7549, Recall: 0.6806, F1: 0.7112
Testing Loss: 0.7777, Accuracy: 0.8803, Precision: 0.7001, Recall: 0.6490, F1: 0.6673
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0031, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0152, Accuracy: 0.9969, Precision: 0.9980, Recall: 0.9950, F1: 0.9965
Validation Loss: 0.6336, Accuracy: 0.8892, Precision: 0.7479, Recall: 0.7425, F1: 0.7356
Testing Loss: 0.8025, Accuracy: 0.8644, Precision: 0.6702, Recall: 0.6358, F1: 0.6490
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0059, Accuracy: 0.9983, Precision: 0.9904, Recall: 0.9933, F1: 0.9918
Validation Loss: 0.6668, Accuracy: 0.8864, Precision: 0.7542, Recall: 0.7397, F1: 0.7434
Testing Loss: 0.8563, Accuracy: 0.8537, Precision: 0.6473, Recall: 0.6338, F1: 0.6382
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0131, Accuracy: 0.9955, Precision: 0.9852, Recall: 0.9879, F1: 0.9865
Validation Loss: 0.7151, Accuracy: 0.8693, Precision: 0.7784, Recall: 0.7044, F1: 0.7266
Testing Loss: 0.7971, Accuracy: 0.8803, Precision: 0.6905, Recall: 0.6728, F1: 0.6715
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0028, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0183, Accuracy: 0.9969, Precision: 0.9906, Recall: 0.9944, F1: 0.9925
Validation Loss: 0.7297, Accuracy: 0.8892, Precision: 0.8201, Recall: 0.7298, F1: 0.7665
Testing Loss: 0.7297, Accuracy: 0.8830, Precision: 0.6908, Recall: 0.6498, F1: 0.6667
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0094, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0147, Accuracy: 0.9965, Precision: 0.9940, Recall: 0.9938, F1: 0.9939
Validation Loss: 0.8090, Accuracy: 0.8636, Precision: 0.7215, Recall: 0.7267, F1: 0.7166
Testing Loss: 0.8207, Accuracy: 0.8564, Precision: 0.6384, Recall: 0.6646, F1: 0.6486
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0141, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0210, Accuracy: 0.9941, Precision: 0.9870, Recall: 0.9909, F1: 0.9889
Validation Loss: 0.6487, Accuracy: 0.8864, Precision: 0.7303, Recall: 0.7152, F1: 0.7072
Testing Loss: 0.7802, Accuracy: 0.8590, Precision: 0.6651, Recall: 0.6334, F1: 0.6363
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0100, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0089, Accuracy: 0.9976, Precision: 0.9942, Recall: 0.9971, F1: 0.9957
Validation Loss: 0.7136, Accuracy: 0.8750, Precision: 0.7222, Recall: 0.7221, F1: 0.7185
Testing Loss: 0.8091, Accuracy: 0.8670, Precision: 0.6506, Recall: 0.6547, F1: 0.6504
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0047, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9986, F1: 0.9992
Validation Loss: 0.7620, Accuracy: 0.8693, Precision: 0.6898, Recall: 0.7095, F1: 0.6912
Testing Loss: 0.9015, Accuracy: 0.8670, Precision: 0.6557, Recall: 0.6728, F1: 0.6601
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7532, Accuracy: 0.8722, Precision: 0.6962, Recall: 0.7125, F1: 0.6956
Testing Loss: 0.8864, Accuracy: 0.8617, Precision: 0.6491, Recall: 0.6564, F1: 0.6496
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8293, Accuracy: 0.8636, Precision: 0.6988, Recall: 0.7024, F1: 0.6916
Testing Loss: 0.9157, Accuracy: 0.8590, Precision: 0.6490, Recall: 0.6474, F1: 0.6462
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0147, Accuracy: 0.9972, Precision: 0.9982, Recall: 0.9970, F1: 0.9976
Validation Loss: 0.8786, Accuracy: 0.8750, Precision: 0.7133, Recall: 0.6494, F1: 0.6690
Testing Loss: 0.9868, Accuracy: 0.8617, Precision: 0.6995, Recall: 0.6302, F1: 0.6492
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0157, Accuracy: 0.9969, Precision: 0.9952, Recall: 0.9954, F1: 0.9953
Validation Loss: 0.8062, Accuracy: 0.8665, Precision: 0.7190, Recall: 0.7771, F1: 0.7162
Testing Loss: 0.8891, Accuracy: 0.8617, Precision: 0.6611, Recall: 0.6638, F1: 0.6594
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0171, Accuracy: 0.9969, Precision: 0.9957, Recall: 0.9959, F1: 0.9958
Validation Loss: 0.8901, Accuracy: 0.8580, Precision: 0.7621, Recall: 0.6666, F1: 0.6941
Testing Loss: 0.9310, Accuracy: 0.8378, Precision: 0.6596, Recall: 0.5624, F1: 0.5932
LM Predictions:  [2, 0, 2, 5, 4, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0979, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9500, F1: 0.9532
Epoch 58/70
Train Loss: 0.0132, Accuracy: 0.9979, Precision: 0.9962, Recall: 0.9943, F1: 0.9952
Validation Loss: 0.7801, Accuracy: 0.8778, Precision: 0.7196, Recall: 0.7115, F1: 0.7085
Testing Loss: 0.9108, Accuracy: 0.8750, Precision: 0.6882, Recall: 0.6605, F1: 0.6682
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8002, Accuracy: 0.8807, Precision: 0.7186, Recall: 0.7143, F1: 0.7076
Testing Loss: 0.9119, Accuracy: 0.8777, Precision: 0.6900, Recall: 0.6617, F1: 0.6701
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0136, Accuracy: 0.9972, Precision: 0.9928, Recall: 0.9930, F1: 0.9929
Validation Loss: 0.8310, Accuracy: 0.8580, Precision: 0.8230, Recall: 0.7101, F1: 0.7377
Testing Loss: 0.8472, Accuracy: 0.8777, Precision: 0.6901, Recall: 0.6563, F1: 0.6707
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 3, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1467, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 61/70
Train Loss: 0.0099, Accuracy: 0.9976, Precision: 0.9971, Recall: 0.9984, F1: 0.9977
Validation Loss: 1.1459, Accuracy: 0.8523, Precision: 0.6967, Recall: 0.6989, F1: 0.6878
Testing Loss: 1.0416, Accuracy: 0.8537, Precision: 0.6459, Recall: 0.6256, F1: 0.6333
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0334, Accuracy: 0.9916, Precision: 0.9753, Recall: 0.9731, F1: 0.9742
Validation Loss: 0.7352, Accuracy: 0.8920, Precision: 0.7454, Recall: 0.7453, F1: 0.7399
Testing Loss: 0.7796, Accuracy: 0.8697, Precision: 0.6589, Recall: 0.6526, F1: 0.6519
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0093, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0207, Accuracy: 0.9955, Precision: 0.9913, Recall: 0.9935, F1: 0.9924
Validation Loss: 1.1481, Accuracy: 0.8267, Precision: 0.7231, Recall: 0.6769, F1: 0.6786
Testing Loss: 1.2931, Accuracy: 0.8298, Precision: 0.6814, Recall: 0.6505, F1: 0.6576
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0084, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0545, Accuracy: 0.9888, Precision: 0.9819, Recall: 0.9774, F1: 0.9796
Validation Loss: 0.7434, Accuracy: 0.8750, Precision: 0.7406, Recall: 0.7058, F1: 0.7097
Testing Loss: 0.8458, Accuracy: 0.8697, Precision: 0.6791, Recall: 0.6678, F1: 0.6700
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1857, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 65/70
Train Loss: 0.0040, Accuracy: 0.9993, Precision: 0.9983, Recall: 0.9967, F1: 0.9975
Validation Loss: 0.8389, Accuracy: 0.8693, Precision: 0.7267, Recall: 0.7100, F1: 0.7083
Testing Loss: 0.9305, Accuracy: 0.8697, Precision: 0.6781, Recall: 0.6765, F1: 0.6740
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 1, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1914, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7667, F1: 0.7778
Epoch 66/70
Train Loss: 0.0080, Accuracy: 0.9986, Precision: 0.9967, Recall: 0.9962, F1: 0.9964
Validation Loss: 0.9525, Accuracy: 0.8636, Precision: 0.7469, Recall: 0.6835, F1: 0.7012
Testing Loss: 1.0061, Accuracy: 0.8484, Precision: 0.6659, Recall: 0.6354, F1: 0.6416
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0120, Accuracy: 0.9979, Precision: 0.9959, Recall: 0.9957, F1: 0.9958
Validation Loss: 0.8214, Accuracy: 0.8608, Precision: 0.7444, Recall: 0.7476, F1: 0.7325
Testing Loss: 0.8647, Accuracy: 0.8617, Precision: 0.6669, Recall: 0.6469, F1: 0.6541
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8771, Accuracy: 0.8722, Precision: 0.7299, Recall: 0.7186, F1: 0.7192
Testing Loss: 0.9367, Accuracy: 0.8750, Precision: 0.6806, Recall: 0.6646, F1: 0.6672
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8886, Accuracy: 0.8693, Precision: 0.7285, Recall: 0.7173, F1: 0.7178
Testing Loss: 0.9422, Accuracy: 0.8750, Precision: 0.6822, Recall: 0.6691, F1: 0.6712
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0145, Accuracy: 0.9983, Precision: 0.9964, Recall: 0.9951, F1: 0.9957
Validation Loss: 0.8742, Accuracy: 0.8608, Precision: 0.7199, Recall: 0.7150, F1: 0.7139
Testing Loss: 0.8138, Accuracy: 0.8697, Precision: 0.6655, Recall: 0.6736, F1: 0.6692
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



