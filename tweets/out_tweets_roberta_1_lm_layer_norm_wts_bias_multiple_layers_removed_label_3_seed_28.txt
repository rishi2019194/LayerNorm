---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1952, Accuracy: 0.5353, Precision: 0.2612, Recall: 0.2586, F1: 0.2450
Validation Loss: 0.8508, Accuracy: 0.7045, Precision: 0.3472, Recall: 0.3976, F1: 0.3638
Testing Loss: 0.8471, Accuracy: 0.7048, Precision: 0.3545, Recall: 0.4088, F1: 0.3659
LM Predictions:  [5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9246, Accuracy: 0.2143, Precision: 0.0462, Recall: 0.1714, F1: 0.0727
Epoch 2/70
Train Loss: 0.7703, Accuracy: 0.7358, Precision: 0.4742, Recall: 0.4334, F1: 0.4289
Validation Loss: 0.6904, Accuracy: 0.7756, Precision: 0.4388, Recall: 0.4720, F1: 0.4542
Testing Loss: 0.7288, Accuracy: 0.7793, Precision: 0.5879, Recall: 0.4920, F1: 0.5024
LM Predictions:  [3, 3, 3, 3, 5, 2, 5, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 5, 3, 5, 2, 4, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0275, Accuracy: 0.2143, Precision: 0.2292, Recall: 0.1524, F1: 0.1749
Epoch 3/70
Train Loss: 0.6080, Accuracy: 0.7932, Precision: 0.5293, Recall: 0.5256, F1: 0.5149
Validation Loss: 0.6509, Accuracy: 0.7841, Precision: 0.5283, Recall: 0.4878, F1: 0.4746
Testing Loss: 0.6220, Accuracy: 0.8112, Precision: 0.5774, Recall: 0.5377, F1: 0.5473
LM Predictions:  [3, 3, 3, 5, 5, 2, 3, 5, 3, 3, 3, 3, 5, 4, 4, 5, 3, 3, 3, 3, 2, 5, 5, 2, 4, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1627, Accuracy: 0.1786, Precision: 0.1667, Recall: 0.1286, F1: 0.1375
Epoch 4/70
Train Loss: 0.5264, Accuracy: 0.8310, Precision: 0.5633, Recall: 0.5765, F1: 0.5663
Validation Loss: 0.6136, Accuracy: 0.7841, Precision: 0.5269, Recall: 0.5177, F1: 0.4992
Testing Loss: 0.5896, Accuracy: 0.8112, Precision: 0.5378, Recall: 0.5454, F1: 0.5302
LM Predictions:  [3, 3, 3, 3, 5, 2, 2, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 5, 2, 2, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0013, Accuracy: 0.2143, Precision: 0.2889, Recall: 0.1524, F1: 0.1736
Epoch 5/70
Train Loss: 0.4718, Accuracy: 0.8418, Precision: 0.5875, Recall: 0.6050, F1: 0.5930
Validation Loss: 0.5864, Accuracy: 0.8210, Precision: 0.6205, Recall: 0.5504, F1: 0.5760
Testing Loss: 0.5718, Accuracy: 0.8165, Precision: 0.7505, Recall: 0.5900, F1: 0.6113
LM Predictions:  [3, 3, 3, 3, 2, 2, 2, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 5, 2, 1, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9261, Accuracy: 0.2500, Precision: 0.4722, Recall: 0.1940, F1: 0.2453
Epoch 6/70
Train Loss: 0.4006, Accuracy: 0.8712, Precision: 0.6897, Recall: 0.6455, F1: 0.6520
Validation Loss: 0.6030, Accuracy: 0.8239, Precision: 0.6316, Recall: 0.5742, F1: 0.5836
Testing Loss: 0.5840, Accuracy: 0.8298, Precision: 0.6168, Recall: 0.5817, F1: 0.5897
LM Predictions:  [3, 3, 3, 3, 5, 2, 4, 5, 3, 3, 3, 3, 2, 5, 4, 5, 3, 3, 3, 3, 2, 5, 5, 2, 1, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9399, Accuracy: 0.2857, Precision: 0.4792, Recall: 0.2274, F1: 0.2892
Epoch 7/70
Train Loss: 0.3600, Accuracy: 0.8786, Precision: 0.7135, Recall: 0.6710, F1: 0.6749
Validation Loss: 0.6200, Accuracy: 0.8153, Precision: 0.6305, Recall: 0.5958, F1: 0.6055
Testing Loss: 0.5868, Accuracy: 0.8271, Precision: 0.6014, Recall: 0.6058, F1: 0.5939
LM Predictions:  [3, 3, 3, 3, 3, 2, 2, 5, 3, 3, 3, 3, 2, 3, 4, 5, 3, 3, 3, 3, 2, 2, 1, 3, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4731, Accuracy: 0.2143, Precision: 0.4048, Recall: 0.1524, F1: 0.2011
Epoch 8/70
Train Loss: 0.3282, Accuracy: 0.9006, Precision: 0.7578, Recall: 0.7295, F1: 0.7378
Validation Loss: 0.5929, Accuracy: 0.8097, Precision: 0.6487, Recall: 0.5537, F1: 0.5739
Testing Loss: 0.6035, Accuracy: 0.8218, Precision: 0.6269, Recall: 0.5725, F1: 0.5903
LM Predictions:  [3, 3, 3, 2, 2, 2, 2, 5, 3, 3, 3, 3, 2, 2, 4, 2, 3, 3, 3, 3, 2, 2, 1, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3930, Accuracy: 0.2143, Precision: 0.3889, Recall: 0.1524, F1: 0.1674
Epoch 9/70
Train Loss: 0.3005, Accuracy: 0.9062, Precision: 0.7635, Recall: 0.7445, F1: 0.7508
Validation Loss: 0.5658, Accuracy: 0.8097, Precision: 0.6237, Recall: 0.5823, F1: 0.5847
Testing Loss: 0.6183, Accuracy: 0.8059, Precision: 0.5984, Recall: 0.5881, F1: 0.5618
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 3, 2, 1, 2, 1, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0315, Accuracy: 0.3214, Precision: 0.5119, Recall: 0.2512, F1: 0.3201
Epoch 10/70
Train Loss: 0.2443, Accuracy: 0.9209, Precision: 0.7884, Recall: 0.7721, F1: 0.7791
Validation Loss: 0.6004, Accuracy: 0.8381, Precision: 0.6400, Recall: 0.6146, F1: 0.6259
Testing Loss: 0.6684, Accuracy: 0.8298, Precision: 0.6059, Recall: 0.6154, F1: 0.6017
LM Predictions:  [3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 3, 3, 2, 5, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0479, Accuracy: 0.3571, Precision: 0.7063, Recall: 0.2845, F1: 0.3794
Epoch 11/70
Train Loss: 0.2406, Accuracy: 0.9251, Precision: 0.8169, Recall: 0.8001, F1: 0.8061
Validation Loss: 0.6154, Accuracy: 0.8239, Precision: 0.6223, Recall: 0.6186, F1: 0.6191
Testing Loss: 0.6023, Accuracy: 0.8404, Precision: 0.6188, Recall: 0.6408, F1: 0.6233
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 3, 3, 1, 2, 2, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0135, Accuracy: 0.3214, Precision: 0.4008, Recall: 0.2512, F1: 0.2987
Epoch 12/70
Train Loss: 0.2039, Accuracy: 0.9412, Precision: 0.8439, Recall: 0.8205, F1: 0.8286
Validation Loss: 0.6170, Accuracy: 0.8324, Precision: 0.6540, Recall: 0.6138, F1: 0.6315
Testing Loss: 0.6797, Accuracy: 0.8324, Precision: 0.6592, Recall: 0.6463, F1: 0.6473
LM Predictions:  [0, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 3, 2, 0, 4, 5, 3, 3, 4, 1, 3, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9463, Accuracy: 0.4643, Precision: 0.5972, Recall: 0.3929, F1: 0.4683
Epoch 13/70
Train Loss: 0.1642, Accuracy: 0.9535, Precision: 0.8831, Recall: 0.8778, F1: 0.8797
Validation Loss: 0.6753, Accuracy: 0.8352, Precision: 0.6691, Recall: 0.5871, F1: 0.6098
Testing Loss: 0.7385, Accuracy: 0.8351, Precision: 0.6134, Recall: 0.6141, F1: 0.6040
LM Predictions:  [5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 3, 2, 5, 4, 5, 3, 3, 1, 1, 1, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3918, Accuracy: 0.5357, Precision: 0.7000, Recall: 0.4583, F1: 0.5359
Epoch 14/70
Train Loss: 0.1644, Accuracy: 0.9545, Precision: 0.8642, Recall: 0.8812, F1: 0.8717
Validation Loss: 0.6938, Accuracy: 0.8551, Precision: 0.6856, Recall: 0.6431, F1: 0.6615
Testing Loss: 0.6886, Accuracy: 0.8484, Precision: 0.6484, Recall: 0.6592, F1: 0.6536
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 3, 2, 2, 4, 5, 3, 4, 3, 5, 3, 2, 5, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6107, Accuracy: 0.4286, Precision: 0.5917, Recall: 0.3417, F1: 0.4168
Epoch 15/70
Train Loss: 0.1561, Accuracy: 0.9535, Precision: 0.8741, Recall: 0.8622, F1: 0.8662
Validation Loss: 0.7824, Accuracy: 0.8153, Precision: 0.6735, Recall: 0.5730, F1: 0.6103
Testing Loss: 0.7448, Accuracy: 0.8351, Precision: 0.6823, Recall: 0.6721, F1: 0.6674
LM Predictions:  [0, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 2, 4, 5, 3, 3, 3, 5, 0, 2, 5, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4858, Accuracy: 0.5000, Precision: 0.6750, Recall: 0.4167, F1: 0.5122
Epoch 16/70
Train Loss: 0.1435, Accuracy: 0.9598, Precision: 0.9015, Recall: 0.8904, F1: 0.8943
Validation Loss: 0.7183, Accuracy: 0.8381, Precision: 0.6853, Recall: 0.6263, F1: 0.6512
Testing Loss: 0.6851, Accuracy: 0.8404, Precision: 0.6395, Recall: 0.6342, F1: 0.6358
LM Predictions:  [0, 3, 3, 3, 1, 4, 4, 5, 0, 3, 3, 0, 2, 5, 4, 5, 3, 3, 1, 5, 0, 2, 5, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4767, Accuracy: 0.5714, Precision: 0.7167, Recall: 0.4738, F1: 0.5593
Epoch 17/70
Train Loss: 0.1274, Accuracy: 0.9598, Precision: 0.8869, Recall: 0.8830, F1: 0.8838
Validation Loss: 0.7038, Accuracy: 0.8523, Precision: 0.6954, Recall: 0.6605, F1: 0.6766
Testing Loss: 0.6738, Accuracy: 0.8644, Precision: 0.6807, Recall: 0.6887, F1: 0.6842
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 5, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6106, Accuracy: 0.5357, Precision: 0.8056, Recall: 0.4226, F1: 0.5363
Epoch 18/70
Train Loss: 0.1218, Accuracy: 0.9636, Precision: 0.9012, Recall: 0.8974, F1: 0.8988
Validation Loss: 0.6719, Accuracy: 0.8381, Precision: 0.6544, Recall: 0.6209, F1: 0.6341
Testing Loss: 0.7584, Accuracy: 0.8218, Precision: 0.6254, Recall: 0.6349, F1: 0.6256
LM Predictions:  [3, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2388, Accuracy: 0.6071, Precision: 0.8056, Recall: 0.4976, F1: 0.6105
Epoch 19/70
Train Loss: 0.1004, Accuracy: 0.9706, Precision: 0.9083, Recall: 0.9169, F1: 0.9121
Validation Loss: 0.7554, Accuracy: 0.8267, Precision: 0.6786, Recall: 0.5908, F1: 0.6210
Testing Loss: 0.7931, Accuracy: 0.8165, Precision: 0.6444, Recall: 0.5907, F1: 0.6118
LM Predictions:  [2, 3, 3, 3, 1, 4, 4, 5, 2, 3, 3, 0, 2, 2, 4, 5, 3, 3, 1, 5, 5, 2, 5, 2, 4, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4887, Accuracy: 0.6429, Precision: 0.7345, Recall: 0.5214, F1: 0.6012
Epoch 20/70
Train Loss: 0.1058, Accuracy: 0.9703, Precision: 0.9277, Recall: 0.9156, F1: 0.9210
Validation Loss: 0.8299, Accuracy: 0.8239, Precision: 0.6582, Recall: 0.6894, F1: 0.6544
Testing Loss: 0.8686, Accuracy: 0.8218, Precision: 0.6485, Recall: 0.6759, F1: 0.6494
LM Predictions:  [5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 0, 4, 5, 3, 4, 4, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4501, Accuracy: 0.6071, Precision: 0.6694, Recall: 0.5071, F1: 0.5720
Epoch 21/70
Train Loss: 0.0760, Accuracy: 0.9804, Precision: 0.9489, Recall: 0.9542, F1: 0.9511
Validation Loss: 0.9601, Accuracy: 0.8210, Precision: 0.6695, Recall: 0.5981, F1: 0.6268
Testing Loss: 0.9640, Accuracy: 0.8351, Precision: 0.6913, Recall: 0.6717, F1: 0.6785
LM Predictions:  [2, 3, 3, 3, 1, 4, 4, 5, 2, 3, 3, 0, 2, 2, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 4, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3456, Accuracy: 0.6786, Precision: 0.7206, Recall: 0.5548, F1: 0.6176
Epoch 22/70
Train Loss: 0.1156, Accuracy: 0.9671, Precision: 0.9316, Recall: 0.9361, F1: 0.9335
Validation Loss: 0.7366, Accuracy: 0.8608, Precision: 0.6950, Recall: 0.6622, F1: 0.6768
Testing Loss: 0.8672, Accuracy: 0.8431, Precision: 0.6733, Recall: 0.6682, F1: 0.6692
LM Predictions:  [5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 0, 2, 0, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2519, Accuracy: 0.6429, Precision: 0.7361, Recall: 0.5488, F1: 0.6259
Epoch 23/70
Train Loss: 0.0781, Accuracy: 0.9783, Precision: 0.9461, Recall: 0.9405, F1: 0.9429
Validation Loss: 0.8900, Accuracy: 0.8466, Precision: 0.7036, Recall: 0.6147, F1: 0.6385
Testing Loss: 0.8925, Accuracy: 0.8271, Precision: 0.6510, Recall: 0.6366, F1: 0.6316
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 3, 3, 2, 0, 2, 0, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0797, Accuracy: 0.6786, Precision: 0.6889, Recall: 0.5726, F1: 0.6236
Epoch 24/70
Train Loss: 0.0665, Accuracy: 0.9818, Precision: 0.9611, Recall: 0.9602, F1: 0.9605
Validation Loss: 0.8068, Accuracy: 0.8466, Precision: 0.7045, Recall: 0.6347, F1: 0.6573
Testing Loss: 0.9225, Accuracy: 0.8218, Precision: 0.6374, Recall: 0.6341, F1: 0.6238
LM Predictions:  [5, 3, 5, 2, 1, 4, 4, 5, 3, 3, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9397, Accuracy: 0.6786, Precision: 0.6885, Recall: 0.5726, F1: 0.6198
Epoch 25/70
Train Loss: 0.0895, Accuracy: 0.9748, Precision: 0.9408, Recall: 0.9320, F1: 0.9361
Validation Loss: 0.8568, Accuracy: 0.8352, Precision: 0.6639, Recall: 0.6644, F1: 0.6533
Testing Loss: 0.8412, Accuracy: 0.8298, Precision: 0.6538, Recall: 0.6734, F1: 0.6562
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 0, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8976, Accuracy: 0.7143, Precision: 0.6972, Recall: 0.5964, F1: 0.6402
Epoch 26/70
Train Loss: 0.0766, Accuracy: 0.9808, Precision: 0.9498, Recall: 0.9551, F1: 0.9521
Validation Loss: 0.7807, Accuracy: 0.8523, Precision: 0.6452, Recall: 0.6193, F1: 0.6175
Testing Loss: 0.7987, Accuracy: 0.8617, Precision: 0.7194, Recall: 0.6747, F1: 0.6878
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 4, 3, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1570, Accuracy: 0.7500, Precision: 0.7963, Recall: 0.6024, F1: 0.6690
Epoch 27/70
Train Loss: 0.0671, Accuracy: 0.9797, Precision: 0.9421, Recall: 0.9434, F1: 0.9425
Validation Loss: 0.7638, Accuracy: 0.8466, Precision: 0.6457, Recall: 0.6447, F1: 0.6447
Testing Loss: 0.8924, Accuracy: 0.8457, Precision: 0.6242, Recall: 0.6428, F1: 0.6303
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 2, 3, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8378, Accuracy: 0.7143, Precision: 0.7278, Recall: 0.5690, F1: 0.6214
Epoch 28/70
Train Loss: 0.0573, Accuracy: 0.9867, Precision: 0.9657, Recall: 0.9631, F1: 0.9643
Validation Loss: 0.8313, Accuracy: 0.8352, Precision: 0.6368, Recall: 0.6578, F1: 0.6457
Testing Loss: 0.8695, Accuracy: 0.8564, Precision: 0.6453, Recall: 0.6618, F1: 0.6526
LM Predictions:  [5, 3, 5, 3, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 3, 1, 5, 5, 2, 3, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0077, Accuracy: 0.6786, Precision: 0.7778, Recall: 0.5536, F1: 0.6270
Epoch 29/70
Train Loss: 0.0734, Accuracy: 0.9815, Precision: 0.9512, Recall: 0.9446, F1: 0.9477
Validation Loss: 0.8872, Accuracy: 0.8210, Precision: 0.6573, Recall: 0.6219, F1: 0.6377
Testing Loss: 0.7143, Accuracy: 0.8617, Precision: 0.6777, Recall: 0.6874, F1: 0.6814
LM Predictions:  [5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 0, 0, 2, 0, 4, 5, 3, 3, 3, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3499, Accuracy: 0.5714, Precision: 0.7333, Recall: 0.4738, F1: 0.5684
Epoch 30/70
Train Loss: 0.0774, Accuracy: 0.9776, Precision: 0.9358, Recall: 0.9381, F1: 0.9367
Validation Loss: 0.8040, Accuracy: 0.8494, Precision: 0.6892, Recall: 0.7240, F1: 0.6966
Testing Loss: 0.8416, Accuracy: 0.8511, Precision: 0.6958, Recall: 0.7136, F1: 0.7019
LM Predictions:  [2, 3, 3, 3, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8864, Accuracy: 0.7500, Precision: 0.8333, Recall: 0.6107, F1: 0.6939
Epoch 31/70
Train Loss: 0.0784, Accuracy: 0.9811, Precision: 0.9561, Recall: 0.9615, F1: 0.9587
Validation Loss: 1.1945, Accuracy: 0.8153, Precision: 0.6997, Recall: 0.5498, F1: 0.5957
Testing Loss: 1.1505, Accuracy: 0.8059, Precision: 0.6973, Recall: 0.5761, F1: 0.6133
LM Predictions:  [2, 5, 5, 2, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 4, 5, 5, 5, 2, 5, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1264, Accuracy: 0.6786, Precision: 0.7037, Recall: 0.5452, F1: 0.5837
Epoch 32/70
Train Loss: 0.0706, Accuracy: 0.9773, Precision: 0.9427, Recall: 0.9451, F1: 0.9438
Validation Loss: 0.9047, Accuracy: 0.8466, Precision: 0.6889, Recall: 0.6240, F1: 0.6483
Testing Loss: 0.9052, Accuracy: 0.8404, Precision: 0.6652, Recall: 0.6529, F1: 0.6527
LM Predictions:  [5, 3, 3, 1, 1, 4, 4, 5, 5, 3, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0420, Accuracy: 0.6786, Precision: 0.6857, Recall: 0.5726, F1: 0.6144
Epoch 33/70
Train Loss: 0.0898, Accuracy: 0.9790, Precision: 0.9665, Recall: 0.9654, F1: 0.9659
Validation Loss: 0.9973, Accuracy: 0.7983, Precision: 0.6382, Recall: 0.6109, F1: 0.6192
Testing Loss: 0.9070, Accuracy: 0.8218, Precision: 0.6183, Recall: 0.6228, F1: 0.6135
LM Predictions:  [2, 3, 5, 5, 1, 4, 2, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 2, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8923, Accuracy: 0.7500, Precision: 0.7292, Recall: 0.6012, F1: 0.6341
Epoch 34/70
Train Loss: 0.0783, Accuracy: 0.9787, Precision: 0.9584, Recall: 0.9436, F1: 0.9506
Validation Loss: 0.8117, Accuracy: 0.8381, Precision: 0.6763, Recall: 0.6246, F1: 0.6421
Testing Loss: 0.7561, Accuracy: 0.8457, Precision: 0.6740, Recall: 0.6578, F1: 0.6536
LM Predictions:  [5, 0, 5, 5, 1, 4, 4, 5, 5, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8294, Accuracy: 0.7857, Precision: 0.7417, Recall: 0.6536, F1: 0.6798
Epoch 35/70
Train Loss: 0.0471, Accuracy: 0.9885, Precision: 0.9655, Recall: 0.9777, F1: 0.9715
Validation Loss: 0.8702, Accuracy: 0.8523, Precision: 0.6819, Recall: 0.6536, F1: 0.6641
Testing Loss: 0.8944, Accuracy: 0.8378, Precision: 0.6345, Recall: 0.6342, F1: 0.6306
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6015, Accuracy: 0.8214, Precision: 0.7546, Recall: 0.6679, F1: 0.6978
Epoch 36/70
Train Loss: 0.0301, Accuracy: 0.9934, Precision: 0.9801, Recall: 0.9744, F1: 0.9772
Validation Loss: 0.9637, Accuracy: 0.8466, Precision: 0.6789, Recall: 0.6138, F1: 0.6351
Testing Loss: 0.9958, Accuracy: 0.8245, Precision: 0.6215, Recall: 0.6079, F1: 0.6068
LM Predictions:  [2, 5, 2, 1, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7383, Accuracy: 0.7857, Precision: 0.7181, Recall: 0.6440, F1: 0.6680
Epoch 37/70
Train Loss: 0.0310, Accuracy: 0.9892, Precision: 0.9723, Recall: 0.9740, F1: 0.9732
Validation Loss: 0.9133, Accuracy: 0.8608, Precision: 0.7279, Recall: 0.6083, F1: 0.6350
Testing Loss: 1.0379, Accuracy: 0.8324, Precision: 0.6299, Recall: 0.5988, F1: 0.6049
LM Predictions:  [2, 5, 2, 1, 1, 4, 4, 5, 2, 5, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5038, Accuracy: 0.7857, Precision: 0.6982, Recall: 0.6440, F1: 0.6589
Epoch 38/70
Train Loss: 0.0657, Accuracy: 0.9797, Precision: 0.9535, Recall: 0.9326, F1: 0.9421
Validation Loss: 0.9775, Accuracy: 0.8409, Precision: 0.6536, Recall: 0.6541, F1: 0.6510
Testing Loss: 1.0791, Accuracy: 0.8378, Precision: 0.6254, Recall: 0.6572, F1: 0.6321
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 3, 2, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0131, Accuracy: 0.7500, Precision: 0.7679, Recall: 0.6107, F1: 0.6651
Epoch 39/70
Train Loss: 0.0484, Accuracy: 0.9878, Precision: 0.9710, Recall: 0.9630, F1: 0.9668
Validation Loss: 0.9453, Accuracy: 0.8466, Precision: 0.7347, Recall: 0.6550, F1: 0.6608
Testing Loss: 0.9603, Accuracy: 0.8324, Precision: 0.6588, Recall: 0.6232, F1: 0.6264
LM Predictions:  [2, 0, 0, 0, 0, 4, 4, 5, 2, 5, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7079, Accuracy: 0.7857, Precision: 0.8429, Recall: 0.7743, F1: 0.7839
Epoch 40/70
Train Loss: 0.0291, Accuracy: 0.9930, Precision: 0.9786, Recall: 0.9819, F1: 0.9802
Validation Loss: 1.0729, Accuracy: 0.8494, Precision: 0.6539, Recall: 0.6382, F1: 0.6453
Testing Loss: 1.0572, Accuracy: 0.8351, Precision: 0.6239, Recall: 0.6321, F1: 0.6211
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5411, Accuracy: 0.7857, Precision: 0.6944, Recall: 0.6440, F1: 0.6631
Epoch 41/70
Train Loss: 0.0475, Accuracy: 0.9878, Precision: 0.9674, Recall: 0.9690, F1: 0.9681
Validation Loss: 1.0163, Accuracy: 0.8580, Precision: 0.6805, Recall: 0.6406, F1: 0.6569
Testing Loss: 1.0430, Accuracy: 0.8431, Precision: 0.6555, Recall: 0.6412, F1: 0.6473
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4461, Accuracy: 0.8571, Precision: 0.7963, Recall: 0.6917, F1: 0.7285
Epoch 42/70
Train Loss: 0.0430, Accuracy: 0.9899, Precision: 0.9678, Recall: 0.9740, F1: 0.9708
Validation Loss: 0.9102, Accuracy: 0.8466, Precision: 0.6656, Recall: 0.6455, F1: 0.6547
Testing Loss: 0.9064, Accuracy: 0.8590, Precision: 0.6557, Recall: 0.6736, F1: 0.6629
LM Predictions:  [2, 5, 5, 0, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5423, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6440, F1: 0.6810
Epoch 43/70
Train Loss: 0.0259, Accuracy: 0.9934, Precision: 0.9794, Recall: 0.9725, F1: 0.9759
Validation Loss: 1.0427, Accuracy: 0.8409, Precision: 0.6750, Recall: 0.6173, F1: 0.6353
Testing Loss: 1.0636, Accuracy: 0.8404, Precision: 0.6470, Recall: 0.6198, F1: 0.6105
LM Predictions:  [2, 5, 5, 0, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3819, Accuracy: 0.7857, Precision: 0.7250, Recall: 0.6440, F1: 0.6736
Epoch 44/70
Train Loss: 0.0336, Accuracy: 0.9906, Precision: 0.9851, Recall: 0.9700, F1: 0.9773
Validation Loss: 1.0364, Accuracy: 0.8352, Precision: 0.6681, Recall: 0.6183, F1: 0.6355
Testing Loss: 1.0648, Accuracy: 0.8378, Precision: 0.6442, Recall: 0.6272, F1: 0.6270
LM Predictions:  [5, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3756, Accuracy: 0.7857, Precision: 0.7639, Recall: 0.6440, F1: 0.6777
Epoch 45/70
Train Loss: 0.0862, Accuracy: 0.9773, Precision: 0.9279, Recall: 0.9140, F1: 0.9206
Validation Loss: 0.9440, Accuracy: 0.8153, Precision: 0.6742, Recall: 0.6403, F1: 0.6426
Testing Loss: 0.9283, Accuracy: 0.8298, Precision: 0.6622, Recall: 0.6374, F1: 0.6392
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6980, Accuracy: 0.8214, Precision: 0.7727, Recall: 0.6679, F1: 0.6995
Epoch 46/70
Train Loss: 0.0501, Accuracy: 0.9857, Precision: 0.9569, Recall: 0.9600, F1: 0.9585
Validation Loss: 0.9533, Accuracy: 0.8324, Precision: 0.6369, Recall: 0.6173, F1: 0.6197
Testing Loss: 0.9818, Accuracy: 0.8431, Precision: 0.6190, Recall: 0.6207, F1: 0.6097
LM Predictions:  [5, 5, 1, 5, 1, 4, 4, 5, 5, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6270, Accuracy: 0.7857, Precision: 0.8543, Recall: 0.7943, F1: 0.7834
Epoch 47/70
Train Loss: 0.0365, Accuracy: 0.9899, Precision: 0.9662, Recall: 0.9679, F1: 0.9670
Validation Loss: 1.0541, Accuracy: 0.8352, Precision: 0.6503, Recall: 0.6076, F1: 0.6212
Testing Loss: 1.2280, Accuracy: 0.8271, Precision: 0.6051, Recall: 0.5990, F1: 0.5963
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3881, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7250, F1: 0.7517
Epoch 48/70
Train Loss: 0.0350, Accuracy: 0.9909, Precision: 0.9779, Recall: 0.9716, F1: 0.9747
Validation Loss: 0.9695, Accuracy: 0.8352, Precision: 0.6750, Recall: 0.6193, F1: 0.6321
Testing Loss: 1.0025, Accuracy: 0.8351, Precision: 0.6526, Recall: 0.6261, F1: 0.6240
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3299, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9100, F1: 0.9144
Epoch 49/70
Train Loss: 0.0287, Accuracy: 0.9920, Precision: 0.9787, Recall: 0.9767, F1: 0.9776
Validation Loss: 0.9107, Accuracy: 0.8523, Precision: 0.7072, Recall: 0.6950, F1: 0.6818
Testing Loss: 0.9931, Accuracy: 0.8324, Precision: 0.6131, Recall: 0.6243, F1: 0.6086
LM Predictions:  [2, 5, 1, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5556, Accuracy: 0.8214, Precision: 0.7296, Recall: 0.6679, F1: 0.6839
Epoch 50/70
Train Loss: 0.0169, Accuracy: 0.9955, Precision: 0.9852, Recall: 0.9849, F1: 0.9850
Validation Loss: 1.1679, Accuracy: 0.8438, Precision: 0.6695, Recall: 0.6306, F1: 0.6430
Testing Loss: 1.2397, Accuracy: 0.8271, Precision: 0.6180, Recall: 0.6215, F1: 0.6088
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0882, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 51/70
Train Loss: 0.0645, Accuracy: 0.9850, Precision: 0.9699, Recall: 0.9590, F1: 0.9644
Validation Loss: 0.8342, Accuracy: 0.8523, Precision: 0.6689, Recall: 0.6543, F1: 0.6597
Testing Loss: 0.9316, Accuracy: 0.8511, Precision: 0.6315, Recall: 0.6367, F1: 0.6290
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2000, Accuracy: 0.8571, Precision: 0.8900, Recall: 0.8300, F1: 0.8425
Epoch 52/70
Train Loss: 0.0277, Accuracy: 0.9937, Precision: 0.9842, Recall: 0.9841, F1: 0.9842
Validation Loss: 1.1560, Accuracy: 0.8466, Precision: 0.6642, Recall: 0.6487, F1: 0.6550
Testing Loss: 1.1473, Accuracy: 0.8404, Precision: 0.6423, Recall: 0.6359, F1: 0.6350
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1377, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9200, F1: 0.9200
Epoch 53/70
Train Loss: 0.0400, Accuracy: 0.9920, Precision: 0.9800, Recall: 0.9775, F1: 0.9787
Validation Loss: 1.0238, Accuracy: 0.8267, Precision: 0.6397, Recall: 0.6256, F1: 0.6274
Testing Loss: 1.1169, Accuracy: 0.8245, Precision: 0.6095, Recall: 0.5900, F1: 0.5906
LM Predictions:  [5, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1811, Accuracy: 0.8929, Precision: 0.9400, Recall: 0.8914, F1: 0.8993
Epoch 54/70
Train Loss: 0.0389, Accuracy: 0.9916, Precision: 0.9753, Recall: 0.9787, F1: 0.9767
Validation Loss: 1.0838, Accuracy: 0.8494, Precision: 0.7120, Recall: 0.6078, F1: 0.6386
Testing Loss: 1.2048, Accuracy: 0.8218, Precision: 0.6279, Recall: 0.5886, F1: 0.5851
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 4, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2680, Accuracy: 0.8929, Precision: 0.7792, Recall: 0.7250, F1: 0.7466
Epoch 55/70
Train Loss: 0.0393, Accuracy: 0.9920, Precision: 0.9766, Recall: 0.9841, F1: 0.9802
Validation Loss: 1.0762, Accuracy: 0.8409, Precision: 0.6659, Recall: 0.6149, F1: 0.6246
Testing Loss: 1.1382, Accuracy: 0.8404, Precision: 0.6116, Recall: 0.6187, F1: 0.5973
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 4, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3122, Accuracy: 0.8929, Precision: 0.9222, Recall: 0.8700, F1: 0.8782
Epoch 56/70
Train Loss: 0.0255, Accuracy: 0.9937, Precision: 0.9866, Recall: 0.9787, F1: 0.9826
Validation Loss: 0.8655, Accuracy: 0.8636, Precision: 0.6986, Recall: 0.6149, F1: 0.6417
Testing Loss: 0.9379, Accuracy: 0.8537, Precision: 0.6897, Recall: 0.6120, F1: 0.6357
LM Predictions:  [3, 5, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 3, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7475, Accuracy: 0.8214, Precision: 0.7963, Recall: 0.6595, F1: 0.7025
Epoch 57/70
Train Loss: 0.0393, Accuracy: 0.9902, Precision: 0.9857, Recall: 0.9651, F1: 0.9749
Validation Loss: 0.9148, Accuracy: 0.8523, Precision: 0.6657, Recall: 0.6495, F1: 0.6568
Testing Loss: 0.9617, Accuracy: 0.8484, Precision: 0.6211, Recall: 0.6367, F1: 0.6273
LM Predictions:  [3, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5647, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7429, F1: 0.7580
Epoch 58/70
Train Loss: 0.0306, Accuracy: 0.9941, Precision: 0.9913, Recall: 0.9830, F1: 0.9870
Validation Loss: 0.9281, Accuracy: 0.8551, Precision: 0.6665, Recall: 0.6176, F1: 0.6355
Testing Loss: 0.9368, Accuracy: 0.8457, Precision: 0.6340, Recall: 0.6235, F1: 0.6237
LM Predictions:  [3, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3350, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7429, F1: 0.7580
Epoch 59/70
Train Loss: 0.0237, Accuracy: 0.9944, Precision: 0.9884, Recall: 0.9765, F1: 0.9822
Validation Loss: 1.0563, Accuracy: 0.8381, Precision: 0.6422, Recall: 0.6370, F1: 0.6388
Testing Loss: 1.1578, Accuracy: 0.8324, Precision: 0.6259, Recall: 0.6289, F1: 0.6260
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1076, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 60/70
Train Loss: 0.0305, Accuracy: 0.9944, Precision: 0.9807, Recall: 0.9835, F1: 0.9820
Validation Loss: 0.8898, Accuracy: 0.8523, Precision: 0.6651, Recall: 0.6474, F1: 0.6463
Testing Loss: 0.9567, Accuracy: 0.8511, Precision: 0.6420, Recall: 0.6519, F1: 0.6458
LM Predictions:  [3, 1, 2, 5, 1, 4, 4, 5, 2, 1, 1, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4749, Accuracy: 0.8571, Precision: 0.7778, Recall: 0.7190, F1: 0.7327
Epoch 61/70
Train Loss: 0.0738, Accuracy: 0.9850, Precision: 0.9806, Recall: 0.9647, F1: 0.9723
Validation Loss: 1.0637, Accuracy: 0.8494, Precision: 0.7232, Recall: 0.6233, F1: 0.6620
Testing Loss: 0.9644, Accuracy: 0.8324, Precision: 0.6285, Recall: 0.6194, F1: 0.6234
LM Predictions:  [3, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6892, Accuracy: 0.8571, Precision: 0.7630, Recall: 0.7095, F1: 0.7210
Epoch 62/70
Train Loss: 0.0505, Accuracy: 0.9892, Precision: 0.9766, Recall: 0.9758, F1: 0.9761
Validation Loss: 0.8570, Accuracy: 0.8494, Precision: 0.7092, Recall: 0.6436, F1: 0.6625
Testing Loss: 1.0909, Accuracy: 0.8085, Precision: 0.5971, Recall: 0.5850, F1: 0.5816
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1730, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9200, F1: 0.9200
Epoch 63/70
Train Loss: 0.0323, Accuracy: 0.9927, Precision: 0.9869, Recall: 0.9835, F1: 0.9851
Validation Loss: 0.9665, Accuracy: 0.8267, Precision: 0.6275, Recall: 0.5919, F1: 0.6074
Testing Loss: 1.0979, Accuracy: 0.8138, Precision: 0.5974, Recall: 0.5858, F1: 0.5873
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2871, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7333, F1: 0.7620
Epoch 64/70
Train Loss: 0.0217, Accuracy: 0.9955, Precision: 0.9897, Recall: 0.9861, F1: 0.9879
Validation Loss: 0.9132, Accuracy: 0.8466, Precision: 0.6846, Recall: 0.6279, F1: 0.6454
Testing Loss: 1.1081, Accuracy: 0.8298, Precision: 0.6154, Recall: 0.6141, F1: 0.6055
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0772, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 65/70
Train Loss: 0.0179, Accuracy: 0.9962, Precision: 0.9928, Recall: 0.9882, F1: 0.9905
Validation Loss: 1.1163, Accuracy: 0.8381, Precision: 0.6811, Recall: 0.6059, F1: 0.6338
Testing Loss: 1.2828, Accuracy: 0.8165, Precision: 0.6183, Recall: 0.5924, F1: 0.5948
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1029, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 66/70
Train Loss: 0.0830, Accuracy: 0.9832, Precision: 0.9720, Recall: 0.9460, F1: 0.9559
Validation Loss: 1.1262, Accuracy: 0.7727, Precision: 0.6474, Recall: 0.6763, F1: 0.6017
Testing Loss: 1.2533, Accuracy: 0.7553, Precision: 0.6082, Recall: 0.6058, F1: 0.5544
LM Predictions:  [2, 0, 3, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6587, Accuracy: 0.8571, Precision: 0.7444, Recall: 0.7190, F1: 0.7225
Epoch 67/70
Train Loss: 0.0529, Accuracy: 0.9871, Precision: 0.9627, Recall: 0.9641, F1: 0.9634
Validation Loss: 1.0360, Accuracy: 0.8324, Precision: 0.6645, Recall: 0.5905, F1: 0.6151
Testing Loss: 0.9981, Accuracy: 0.8298, Precision: 0.6220, Recall: 0.5767, F1: 0.5912
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3470, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9200, F1: 0.9144
Epoch 68/70
Train Loss: 0.0389, Accuracy: 0.9927, Precision: 0.9805, Recall: 0.9781, F1: 0.9793
Validation Loss: 1.0284, Accuracy: 0.8352, Precision: 0.6692, Recall: 0.6198, F1: 0.6397
Testing Loss: 1.0009, Accuracy: 0.8245, Precision: 0.6083, Recall: 0.5796, F1: 0.5861
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2113, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 69/70
Train Loss: 0.0250, Accuracy: 0.9937, Precision: 0.9822, Recall: 0.9829, F1: 0.9825
Validation Loss: 1.0953, Accuracy: 0.8324, Precision: 0.6423, Recall: 0.6274, F1: 0.6336
Testing Loss: 1.0148, Accuracy: 0.8457, Precision: 0.6290, Recall: 0.6207, F1: 0.6209
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1961, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 70/70
Train Loss: 0.0265, Accuracy: 0.9941, Precision: 0.9868, Recall: 0.9832, F1: 0.9850
Validation Loss: 1.0667, Accuracy: 0.8295, Precision: 0.6309, Recall: 0.6551, F1: 0.6402
Testing Loss: 1.2686, Accuracy: 0.8085, Precision: 0.5942, Recall: 0.6134, F1: 0.5928
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 4, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5467, Accuracy: 0.8929, Precision: 0.8767, Recall: 0.8814, F1: 0.8764
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.0907, Accuracy: 0.5875, Precision: 0.2866, Recall: 0.2913, F1: 0.2790
Validation Loss: 0.7441, Accuracy: 0.7585, Precision: 0.3651, Recall: 0.4192, F1: 0.3891
Testing Loss: 0.6422, Accuracy: 0.7872, Precision: 0.3756, Recall: 0.4388, F1: 0.4027
LM Predictions:  [5, 2, 2, 2, 5, 2, 5, 2, 5, 2, 5, 5, 2, 5, 2, 5, 5, 5, 2, 5, 5, 5, 5, 2, 2, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9871, Accuracy: 0.3214, Precision: 0.1282, Recall: 0.2571, F1: 0.1709
Epoch 2/70
Train Loss: 0.6100, Accuracy: 0.8041, Precision: 0.5381, Recall: 0.5110, F1: 0.5049
Validation Loss: 0.6428, Accuracy: 0.7983, Precision: 0.4861, Recall: 0.5247, F1: 0.5027
Testing Loss: 0.5665, Accuracy: 0.8457, Precision: 0.6034, Recall: 0.5681, F1: 0.5787
LM Predictions:  [3, 3, 3, 2, 2, 2, 5, 5, 5, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 2, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5306, Accuracy: 0.1071, Precision: 0.1083, Recall: 0.0714, F1: 0.0859
Epoch 3/70
Train Loss: 0.4767, Accuracy: 0.8488, Precision: 0.5804, Recall: 0.5981, F1: 0.5830
Validation Loss: 0.4748, Accuracy: 0.8580, Precision: 0.6533, Recall: 0.6318, F1: 0.6320
Testing Loss: 0.5118, Accuracy: 0.8564, Precision: 0.6258, Recall: 0.6400, F1: 0.6276
LM Predictions:  [3, 3, 3, 5, 5, 5, 5, 5, 5, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 1, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2672, Accuracy: 0.1429, Precision: 0.3056, Recall: 0.1131, F1: 0.1485
Epoch 4/70
Train Loss: 0.3612, Accuracy: 0.8905, Precision: 0.6517, Recall: 0.6735, F1: 0.6612
Validation Loss: 0.5415, Accuracy: 0.8381, Precision: 0.6360, Recall: 0.6185, F1: 0.6233
Testing Loss: 0.5621, Accuracy: 0.8484, Precision: 0.6183, Recall: 0.6502, F1: 0.6191
LM Predictions:  [3, 3, 3, 2, 1, 2, 4, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4792, Accuracy: 0.1429, Precision: 0.3333, Recall: 0.1405, F1: 0.1759
Epoch 5/70
Train Loss: 0.3211, Accuracy: 0.9017, Precision: 0.6627, Recall: 0.6926, F1: 0.6759
Validation Loss: 0.4463, Accuracy: 0.8722, Precision: 0.6972, Recall: 0.6822, F1: 0.6865
Testing Loss: 0.4700, Accuracy: 0.8590, Precision: 0.6384, Recall: 0.6544, F1: 0.6445
LM Predictions:  [3, 3, 3, 2, 5, 2, 4, 5, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 2, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3236, Accuracy: 0.1429, Precision: 0.3333, Recall: 0.1048, F1: 0.1532
Epoch 6/70
Train Loss: 0.2610, Accuracy: 0.9227, Precision: 0.7999, Recall: 0.7902, F1: 0.7884
Validation Loss: 0.5096, Accuracy: 0.8352, Precision: 0.6503, Recall: 0.6827, F1: 0.6544
Testing Loss: 0.4586, Accuracy: 0.8617, Precision: 0.6423, Recall: 0.6621, F1: 0.6445
LM Predictions:  [3, 3, 3, 5, 1, 4, 4, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0816, Accuracy: 0.1786, Precision: 0.4167, Recall: 0.1560, F1: 0.2249
Epoch 7/70
Train Loss: 0.2340, Accuracy: 0.9300, Precision: 0.7957, Recall: 0.8024, F1: 0.7947
Validation Loss: 0.4651, Accuracy: 0.8693, Precision: 0.7280, Recall: 0.7238, F1: 0.7218
Testing Loss: 0.4910, Accuracy: 0.8617, Precision: 0.6458, Recall: 0.6395, F1: 0.6388
LM Predictions:  [3, 3, 3, 5, 1, 3, 4, 5, 5, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 5, 2, 3, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3897, Accuracy: 0.3214, Precision: 0.4861, Recall: 0.2595, F1: 0.3186
Epoch 8/70
Train Loss: 0.1614, Accuracy: 0.9538, Precision: 0.8692, Recall: 0.8782, F1: 0.8719
Validation Loss: 0.5780, Accuracy: 0.8523, Precision: 0.6899, Recall: 0.7034, F1: 0.6838
Testing Loss: 0.5142, Accuracy: 0.8723, Precision: 0.6743, Recall: 0.6855, F1: 0.6786
LM Predictions:  [3, 3, 3, 5, 1, 3, 4, 5, 2, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 5, 2, 3, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1539, Accuracy: 0.3929, Precision: 0.7222, Recall: 0.3167, F1: 0.4089
Epoch 9/70
Train Loss: 0.1593, Accuracy: 0.9538, Precision: 0.8701, Recall: 0.8730, F1: 0.8703
Validation Loss: 0.4706, Accuracy: 0.8778, Precision: 0.6815, Recall: 0.6504, F1: 0.6626
Testing Loss: 0.5958, Accuracy: 0.8697, Precision: 0.6396, Recall: 0.6604, F1: 0.6363
LM Predictions:  [3, 3, 3, 4, 1, 4, 4, 5, 5, 3, 3, 0, 2, 3, 3, 3, 3, 3, 1, 5, 2, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6957, Accuracy: 0.4286, Precision: 0.6250, Recall: 0.3679, F1: 0.4331
Epoch 10/70
Train Loss: 0.1409, Accuracy: 0.9633, Precision: 0.9001, Recall: 0.9105, F1: 0.9043
Validation Loss: 0.4609, Accuracy: 0.8722, Precision: 0.7471, Recall: 0.7385, F1: 0.7249
Testing Loss: 0.5789, Accuracy: 0.8537, Precision: 0.6395, Recall: 0.6699, F1: 0.6477
LM Predictions:  [3, 3, 3, 0, 1, 4, 4, 5, 5, 3, 3, 0, 2, 3, 3, 4, 3, 4, 1, 5, 2, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4219, Accuracy: 0.5000, Precision: 0.6056, Recall: 0.4345, F1: 0.4972
Epoch 11/70
Train Loss: 0.1054, Accuracy: 0.9689, Precision: 0.9045, Recall: 0.9124, F1: 0.9078
Validation Loss: 0.5030, Accuracy: 0.8693, Precision: 0.6893, Recall: 0.6543, F1: 0.6677
Testing Loss: 0.6412, Accuracy: 0.8564, Precision: 0.6417, Recall: 0.6329, F1: 0.6274
LM Predictions:  [2, 3, 3, 0, 1, 4, 4, 5, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 1, 5, 2, 2, 1, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7898, Accuracy: 0.4286, Precision: 0.5069, Recall: 0.3583, F1: 0.3980
Epoch 12/70
Train Loss: 0.1214, Accuracy: 0.9685, Precision: 0.9060, Recall: 0.9034, F1: 0.9041
Validation Loss: 0.6143, Accuracy: 0.8665, Precision: 0.7026, Recall: 0.6432, F1: 0.6665
Testing Loss: 0.6003, Accuracy: 0.8750, Precision: 0.6650, Recall: 0.6465, F1: 0.6488
LM Predictions:  [2, 4, 2, 3, 1, 2, 4, 5, 2, 4, 4, 0, 2, 5, 4, 3, 3, 4, 1, 5, 2, 2, 2, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3558, Accuracy: 0.5714, Precision: 0.6477, Recall: 0.4548, F1: 0.4781
Epoch 13/70
Train Loss: 0.0921, Accuracy: 0.9731, Precision: 0.9177, Recall: 0.9265, F1: 0.9218
Validation Loss: 0.6242, Accuracy: 0.8636, Precision: 0.6937, Recall: 0.6305, F1: 0.6557
Testing Loss: 0.7388, Accuracy: 0.8617, Precision: 0.6380, Recall: 0.6354, F1: 0.6342
LM Predictions:  [2, 2, 2, 5, 1, 4, 4, 5, 5, 3, 2, 0, 2, 5, 2, 5, 3, 4, 1, 5, 5, 5, 2, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1587, Accuracy: 0.6429, Precision: 0.7037, Recall: 0.5202, F1: 0.5526
Epoch 14/70
Train Loss: 0.0789, Accuracy: 0.9801, Precision: 0.9466, Recall: 0.9510, F1: 0.9486
Validation Loss: 0.5733, Accuracy: 0.8778, Precision: 0.7405, Recall: 0.6925, F1: 0.7068
Testing Loss: 0.7780, Accuracy: 0.8484, Precision: 0.6075, Recall: 0.6186, F1: 0.6007
LM Predictions:  [5, 4, 2, 5, 1, 4, 4, 5, 2, 3, 4, 0, 2, 5, 4, 4, 4, 4, 1, 5, 5, 5, 1, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3439, Accuracy: 0.6786, Precision: 0.6667, Recall: 0.5631, F1: 0.5588
Epoch 15/70
Train Loss: 0.0880, Accuracy: 0.9811, Precision: 0.9634, Recall: 0.9531, F1: 0.9580
Validation Loss: 0.6096, Accuracy: 0.8778, Precision: 0.7628, Recall: 0.7231, F1: 0.7328
Testing Loss: 0.6719, Accuracy: 0.8590, Precision: 0.6608, Recall: 0.6317, F1: 0.6392
LM Predictions:  [2, 4, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 4, 4, 5, 3, 4, 1, 5, 2, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9665, Accuracy: 0.7143, Precision: 0.7569, Recall: 0.5869, F1: 0.6361
Epoch 16/70
Train Loss: 0.0615, Accuracy: 0.9839, Precision: 0.9579, Recall: 0.9576, F1: 0.9576
Validation Loss: 0.5781, Accuracy: 0.8864, Precision: 0.7047, Recall: 0.6894, F1: 0.6965
Testing Loss: 0.7299, Accuracy: 0.8590, Precision: 0.6525, Recall: 0.6714, F1: 0.6596
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 5, 4, 1, 5, 5, 2, 0, 0, 1, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7339, Accuracy: 0.7857, Precision: 0.6991, Recall: 0.6345, F1: 0.6562
Epoch 17/70
Train Loss: 0.0543, Accuracy: 0.9871, Precision: 0.9604, Recall: 0.9637, F1: 0.9620
Validation Loss: 0.6672, Accuracy: 0.8636, Precision: 0.6686, Recall: 0.6667, F1: 0.6664
Testing Loss: 0.6339, Accuracy: 0.8830, Precision: 0.6685, Recall: 0.6711, F1: 0.6693
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 3, 4, 4, 1, 5, 5, 2, 0, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6010, Accuracy: 0.8571, Precision: 0.8095, Recall: 0.7012, F1: 0.7440
Epoch 18/70
Train Loss: 0.0637, Accuracy: 0.9836, Precision: 0.9679, Recall: 0.9477, F1: 0.9567
Validation Loss: 0.9556, Accuracy: 0.8523, Precision: 0.7291, Recall: 0.6357, F1: 0.6684
Testing Loss: 0.9249, Accuracy: 0.8564, Precision: 0.6599, Recall: 0.6239, F1: 0.6302
LM Predictions:  [2, 4, 2, 4, 1, 4, 4, 5, 2, 3, 4, 2, 2, 5, 4, 3, 4, 4, 1, 5, 5, 2, 2, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8840, Accuracy: 0.7500, Precision: 0.7338, Recall: 0.6202, F1: 0.6333
Epoch 19/70
Train Loss: 0.0720, Accuracy: 0.9808, Precision: 0.9574, Recall: 0.9503, F1: 0.9536
Validation Loss: 0.7430, Accuracy: 0.8693, Precision: 0.7141, Recall: 0.6441, F1: 0.6644
Testing Loss: 0.7390, Accuracy: 0.8564, Precision: 0.6918, Recall: 0.6256, F1: 0.6497
LM Predictions:  [2, 4, 2, 3, 1, 4, 4, 5, 2, 3, 4, 0, 2, 2, 4, 5, 3, 4, 1, 5, 2, 2, 3, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8150, Accuracy: 0.6429, Precision: 0.6741, Recall: 0.5298, F1: 0.5488
Epoch 20/70
Train Loss: 0.0591, Accuracy: 0.9857, Precision: 0.9553, Recall: 0.9492, F1: 0.9521
Validation Loss: 0.6014, Accuracy: 0.8835, Precision: 0.7195, Recall: 0.6759, F1: 0.6938
Testing Loss: 0.5932, Accuracy: 0.8670, Precision: 0.6744, Recall: 0.6575, F1: 0.6611
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 2, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5497, Accuracy: 0.8214, Precision: 0.7917, Recall: 0.6583, F1: 0.6974
Epoch 21/70
Train Loss: 0.0574, Accuracy: 0.9850, Precision: 0.9775, Recall: 0.9680, F1: 0.9726
Validation Loss: 0.7015, Accuracy: 0.8580, Precision: 0.6808, Recall: 0.6493, F1: 0.6623
Testing Loss: 0.5718, Accuracy: 0.8777, Precision: 0.6702, Recall: 0.6765, F1: 0.6668
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5792, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7250, F1: 0.7679
Epoch 22/70
Train Loss: 0.0503, Accuracy: 0.9899, Precision: 0.9764, Recall: 0.9787, F1: 0.9775
Validation Loss: 0.8475, Accuracy: 0.8636, Precision: 0.7304, Recall: 0.6249, F1: 0.6494
Testing Loss: 0.7072, Accuracy: 0.8803, Precision: 0.7401, Recall: 0.6326, F1: 0.6633
LM Predictions:  [2, 4, 2, 5, 1, 4, 4, 5, 2, 3, 4, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5203, Accuracy: 0.8571, Precision: 0.7857, Recall: 0.7012, F1: 0.7273
Epoch 23/70
Train Loss: 0.0550, Accuracy: 0.9853, Precision: 0.9725, Recall: 0.9689, F1: 0.9706
Validation Loss: 0.8236, Accuracy: 0.8551, Precision: 0.7193, Recall: 0.5952, F1: 0.6229
Testing Loss: 0.6689, Accuracy: 0.8830, Precision: 0.7234, Recall: 0.6768, F1: 0.6847
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1515, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 24/70
Train Loss: 0.0438, Accuracy: 0.9906, Precision: 0.9719, Recall: 0.9743, F1: 0.9731
Validation Loss: 0.8489, Accuracy: 0.8665, Precision: 0.7711, Recall: 0.6825, F1: 0.7099
Testing Loss: 0.7090, Accuracy: 0.8830, Precision: 0.7066, Recall: 0.6503, F1: 0.6695
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 2, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1321, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 25/70
Train Loss: 0.0207, Accuracy: 0.9955, Precision: 0.9940, Recall: 0.9864, F1: 0.9901
Validation Loss: 0.8040, Accuracy: 0.8636, Precision: 0.6999, Recall: 0.7130, F1: 0.7013
Testing Loss: 0.7243, Accuracy: 0.8723, Precision: 0.6632, Recall: 0.6851, F1: 0.6714
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 0, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3505, Accuracy: 0.8929, Precision: 0.8056, Recall: 0.7250, F1: 0.7527
Epoch 26/70
Train Loss: 0.0262, Accuracy: 0.9955, Precision: 0.9850, Recall: 0.9880, F1: 0.9864
Validation Loss: 0.8635, Accuracy: 0.8636, Precision: 0.7008, Recall: 0.6172, F1: 0.6429
Testing Loss: 0.7453, Accuracy: 0.8777, Precision: 0.6856, Recall: 0.6482, F1: 0.6605
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2553, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7583, F1: 0.7910
Epoch 27/70
Train Loss: 0.0758, Accuracy: 0.9808, Precision: 0.9679, Recall: 0.9600, F1: 0.9639
Validation Loss: 0.6684, Accuracy: 0.8778, Precision: 0.7935, Recall: 0.6986, F1: 0.7215
Testing Loss: 0.6909, Accuracy: 0.8750, Precision: 0.6845, Recall: 0.6244, F1: 0.6473
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 5, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2672, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7250, F1: 0.7614
Epoch 28/70
Train Loss: 0.0344, Accuracy: 0.9909, Precision: 0.9724, Recall: 0.9770, F1: 0.9746
Validation Loss: 0.6493, Accuracy: 0.8665, Precision: 0.7542, Recall: 0.6366, F1: 0.6750
Testing Loss: 0.7550, Accuracy: 0.8590, Precision: 0.6601, Recall: 0.6190, F1: 0.6309
LM Predictions:  [2, 4, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4499, Accuracy: 0.8929, Precision: 0.7847, Recall: 0.7250, F1: 0.7416
Epoch 29/70
Train Loss: 0.0223, Accuracy: 0.9944, Precision: 0.9883, Recall: 0.9815, F1: 0.9849
Validation Loss: 0.6255, Accuracy: 0.8920, Precision: 0.7944, Recall: 0.7603, F1: 0.7752
Testing Loss: 0.8042, Accuracy: 0.8723, Precision: 0.6644, Recall: 0.6613, F1: 0.6601
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0367, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0159, Accuracy: 0.9965, Precision: 0.9957, Recall: 0.9957, F1: 0.9957
Validation Loss: 0.7027, Accuracy: 0.8864, Precision: 0.7746, Recall: 0.7411, F1: 0.7545
Testing Loss: 0.8228, Accuracy: 0.8670, Precision: 0.6416, Recall: 0.6527, F1: 0.6424
LM Predictions:  [3, 4, 3, 5, 1, 4, 4, 5, 3, 3, 3, 0, 3, 3, 4, 5, 4, 4, 1, 3, 5, 2, 0, 3, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4246, Accuracy: 0.6429, Precision: 0.8056, Recall: 0.5679, F1: 0.6378
Epoch 31/70
Train Loss: 0.0330, Accuracy: 0.9920, Precision: 0.9786, Recall: 0.9824, F1: 0.9804
Validation Loss: 0.7277, Accuracy: 0.8864, Precision: 0.7901, Recall: 0.7269, F1: 0.7441
Testing Loss: 0.6975, Accuracy: 0.8883, Precision: 0.7034, Recall: 0.6814, F1: 0.6887
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 0, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1287, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9100, F1: 0.9181
Epoch 32/70
Train Loss: 0.0164, Accuracy: 0.9955, Precision: 0.9895, Recall: 0.9895, F1: 0.9895
Validation Loss: 0.7485, Accuracy: 0.8920, Precision: 0.7976, Recall: 0.7354, F1: 0.7532
Testing Loss: 0.7978, Accuracy: 0.8777, Precision: 0.6756, Recall: 0.6630, F1: 0.6634
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 1, 3, 0, 2, 5, 4, 3, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3891, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7524, F1: 0.7870
Epoch 33/70
Train Loss: 0.0573, Accuracy: 0.9867, Precision: 0.9664, Recall: 0.9699, F1: 0.9681
Validation Loss: 0.7609, Accuracy: 0.8665, Precision: 0.7259, Recall: 0.6475, F1: 0.6764
Testing Loss: 0.7338, Accuracy: 0.8723, Precision: 0.6561, Recall: 0.6404, F1: 0.6449
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1654, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 34/70
Train Loss: 0.0226, Accuracy: 0.9944, Precision: 0.9865, Recall: 0.9879, F1: 0.9872
Validation Loss: 0.7785, Accuracy: 0.8665, Precision: 0.7094, Recall: 0.6652, F1: 0.6835
Testing Loss: 0.7208, Accuracy: 0.8750, Precision: 0.6710, Recall: 0.6674, F1: 0.6643
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0604, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 35/70
Train Loss: 0.0341, Accuracy: 0.9916, Precision: 0.9756, Recall: 0.9744, F1: 0.9750
Validation Loss: 0.8241, Accuracy: 0.8381, Precision: 0.7144, Recall: 0.6252, F1: 0.6612
Testing Loss: 0.7294, Accuracy: 0.8644, Precision: 0.7148, Recall: 0.6891, F1: 0.6976
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 2, 4, 5, 4, 4, 1, 5, 2, 2, 0, 2, 1, 0, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4369, Accuracy: 0.8214, Precision: 0.7487, Recall: 0.6774, F1: 0.6994
Epoch 36/70
Train Loss: 0.0302, Accuracy: 0.9923, Precision: 0.9836, Recall: 0.9813, F1: 0.9824
Validation Loss: 0.8388, Accuracy: 0.8551, Precision: 0.7143, Recall: 0.6702, F1: 0.6831
Testing Loss: 0.7651, Accuracy: 0.8697, Precision: 0.6399, Recall: 0.6657, F1: 0.6466
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 5, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1250, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9714, F1: 0.9713
Epoch 37/70
Train Loss: 0.0193, Accuracy: 0.9951, Precision: 0.9933, Recall: 0.9935, F1: 0.9934
Validation Loss: 0.6898, Accuracy: 0.8864, Precision: 0.7368, Recall: 0.6934, F1: 0.7125
Testing Loss: 0.7617, Accuracy: 0.8777, Precision: 0.6670, Recall: 0.6794, F1: 0.6716
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 4, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0517, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9500, F1: 0.9532
Epoch 38/70
Train Loss: 0.0117, Accuracy: 0.9986, Precision: 0.9965, Recall: 0.9944, F1: 0.9954
Validation Loss: 0.8281, Accuracy: 0.8665, Precision: 0.7126, Recall: 0.6645, F1: 0.6825
Testing Loss: 0.9018, Accuracy: 0.8723, Precision: 0.6638, Recall: 0.6634, F1: 0.6493
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0120, Accuracy: 0.9986, Precision: 0.9990, Recall: 0.9981, F1: 0.9986
Validation Loss: 0.8577, Accuracy: 0.8608, Precision: 0.7292, Recall: 0.6263, F1: 0.6653
Testing Loss: 0.9158, Accuracy: 0.8723, Precision: 0.6926, Recall: 0.6297, F1: 0.6521
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0334, Accuracy: 0.9913, Precision: 0.9599, Recall: 0.9751, F1: 0.9672
Validation Loss: 0.7689, Accuracy: 0.8494, Precision: 0.6803, Recall: 0.6539, F1: 0.6605
Testing Loss: 0.8441, Accuracy: 0.8564, Precision: 0.6802, Recall: 0.6743, F1: 0.6655
LM Predictions:  [2, 5, 5, 5, 1, 4, 4, 5, 2, 1, 5, 5, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3305, Accuracy: 0.8214, Precision: 0.9167, Recall: 0.8114, F1: 0.7987
Epoch 41/70
Train Loss: 0.0482, Accuracy: 0.9871, Precision: 0.9650, Recall: 0.9554, F1: 0.9598
Validation Loss: 0.7935, Accuracy: 0.8693, Precision: 0.7332, Recall: 0.6531, F1: 0.6808
Testing Loss: 0.7803, Accuracy: 0.8803, Precision: 0.6715, Recall: 0.6642, F1: 0.6586
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0177, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0285, Accuracy: 0.9937, Precision: 0.9895, Recall: 0.9791, F1: 0.9841
Validation Loss: 0.7720, Accuracy: 0.8665, Precision: 0.7601, Recall: 0.7059, F1: 0.7213
Testing Loss: 0.8352, Accuracy: 0.8670, Precision: 0.6665, Recall: 0.6591, F1: 0.6572
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0062, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0231, Accuracy: 0.9955, Precision: 0.9973, Recall: 0.9961, F1: 0.9967
Validation Loss: 0.6458, Accuracy: 0.8920, Precision: 0.7675, Recall: 0.7338, F1: 0.7435
Testing Loss: 0.7694, Accuracy: 0.8750, Precision: 0.6745, Recall: 0.6638, F1: 0.6666
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0046, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0036, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9998, F1: 0.9997
Validation Loss: 0.7777, Accuracy: 0.8864, Precision: 0.7871, Recall: 0.7234, F1: 0.7448
Testing Loss: 0.8451, Accuracy: 0.8803, Precision: 0.6790, Recall: 0.6683, F1: 0.6711
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0026, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9997, F1: 0.9998
Validation Loss: 0.8285, Accuracy: 0.8835, Precision: 0.7642, Recall: 0.7230, F1: 0.7361
Testing Loss: 0.8739, Accuracy: 0.8830, Precision: 0.6801, Recall: 0.6699, F1: 0.6700
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0073, Accuracy: 0.9983, Precision: 0.9926, Recall: 0.9899, F1: 0.9912
Validation Loss: 0.8023, Accuracy: 0.8864, Precision: 0.7658, Recall: 0.7312, F1: 0.7412
Testing Loss: 0.8571, Accuracy: 0.8830, Precision: 0.6798, Recall: 0.6728, F1: 0.6734
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0218, Accuracy: 0.9955, Precision: 0.9872, Recall: 0.9858, F1: 0.9865
Validation Loss: 0.6852, Accuracy: 0.8920, Precision: 0.7952, Recall: 0.7525, F1: 0.7672
Testing Loss: 0.7963, Accuracy: 0.8750, Precision: 0.6568, Recall: 0.6556, F1: 0.6515
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 1, 5, 5, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4548, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9200, F1: 0.9144
Epoch 48/70
Train Loss: 0.0245, Accuracy: 0.9951, Precision: 0.9907, Recall: 0.9930, F1: 0.9918
Validation Loss: 0.7937, Accuracy: 0.8750, Precision: 0.7497, Recall: 0.7269, F1: 0.7348
Testing Loss: 0.8551, Accuracy: 0.8750, Precision: 0.6674, Recall: 0.6630, F1: 0.6610
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 5, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2798, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 49/70
Train Loss: 0.0438, Accuracy: 0.9913, Precision: 0.9789, Recall: 0.9766, F1: 0.9774
Validation Loss: 0.8505, Accuracy: 0.8636, Precision: 0.6843, Recall: 0.6239, F1: 0.6429
Testing Loss: 0.8670, Accuracy: 0.8670, Precision: 0.6560, Recall: 0.6112, F1: 0.6158
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0279, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0478, Accuracy: 0.9885, Precision: 0.9750, Recall: 0.9782, F1: 0.9766
Validation Loss: 0.7084, Accuracy: 0.8835, Precision: 0.7152, Recall: 0.7163, F1: 0.7156
Testing Loss: 0.8243, Accuracy: 0.8750, Precision: 0.6550, Recall: 0.6580, F1: 0.6544
LM Predictions:  [2, 0, 3, 5, 1, 5, 4, 5, 2, 1, 5, 0, 3, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2845, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7524, F1: 0.7759
Epoch 51/70
Train Loss: 0.0234, Accuracy: 0.9948, Precision: 0.9932, Recall: 0.9929, F1: 0.9930
Validation Loss: 0.8236, Accuracy: 0.8409, Precision: 0.7388, Recall: 0.7059, F1: 0.7158
Testing Loss: 0.8664, Accuracy: 0.8537, Precision: 0.6460, Recall: 0.6392, F1: 0.6261
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0095, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0223, Accuracy: 0.9965, Precision: 0.9953, Recall: 0.9957, F1: 0.9955
Validation Loss: 0.9256, Accuracy: 0.8551, Precision: 0.7577, Recall: 0.7099, F1: 0.7287
Testing Loss: 0.9190, Accuracy: 0.8697, Precision: 0.6686, Recall: 0.6605, F1: 0.6604
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0084, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0103, Accuracy: 0.9972, Precision: 0.9962, Recall: 0.9973, F1: 0.9967
Validation Loss: 0.9010, Accuracy: 0.8608, Precision: 0.6996, Recall: 0.6769, F1: 0.6866
Testing Loss: 0.8565, Accuracy: 0.8856, Precision: 0.6842, Recall: 0.6785, F1: 0.6801
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0048, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0034, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9997, F1: 0.9997
Validation Loss: 0.9050, Accuracy: 0.8693, Precision: 0.7104, Recall: 0.6738, F1: 0.6893
Testing Loss: 0.9885, Accuracy: 0.8723, Precision: 0.6853, Recall: 0.6555, F1: 0.6662
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0147, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0031, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.9462, Accuracy: 0.8636, Precision: 0.7005, Recall: 0.6634, F1: 0.6792
Testing Loss: 0.9705, Accuracy: 0.8750, Precision: 0.6664, Recall: 0.6535, F1: 0.6580
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0151, Accuracy: 0.9951, Precision: 0.9962, Recall: 0.9931, F1: 0.9946
Validation Loss: 0.7386, Accuracy: 0.8580, Precision: 0.7321, Recall: 0.6229, F1: 0.6416
Testing Loss: 0.7997, Accuracy: 0.8750, Precision: 0.6753, Recall: 0.6802, F1: 0.6709
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0229, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0210, Accuracy: 0.9948, Precision: 0.9851, Recall: 0.9877, F1: 0.9864
Validation Loss: 0.8943, Accuracy: 0.8267, Precision: 0.6551, Recall: 0.6513, F1: 0.6315
Testing Loss: 0.8214, Accuracy: 0.8457, Precision: 0.6239, Recall: 0.6820, F1: 0.6445
LM Predictions:  [3, 5, 3, 5, 1, 4, 4, 5, 5, 5, 5, 0, 3, 5, 4, 5, 4, 4, 1, 5, 5, 3, 0, 3, 1, 5, 0, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3798, Accuracy: 0.6429, Precision: 0.6061, Recall: 0.5583, F1: 0.5642
Epoch 58/70
Train Loss: 0.0510, Accuracy: 0.9860, Precision: 0.9565, Recall: 0.9421, F1: 0.9490
Validation Loss: 0.8470, Accuracy: 0.8580, Precision: 0.6705, Recall: 0.6758, F1: 0.6675
Testing Loss: 0.9300, Accuracy: 0.8590, Precision: 0.6250, Recall: 0.6265, F1: 0.6196
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 3, 4, 4, 1, 5, 5, 2, 0, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5353, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7429, F1: 0.7788
Epoch 59/70
Train Loss: 0.0248, Accuracy: 0.9923, Precision: 0.9710, Recall: 0.9757, F1: 0.9733
Validation Loss: 0.8204, Accuracy: 0.8636, Precision: 0.6889, Recall: 0.6764, F1: 0.6772
Testing Loss: 0.8633, Accuracy: 0.8644, Precision: 0.6578, Recall: 0.6700, F1: 0.6534
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 3, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4234, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7667, F1: 0.7731
Epoch 60/70
Train Loss: 0.0320, Accuracy: 0.9934, Precision: 0.9915, Recall: 0.9903, F1: 0.9908
Validation Loss: 0.9373, Accuracy: 0.8608, Precision: 0.7080, Recall: 0.6511, F1: 0.6706
Testing Loss: 0.9207, Accuracy: 0.8697, Precision: 0.6672, Recall: 0.6466, F1: 0.6552
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2205, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7667, F1: 0.7917
Epoch 61/70
Train Loss: 0.0277, Accuracy: 0.9916, Precision: 0.9766, Recall: 0.9813, F1: 0.9789
Validation Loss: 0.8879, Accuracy: 0.8551, Precision: 0.7293, Recall: 0.6924, F1: 0.7060
Testing Loss: 0.8767, Accuracy: 0.8564, Precision: 0.6441, Recall: 0.6264, F1: 0.6297
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0788, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 62/70
Train Loss: 0.0095, Accuracy: 0.9965, Precision: 0.9933, Recall: 0.9925, F1: 0.9929
Validation Loss: 0.9190, Accuracy: 0.8636, Precision: 0.7440, Recall: 0.6880, F1: 0.7018
Testing Loss: 0.9944, Accuracy: 0.8644, Precision: 0.6706, Recall: 0.6338, F1: 0.6466
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0034, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0059, Accuracy: 0.9983, Precision: 0.9976, Recall: 0.9964, F1: 0.9970
Validation Loss: 0.7972, Accuracy: 0.8722, Precision: 0.7040, Recall: 0.6868, F1: 0.6917
Testing Loss: 1.0588, Accuracy: 0.8644, Precision: 0.6486, Recall: 0.6646, F1: 0.6489
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0047, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7766, Accuracy: 0.8835, Precision: 0.7443, Recall: 0.7417, F1: 0.7368
Testing Loss: 1.0891, Accuracy: 0.8644, Precision: 0.6523, Recall: 0.6596, F1: 0.6519
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0052, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0011, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.9000, Accuracy: 0.8693, Precision: 0.7019, Recall: 0.6617, F1: 0.6789
Testing Loss: 1.0256, Accuracy: 0.8723, Precision: 0.6620, Recall: 0.6679, F1: 0.6622
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0269, Accuracy: 0.9937, Precision: 0.9887, Recall: 0.9908, F1: 0.9897
Validation Loss: 1.0108, Accuracy: 0.8494, Precision: 0.6732, Recall: 0.6366, F1: 0.6503
Testing Loss: 1.0332, Accuracy: 0.8723, Precision: 0.6619, Recall: 0.6879, F1: 0.6698
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0041, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0232, Accuracy: 0.9955, Precision: 0.9840, Recall: 0.9863, F1: 0.9851
Validation Loss: 0.8967, Accuracy: 0.8608, Precision: 0.7217, Recall: 0.6320, F1: 0.6665
Testing Loss: 1.0771, Accuracy: 0.8404, Precision: 0.6242, Recall: 0.6005, F1: 0.6102
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0030, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0923, Accuracy: 0.9808, Precision: 0.9666, Recall: 0.9186, F1: 0.9378
Validation Loss: 0.8636, Accuracy: 0.8722, Precision: 0.7128, Recall: 0.6547, F1: 0.6764
Testing Loss: 0.7628, Accuracy: 0.8723, Precision: 0.6701, Recall: 0.6618, F1: 0.6607
LM Predictions:  [2, 0, 2, 4, 1, 4, 4, 5, 2, 4, 5, 0, 4, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6529, Accuracy: 0.8929, Precision: 0.9250, Recall: 0.8929, F1: 0.8945
Epoch 69/70
Train Loss: 0.0299, Accuracy: 0.9944, Precision: 0.9836, Recall: 0.9693, F1: 0.9759
Validation Loss: 1.0050, Accuracy: 0.8409, Precision: 0.7168, Recall: 0.6485, F1: 0.6503
Testing Loss: 0.9235, Accuracy: 0.8537, Precision: 0.6630, Recall: 0.6264, F1: 0.6408
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 4, 5, 4, 5, 4, 4, 1, 5, 0, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3274, Accuracy: 0.9286, Precision: 0.9333, Recall: 0.9429, F1: 0.9329
Epoch 70/70
Train Loss: 0.0313, Accuracy: 0.9930, Precision: 0.9824, Recall: 0.9854, F1: 0.9839
Validation Loss: 0.8649, Accuracy: 0.8409, Precision: 0.7191, Recall: 0.6760, F1: 0.6934
Testing Loss: 0.8857, Accuracy: 0.8511, Precision: 0.6551, Recall: 0.6425, F1: 0.6419
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 4, 5, 4, 5, 4, 4, 1, 5, 5, 2, 2, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3666, Accuracy: 0.9286, Precision: 0.9381, Recall: 0.9314, F1: 0.9310
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.8100, Accuracy: 0.7194, Precision: 0.5498, Recall: 0.4205, F1: 0.4363
Validation Loss: 0.4273, Accuracy: 0.8722, Precision: 0.6854, Recall: 0.6467, F1: 0.6556
Testing Loss: 0.4344, Accuracy: 0.8564, Precision: 0.6344, Recall: 0.6391, F1: 0.6154
LM Predictions:  [3, 3, 3, 2, 1, 2, 2, 5, 1, 3, 3, 3, 3, 1, 3, 2, 3, 3, 3, 1, 3, 3, 1, 3, 2, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3409, Accuracy: 0.1071, Precision: 0.2238, Recall: 0.0893, F1: 0.1025
Epoch 2/70
Train Loss: 0.3710, Accuracy: 0.8838, Precision: 0.7913, Recall: 0.6826, F1: 0.6863
Validation Loss: 0.3434, Accuracy: 0.8977, Precision: 0.7050, Recall: 0.6962, F1: 0.7001
Testing Loss: 0.3908, Accuracy: 0.8830, Precision: 0.6682, Recall: 0.7040, F1: 0.6831
LM Predictions:  [3, 3, 3, 2, 1, 3, 4, 5, 5, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1685, Accuracy: 0.1786, Precision: 0.5000, Recall: 0.1464, F1: 0.2199
Epoch 3/70
Train Loss: 0.2546, Accuracy: 0.9209, Precision: 0.8134, Recall: 0.7968, F1: 0.7998
Validation Loss: 0.3346, Accuracy: 0.8807, Precision: 0.7430, Recall: 0.7448, F1: 0.7403
Testing Loss: 0.3610, Accuracy: 0.8910, Precision: 0.6690, Recall: 0.7138, F1: 0.6874
LM Predictions:  [3, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 2, 1, 2, 5, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1666, Accuracy: 0.2857, Precision: 0.4583, Recall: 0.2274, F1: 0.3023
Epoch 4/70
Train Loss: 0.1766, Accuracy: 0.9475, Precision: 0.8791, Recall: 0.8690, F1: 0.8728
Validation Loss: 0.3762, Accuracy: 0.8807, Precision: 0.7566, Recall: 0.6526, F1: 0.6873
Testing Loss: 0.4511, Accuracy: 0.8670, Precision: 0.6635, Recall: 0.6670, F1: 0.6610
LM Predictions:  [2, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 3, 3, 1, 4, 1, 5, 2, 2, 3, 2, 5, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3892, Accuracy: 0.5000, Precision: 0.6357, Recall: 0.4071, F1: 0.4663
Epoch 5/70
Train Loss: 0.1124, Accuracy: 0.9661, Precision: 0.9279, Recall: 0.9197, F1: 0.9229
Validation Loss: 0.5396, Accuracy: 0.8778, Precision: 0.7577, Recall: 0.6364, F1: 0.6781
Testing Loss: 0.4856, Accuracy: 0.8883, Precision: 0.6981, Recall: 0.6695, F1: 0.6795
LM Predictions:  [2, 3, 3, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 5, 4, 1, 5, 2, 2, 3, 2, 5, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2012, Accuracy: 0.6071, Precision: 0.7361, Recall: 0.4881, F1: 0.5507
Epoch 6/70
Train Loss: 0.0759, Accuracy: 0.9790, Precision: 0.9539, Recall: 0.9479, F1: 0.9506
Validation Loss: 0.4766, Accuracy: 0.8864, Precision: 0.7461, Recall: 0.7592, F1: 0.7418
Testing Loss: 0.5330, Accuracy: 0.8750, Precision: 0.6701, Recall: 0.6999, F1: 0.6727
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 3, 3, 3, 0, 2, 5, 3, 5, 1, 4, 1, 5, 5, 2, 3, 2, 1, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9397, Accuracy: 0.6429, Precision: 0.7429, Recall: 0.5202, F1: 0.5734
Epoch 7/70
Train Loss: 0.0648, Accuracy: 0.9815, Precision: 0.9511, Recall: 0.9461, F1: 0.9482
Validation Loss: 0.5049, Accuracy: 0.8778, Precision: 0.7470, Recall: 0.7873, F1: 0.7369
Testing Loss: 0.5213, Accuracy: 0.8856, Precision: 0.6727, Recall: 0.6962, F1: 0.6801
LM Predictions:  [2, 3, 2, 3, 1, 4, 4, 5, 2, 3, 3, 3, 2, 5, 4, 5, 5, 4, 1, 5, 5, 2, 3, 2, 1, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7914, Accuracy: 0.6786, Precision: 0.5972, Recall: 0.5440, F1: 0.5680
Epoch 8/70
Train Loss: 0.0332, Accuracy: 0.9906, Precision: 0.9724, Recall: 0.9690, F1: 0.9705
Validation Loss: 0.5690, Accuracy: 0.8835, Precision: 0.7269, Recall: 0.6739, F1: 0.6960
Testing Loss: 0.5371, Accuracy: 0.8777, Precision: 0.6836, Recall: 0.6629, F1: 0.6644
LM Predictions:  [2, 0, 2, 3, 1, 4, 4, 5, 2, 4, 3, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4996, Accuracy: 0.8214, Precision: 0.7847, Recall: 0.6774, F1: 0.7138
Epoch 9/70
Train Loss: 0.0325, Accuracy: 0.9885, Precision: 0.9704, Recall: 0.9748, F1: 0.9725
Validation Loss: 0.5856, Accuracy: 0.8807, Precision: 0.7411, Recall: 0.6525, F1: 0.6802
Testing Loss: 0.5619, Accuracy: 0.8803, Precision: 0.6797, Recall: 0.6818, F1: 0.6764
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1577, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 10/70
Train Loss: 0.0684, Accuracy: 0.9832, Precision: 0.9757, Recall: 0.9796, F1: 0.9776
Validation Loss: 0.5210, Accuracy: 0.8864, Precision: 0.7040, Recall: 0.7004, F1: 0.6944
Testing Loss: 0.6282, Accuracy: 0.8564, Precision: 0.6313, Recall: 0.6741, F1: 0.6429
LM Predictions:  [0, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2734, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7524, F1: 0.7622
Epoch 11/70
Train Loss: 0.0787, Accuracy: 0.9773, Precision: 0.9480, Recall: 0.9580, F1: 0.9528
Validation Loss: 0.5269, Accuracy: 0.8778, Precision: 0.7438, Recall: 0.6258, F1: 0.6512
Testing Loss: 0.6327, Accuracy: 0.8511, Precision: 0.6464, Recall: 0.5997, F1: 0.6064
LM Predictions:  [2, 0, 2, 2, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1393, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7345, F1: 0.7574
Epoch 12/70
Train Loss: 0.0324, Accuracy: 0.9892, Precision: 0.9828, Recall: 0.9828, F1: 0.9828
Validation Loss: 0.5817, Accuracy: 0.8778, Precision: 0.7410, Recall: 0.6533, F1: 0.6844
Testing Loss: 0.6535, Accuracy: 0.8644, Precision: 0.6638, Recall: 0.6395, F1: 0.6365
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0805, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 13/70
Train Loss: 0.0220, Accuracy: 0.9937, Precision: 0.9921, Recall: 0.9933, F1: 0.9927
Validation Loss: 0.4898, Accuracy: 0.8864, Precision: 0.7062, Recall: 0.7091, F1: 0.7055
Testing Loss: 0.6485, Accuracy: 0.8617, Precision: 0.6381, Recall: 0.6712, F1: 0.6466
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0705, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 14/70
Train Loss: 0.0234, Accuracy: 0.9944, Precision: 0.9888, Recall: 0.9910, F1: 0.9899
Validation Loss: 0.6196, Accuracy: 0.8864, Precision: 0.7179, Recall: 0.6715, F1: 0.6889
Testing Loss: 0.6494, Accuracy: 0.8856, Precision: 0.6659, Recall: 0.6552, F1: 0.6546
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0331, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 15/70
Train Loss: 0.0219, Accuracy: 0.9951, Precision: 0.9894, Recall: 0.9856, F1: 0.9875
Validation Loss: 0.5544, Accuracy: 0.8722, Precision: 0.7363, Recall: 0.6528, F1: 0.6739
Testing Loss: 0.6360, Accuracy: 0.8723, Precision: 0.6559, Recall: 0.6605, F1: 0.6494
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0121, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 16/70
Train Loss: 0.0240, Accuracy: 0.9930, Precision: 0.9900, Recall: 0.9874, F1: 0.9887
Validation Loss: 0.4817, Accuracy: 0.8920, Precision: 0.7482, Recall: 0.7216, F1: 0.7339
Testing Loss: 0.6255, Accuracy: 0.8723, Precision: 0.6720, Recall: 0.6884, F1: 0.6752
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0452, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 17/70
Train Loss: 0.0146, Accuracy: 0.9962, Precision: 0.9952, Recall: 0.9941, F1: 0.9947
Validation Loss: 0.5283, Accuracy: 0.8892, Precision: 0.7529, Recall: 0.7387, F1: 0.7315
Testing Loss: 0.7052, Accuracy: 0.8644, Precision: 0.6453, Recall: 0.6432, F1: 0.6294
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0030, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 18/70
Train Loss: 0.0067, Accuracy: 0.9969, Precision: 0.9928, Recall: 0.9968, F1: 0.9948
Validation Loss: 0.7103, Accuracy: 0.8778, Precision: 0.7236, Recall: 0.6689, F1: 0.6825
Testing Loss: 0.7988, Accuracy: 0.8644, Precision: 0.6735, Recall: 0.6445, F1: 0.6298
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0028, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0336, Accuracy: 0.9909, Precision: 0.9843, Recall: 0.9790, F1: 0.9816
Validation Loss: 0.6441, Accuracy: 0.8778, Precision: 0.7258, Recall: 0.6474, F1: 0.6726
Testing Loss: 0.6968, Accuracy: 0.8723, Precision: 0.6848, Recall: 0.6707, F1: 0.6708
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 20/70
Train Loss: 0.0296, Accuracy: 0.9927, Precision: 0.9870, Recall: 0.9795, F1: 0.9832
Validation Loss: 0.5831, Accuracy: 0.8977, Precision: 0.7347, Recall: 0.7021, F1: 0.7135
Testing Loss: 0.6908, Accuracy: 0.8644, Precision: 0.6698, Recall: 0.6510, F1: 0.6581
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0469, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0129, Accuracy: 0.9969, Precision: 0.9897, Recall: 0.9914, F1: 0.9905
Validation Loss: 0.5898, Accuracy: 0.8920, Precision: 0.7462, Recall: 0.6828, F1: 0.7093
Testing Loss: 0.6676, Accuracy: 0.8750, Precision: 0.6651, Recall: 0.6686, F1: 0.6632
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0824, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 22/70
Train Loss: 0.0125, Accuracy: 0.9969, Precision: 0.9921, Recall: 0.9907, F1: 0.9914
Validation Loss: 0.5907, Accuracy: 0.8835, Precision: 0.7222, Recall: 0.6898, F1: 0.7023
Testing Loss: 0.7403, Accuracy: 0.8670, Precision: 0.6640, Recall: 0.6806, F1: 0.6669
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0210, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 23/70
Train Loss: 0.0194, Accuracy: 0.9941, Precision: 0.9834, Recall: 0.9864, F1: 0.9849
Validation Loss: 0.6056, Accuracy: 0.8920, Precision: 0.7680, Recall: 0.7376, F1: 0.7319
Testing Loss: 0.8749, Accuracy: 0.8484, Precision: 0.6459, Recall: 0.6202, F1: 0.6242
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0030, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0150, Accuracy: 0.9958, Precision: 0.9900, Recall: 0.9941, F1: 0.9920
Validation Loss: 0.5957, Accuracy: 0.9006, Precision: 0.7780, Recall: 0.7542, F1: 0.7537
Testing Loss: 0.7518, Accuracy: 0.8803, Precision: 0.6632, Recall: 0.6670, F1: 0.6631
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0080, Accuracy: 0.9979, Precision: 0.9989, Recall: 0.9990, F1: 0.9989
Validation Loss: 0.8126, Accuracy: 0.8580, Precision: 0.7777, Recall: 0.7160, F1: 0.7392
Testing Loss: 0.9971, Accuracy: 0.8378, Precision: 0.6734, Recall: 0.6261, F1: 0.6424
LM Predictions:  [2, 0, 2, 4, 1, 4, 4, 5, 2, 1, 5, 0, 2, 4, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2165, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9429, F1: 0.9333
Epoch 26/70
Train Loss: 0.0275, Accuracy: 0.9916, Precision: 0.9886, Recall: 0.9872, F1: 0.9879
Validation Loss: 0.6102, Accuracy: 0.8835, Precision: 0.7208, Recall: 0.7159, F1: 0.7145
Testing Loss: 0.7975, Accuracy: 0.8590, Precision: 0.6399, Recall: 0.6552, F1: 0.6448
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0093, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0168, Accuracy: 0.9958, Precision: 0.9888, Recall: 0.9911, F1: 0.9899
Validation Loss: 0.7058, Accuracy: 0.8807, Precision: 0.7072, Recall: 0.6374, F1: 0.6539
Testing Loss: 0.8845, Accuracy: 0.8511, Precision: 0.6436, Recall: 0.6005, F1: 0.5905
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0070, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0204, Accuracy: 0.9941, Precision: 0.9912, Recall: 0.9890, F1: 0.9901
Validation Loss: 0.6173, Accuracy: 0.8892, Precision: 0.7116, Recall: 0.6875, F1: 0.6955
Testing Loss: 0.8346, Accuracy: 0.8670, Precision: 0.6478, Recall: 0.6592, F1: 0.6437
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0106, Accuracy: 0.9976, Precision: 0.9975, Recall: 0.9974, F1: 0.9974
Validation Loss: 0.6893, Accuracy: 0.8807, Precision: 0.7216, Recall: 0.6869, F1: 0.6995
Testing Loss: 0.8494, Accuracy: 0.8644, Precision: 0.6553, Recall: 0.6753, F1: 0.6564
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0174, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0190, Accuracy: 0.9941, Precision: 0.9903, Recall: 0.9884, F1: 0.9893
Validation Loss: 0.6854, Accuracy: 0.8864, Precision: 0.7525, Recall: 0.7291, F1: 0.7299
Testing Loss: 0.8030, Accuracy: 0.8590, Precision: 0.6420, Recall: 0.5980, F1: 0.6135
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1124, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 31/70
Train Loss: 0.0221, Accuracy: 0.9955, Precision: 0.9872, Recall: 0.9831, F1: 0.9851
Validation Loss: 0.5897, Accuracy: 0.8807, Precision: 0.6920, Recall: 0.6960, F1: 0.6909
Testing Loss: 0.8484, Accuracy: 0.8537, Precision: 0.6263, Recall: 0.6714, F1: 0.6398
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0365, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0198, Accuracy: 0.9955, Precision: 0.9935, Recall: 0.9906, F1: 0.9920
Validation Loss: 0.6311, Accuracy: 0.8892, Precision: 0.7479, Recall: 0.7329, F1: 0.7306
Testing Loss: 0.9142, Accuracy: 0.8564, Precision: 0.6348, Recall: 0.6441, F1: 0.6334
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0698, Accuracy: 0.9829, Precision: 0.9720, Recall: 0.9722, F1: 0.9721
Validation Loss: 0.6955, Accuracy: 0.8835, Precision: 0.7331, Recall: 0.6669, F1: 0.6945
Testing Loss: 0.8124, Accuracy: 0.8697, Precision: 0.6872, Recall: 0.6621, F1: 0.6694
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0093, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0088, Accuracy: 0.9976, Precision: 0.9963, Recall: 0.9975, F1: 0.9969
Validation Loss: 0.6140, Accuracy: 0.8835, Precision: 0.7380, Recall: 0.7378, F1: 0.7242
Testing Loss: 0.9517, Accuracy: 0.8431, Precision: 0.6430, Recall: 0.6346, F1: 0.6369
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0105, Accuracy: 0.9979, Precision: 0.9964, Recall: 0.9963, F1: 0.9963
Validation Loss: 0.7027, Accuracy: 0.8750, Precision: 0.7198, Recall: 0.6605, F1: 0.6846
Testing Loss: 0.8641, Accuracy: 0.8617, Precision: 0.6896, Recall: 0.6472, F1: 0.6650
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0057, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0155, Accuracy: 0.9962, Precision: 0.9975, Recall: 0.9979, F1: 0.9977
Validation Loss: 0.7598, Accuracy: 0.8807, Precision: 0.7346, Recall: 0.7219, F1: 0.7232
Testing Loss: 0.9123, Accuracy: 0.8617, Precision: 0.6754, Recall: 0.6538, F1: 0.6614
LM Predictions:  [4, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0278, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 37/70
Train Loss: 0.0184, Accuracy: 0.9944, Precision: 0.9863, Recall: 0.9852, F1: 0.9857
Validation Loss: 0.7463, Accuracy: 0.8665, Precision: 0.7127, Recall: 0.7434, F1: 0.7076
Testing Loss: 0.8853, Accuracy: 0.8590, Precision: 0.6613, Recall: 0.6894, F1: 0.6575
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0031, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0124, Accuracy: 0.9976, Precision: 0.9946, Recall: 0.9944, F1: 0.9945
Validation Loss: 0.7212, Accuracy: 0.8665, Precision: 0.7204, Recall: 0.7228, F1: 0.7106
Testing Loss: 0.7975, Accuracy: 0.8697, Precision: 0.6819, Recall: 0.7038, F1: 0.6836
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0018, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0097, Accuracy: 0.9972, Precision: 0.9957, Recall: 0.9958, F1: 0.9957
Validation Loss: 0.6712, Accuracy: 0.8665, Precision: 0.7433, Recall: 0.7161, F1: 0.7157
Testing Loss: 0.8837, Accuracy: 0.8617, Precision: 0.6652, Recall: 0.6735, F1: 0.6663
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0071, Accuracy: 0.9983, Precision: 0.9992, Recall: 0.9992, F1: 0.9992
Validation Loss: 0.7078, Accuracy: 0.8835, Precision: 0.7190, Recall: 0.6988, F1: 0.7071
Testing Loss: 0.9337, Accuracy: 0.8723, Precision: 0.6587, Recall: 0.6879, F1: 0.6666
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0221, Accuracy: 0.9937, Precision: 0.9835, Recall: 0.9870, F1: 0.9852
Validation Loss: 0.7206, Accuracy: 0.8778, Precision: 0.7459, Recall: 0.7579, F1: 0.7352
Testing Loss: 0.8880, Accuracy: 0.8644, Precision: 0.6763, Recall: 0.6321, F1: 0.6487
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1452, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 42/70
Train Loss: 0.0183, Accuracy: 0.9948, Precision: 0.9884, Recall: 0.9841, F1: 0.9862
Validation Loss: 0.6045, Accuracy: 0.8920, Precision: 0.7779, Recall: 0.8416, F1: 0.7713
Testing Loss: 0.7871, Accuracy: 0.8803, Precision: 0.6776, Recall: 0.6879, F1: 0.6781
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0041, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0042, Accuracy: 0.9990, Precision: 0.9966, Recall: 0.9993, F1: 0.9980
Validation Loss: 0.6763, Accuracy: 0.8835, Precision: 0.7321, Recall: 0.6983, F1: 0.7115
Testing Loss: 0.8343, Accuracy: 0.8750, Precision: 0.6867, Recall: 0.6769, F1: 0.6767
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0035, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0019, Accuracy: 0.9993, Precision: 0.9995, Recall: 0.9967, F1: 0.9981
Validation Loss: 0.6996, Accuracy: 0.8949, Precision: 0.7814, Recall: 0.8475, F1: 0.7768
Testing Loss: 0.8539, Accuracy: 0.8723, Precision: 0.6723, Recall: 0.6781, F1: 0.6701
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0034, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.6437, Accuracy: 0.9062, Precision: 0.7915, Recall: 0.8202, F1: 0.7822
Testing Loss: 0.8132, Accuracy: 0.8723, Precision: 0.6775, Recall: 0.6802, F1: 0.6739
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0087, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9988, F1: 0.9984
Validation Loss: 0.6408, Accuracy: 0.8920, Precision: 0.7620, Recall: 0.7593, F1: 0.7544
Testing Loss: 0.7662, Accuracy: 0.8803, Precision: 0.6823, Recall: 0.6658, F1: 0.6707
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0264, Accuracy: 0.9930, Precision: 0.9799, Recall: 0.9842, F1: 0.9820
Validation Loss: 0.7206, Accuracy: 0.8892, Precision: 0.7319, Recall: 0.7161, F1: 0.7237
Testing Loss: 0.8403, Accuracy: 0.8777, Precision: 0.6764, Recall: 0.6888, F1: 0.6702
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0192, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0163, Accuracy: 0.9969, Precision: 0.9906, Recall: 0.9924, F1: 0.9914
Validation Loss: 0.7859, Accuracy: 0.8892, Precision: 0.7236, Recall: 0.7071, F1: 0.7142
Testing Loss: 0.8916, Accuracy: 0.8697, Precision: 0.6459, Recall: 0.6772, F1: 0.6564
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0279, Accuracy: 0.9927, Precision: 0.9841, Recall: 0.9673, F1: 0.9753
Validation Loss: 0.8051, Accuracy: 0.8835, Precision: 0.7160, Recall: 0.6901, F1: 0.7008
Testing Loss: 0.9221, Accuracy: 0.8644, Precision: 0.6534, Recall: 0.6493, F1: 0.6404
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0240, Accuracy: 0.9958, Precision: 0.9971, Recall: 0.9961, F1: 0.9966
Validation Loss: 0.5749, Accuracy: 0.8949, Precision: 0.7484, Recall: 0.6857, F1: 0.7105
Testing Loss: 0.7881, Accuracy: 0.8697, Precision: 0.7080, Recall: 0.6521, F1: 0.6662
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0554, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 51/70
Train Loss: 0.0090, Accuracy: 0.9983, Precision: 0.9948, Recall: 0.9960, F1: 0.9954
Validation Loss: 0.7221, Accuracy: 0.9062, Precision: 0.7839, Recall: 0.7682, F1: 0.7750
Testing Loss: 1.0265, Accuracy: 0.8590, Precision: 0.6490, Recall: 0.6658, F1: 0.6497
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0140, Accuracy: 0.9958, Precision: 0.9960, Recall: 0.9944, F1: 0.9951
Validation Loss: 0.6864, Accuracy: 0.8864, Precision: 0.7516, Recall: 0.7559, F1: 0.7399
Testing Loss: 0.9693, Accuracy: 0.8590, Precision: 0.6660, Recall: 0.6564, F1: 0.6537
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0110, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0163, Accuracy: 0.9972, Precision: 0.9966, Recall: 0.9959, F1: 0.9962
Validation Loss: 0.7418, Accuracy: 0.9034, Precision: 0.8045, Recall: 0.7488, F1: 0.7719
Testing Loss: 0.8697, Accuracy: 0.8830, Precision: 0.6881, Recall: 0.6674, F1: 0.6714
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0156, Accuracy: 0.9969, Precision: 0.9940, Recall: 0.9938, F1: 0.9939
Validation Loss: 0.6716, Accuracy: 0.9034, Precision: 0.7701, Recall: 0.7736, F1: 0.7674
Testing Loss: 0.9360, Accuracy: 0.8644, Precision: 0.6444, Recall: 0.6428, F1: 0.6406
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7265, Accuracy: 0.9091, Precision: 0.7365, Recall: 0.7099, F1: 0.7207
Testing Loss: 0.9854, Accuracy: 0.8697, Precision: 0.6572, Recall: 0.6560, F1: 0.6526
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0017, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.7711, Accuracy: 0.8977, Precision: 0.7735, Recall: 0.7543, F1: 0.7607
Testing Loss: 1.0158, Accuracy: 0.8564, Precision: 0.6460, Recall: 0.6284, F1: 0.6321
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7607, Accuracy: 0.9006, Precision: 0.7287, Recall: 0.7030, F1: 0.7142
Testing Loss: 1.0196, Accuracy: 0.8617, Precision: 0.6493, Recall: 0.6432, F1: 0.6405
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0174, Accuracy: 0.9948, Precision: 0.9786, Recall: 0.9724, F1: 0.9754
Validation Loss: 0.6027, Accuracy: 0.8920, Precision: 0.7276, Recall: 0.6592, F1: 0.6848
Testing Loss: 0.8889, Accuracy: 0.8644, Precision: 0.6546, Recall: 0.6128, F1: 0.6261
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2127, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 59/70
Train Loss: 0.0276, Accuracy: 0.9944, Precision: 0.9837, Recall: 0.9789, F1: 0.9813
Validation Loss: 0.6330, Accuracy: 0.8807, Precision: 0.7520, Recall: 0.7462, F1: 0.7410
Testing Loss: 0.8508, Accuracy: 0.8644, Precision: 0.6627, Recall: 0.6490, F1: 0.6550
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 4, 4, 1, 5, 3, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0770, Accuracy: 0.9286, Precision: 0.8056, Recall: 0.7857, F1: 0.7904
Epoch 60/70
Train Loss: 0.0376, Accuracy: 0.9923, Precision: 0.9897, Recall: 0.9875, F1: 0.9886
Validation Loss: 0.6591, Accuracy: 0.8892, Precision: 0.7682, Recall: 0.7461, F1: 0.7483
Testing Loss: 0.8546, Accuracy: 0.8644, Precision: 0.6585, Recall: 0.6564, F1: 0.6549
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0149, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0043, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9983, F1: 0.9990
Validation Loss: 0.6622, Accuracy: 0.8864, Precision: 0.7521, Recall: 0.7537, F1: 0.7437
Testing Loss: 0.8824, Accuracy: 0.8697, Precision: 0.6530, Recall: 0.6789, F1: 0.6607
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0011, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9986, F1: 0.9992
Validation Loss: 0.7384, Accuracy: 0.8949, Precision: 0.7552, Recall: 0.7624, F1: 0.7470
Testing Loss: 0.9089, Accuracy: 0.8644, Precision: 0.6446, Recall: 0.6732, F1: 0.6527
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7551, Accuracy: 0.8892, Precision: 0.7572, Recall: 0.7584, F1: 0.7482
Testing Loss: 0.9283, Accuracy: 0.8617, Precision: 0.6436, Recall: 0.6597, F1: 0.6476
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0154, Accuracy: 0.9962, Precision: 0.9932, Recall: 0.9951, F1: 0.9941
Validation Loss: 0.8966, Accuracy: 0.8778, Precision: 0.7380, Recall: 0.6467, F1: 0.6740
Testing Loss: 1.0125, Accuracy: 0.8564, Precision: 0.6548, Recall: 0.6206, F1: 0.6307
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0029, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0255, Accuracy: 0.9934, Precision: 0.9892, Recall: 0.9872, F1: 0.9881
Validation Loss: 0.8430, Accuracy: 0.8665, Precision: 0.7595, Recall: 0.7398, F1: 0.7450
Testing Loss: 0.9911, Accuracy: 0.8511, Precision: 0.6575, Recall: 0.6425, F1: 0.6467
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0327, Accuracy: 0.9923, Precision: 0.9835, Recall: 0.9850, F1: 0.9843
Validation Loss: 0.6027, Accuracy: 0.8750, Precision: 0.6955, Recall: 0.6818, F1: 0.6874
Testing Loss: 0.8144, Accuracy: 0.8723, Precision: 0.6735, Recall: 0.6678, F1: 0.6604
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0886, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 67/70
Train Loss: 0.0121, Accuracy: 0.9962, Precision: 0.9907, Recall: 0.9879, F1: 0.9893
Validation Loss: 0.7495, Accuracy: 0.8665, Precision: 0.7246, Recall: 0.7229, F1: 0.7102
Testing Loss: 0.9569, Accuracy: 0.8644, Precision: 0.6578, Recall: 0.6658, F1: 0.6609
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0054, Accuracy: 0.9986, Precision: 0.9977, Recall: 0.9977, F1: 0.9977
Validation Loss: 0.7245, Accuracy: 0.8864, Precision: 0.7227, Recall: 0.6950, F1: 0.7065
Testing Loss: 0.9750, Accuracy: 0.8670, Precision: 0.6683, Recall: 0.6699, F1: 0.6642
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0108, Accuracy: 0.9976, Precision: 0.9912, Recall: 0.9912, F1: 0.9912
Validation Loss: 0.7826, Accuracy: 0.8807, Precision: 0.7343, Recall: 0.6731, F1: 0.6991
Testing Loss: 0.9523, Accuracy: 0.8564, Precision: 0.6515, Recall: 0.6576, F1: 0.6454
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0149, Accuracy: 0.9965, Precision: 0.9927, Recall: 0.9952, F1: 0.9940
Validation Loss: 0.8046, Accuracy: 0.8523, Precision: 0.7100, Recall: 0.6120, F1: 0.6467
Testing Loss: 0.9413, Accuracy: 0.8404, Precision: 0.6475, Recall: 0.6338, F1: 0.6242
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0069, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------