---------------------------------------------------------------------------
Results for seed:  28
Model: andreasmadsen/efficient_mlm_m0.40, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4167, Accuracy: 0.3747, Precision: 0.1439, Recall: 0.1681, F1: 0.1392
Validation Loss: 1.4033, Accuracy: 0.4119, Precision: 0.1355, Recall: 0.1906, F1: 0.1573
Testing Loss: 1.4326, Accuracy: 0.4122, Precision: 0.1381, Recall: 0.1916, F1: 0.1586
LM Predictions:  [2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1078, Accuracy: 0.2857, Precision: 0.1227, Recall: 0.2400, F1: 0.1375
Epoch 2/70
Train Loss: 1.3834, Accuracy: 0.3842, Precision: 0.2365, Recall: 0.1734, F1: 0.1455
Validation Loss: 1.3988, Accuracy: 0.4034, Precision: 0.1405, Recall: 0.1915, F1: 0.1583
Testing Loss: 1.4163, Accuracy: 0.4388, Precision: 0.2069, Recall: 0.2081, F1: 0.1805
LM Predictions:  [4, 4, 4, 5, 4, 4, 2, 2, 4, 4, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 2, 4, 4, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0409, Accuracy: 0.1786, Precision: 0.2667, Recall: 0.1657, F1: 0.1348
Epoch 3/70
Train Loss: 1.3711, Accuracy: 0.3863, Precision: 0.1567, Recall: 0.1732, F1: 0.1433
Validation Loss: 1.3999, Accuracy: 0.3807, Precision: 0.1275, Recall: 0.1696, F1: 0.1134
Testing Loss: 1.4211, Accuracy: 0.3883, Precision: 0.1462, Recall: 0.1794, F1: 0.1239
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0680, Accuracy: 0.2857, Precision: 0.1227, Recall: 0.2400, F1: 0.1375
Epoch 4/70
Train Loss: 1.3667, Accuracy: 0.4090, Precision: 0.1679, Recall: 0.1842, F1: 0.1535
Validation Loss: 1.3840, Accuracy: 0.4176, Precision: 0.1414, Recall: 0.1907, F1: 0.1534
Testing Loss: 1.4079, Accuracy: 0.4495, Precision: 0.3221, Recall: 0.2103, F1: 0.1752
LM Predictions:  [2, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0421, Accuracy: 0.1429, Precision: 0.0381, Recall: 0.1143, F1: 0.0571
Epoch 5/70
Train Loss: 1.3516, Accuracy: 0.4297, Precision: 0.2175, Recall: 0.1960, F1: 0.1683
Validation Loss: 1.3268, Accuracy: 0.4886, Precision: 0.1693, Recall: 0.2256, F1: 0.1876
Testing Loss: 1.3399, Accuracy: 0.5027, Precision: 0.2393, Recall: 0.2371, F1: 0.2053
LM Predictions:  [2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0345, Accuracy: 0.2500, Precision: 0.0971, Recall: 0.2229, F1: 0.1341
Epoch 6/70
Train Loss: 1.3341, Accuracy: 0.4409, Precision: 0.1930, Recall: 0.2041, F1: 0.1816
Validation Loss: 1.2898, Accuracy: 0.5199, Precision: 0.1864, Recall: 0.2399, F1: 0.2007
Testing Loss: 1.3155, Accuracy: 0.5186, Precision: 0.1800, Recall: 0.2410, F1: 0.2005
LM Predictions:  [2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1935, Accuracy: 0.1786, Precision: 0.0476, Recall: 0.1429, F1: 0.0714
Epoch 7/70
Train Loss: 1.2881, Accuracy: 0.4846, Precision: 0.2290, Recall: 0.2297, F1: 0.2110
Validation Loss: 1.3081, Accuracy: 0.4830, Precision: 0.3045, Recall: 0.2268, F1: 0.1991
Testing Loss: 1.3267, Accuracy: 0.4894, Precision: 0.2839, Recall: 0.2383, F1: 0.2121
LM Predictions:  [2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1785, Accuracy: 0.2500, Precision: 0.0560, Recall: 0.2000, F1: 0.0875
Epoch 8/70
Train Loss: 1.2341, Accuracy: 0.5332, Precision: 0.2554, Recall: 0.2566, F1: 0.2402
Validation Loss: 1.0969, Accuracy: 0.5938, Precision: 0.2770, Recall: 0.2902, F1: 0.2677
Testing Loss: 1.1729, Accuracy: 0.5931, Precision: 0.2876, Recall: 0.3038, F1: 0.2867
LM Predictions:  [5, 2, 5, 5, 4, 2, 2, 2, 5, 2, 2, 4, 2, 4, 4, 4, 5, 2, 2, 4, 5, 2, 4, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1678, Accuracy: 0.2143, Precision: 0.1350, Recall: 0.1829, F1: 0.1501
Epoch 9/70
Train Loss: 1.1709, Accuracy: 0.5700, Precision: 0.2656, Recall: 0.2779, F1: 0.2622
Validation Loss: 1.0692, Accuracy: 0.5881, Precision: 0.2704, Recall: 0.2880, F1: 0.2676
Testing Loss: 1.1016, Accuracy: 0.6356, Precision: 0.3043, Recall: 0.3254, F1: 0.3067
LM Predictions:  [5, 2, 5, 2, 4, 2, 2, 2, 5, 2, 2, 4, 2, 2, 4, 4, 5, 2, 2, 5, 2, 2, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3091, Accuracy: 0.1786, Precision: 0.1086, Recall: 0.1543, F1: 0.1197
Epoch 10/70
Train Loss: 1.1259, Accuracy: 0.5990, Precision: 0.2833, Recall: 0.2976, F1: 0.2839
Validation Loss: 1.0501, Accuracy: 0.5938, Precision: 0.2675, Recall: 0.2975, F1: 0.2780
Testing Loss: 1.0391, Accuracy: 0.6676, Precision: 0.3178, Recall: 0.3519, F1: 0.3315
LM Predictions:  [5, 2, 5, 2, 4, 2, 5, 2, 5, 2, 2, 4, 2, 4, 4, 5, 5, 2, 4, 4, 2, 2, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3271, Accuracy: 0.1786, Precision: 0.1012, Recall: 0.1543, F1: 0.1187
Epoch 11/70
Train Loss: 1.0769, Accuracy: 0.6207, Precision: 0.2884, Recall: 0.3070, F1: 0.2915
Validation Loss: 1.0740, Accuracy: 0.6193, Precision: 0.2818, Recall: 0.3163, F1: 0.2925
Testing Loss: 1.0933, Accuracy: 0.6410, Precision: 0.3009, Recall: 0.3300, F1: 0.3090
LM Predictions:  [5, 2, 5, 2, 4, 4, 4, 2, 4, 2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 5, 5, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2313, Accuracy: 0.2857, Precision: 0.1614, Recall: 0.2857, F1: 0.1887
Epoch 12/70
Train Loss: 1.0572, Accuracy: 0.6225, Precision: 0.2898, Recall: 0.3092, F1: 0.2943
Validation Loss: 1.0110, Accuracy: 0.6420, Precision: 0.3049, Recall: 0.3356, F1: 0.3176
Testing Loss: 1.0264, Accuracy: 0.6596, Precision: 0.3117, Recall: 0.3546, F1: 0.3303
LM Predictions:  [5, 2, 5, 2, 4, 2, 5, 2, 2, 2, 2, 4, 2, 4, 4, 5, 5, 2, 4, 5, 5, 2, 4, 2, 2, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1946, Accuracy: 0.2857, Precision: 0.1655, Recall: 0.2400, F1: 0.1926
Epoch 13/70
Train Loss: 1.0355, Accuracy: 0.6337, Precision: 0.3005, Recall: 0.3239, F1: 0.3097
Validation Loss: 1.0106, Accuracy: 0.6335, Precision: 0.2987, Recall: 0.3201, F1: 0.3013
Testing Loss: 1.1045, Accuracy: 0.6356, Precision: 0.3016, Recall: 0.3287, F1: 0.3096
LM Predictions:  [2, 2, 4, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 4, 5, 2, 4, 4, 5, 4, 4, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0618, Accuracy: 0.2857, Precision: 0.2030, Recall: 0.2514, F1: 0.1854
Epoch 14/70
Train Loss: 0.9985, Accuracy: 0.6505, Precision: 0.3082, Recall: 0.3329, F1: 0.3182
Validation Loss: 0.9788, Accuracy: 0.6534, Precision: 0.3098, Recall: 0.3407, F1: 0.3224
Testing Loss: 0.9845, Accuracy: 0.6755, Precision: 0.3231, Recall: 0.3654, F1: 0.3418
LM Predictions:  [5, 2, 5, 2, 4, 2, 5, 2, 2, 2, 2, 4, 2, 2, 4, 5, 5, 2, 4, 5, 5, 4, 5, 2, 4, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1867, Accuracy: 0.2500, Precision: 0.1462, Recall: 0.2114, F1: 0.1714
Epoch 15/70
Train Loss: 0.9903, Accuracy: 0.6428, Precision: 0.3043, Recall: 0.3298, F1: 0.3151
Validation Loss: 1.0480, Accuracy: 0.6222, Precision: 0.3030, Recall: 0.3120, F1: 0.2948
Testing Loss: 1.0657, Accuracy: 0.6729, Precision: 0.3238, Recall: 0.3461, F1: 0.3272
LM Predictions:  [2, 2, 2, 5, 4, 2, 2, 2, 5, 2, 2, 2, 2, 4, 4, 4, 5, 2, 4, 5, 5, 2, 4, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0500, Accuracy: 0.3214, Precision: 0.2117, Recall: 0.2686, F1: 0.2217
Epoch 16/70
Train Loss: 0.9590, Accuracy: 0.6592, Precision: 0.3133, Recall: 0.3403, F1: 0.3250
Validation Loss: 1.0481, Accuracy: 0.6307, Precision: 0.2921, Recall: 0.3167, F1: 0.2971
Testing Loss: 1.0668, Accuracy: 0.6596, Precision: 0.3227, Recall: 0.3414, F1: 0.3238
LM Predictions:  [5, 2, 2, 5, 4, 2, 2, 2, 2, 2, 2, 4, 2, 2, 4, 5, 5, 2, 4, 5, 5, 4, 5, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9801, Accuracy: 0.3214, Precision: 0.2000, Recall: 0.2686, F1: 0.2238
Epoch 17/70
Train Loss: 0.9603, Accuracy: 0.6606, Precision: 0.3147, Recall: 0.3421, F1: 0.3268
Validation Loss: 1.0099, Accuracy: 0.6364, Precision: 0.2887, Recall: 0.3251, F1: 0.3038
Testing Loss: 1.0279, Accuracy: 0.6676, Precision: 0.3208, Recall: 0.3504, F1: 0.3309
LM Predictions:  [5, 4, 4, 5, 5, 2, 2, 5, 5, 2, 2, 4, 2, 2, 4, 4, 5, 2, 4, 5, 5, 4, 5, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9273, Accuracy: 0.2500, Precision: 0.1511, Recall: 0.2114, F1: 0.1756
Epoch 18/70
Train Loss: 0.9480, Accuracy: 0.6620, Precision: 0.3145, Recall: 0.3458, F1: 0.3289
Validation Loss: 0.9751, Accuracy: 0.6534, Precision: 0.3072, Recall: 0.3384, F1: 0.3195
Testing Loss: 0.9899, Accuracy: 0.6729, Precision: 0.3154, Recall: 0.3576, F1: 0.3345
LM Predictions:  [5, 4, 4, 5, 4, 2, 5, 2, 2, 2, 2, 4, 2, 2, 4, 4, 5, 5, 2, 5, 5, 4, 5, 2, 2, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0554, Accuracy: 0.2500, Precision: 0.1452, Recall: 0.2114, F1: 0.1715
Epoch 19/70
Train Loss: 0.9088, Accuracy: 0.6802, Precision: 0.3238, Recall: 0.3562, F1: 0.3388
Validation Loss: 0.9741, Accuracy: 0.6420, Precision: 0.3054, Recall: 0.3449, F1: 0.3233
Testing Loss: 0.9642, Accuracy: 0.6835, Precision: 0.3242, Recall: 0.3773, F1: 0.3474
LM Predictions:  [5, 2, 2, 5, 5, 2, 5, 5, 5, 2, 2, 2, 2, 5, 4, 4, 5, 5, 2, 5, 5, 5, 5, 2, 4, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0520, Accuracy: 0.3214, Precision: 0.1926, Recall: 0.2686, F1: 0.2119
Epoch 20/70
Train Loss: 0.9039, Accuracy: 0.6704, Precision: 0.3205, Recall: 0.3529, F1: 0.3355
Validation Loss: 0.9904, Accuracy: 0.6562, Precision: 0.3133, Recall: 0.3417, F1: 0.3238
Testing Loss: 0.9451, Accuracy: 0.6968, Precision: 0.3365, Recall: 0.3785, F1: 0.3541
LM Predictions:  [5, 5, 2, 5, 5, 2, 5, 5, 2, 2, 2, 4, 2, 5, 4, 4, 5, 5, 2, 5, 5, 5, 5, 2, 2, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0178, Accuracy: 0.3571, Precision: 0.2108, Recall: 0.2971, F1: 0.2341
Epoch 21/70
Train Loss: 0.9282, Accuracy: 0.6683, Precision: 0.3196, Recall: 0.3528, F1: 0.3350
Validation Loss: 0.9646, Accuracy: 0.6676, Precision: 0.3237, Recall: 0.3570, F1: 0.3366
Testing Loss: 0.9422, Accuracy: 0.6915, Precision: 0.3388, Recall: 0.3907, F1: 0.3574
LM Predictions:  [5, 2, 2, 5, 5, 2, 5, 5, 5, 2, 2, 2, 2, 5, 4, 5, 5, 5, 2, 5, 5, 5, 5, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9604, Accuracy: 0.3571, Precision: 0.2357, Recall: 0.2971, F1: 0.2346
Epoch 22/70
Train Loss: 0.8786, Accuracy: 0.6949, Precision: 0.3312, Recall: 0.3654, F1: 0.3471
Validation Loss: 0.9520, Accuracy: 0.6705, Precision: 0.3226, Recall: 0.3478, F1: 0.3300
Testing Loss: 0.9167, Accuracy: 0.7074, Precision: 0.3378, Recall: 0.3788, F1: 0.3560
LM Predictions:  [5, 2, 5, 2, 4, 2, 5, 5, 2, 2, 2, 4, 2, 2, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0276, Accuracy: 0.2857, Precision: 0.1615, Recall: 0.2400, F1: 0.1914
Epoch 23/70
Train Loss: 0.8635, Accuracy: 0.6966, Precision: 0.3321, Recall: 0.3654, F1: 0.3476
Validation Loss: 0.9085, Accuracy: 0.6648, Precision: 0.3162, Recall: 0.3510, F1: 0.3316
Testing Loss: 0.8940, Accuracy: 0.7261, Precision: 0.3488, Recall: 0.4020, F1: 0.3720
LM Predictions:  [5, 2, 5, 5, 5, 2, 5, 5, 2, 2, 2, 2, 2, 2, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8631, Accuracy: 0.3571, Precision: 0.1997, Recall: 0.2971, F1: 0.2333
Epoch 24/70
Train Loss: 0.8707, Accuracy: 0.6893, Precision: 0.3294, Recall: 0.3654, F1: 0.3462
Validation Loss: 0.9453, Accuracy: 0.6392, Precision: 0.3101, Recall: 0.3513, F1: 0.3268
Testing Loss: 0.9024, Accuracy: 0.6809, Precision: 0.3313, Recall: 0.3826, F1: 0.3502
LM Predictions:  [1, 5, 2, 5, 5, 2, 5, 5, 5, 5, 2, 2, 5, 2, 4, 4, 5, 5, 4, 5, 5, 5, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8872, Accuracy: 0.2857, Precision: 0.1838, Recall: 0.2400, F1: 0.1968
Epoch 25/70
Train Loss: 0.8408, Accuracy: 0.7022, Precision: 0.3375, Recall: 0.3731, F1: 0.3539
Validation Loss: 0.9542, Accuracy: 0.6506, Precision: 0.3119, Recall: 0.3451, F1: 0.3256
Testing Loss: 0.9240, Accuracy: 0.6941, Precision: 0.3369, Recall: 0.3822, F1: 0.3555
LM Predictions:  [5, 2, 2, 5, 5, 2, 2, 5, 5, 2, 2, 2, 2, 2, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8981, Accuracy: 0.3571, Precision: 0.2133, Recall: 0.2971, F1: 0.2350
Epoch 26/70
Train Loss: 0.8553, Accuracy: 0.7050, Precision: 0.3950, Recall: 0.3769, F1: 0.3587
Validation Loss: 0.9504, Accuracy: 0.6761, Precision: 0.3174, Recall: 0.3588, F1: 0.3367
Testing Loss: 0.8904, Accuracy: 0.7181, Precision: 0.3409, Recall: 0.3951, F1: 0.3657
LM Predictions:  [3, 4, 2, 5, 5, 2, 5, 5, 5, 5, 2, 2, 2, 2, 4, 4, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8312, Accuracy: 0.3214, Precision: 0.1674, Recall: 0.2238, F1: 0.1824
Epoch 27/70
Train Loss: 0.8334, Accuracy: 0.7071, Precision: 0.5343, Recall: 0.3824, F1: 0.3699
Validation Loss: 0.9229, Accuracy: 0.6847, Precision: 0.4978, Recall: 0.3639, F1: 0.3561
Testing Loss: 0.8651, Accuracy: 0.7261, Precision: 0.4133, Recall: 0.4000, F1: 0.3836
LM Predictions:  [3, 2, 3, 2, 5, 5, 2, 5, 2, 5, 2, 2, 2, 2, 4, 4, 5, 2, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7974, Accuracy: 0.3214, Precision: 0.1736, Recall: 0.2238, F1: 0.1841
Epoch 28/70
Train Loss: 0.8225, Accuracy: 0.7071, Precision: 0.3574, Recall: 0.3768, F1: 0.3596
Validation Loss: 0.9389, Accuracy: 0.6847, Precision: 0.4920, Recall: 0.3702, F1: 0.3596
Testing Loss: 0.8772, Accuracy: 0.7207, Precision: 0.5253, Recall: 0.4107, F1: 0.4104
LM Predictions:  [3, 2, 2, 5, 5, 2, 4, 2, 2, 2, 3, 2, 2, 2, 4, 4, 5, 3, 5, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9424, Accuracy: 0.3929, Precision: 0.2492, Recall: 0.2810, F1: 0.2457
Epoch 29/70
Train Loss: 0.8135, Accuracy: 0.7148, Precision: 0.4020, Recall: 0.3866, F1: 0.3720
Validation Loss: 0.8753, Accuracy: 0.6847, Precision: 0.5982, Recall: 0.3897, F1: 0.3967
Testing Loss: 0.8691, Accuracy: 0.7021, Precision: 0.4429, Recall: 0.4039, F1: 0.4010
LM Predictions:  [3, 2, 3, 2, 4, 5, 2, 5, 2, 5, 2, 4, 2, 5, 4, 4, 5, 5, 4, 4, 5, 4, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0088, Accuracy: 0.2857, Precision: 0.1500, Recall: 0.2000, F1: 0.1707
Epoch 30/70
Train Loss: 0.8018, Accuracy: 0.7176, Precision: 0.4226, Recall: 0.3893, F1: 0.3764
Validation Loss: 0.9067, Accuracy: 0.6932, Precision: 0.5826, Recall: 0.3909, F1: 0.4001
Testing Loss: 0.8686, Accuracy: 0.7048, Precision: 0.5498, Recall: 0.4000, F1: 0.4011
LM Predictions:  [3, 2, 2, 2, 4, 2, 3, 5, 2, 2, 2, 2, 2, 2, 4, 4, 5, 3, 4, 5, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0313, Accuracy: 0.3571, Precision: 0.2000, Recall: 0.2476, F1: 0.2076
Epoch 31/70
Train Loss: 0.8049, Accuracy: 0.7173, Precision: 0.4683, Recall: 0.4012, F1: 0.3988
Validation Loss: 0.9327, Accuracy: 0.7045, Precision: 0.5106, Recall: 0.3727, F1: 0.3629
Testing Loss: 0.9313, Accuracy: 0.7128, Precision: 0.4285, Recall: 0.3905, F1: 0.3734
LM Predictions:  [5, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2, 2, 4, 4, 5, 2, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0560, Accuracy: 0.3571, Precision: 0.2267, Recall: 0.3086, F1: 0.2427
Epoch 32/70
Train Loss: 0.7999, Accuracy: 0.7187, Precision: 0.4449, Recall: 0.3981, F1: 0.3922
Validation Loss: 0.8654, Accuracy: 0.7045, Precision: 0.6749, Recall: 0.3828, F1: 0.3819
Testing Loss: 0.8483, Accuracy: 0.7048, Precision: 0.4491, Recall: 0.3959, F1: 0.3864
LM Predictions:  [5, 2, 3, 5, 5, 2, 2, 5, 5, 5, 2, 2, 2, 5, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0026, Accuracy: 0.3571, Precision: 0.1717, Recall: 0.2476, F1: 0.1988
Epoch 33/70
Train Loss: 0.7940, Accuracy: 0.7201, Precision: 0.4786, Recall: 0.4011, F1: 0.3972
Validation Loss: 0.8546, Accuracy: 0.6989, Precision: 0.5592, Recall: 0.3922, F1: 0.3922
Testing Loss: 0.8436, Accuracy: 0.6995, Precision: 0.3840, Recall: 0.3918, F1: 0.3787
LM Predictions:  [3, 2, 3, 5, 5, 2, 3, 5, 5, 5, 3, 2, 2, 5, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0788, Accuracy: 0.3571, Precision: 0.1980, Recall: 0.2476, F1: 0.2127
Epoch 34/70
Train Loss: 0.7869, Accuracy: 0.7162, Precision: 0.4517, Recall: 0.3949, F1: 0.3864
Validation Loss: 0.8446, Accuracy: 0.6989, Precision: 0.5694, Recall: 0.4001, F1: 0.4037
Testing Loss: 0.8029, Accuracy: 0.7128, Precision: 0.5683, Recall: 0.4221, F1: 0.4257
LM Predictions:  [3, 2, 3, 2, 4, 2, 3, 2, 2, 5, 3, 2, 2, 5, 4, 4, 3, 3, 4, 5, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9436, Accuracy: 0.3214, Precision: 0.2028, Recall: 0.2238, F1: 0.2044
Epoch 35/70
Train Loss: 0.7655, Accuracy: 0.7309, Precision: 0.4687, Recall: 0.4135, F1: 0.4131
Validation Loss: 0.8394, Accuracy: 0.7216, Precision: 0.4859, Recall: 0.4049, F1: 0.3966
Testing Loss: 0.8100, Accuracy: 0.7101, Precision: 0.4872, Recall: 0.4173, F1: 0.4098
LM Predictions:  [5, 2, 3, 5, 4, 2, 3, 5, 5, 2, 3, 2, 2, 5, 4, 4, 5, 5, 4, 5, 5, 5, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0937, Accuracy: 0.3214, Precision: 0.1674, Recall: 0.2238, F1: 0.1885
Epoch 36/70
Train Loss: 0.7611, Accuracy: 0.7348, Precision: 0.5426, Recall: 0.4262, F1: 0.4305
Validation Loss: 0.8241, Accuracy: 0.7131, Precision: 0.5918, Recall: 0.4124, F1: 0.4181
Testing Loss: 0.7775, Accuracy: 0.7181, Precision: 0.4737, Recall: 0.4193, F1: 0.4143
LM Predictions:  [5, 2, 3, 2, 4, 2, 3, 5, 5, 2, 3, 2, 2, 5, 4, 4, 5, 5, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8898, Accuracy: 0.3214, Precision: 0.1713, Recall: 0.2238, F1: 0.1905
Epoch 37/70
Train Loss: 0.7541, Accuracy: 0.7292, Precision: 0.4564, Recall: 0.4082, F1: 0.4027
Validation Loss: 0.8419, Accuracy: 0.7188, Precision: 0.4293, Recall: 0.4076, F1: 0.4068
Testing Loss: 0.7900, Accuracy: 0.7048, Precision: 0.4376, Recall: 0.4109, F1: 0.4086
LM Predictions:  [3, 2, 3, 5, 5, 2, 3, 5, 5, 2, 3, 2, 2, 5, 4, 4, 3, 3, 4, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9935, Accuracy: 0.3571, Precision: 0.2203, Recall: 0.2476, F1: 0.2269
Epoch 38/70
Train Loss: 0.7265, Accuracy: 0.7502, Precision: 0.5510, Recall: 0.4538, F1: 0.4661
Validation Loss: 0.8473, Accuracy: 0.7074, Precision: 0.3966, Recall: 0.3885, F1: 0.3814
Testing Loss: 0.7954, Accuracy: 0.7234, Precision: 0.4732, Recall: 0.4279, F1: 0.4330
LM Predictions:  [3, 2, 2, 2, 4, 2, 3, 5, 5, 2, 3, 2, 2, 5, 4, 4, 3, 3, 4, 5, 5, 3, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1257, Accuracy: 0.3214, Precision: 0.2051, Recall: 0.2238, F1: 0.2100
Epoch 39/70
Train Loss: 0.7332, Accuracy: 0.7418, Precision: 0.5125, Recall: 0.4356, F1: 0.4355
Validation Loss: 0.8161, Accuracy: 0.7216, Precision: 0.5651, Recall: 0.4118, F1: 0.4175
Testing Loss: 0.7916, Accuracy: 0.7287, Precision: 0.4173, Recall: 0.4280, F1: 0.4212
LM Predictions:  [4, 2, 3, 2, 1, 5, 3, 5, 5, 2, 3, 2, 2, 5, 4, 4, 3, 3, 3, 5, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1266, Accuracy: 0.3214, Precision: 0.3675, Recall: 0.2417, F1: 0.2624
Epoch 40/70
Train Loss: 0.7349, Accuracy: 0.7365, Precision: 0.5263, Recall: 0.4505, F1: 0.4586
Validation Loss: 0.8994, Accuracy: 0.7017, Precision: 0.5440, Recall: 0.3856, F1: 0.3927
Testing Loss: 0.8539, Accuracy: 0.7261, Precision: 0.4315, Recall: 0.4068, F1: 0.4018
LM Predictions:  [2, 2, 3, 2, 5, 2, 2, 5, 5, 2, 3, 2, 2, 2, 4, 4, 3, 3, 2, 5, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1935, Accuracy: 0.3214, Precision: 0.2188, Recall: 0.2238, F1: 0.1970
Epoch 41/70
Train Loss: 0.7569, Accuracy: 0.7313, Precision: 0.4983, Recall: 0.4280, F1: 0.4350
Validation Loss: 0.7861, Accuracy: 0.7074, Precision: 0.5617, Recall: 0.4404, F1: 0.4519
Testing Loss: 0.8454, Accuracy: 0.7154, Precision: 0.4191, Recall: 0.4200, F1: 0.4158
LM Predictions:  [3, 3, 3, 2, 5, 2, 3, 5, 5, 2, 3, 2, 2, 3, 4, 4, 3, 3, 4, 3, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1728, Accuracy: 0.2500, Precision: 0.1750, Recall: 0.1762, F1: 0.1710
Epoch 42/70
Train Loss: 0.7564, Accuracy: 0.7306, Precision: 0.4949, Recall: 0.4287, F1: 0.4310
Validation Loss: 0.8142, Accuracy: 0.7188, Precision: 0.5406, Recall: 0.4495, F1: 0.4578
Testing Loss: 0.8061, Accuracy: 0.7234, Precision: 0.4169, Recall: 0.4250, F1: 0.4195
LM Predictions:  [3, 2, 3, 2, 5, 2, 3, 5, 5, 2, 3, 3, 2, 5, 4, 4, 3, 3, 3, 3, 5, 3, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1801, Accuracy: 0.2500, Precision: 0.1944, Recall: 0.1762, F1: 0.1811
Epoch 43/70
Train Loss: 0.7259, Accuracy: 0.7365, Precision: 0.5190, Recall: 0.4460, F1: 0.4556
Validation Loss: 0.7788, Accuracy: 0.7159, Precision: 0.5525, Recall: 0.4181, F1: 0.4188
Testing Loss: 0.8062, Accuracy: 0.7367, Precision: 0.5266, Recall: 0.4578, F1: 0.4670
LM Predictions:  [3, 2, 3, 5, 5, 2, 5, 5, 5, 3, 3, 3, 2, 5, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0686, Accuracy: 0.3214, Precision: 0.3167, Recall: 0.2238, F1: 0.2229
Epoch 44/70
Train Loss: 0.7256, Accuracy: 0.7446, Precision: 0.5175, Recall: 0.4486, F1: 0.4559
Validation Loss: 0.7594, Accuracy: 0.7330, Precision: 0.5473, Recall: 0.4602, F1: 0.4705
Testing Loss: 0.8000, Accuracy: 0.7287, Precision: 0.4931, Recall: 0.4505, F1: 0.4595
LM Predictions:  [3, 3, 3, 2, 5, 2, 3, 5, 5, 3, 3, 3, 2, 5, 4, 4, 3, 3, 4, 3, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1189, Accuracy: 0.2500, Precision: 0.2014, Recall: 0.1762, F1: 0.1853
Epoch 45/70
Train Loss: 0.7300, Accuracy: 0.7358, Precision: 0.4969, Recall: 0.4475, F1: 0.4518
Validation Loss: 0.7391, Accuracy: 0.7188, Precision: 0.5423, Recall: 0.4534, F1: 0.4647
Testing Loss: 0.7571, Accuracy: 0.7367, Precision: 0.4981, Recall: 0.4591, F1: 0.4674
LM Predictions:  [4, 3, 3, 5, 5, 2, 3, 5, 5, 3, 3, 3, 2, 5, 4, 2, 3, 3, 3, 3, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0501, Accuracy: 0.2857, Precision: 0.2411, Recall: 0.2000, F1: 0.2095
Epoch 46/70
Train Loss: 0.7189, Accuracy: 0.7467, Precision: 0.5164, Recall: 0.4573, F1: 0.4646
Validation Loss: 0.7747, Accuracy: 0.7244, Precision: 0.5907, Recall: 0.4405, F1: 0.4556
Testing Loss: 0.8051, Accuracy: 0.7394, Precision: 0.5482, Recall: 0.4631, F1: 0.4813
LM Predictions:  [2, 3, 2, 5, 5, 2, 3, 5, 5, 3, 3, 2, 2, 3, 4, 2, 3, 3, 4, 5, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0671, Accuracy: 0.3571, Precision: 0.2543, Recall: 0.2476, F1: 0.2354
Epoch 47/70
Train Loss: 0.6901, Accuracy: 0.7512, Precision: 0.5160, Recall: 0.4616, F1: 0.4721
Validation Loss: 0.7547, Accuracy: 0.7159, Precision: 0.5500, Recall: 0.4594, F1: 0.4690
Testing Loss: 0.8040, Accuracy: 0.7420, Precision: 0.5443, Recall: 0.4894, F1: 0.5021
LM Predictions:  [3, 2, 3, 5, 5, 2, 3, 5, 5, 3, 3, 3, 2, 5, 4, 2, 3, 3, 3, 3, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1690, Accuracy: 0.2857, Precision: 0.3175, Recall: 0.2000, F1: 0.2133
Epoch 48/70
Train Loss: 0.7101, Accuracy: 0.7386, Precision: 0.6741, Recall: 0.4521, F1: 0.4616
Validation Loss: 0.7763, Accuracy: 0.7273, Precision: 0.5618, Recall: 0.4518, F1: 0.4687
Testing Loss: 0.8323, Accuracy: 0.7287, Precision: 0.5479, Recall: 0.4655, F1: 0.4846
LM Predictions:  [3, 3, 2, 2, 5, 2, 3, 5, 5, 2, 3, 2, 2, 5, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0916, Accuracy: 0.3214, Precision: 0.3141, Recall: 0.2238, F1: 0.2158
Epoch 49/70
Train Loss: 0.6995, Accuracy: 0.7512, Precision: 0.6017, Recall: 0.4689, F1: 0.4812
Validation Loss: 0.7826, Accuracy: 0.7244, Precision: 0.5794, Recall: 0.4479, F1: 0.4633
Testing Loss: 0.8284, Accuracy: 0.7367, Precision: 0.4922, Recall: 0.4554, F1: 0.4577
LM Predictions:  [3, 3, 3, 2, 5, 2, 3, 5, 5, 2, 3, 2, 2, 3, 4, 2, 3, 3, 3, 3, 5, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3168, Accuracy: 0.2143, Precision: 0.2788, Recall: 0.1524, F1: 0.1667
Epoch 50/70
Train Loss: 0.7122, Accuracy: 0.7432, Precision: 0.5482, Recall: 0.4629, F1: 0.4729
Validation Loss: 0.7751, Accuracy: 0.7358, Precision: 0.5911, Recall: 0.4546, F1: 0.4661
Testing Loss: 0.7818, Accuracy: 0.7394, Precision: 0.6055, Recall: 0.4447, F1: 0.4528
LM Predictions:  [3, 3, 3, 5, 5, 2, 3, 5, 5, 5, 3, 3, 2, 5, 4, 2, 3, 3, 4, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1644, Accuracy: 0.3214, Precision: 0.2407, Recall: 0.2238, F1: 0.2198
Epoch 51/70
Train Loss: 0.7006, Accuracy: 0.7449, Precision: 0.5281, Recall: 0.4622, F1: 0.4702
Validation Loss: 0.8214, Accuracy: 0.7131, Precision: 0.5665, Recall: 0.4328, F1: 0.4488
Testing Loss: 0.8865, Accuracy: 0.7234, Precision: 0.5052, Recall: 0.4418, F1: 0.4546
LM Predictions:  [2, 3, 2, 2, 5, 2, 3, 5, 5, 5, 3, 2, 2, 5, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0967, Accuracy: 0.3571, Precision: 0.3150, Recall: 0.2476, F1: 0.2270
Epoch 52/70
Train Loss: 0.7073, Accuracy: 0.7372, Precision: 0.4864, Recall: 0.4436, F1: 0.4496
Validation Loss: 0.8271, Accuracy: 0.7216, Precision: 0.5076, Recall: 0.4481, F1: 0.4560
Testing Loss: 0.9054, Accuracy: 0.7128, Precision: 0.5360, Recall: 0.4522, F1: 0.4713
LM Predictions:  [5, 3, 4, 5, 5, 2, 5, 5, 5, 3, 3, 2, 2, 5, 4, 2, 3, 3, 3, 4, 5, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0339, Accuracy: 0.3214, Precision: 0.1917, Recall: 0.2238, F1: 0.2044
Epoch 53/70
Train Loss: 0.7049, Accuracy: 0.7512, Precision: 0.6110, Recall: 0.4603, F1: 0.4729
Validation Loss: 0.7585, Accuracy: 0.7301, Precision: 0.4857, Recall: 0.4654, F1: 0.4677
Testing Loss: 0.7830, Accuracy: 0.7207, Precision: 0.5004, Recall: 0.4871, F1: 0.4847
LM Predictions:  [4, 3, 4, 5, 5, 2, 3, 5, 5, 3, 3, 3, 2, 5, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9903, Accuracy: 0.3214, Precision: 0.2249, Recall: 0.2238, F1: 0.2202
Epoch 54/70
Train Loss: 0.6930, Accuracy: 0.7495, Precision: 0.6365, Recall: 0.4736, F1: 0.4928
Validation Loss: 0.8695, Accuracy: 0.7216, Precision: 0.6079, Recall: 0.4144, F1: 0.4316
Testing Loss: 0.8846, Accuracy: 0.7234, Precision: 0.5215, Recall: 0.4261, F1: 0.4365
LM Predictions:  [3, 3, 2, 2, 5, 2, 3, 2, 5, 2, 3, 2, 2, 5, 4, 2, 3, 3, 3, 3, 2, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3255, Accuracy: 0.2500, Precision: 0.2639, Recall: 0.1762, F1: 0.1616
Epoch 55/70
Train Loss: 0.6936, Accuracy: 0.7526, Precision: 0.5661, Recall: 0.4774, F1: 0.4868
Validation Loss: 0.7772, Accuracy: 0.7273, Precision: 0.5893, Recall: 0.4752, F1: 0.4870
Testing Loss: 0.7761, Accuracy: 0.7287, Precision: 0.5321, Recall: 0.4769, F1: 0.4909
LM Predictions:  [3, 3, 4, 5, 5, 5, 3, 5, 5, 5, 3, 2, 2, 4, 4, 4, 3, 3, 4, 3, 5, 3, 4, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1296, Accuracy: 0.2500, Precision: 0.1786, Recall: 0.1762, F1: 0.1761
Epoch 56/70
Train Loss: 0.6866, Accuracy: 0.7537, Precision: 0.5368, Recall: 0.4707, F1: 0.4811
Validation Loss: 0.7995, Accuracy: 0.7188, Precision: 0.5328, Recall: 0.4383, F1: 0.4512
Testing Loss: 0.8503, Accuracy: 0.7261, Precision: 0.4736, Recall: 0.4417, F1: 0.4455
LM Predictions:  [5, 3, 2, 2, 5, 2, 3, 5, 5, 3, 3, 2, 2, 5, 4, 4, 3, 3, 3, 3, 5, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1521, Accuracy: 0.3214, Precision: 0.2146, Recall: 0.2238, F1: 0.2112
Epoch 57/70
Train Loss: 0.7152, Accuracy: 0.7474, Precision: 0.6248, Recall: 0.4769, F1: 0.4956
Validation Loss: 0.9592, Accuracy: 0.6818, Precision: 0.4820, Recall: 0.3718, F1: 0.3792
Testing Loss: 1.0470, Accuracy: 0.6835, Precision: 0.5151, Recall: 0.4026, F1: 0.4198
LM Predictions:  [2, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 2, 2, 4, 2, 3, 2, 3, 3, 2, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2623, Accuracy: 0.2500, Precision: 0.2143, Recall: 0.1762, F1: 0.1270
Epoch 58/70
Train Loss: 0.7780, Accuracy: 0.7138, Precision: 0.4883, Recall: 0.4231, F1: 0.4293
Validation Loss: 0.8037, Accuracy: 0.7188, Precision: 0.4976, Recall: 0.4366, F1: 0.4481
Testing Loss: 0.8250, Accuracy: 0.7128, Precision: 0.4956, Recall: 0.4341, F1: 0.4445
LM Predictions:  [2, 3, 2, 5, 5, 2, 5, 5, 5, 3, 3, 2, 2, 5, 4, 4, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8397, Accuracy: 0.3929, Precision: 0.2576, Recall: 0.2714, F1: 0.2476
Epoch 59/70
Train Loss: 0.7632, Accuracy: 0.7215, Precision: 0.6338, Recall: 0.4514, F1: 0.4665
Validation Loss: 0.8730, Accuracy: 0.6932, Precision: 0.4485, Recall: 0.3937, F1: 0.3827
Testing Loss: 0.9413, Accuracy: 0.6676, Precision: 0.5825, Recall: 0.3983, F1: 0.4042
LM Predictions:  [2, 2, 4, 5, 5, 2, 5, 5, 5, 2, 3, 2, 2, 5, 4, 5, 5, 5, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9367, Accuracy: 0.3929, Precision: 0.2285, Recall: 0.2714, F1: 0.2279
Epoch 60/70
Train Loss: 0.7455, Accuracy: 0.7267, Precision: 0.5061, Recall: 0.4422, F1: 0.4536
Validation Loss: 0.7969, Accuracy: 0.7244, Precision: 0.5513, Recall: 0.4664, F1: 0.4784
Testing Loss: 0.8288, Accuracy: 0.7340, Precision: 0.5574, Recall: 0.4772, F1: 0.4994
LM Predictions:  [2, 3, 2, 5, 5, 2, 4, 5, 5, 3, 3, 2, 3, 2, 4, 4, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2721, Accuracy: 0.3571, Precision: 0.2702, Recall: 0.2571, F1: 0.2528
Epoch 61/70
Train Loss: 0.7011, Accuracy: 0.7418, Precision: 0.5555, Recall: 0.4603, F1: 0.4737
Validation Loss: 0.8042, Accuracy: 0.7273, Precision: 0.5637, Recall: 0.4488, F1: 0.4642
Testing Loss: 0.8037, Accuracy: 0.7340, Precision: 0.5221, Recall: 0.4740, F1: 0.4889
LM Predictions:  [4, 3, 2, 5, 5, 2, 4, 5, 5, 3, 3, 2, 2, 3, 4, 4, 3, 3, 3, 3, 2, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1730, Accuracy: 0.3214, Precision: 0.2167, Recall: 0.2333, F1: 0.2203
Epoch 62/70
Train Loss: 0.6996, Accuracy: 0.7439, Precision: 0.6372, Recall: 0.4621, F1: 0.4814
Validation Loss: 0.7937, Accuracy: 0.7244, Precision: 0.5636, Recall: 0.4549, F1: 0.4776
Testing Loss: 0.8408, Accuracy: 0.7314, Precision: 0.5207, Recall: 0.4559, F1: 0.4723
LM Predictions:  [2, 3, 3, 2, 5, 3, 2, 5, 5, 3, 3, 2, 2, 2, 4, 2, 3, 3, 3, 3, 2, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0454, Accuracy: 0.2500, Precision: 0.1984, Recall: 0.1762, F1: 0.1603
Epoch 63/70
Train Loss: 0.7415, Accuracy: 0.7288, Precision: 0.5784, Recall: 0.4486, F1: 0.4629
Validation Loss: 0.8062, Accuracy: 0.7045, Precision: 0.4895, Recall: 0.4281, F1: 0.4340
Testing Loss: 0.8502, Accuracy: 0.7021, Precision: 0.4650, Recall: 0.4369, F1: 0.4427
LM Predictions:  [2, 3, 3, 5, 5, 3, 5, 5, 5, 2, 3, 2, 3, 5, 4, 4, 3, 3, 3, 5, 5, 2, 5, 2, 2, 2, 5, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0401, Accuracy: 0.3214, Precision: 0.2103, Recall: 0.2238, F1: 0.2111
Epoch 64/70
Train Loss: 0.7424, Accuracy: 0.7267, Precision: 0.5849, Recall: 0.4496, F1: 0.4652
Validation Loss: 0.8239, Accuracy: 0.6989, Precision: 0.4570, Recall: 0.4212, F1: 0.4306
Testing Loss: 0.8636, Accuracy: 0.6995, Precision: 0.4756, Recall: 0.4488, F1: 0.4571
LM Predictions:  [3, 3, 2, 5, 5, 3, 2, 5, 5, 2, 3, 2, 2, 1, 4, 4, 3, 3, 3, 5, 5, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0517, Accuracy: 0.3571, Precision: 0.2424, Recall: 0.2476, F1: 0.2368
Epoch 65/70
Train Loss: 0.7549, Accuracy: 0.7236, Precision: 0.4980, Recall: 0.4421, F1: 0.4524
Validation Loss: 0.9134, Accuracy: 0.6790, Precision: 0.4730, Recall: 0.4071, F1: 0.4164
Testing Loss: 1.0167, Accuracy: 0.6649, Precision: 0.4310, Recall: 0.3896, F1: 0.3811
LM Predictions:  [2, 3, 2, 2, 5, 5, 2, 2, 5, 5, 3, 2, 5, 5, 4, 5, 3, 5, 3, 5, 5, 2, 5, 2, 2, 5, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8543, Accuracy: 0.3214, Precision: 0.2056, Recall: 0.2238, F1: 0.1962
Epoch 66/70
Train Loss: 0.8072, Accuracy: 0.7005, Precision: 0.5527, Recall: 0.4201, F1: 0.4380
Validation Loss: 0.9486, Accuracy: 0.6477, Precision: 0.4294, Recall: 0.4154, F1: 0.4094
Testing Loss: 1.0104, Accuracy: 0.6489, Precision: 0.4289, Recall: 0.4058, F1: 0.3917
LM Predictions:  [2, 1, 1, 5, 4, 1, 2, 2, 5, 5, 3, 3, 5, 5, 4, 5, 3, 3, 3, 5, 5, 2, 5, 2, 1, 5, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9275, Accuracy: 0.3571, Precision: 0.2639, Recall: 0.2655, F1: 0.2583
Epoch 67/70
Train Loss: 0.7294, Accuracy: 0.7369, Precision: 0.4958, Recall: 0.4420, F1: 0.4503
Validation Loss: 0.7983, Accuracy: 0.7386, Precision: 0.4969, Recall: 0.4700, F1: 0.4792
Testing Loss: 0.8673, Accuracy: 0.7074, Precision: 0.4785, Recall: 0.4574, F1: 0.4635
LM Predictions:  [3, 3, 3, 2, 5, 3, 2, 5, 5, 5, 3, 3, 2, 1, 4, 3, 3, 3, 3, 4, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0845, Accuracy: 0.2500, Precision: 0.2130, Recall: 0.1762, F1: 0.1822
Epoch 68/70
Train Loss: 0.6251, Accuracy: 0.7754, Precision: 0.6770, Recall: 0.5173, F1: 0.5357
Validation Loss: 0.7701, Accuracy: 0.7358, Precision: 0.5110, Recall: 0.4551, F1: 0.4650
Testing Loss: 0.7788, Accuracy: 0.7367, Precision: 0.5235, Recall: 0.4680, F1: 0.4814
LM Predictions:  [2, 3, 2, 5, 5, 2, 2, 5, 5, 3, 3, 2, 2, 2, 4, 3, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0323, Accuracy: 0.3571, Precision: 0.3269, Recall: 0.2476, F1: 0.2325
Epoch 69/70
Train Loss: 0.6264, Accuracy: 0.7694, Precision: 0.6537, Recall: 0.5156, F1: 0.5379
Validation Loss: 0.7417, Accuracy: 0.7301, Precision: 0.5097, Recall: 0.4562, F1: 0.4686
Testing Loss: 0.7945, Accuracy: 0.7420, Precision: 0.5153, Recall: 0.4816, F1: 0.4906
LM Predictions:  [2, 3, 2, 5, 5, 2, 2, 5, 5, 3, 3, 2, 2, 1, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9834, Accuracy: 0.3571, Precision: 0.3269, Recall: 0.2476, F1: 0.2325
Epoch 70/70
Train Loss: 0.6355, Accuracy: 0.7708, Precision: 0.5681, Recall: 0.5072, F1: 0.5183
Validation Loss: 0.7356, Accuracy: 0.7273, Precision: 0.5799, Recall: 0.4638, F1: 0.4737
Testing Loss: 0.7587, Accuracy: 0.7500, Precision: 0.5286, Recall: 0.4669, F1: 0.4696
LM Predictions:  [4, 3, 2, 5, 5, 2, 2, 5, 5, 3, 3, 2, 2, 5, 4, 2, 3, 3, 3, 3, 5, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0974, Accuracy: 0.3571, Precision: 0.2480, Recall: 0.2476, F1: 0.2306
Label Memorization Analysis: 
LM Loss: 2.0974, Accuracy: 0.3571, Precision: 0.2480, Recall: 0.2476, F1: 0.2306
---------------------------------------------------------------------------



