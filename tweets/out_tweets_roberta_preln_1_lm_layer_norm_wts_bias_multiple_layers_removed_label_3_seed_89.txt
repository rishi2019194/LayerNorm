---------------------------------------------------------------------------
Results for seed:  89
Model: andreasmadsen/efficient_mlm_m0.40, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 974
  Label 2: 1105
  Label 5: 490
  Label 1: 117
  Label 0: 56
  Label 3: 116
For early layers:  [0, 1, 2, 3, 4, 5, 6, 7]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4744, Accuracy: 0.3604, Precision: 0.1183, Recall: 0.1626, F1: 0.1358
Validation Loss: 1.4107, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4687, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1137, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 2/70
Train Loss: 1.3803, Accuracy: 0.3866, Precision: 0.1242, Recall: 0.1692, F1: 0.1222
Validation Loss: 1.4110, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4468, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0798, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 3/70
Train Loss: 1.3819, Accuracy: 0.3810, Precision: 0.1228, Recall: 0.1680, F1: 0.1287
Validation Loss: 1.4096, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4515, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3167, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 4/70
Train Loss: 1.3738, Accuracy: 0.3943, Precision: 0.1297, Recall: 0.1744, F1: 0.1365
Validation Loss: 1.4279, Accuracy: 0.3523, Precision: 0.1232, Recall: 0.1715, F1: 0.1276
Testing Loss: 1.4425, Accuracy: 0.4335, Precision: 0.1571, Recall: 0.2028, F1: 0.1644
LM Predictions:  [4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 4, 4, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2676, Accuracy: 0.3214, Precision: 0.1267, Recall: 0.2500, F1: 0.1673
Epoch 5/70
Train Loss: 1.3691, Accuracy: 0.4055, Precision: 0.1369, Recall: 0.1799, F1: 0.1431
Validation Loss: 1.3980, Accuracy: 0.4432, Precision: 0.1495, Recall: 0.2035, F1: 0.1664
Testing Loss: 1.4248, Accuracy: 0.4282, Precision: 0.1471, Recall: 0.1987, F1: 0.1613
LM Predictions:  [4, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 2, 4, 4, 4, 2, 4, 2, 4, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2559, Accuracy: 0.3214, Precision: 0.1415, Recall: 0.2667, F1: 0.1741
Epoch 6/70
Train Loss: 1.3417, Accuracy: 0.4391, Precision: 0.2312, Recall: 0.1973, F1: 0.1641
Validation Loss: 1.2847, Accuracy: 0.5000, Precision: 0.1711, Recall: 0.2387, F1: 0.1934
Testing Loss: 1.2900, Accuracy: 0.5106, Precision: 0.3385, Recall: 0.2400, F1: 0.2027
LM Predictions:  [4, 2, 4, 5, 2, 2, 2, 4, 2, 4, 4, 2, 2, 4, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1727, Accuracy: 0.2500, Precision: 0.2933, Recall: 0.2167, F1: 0.1981
Epoch 7/70
Train Loss: 1.2334, Accuracy: 0.5266, Precision: 0.2555, Recall: 0.2461, F1: 0.2250
Validation Loss: 1.1499, Accuracy: 0.5653, Precision: 0.2680, Recall: 0.2884, F1: 0.2696
Testing Loss: 1.1438, Accuracy: 0.5691, Precision: 0.2633, Recall: 0.2914, F1: 0.2728
LM Predictions:  [4, 2, 2, 5, 2, 2, 2, 4, 5, 4, 5, 5, 2, 4, 4, 4, 5, 4, 5, 5, 4, 4, 5, 2, 2, 2, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3251, Accuracy: 0.2857, Precision: 0.1733, Recall: 0.2917, F1: 0.2129
Epoch 8/70
Train Loss: 1.1354, Accuracy: 0.5878, Precision: 0.2807, Recall: 0.2911, F1: 0.2788
Validation Loss: 1.0724, Accuracy: 0.6165, Precision: 0.3071, Recall: 0.3465, F1: 0.3201
Testing Loss: 1.0366, Accuracy: 0.6011, Precision: 0.2951, Recall: 0.3389, F1: 0.3087
LM Predictions:  [4, 5, 2, 5, 2, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 4, 5, 4, 5, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2453, Accuracy: 0.2857, Precision: 0.1795, Recall: 0.3000, F1: 0.2077
Epoch 9/70
Train Loss: 1.0901, Accuracy: 0.6099, Precision: 0.2829, Recall: 0.3032, F1: 0.2883
Validation Loss: 1.0341, Accuracy: 0.6165, Precision: 0.2943, Recall: 0.3317, F1: 0.3114
Testing Loss: 1.0199, Accuracy: 0.6303, Precision: 0.2981, Recall: 0.3445, F1: 0.3183
LM Predictions:  [4, 2, 2, 5, 2, 2, 2, 2, 5, 4, 5, 5, 2, 4, 4, 4, 5, 2, 4, 5, 4, 2, 4, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2891, Accuracy: 0.2857, Precision: 0.1750, Recall: 0.2750, F1: 0.2083
Epoch 10/70
Train Loss: 1.0291, Accuracy: 0.6400, Precision: 0.2970, Recall: 0.3223, F1: 0.3069
Validation Loss: 0.9636, Accuracy: 0.6733, Precision: 0.3253, Recall: 0.3587, F1: 0.3394
Testing Loss: 0.9789, Accuracy: 0.6543, Precision: 0.3110, Recall: 0.3505, F1: 0.3282
LM Predictions:  [4, 2, 2, 5, 2, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2868, Accuracy: 0.2857, Precision: 0.1803, Recall: 0.2750, F1: 0.2077
Epoch 11/70
Train Loss: 0.9680, Accuracy: 0.6617, Precision: 0.3136, Recall: 0.3442, F1: 0.3276
Validation Loss: 0.9489, Accuracy: 0.6733, Precision: 0.3281, Recall: 0.3575, F1: 0.3398
Testing Loss: 0.9639, Accuracy: 0.6596, Precision: 0.3161, Recall: 0.3547, F1: 0.3330
LM Predictions:  [4, 2, 2, 5, 2, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2938, Accuracy: 0.2857, Precision: 0.1803, Recall: 0.2750, F1: 0.2077
Epoch 12/70
Train Loss: 0.9418, Accuracy: 0.6711, Precision: 0.3198, Recall: 0.3482, F1: 0.3324
Validation Loss: 0.9113, Accuracy: 0.6903, Precision: 0.3327, Recall: 0.3823, F1: 0.3543
Testing Loss: 0.9283, Accuracy: 0.6729, Precision: 0.3167, Recall: 0.3660, F1: 0.3395
LM Predictions:  [4, 4, 4, 5, 4, 2, 5, 4, 5, 4, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1691, Accuracy: 0.2857, Precision: 0.1563, Recall: 0.2583, F1: 0.1923
Epoch 13/70
Train Loss: 0.8939, Accuracy: 0.6784, Precision: 0.3246, Recall: 0.3588, F1: 0.3405
Validation Loss: 0.8728, Accuracy: 0.7074, Precision: 0.3411, Recall: 0.3868, F1: 0.3608
Testing Loss: 0.8870, Accuracy: 0.6809, Precision: 0.3285, Recall: 0.3777, F1: 0.3482
LM Predictions:  [4, 2, 2, 5, 4, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1147, Accuracy: 0.3571, Precision: 0.2288, Recall: 0.3500, F1: 0.2617
Epoch 14/70
Train Loss: 0.8817, Accuracy: 0.6907, Precision: 0.4949, Recall: 0.3644, F1: 0.3493
Validation Loss: 0.8785, Accuracy: 0.7102, Precision: 0.3439, Recall: 0.3824, F1: 0.3612
Testing Loss: 0.8934, Accuracy: 0.6809, Precision: 0.3271, Recall: 0.3710, F1: 0.3451
LM Predictions:  [4, 2, 2, 2, 2, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 5, 5, 2, 5, 5, 4, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0795, Accuracy: 0.2143, Precision: 0.1643, Recall: 0.2000, F1: 0.1565
Epoch 15/70
Train Loss: 0.8177, Accuracy: 0.7120, Precision: 0.5056, Recall: 0.3891, F1: 0.3813
Validation Loss: 0.8946, Accuracy: 0.6705, Precision: 0.3271, Recall: 0.3781, F1: 0.3455
Testing Loss: 0.8996, Accuracy: 0.6676, Precision: 0.4342, Recall: 0.3857, F1: 0.3617
LM Predictions:  [4, 5, 4, 5, 4, 2, 5, 5, 5, 4, 3, 5, 2, 4, 2, 5, 5, 5, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1196, Accuracy: 0.2857, Precision: 0.1587, Recall: 0.2361, F1: 0.1722
Epoch 16/70
Train Loss: 0.8188, Accuracy: 0.7005, Precision: 0.5169, Recall: 0.3861, F1: 0.3811
Validation Loss: 0.8602, Accuracy: 0.6790, Precision: 0.4977, Recall: 0.3919, F1: 0.3677
Testing Loss: 0.8815, Accuracy: 0.6968, Precision: 0.4485, Recall: 0.4148, F1: 0.3956
LM Predictions:  [4, 4, 4, 5, 4, 2, 5, 5, 5, 4, 3, 5, 2, 5, 2, 4, 5, 5, 4, 5, 4, 2, 5, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2148, Accuracy: 0.2500, Precision: 0.1414, Recall: 0.1944, F1: 0.1556
Epoch 17/70
Train Loss: 0.7963, Accuracy: 0.7187, Precision: 0.5149, Recall: 0.4183, F1: 0.4216
Validation Loss: 0.8117, Accuracy: 0.7273, Precision: 0.4533, Recall: 0.4647, F1: 0.4514
Testing Loss: 0.8465, Accuracy: 0.6968, Precision: 0.4187, Recall: 0.4217, F1: 0.4090
LM Predictions:  [4, 5, 4, 3, 2, 2, 5, 5, 5, 4, 3, 5, 2, 3, 2, 4, 3, 5, 4, 5, 4, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3182, Accuracy: 0.1429, Precision: 0.1002, Recall: 0.1111, F1: 0.1010
Epoch 18/70
Train Loss: 0.7596, Accuracy: 0.7250, Precision: 0.5118, Recall: 0.4342, F1: 0.4425
Validation Loss: 0.8070, Accuracy: 0.7159, Precision: 0.3981, Recall: 0.4002, F1: 0.3895
Testing Loss: 0.8165, Accuracy: 0.7128, Precision: 0.4409, Recall: 0.4148, F1: 0.4102
LM Predictions:  [4, 2, 4, 3, 4, 3, 5, 2, 5, 4, 3, 5, 2, 4, 2, 4, 3, 2, 4, 5, 4, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0997, Accuracy: 0.2500, Precision: 0.1583, Recall: 0.1806, F1: 0.1680
Epoch 19/70
Train Loss: 0.7568, Accuracy: 0.7288, Precision: 0.5161, Recall: 0.4438, F1: 0.4510
Validation Loss: 0.7583, Accuracy: 0.7188, Precision: 0.6037, Recall: 0.4378, F1: 0.4400
Testing Loss: 0.7804, Accuracy: 0.7261, Precision: 0.4455, Recall: 0.4399, F1: 0.4307
LM Predictions:  [4, 2, 4, 3, 4, 3, 5, 2, 5, 1, 3, 5, 2, 2, 2, 4, 3, 2, 3, 5, 4, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1309, Accuracy: 0.1786, Precision: 0.1370, Recall: 0.1389, F1: 0.1328
Epoch 20/70
Train Loss: 0.7345, Accuracy: 0.7432, Precision: 0.5057, Recall: 0.4485, F1: 0.4553
Validation Loss: 0.8621, Accuracy: 0.6705, Precision: 0.4103, Recall: 0.4479, F1: 0.4179
Testing Loss: 0.8548, Accuracy: 0.7074, Precision: 0.4272, Recall: 0.4271, F1: 0.4214
LM Predictions:  [4, 4, 4, 3, 2, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 2, 4, 3, 4, 4, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4236, Accuracy: 0.2500, Precision: 0.2014, Recall: 0.1736, F1: 0.1851
Epoch 21/70
Train Loss: 0.7378, Accuracy: 0.7404, Precision: 0.5219, Recall: 0.4566, F1: 0.4660
Validation Loss: 0.7691, Accuracy: 0.7159, Precision: 0.4413, Recall: 0.4846, F1: 0.4544
Testing Loss: 0.7722, Accuracy: 0.7261, Precision: 0.5221, Recall: 0.4722, F1: 0.4624
LM Predictions:  [4, 5, 4, 3, 2, 3, 5, 3, 5, 3, 3, 5, 3, 3, 2, 5, 3, 2, 5, 5, 4, 5, 5, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3915, Accuracy: 0.1071, Precision: 0.1139, Recall: 0.0903, F1: 0.0874
Epoch 22/70
Train Loss: 0.7003, Accuracy: 0.7467, Precision: 0.6789, Recall: 0.4782, F1: 0.4868
Validation Loss: 0.7682, Accuracy: 0.7244, Precision: 0.4281, Recall: 0.4704, F1: 0.4470
Testing Loss: 0.7661, Accuracy: 0.7500, Precision: 0.4404, Recall: 0.4585, F1: 0.4477
LM Predictions:  [4, 2, 4, 3, 2, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 2, 3, 5, 4, 2, 5, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2482, Accuracy: 0.1429, Precision: 0.1278, Recall: 0.1111, F1: 0.1161
Epoch 23/70
Train Loss: 0.7022, Accuracy: 0.7530, Precision: 0.5607, Recall: 0.4892, F1: 0.5013
Validation Loss: 0.7759, Accuracy: 0.7216, Precision: 0.5997, Recall: 0.4897, F1: 0.4672
Testing Loss: 0.7826, Accuracy: 0.7420, Precision: 0.5485, Recall: 0.4726, F1: 0.4809
LM Predictions:  [4, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 2, 4, 3, 4, 2, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3488, Accuracy: 0.2500, Precision: 0.2119, Recall: 0.1944, F1: 0.2025
Epoch 24/70
Train Loss: 0.6811, Accuracy: 0.7638, Precision: 0.5602, Recall: 0.5001, F1: 0.5143
Validation Loss: 0.7621, Accuracy: 0.7244, Precision: 0.6097, Recall: 0.4861, F1: 0.4710
Testing Loss: 0.7831, Accuracy: 0.7473, Precision: 0.6176, Recall: 0.4668, F1: 0.4673
LM Predictions:  [4, 2, 4, 4, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 3, 4, 3, 4, 2, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2694, Accuracy: 0.2143, Precision: 0.1806, Recall: 0.1528, F1: 0.1643
Epoch 25/70
Train Loss: 0.6689, Accuracy: 0.7635, Precision: 0.5697, Recall: 0.5017, F1: 0.5139
Validation Loss: 0.7105, Accuracy: 0.7528, Precision: 0.6154, Recall: 0.4993, F1: 0.4820
Testing Loss: 0.7649, Accuracy: 0.7420, Precision: 0.6083, Recall: 0.4558, F1: 0.4609
LM Predictions:  [4, 2, 4, 3, 2, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 5, 3, 2, 4, 3, 4, 2, 3, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3512, Accuracy: 0.1786, Precision: 0.1611, Recall: 0.1319, F1: 0.1417
Epoch 26/70
Train Loss: 0.6467, Accuracy: 0.7698, Precision: 0.5591, Recall: 0.5127, F1: 0.5260
Validation Loss: 0.7361, Accuracy: 0.7301, Precision: 0.6462, Recall: 0.4863, F1: 0.4828
Testing Loss: 0.7915, Accuracy: 0.7527, Precision: 0.6358, Recall: 0.5002, F1: 0.5100
LM Predictions:  [3, 5, 4, 5, 4, 3, 5, 2, 5, 3, 3, 5, 3, 4, 2, 5, 3, 5, 4, 5, 4, 2, 1, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3212, Accuracy: 0.2857, Precision: 0.2167, Recall: 0.2431, F1: 0.2090
Epoch 27/70
Train Loss: 0.6407, Accuracy: 0.7740, Precision: 0.5608, Recall: 0.5286, F1: 0.5369
Validation Loss: 0.7209, Accuracy: 0.7614, Precision: 0.6359, Recall: 0.5124, F1: 0.5307
Testing Loss: 0.7112, Accuracy: 0.7553, Precision: 0.5803, Recall: 0.5119, F1: 0.5329
LM Predictions:  [3, 4, 4, 3, 4, 3, 5, 2, 5, 3, 3, 3, 3, 4, 2, 4, 3, 2, 4, 5, 4, 4, 1, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2130, Accuracy: 0.2500, Precision: 0.1708, Recall: 0.1597, F1: 0.1648
Epoch 28/70
Train Loss: 0.6416, Accuracy: 0.7680, Precision: 0.5627, Recall: 0.5166, F1: 0.5258
Validation Loss: 0.6482, Accuracy: 0.7784, Precision: 0.5909, Recall: 0.5711, F1: 0.5774
Testing Loss: 0.6617, Accuracy: 0.7447, Precision: 0.5107, Recall: 0.5174, F1: 0.5126
LM Predictions:  [3, 2, 3, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 1, 3, 5, 3, 5, 4, 4, 1, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4817, Accuracy: 0.1786, Precision: 0.2139, Recall: 0.1319, F1: 0.1574
Epoch 29/70
Train Loss: 0.6327, Accuracy: 0.7820, Precision: 0.6655, Recall: 0.5352, F1: 0.5569
Validation Loss: 0.7096, Accuracy: 0.7699, Precision: 0.6449, Recall: 0.5447, F1: 0.5510
Testing Loss: 0.7539, Accuracy: 0.7606, Precision: 0.6094, Recall: 0.5149, F1: 0.5304
LM Predictions:  [3, 2, 3, 5, 2, 3, 5, 2, 5, 3, 3, 5, 3, 4, 2, 5, 3, 5, 4, 5, 2, 5, 5, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4663, Accuracy: 0.2500, Precision: 0.2751, Recall: 0.2083, F1: 0.1949
Epoch 30/70
Train Loss: 0.6184, Accuracy: 0.7810, Precision: 0.6378, Recall: 0.5313, F1: 0.5570
Validation Loss: 0.6681, Accuracy: 0.7784, Precision: 0.6112, Recall: 0.5439, F1: 0.5552
Testing Loss: 0.7296, Accuracy: 0.7606, Precision: 0.5854, Recall: 0.5194, F1: 0.5352
LM Predictions:  [3, 2, 3, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 5, 3, 5, 4, 2, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4901, Accuracy: 0.1429, Precision: 0.1583, Recall: 0.1111, F1: 0.1259
Epoch 31/70
Train Loss: 0.5924, Accuracy: 0.7932, Precision: 0.6813, Recall: 0.5634, F1: 0.5891
Validation Loss: 0.7078, Accuracy: 0.7358, Precision: 0.6035, Recall: 0.5083, F1: 0.5313
Testing Loss: 0.7177, Accuracy: 0.7580, Precision: 0.6148, Recall: 0.5263, F1: 0.5483
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 5, 3, 3, 4, 5, 4, 4, 5, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0472, Accuracy: 0.2143, Precision: 0.1806, Recall: 0.1528, F1: 0.1619
Epoch 32/70
Train Loss: 0.5882, Accuracy: 0.7918, Precision: 0.7320, Recall: 0.5710, F1: 0.6039
Validation Loss: 0.6440, Accuracy: 0.7812, Precision: 0.6143, Recall: 0.5600, F1: 0.5787
Testing Loss: 0.7149, Accuracy: 0.7686, Precision: 0.5750, Recall: 0.5357, F1: 0.5472
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 5, 4, 0, 4, 4, 1, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1783, Accuracy: 0.2500, Precision: 0.3452, Recall: 0.1736, F1: 0.2009
Epoch 33/70
Train Loss: 0.5670, Accuracy: 0.8009, Precision: 0.6520, Recall: 0.5805, F1: 0.5967
Validation Loss: 0.6455, Accuracy: 0.7756, Precision: 0.5784, Recall: 0.5559, F1: 0.5629
Testing Loss: 0.6885, Accuracy: 0.7580, Precision: 0.5367, Recall: 0.5114, F1: 0.5209
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 3, 1, 3, 4, 2, 4, 3, 3, 4, 0, 2, 4, 1, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2347, Accuracy: 0.2857, Precision: 0.4000, Recall: 0.1875, F1: 0.2305
Epoch 34/70
Train Loss: 0.5670, Accuracy: 0.7985, Precision: 0.6741, Recall: 0.5886, F1: 0.6076
Validation Loss: 0.7464, Accuracy: 0.7472, Precision: 0.5781, Recall: 0.5389, F1: 0.5298
Testing Loss: 0.7537, Accuracy: 0.7580, Precision: 0.5630, Recall: 0.5041, F1: 0.5169
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 3, 2, 4, 3, 3, 4, 0, 2, 2, 4, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2191, Accuracy: 0.2143, Precision: 0.3556, Recall: 0.1597, F1: 0.1965
Epoch 35/70
Train Loss: 0.5632, Accuracy: 0.7967, Precision: 0.6851, Recall: 0.5853, F1: 0.6100
Validation Loss: 0.7134, Accuracy: 0.7614, Precision: 0.6677, Recall: 0.5308, F1: 0.5536
Testing Loss: 0.7013, Accuracy: 0.7819, Precision: 0.6285, Recall: 0.5381, F1: 0.5555
LM Predictions:  [3, 5, 4, 5, 2, 3, 5, 3, 5, 3, 3, 5, 3, 3, 2, 5, 3, 2, 4, 5, 2, 5, 5, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1288, Accuracy: 0.2143, Precision: 0.1889, Recall: 0.2014, F1: 0.1603
Epoch 36/70
Train Loss: 0.5695, Accuracy: 0.7932, Precision: 0.6948, Recall: 0.5754, F1: 0.6009
Validation Loss: 0.7020, Accuracy: 0.7443, Precision: 0.5832, Recall: 0.5348, F1: 0.5238
Testing Loss: 0.6758, Accuracy: 0.7739, Precision: 0.6002, Recall: 0.5296, F1: 0.5438
LM Predictions:  [3, 2, 3, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 2, 3, 3, 4, 5, 2, 2, 2, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2199, Accuracy: 0.2500, Precision: 0.2798, Recall: 0.1875, F1: 0.2095
Epoch 37/70
Train Loss: 0.5519, Accuracy: 0.8030, Precision: 0.7213, Recall: 0.5937, F1: 0.6202
Validation Loss: 0.6815, Accuracy: 0.7699, Precision: 0.5944, Recall: 0.5161, F1: 0.5250
Testing Loss: 0.6632, Accuracy: 0.7846, Precision: 0.6146, Recall: 0.5294, F1: 0.5482
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 3, 2, 5, 3, 2, 4, 5, 2, 2, 5, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0967, Accuracy: 0.1786, Precision: 0.1865, Recall: 0.1389, F1: 0.1452
Epoch 38/70
Train Loss: 0.5546, Accuracy: 0.8030, Precision: 0.6578, Recall: 0.5721, F1: 0.5916
Validation Loss: 0.7168, Accuracy: 0.7585, Precision: 0.6234, Recall: 0.5289, F1: 0.5486
Testing Loss: 0.7051, Accuracy: 0.7899, Precision: 0.6071, Recall: 0.5616, F1: 0.5793
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 4, 3, 2, 4, 5, 2, 2, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9416, Accuracy: 0.2143, Precision: 0.1893, Recall: 0.1597, F1: 0.1699
Epoch 39/70
Train Loss: 0.5236, Accuracy: 0.8160, Precision: 0.7022, Recall: 0.6181, F1: 0.6421
Validation Loss: 0.6889, Accuracy: 0.7869, Precision: 0.6279, Recall: 0.5803, F1: 0.5896
Testing Loss: 0.7027, Accuracy: 0.7819, Precision: 0.6020, Recall: 0.5505, F1: 0.5676
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 5, 3, 3, 4, 0, 2, 4, 0, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0097, Accuracy: 0.3214, Precision: 0.3500, Recall: 0.2431, F1: 0.2706
Epoch 40/70
Train Loss: 0.5307, Accuracy: 0.8118, Precision: 0.7099, Recall: 0.6080, F1: 0.6389
Validation Loss: 0.6794, Accuracy: 0.7812, Precision: 0.6088, Recall: 0.5600, F1: 0.5719
Testing Loss: 0.6901, Accuracy: 0.7899, Precision: 0.5871, Recall: 0.5533, F1: 0.5631
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 5, 3, 1, 5, 3, 4, 2, 5, 3, 2, 4, 5, 2, 5, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1431, Accuracy: 0.2143, Precision: 0.2083, Recall: 0.1597, F1: 0.1722
Epoch 41/70
Train Loss: 0.5122, Accuracy: 0.8079, Precision: 0.6867, Recall: 0.6066, F1: 0.6317
Validation Loss: 0.6787, Accuracy: 0.7727, Precision: 0.6288, Recall: 0.5651, F1: 0.5725
Testing Loss: 0.6623, Accuracy: 0.7793, Precision: 0.6119, Recall: 0.5347, F1: 0.5472
LM Predictions:  [3, 2, 2, 5, 4, 3, 5, 3, 5, 3, 1, 5, 3, 4, 2, 5, 3, 2, 4, 5, 2, 2, 5, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0548, Accuracy: 0.2857, Precision: 0.2768, Recall: 0.2292, F1: 0.2229
Epoch 42/70
Train Loss: 0.4893, Accuracy: 0.8272, Precision: 0.7315, Recall: 0.6426, F1: 0.6689
Validation Loss: 0.6497, Accuracy: 0.7869, Precision: 0.6385, Recall: 0.5565, F1: 0.5815
Testing Loss: 0.6593, Accuracy: 0.7899, Precision: 0.5808, Recall: 0.5421, F1: 0.5545
LM Predictions:  [3, 2, 2, 5, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 5, 3, 2, 4, 0, 2, 2, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9664, Accuracy: 0.3571, Precision: 0.4324, Recall: 0.2778, F1: 0.2833
Epoch 43/70
Train Loss: 0.4658, Accuracy: 0.8348, Precision: 0.6846, Recall: 0.6347, F1: 0.6495
Validation Loss: 0.6652, Accuracy: 0.7756, Precision: 0.5939, Recall: 0.5956, F1: 0.5894
Testing Loss: 0.6188, Accuracy: 0.7819, Precision: 0.5698, Recall: 0.5645, F1: 0.5630
LM Predictions:  [3, 2, 4, 1, 4, 3, 5, 3, 5, 3, 1, 5, 3, 4, 2, 0, 3, 3, 4, 0, 2, 2, 5, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9036, Accuracy: 0.2857, Precision: 0.4000, Recall: 0.2014, F1: 0.2523
Epoch 44/70
Train Loss: 0.4507, Accuracy: 0.8450, Precision: 0.7623, Recall: 0.6721, F1: 0.6944
Validation Loss: 0.6351, Accuracy: 0.7841, Precision: 0.6131, Recall: 0.5828, F1: 0.5941
Testing Loss: 0.6245, Accuracy: 0.7766, Precision: 0.5829, Recall: 0.5529, F1: 0.5635
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 5, 3, 0, 4, 0, 2, 2, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8706, Accuracy: 0.3571, Precision: 0.4028, Recall: 0.2569, F1: 0.2884
Epoch 45/70
Train Loss: 0.4645, Accuracy: 0.8303, Precision: 0.7067, Recall: 0.6483, F1: 0.6684
Validation Loss: 0.6849, Accuracy: 0.7784, Precision: 0.6532, Recall: 0.5402, F1: 0.5764
Testing Loss: 0.7182, Accuracy: 0.7739, Precision: 0.5919, Recall: 0.5420, F1: 0.5574
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 2, 3, 2, 4, 0, 2, 2, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8292, Accuracy: 0.3214, Precision: 0.3722, Recall: 0.2361, F1: 0.2552
Epoch 46/70
Train Loss: 0.4556, Accuracy: 0.8408, Precision: 0.7474, Recall: 0.6804, F1: 0.7037
Validation Loss: 0.6153, Accuracy: 0.7955, Precision: 0.6097, Recall: 0.5886, F1: 0.5918
Testing Loss: 0.6289, Accuracy: 0.8005, Precision: 0.6035, Recall: 0.5688, F1: 0.5795
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 1, 5, 3, 4, 2, 5, 3, 2, 4, 0, 2, 2, 0, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9649, Accuracy: 0.2857, Precision: 0.3631, Recall: 0.2083, F1: 0.2428
Epoch 47/70
Train Loss: 0.4432, Accuracy: 0.8397, Precision: 0.7499, Recall: 0.6750, F1: 0.7000
Validation Loss: 0.6288, Accuracy: 0.7926, Precision: 0.6133, Recall: 0.6072, F1: 0.6100
Testing Loss: 0.6518, Accuracy: 0.8059, Precision: 0.5897, Recall: 0.5980, F1: 0.5903
LM Predictions:  [3, 2, 2, 1, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 1, 3, 0, 4, 0, 2, 2, 0, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9920, Accuracy: 0.3571, Precision: 0.4286, Recall: 0.2569, F1: 0.3017
Epoch 48/70
Train Loss: 0.4395, Accuracy: 0.8418, Precision: 0.7497, Recall: 0.6732, F1: 0.6959
Validation Loss: 0.6585, Accuracy: 0.7926, Precision: 0.6346, Recall: 0.6491, F1: 0.6382
Testing Loss: 0.6599, Accuracy: 0.7926, Precision: 0.5719, Recall: 0.5868, F1: 0.5738
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 0, 5, 3, 4, 2, 0, 3, 0, 3, 0, 2, 2, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9936, Accuracy: 0.3214, Precision: 0.3937, Recall: 0.2292, F1: 0.2681
Epoch 49/70
Train Loss: 0.4330, Accuracy: 0.8506, Precision: 0.7557, Recall: 0.7060, F1: 0.7254
Validation Loss: 0.6493, Accuracy: 0.7869, Precision: 0.5979, Recall: 0.6015, F1: 0.5978
Testing Loss: 0.5886, Accuracy: 0.8138, Precision: 0.5888, Recall: 0.6074, F1: 0.5966
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 2, 5, 3, 3, 2, 1, 3, 0, 3, 0, 2, 4, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0858, Accuracy: 0.3214, Precision: 0.4286, Recall: 0.2361, F1: 0.2775
Epoch 50/70
Train Loss: 0.4129, Accuracy: 0.8513, Precision: 0.7633, Recall: 0.7061, F1: 0.7284
Validation Loss: 0.6676, Accuracy: 0.7869, Precision: 0.6288, Recall: 0.5807, F1: 0.5991
Testing Loss: 0.6333, Accuracy: 0.7952, Precision: 0.5987, Recall: 0.5747, F1: 0.5835
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 2, 5, 3, 3, 5, 3, 4, 2, 4, 3, 2, 4, 0, 2, 4, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8563, Accuracy: 0.3571, Precision: 0.4167, Recall: 0.2569, F1: 0.2751
Epoch 51/70
Train Loss: 0.4002, Accuracy: 0.8541, Precision: 0.7635, Recall: 0.6984, F1: 0.7230
Validation Loss: 0.6843, Accuracy: 0.7756, Precision: 0.6015, Recall: 0.5909, F1: 0.5903
Testing Loss: 0.6160, Accuracy: 0.7899, Precision: 0.5829, Recall: 0.5674, F1: 0.5735
LM Predictions:  [3, 3, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 3, 5, 3, 2, 4, 0, 2, 4, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8868, Accuracy: 0.2857, Precision: 0.4028, Recall: 0.2014, F1: 0.2406
Epoch 52/70
Train Loss: 0.4054, Accuracy: 0.8523, Precision: 0.7476, Recall: 0.6958, F1: 0.7158
Validation Loss: 0.6710, Accuracy: 0.7926, Precision: 0.6414, Recall: 0.6106, F1: 0.6217
Testing Loss: 0.6202, Accuracy: 0.7926, Precision: 0.6070, Recall: 0.5859, F1: 0.5933
LM Predictions:  [3, 5, 4, 3, 4, 3, 5, 3, 5, 3, 3, 5, 3, 4, 2, 5, 3, 0, 4, 0, 2, 4, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5557, Accuracy: 0.3214, Precision: 0.3944, Recall: 0.2222, F1: 0.2656
Epoch 53/70
Train Loss: 0.4041, Accuracy: 0.8579, Precision: 0.7648, Recall: 0.7208, F1: 0.7381
Validation Loss: 0.6148, Accuracy: 0.8125, Precision: 0.6546, Recall: 0.5955, F1: 0.6182
Testing Loss: 0.6159, Accuracy: 0.8032, Precision: 0.6050, Recall: 0.5737, F1: 0.5864
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 5, 3, 0, 4, 0, 2, 2, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6245, Accuracy: 0.3571, Precision: 0.4028, Recall: 0.2569, F1: 0.2884
Epoch 54/70
Train Loss: 0.3940, Accuracy: 0.8611, Precision: 0.7565, Recall: 0.7024, F1: 0.7225
Validation Loss: 0.6467, Accuracy: 0.7812, Precision: 0.6287, Recall: 0.5723, F1: 0.5946
Testing Loss: 0.6676, Accuracy: 0.7979, Precision: 0.5724, Recall: 0.5617, F1: 0.5652
LM Predictions:  [3, 2, 2, 3, 2, 3, 5, 2, 0, 3, 2, 5, 3, 4, 2, 5, 3, 2, 3, 0, 2, 2, 2, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8525, Accuracy: 0.3571, Precision: 0.4658, Recall: 0.2708, F1: 0.2566
Epoch 55/70
Train Loss: 0.3819, Accuracy: 0.8688, Precision: 0.7635, Recall: 0.7150, F1: 0.7343
Validation Loss: 0.6646, Accuracy: 0.7955, Precision: 0.6179, Recall: 0.5938, F1: 0.6021
Testing Loss: 0.6172, Accuracy: 0.8032, Precision: 0.5720, Recall: 0.5555, F1: 0.5613
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 5, 3, 2, 5, 3, 4, 2, 2, 3, 2, 3, 0, 2, 4, 0, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7483, Accuracy: 0.3214, Precision: 0.3722, Recall: 0.2361, F1: 0.2552
Epoch 56/70
Train Loss: 0.3787, Accuracy: 0.8660, Precision: 0.7699, Recall: 0.7112, F1: 0.7339
Validation Loss: 0.6325, Accuracy: 0.8040, Precision: 0.6221, Recall: 0.6167, F1: 0.6175
Testing Loss: 0.6352, Accuracy: 0.8032, Precision: 0.5987, Recall: 0.5882, F1: 0.5904
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 3, 5, 3, 4, 2, 2, 3, 0, 4, 0, 2, 4, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7098, Accuracy: 0.3929, Precision: 0.4464, Recall: 0.2708, F1: 0.3269
Epoch 57/70
Train Loss: 0.3637, Accuracy: 0.8712, Precision: 0.7972, Recall: 0.7423, F1: 0.7624
Validation Loss: 0.6597, Accuracy: 0.7841, Precision: 0.5895, Recall: 0.6138, F1: 0.5960
Testing Loss: 0.6371, Accuracy: 0.8138, Precision: 0.5933, Recall: 0.6297, F1: 0.6015
LM Predictions:  [3, 1, 2, 3, 4, 3, 5, 3, 0, 3, 3, 5, 3, 4, 3, 2, 3, 0, 3, 0, 2, 2, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6546, Accuracy: 0.3214, Precision: 0.4444, Recall: 0.2292, F1: 0.2904
Epoch 58/70
Train Loss: 0.3602, Accuracy: 0.8751, Precision: 0.7892, Recall: 0.7414, F1: 0.7606
Validation Loss: 0.6715, Accuracy: 0.7784, Precision: 0.5952, Recall: 0.5803, F1: 0.5830
Testing Loss: 0.6213, Accuracy: 0.8191, Precision: 0.6077, Recall: 0.5854, F1: 0.5924
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 3, 5, 3, 4, 2, 2, 3, 0, 4, 0, 2, 4, 4, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5713, Accuracy: 0.3929, Precision: 0.4667, Recall: 0.2708, F1: 0.3324
Epoch 59/70
Train Loss: 0.3876, Accuracy: 0.8635, Precision: 0.7752, Recall: 0.7085, F1: 0.7345
Validation Loss: 0.7197, Accuracy: 0.7955, Precision: 0.6321, Recall: 0.5886, F1: 0.6008
Testing Loss: 0.7053, Accuracy: 0.8165, Precision: 0.6279, Recall: 0.5743, F1: 0.5873
LM Predictions:  [3, 2, 4, 3, 4, 3, 5, 3, 0, 3, 3, 5, 3, 4, 2, 4, 3, 0, 4, 0, 2, 4, 4, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6241, Accuracy: 0.3571, Precision: 0.4119, Recall: 0.2431, F1: 0.2960
Epoch 60/70
Train Loss: 0.3840, Accuracy: 0.8646, Precision: 0.7751, Recall: 0.7032, F1: 0.7298
Validation Loss: 0.7028, Accuracy: 0.7812, Precision: 0.6098, Recall: 0.6089, F1: 0.6068
Testing Loss: 0.6261, Accuracy: 0.8005, Precision: 0.5822, Recall: 0.5670, F1: 0.5714
LM Predictions:  [3, 4, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 5, 3, 0, 4, 0, 2, 4, 4, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3596, Accuracy: 0.5000, Precision: 0.5278, Recall: 0.3403, F1: 0.4111
Epoch 61/70
Train Loss: 0.3769, Accuracy: 0.8635, Precision: 0.7638, Recall: 0.7085, F1: 0.7300
Validation Loss: 0.6522, Accuracy: 0.7926, Precision: 0.6053, Recall: 0.5889, F1: 0.5962
Testing Loss: 0.6307, Accuracy: 0.7952, Precision: 0.5878, Recall: 0.5746, F1: 0.5807
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 2, 0, 3, 2, 5, 3, 4, 2, 0, 3, 0, 4, 0, 2, 2, 0, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3119, Accuracy: 0.4643, Precision: 0.4875, Recall: 0.3264, F1: 0.3681
Epoch 62/70
Train Loss: 0.3579, Accuracy: 0.8747, Precision: 0.8074, Recall: 0.7568, F1: 0.7776
Validation Loss: 0.6508, Accuracy: 0.7983, Precision: 0.6459, Recall: 0.6076, F1: 0.6202
Testing Loss: 0.6606, Accuracy: 0.8085, Precision: 0.6155, Recall: 0.5657, F1: 0.5803
LM Predictions:  [3, 4, 2, 3, 4, 3, 5, 2, 0, 3, 2, 5, 3, 4, 2, 5, 3, 0, 4, 0, 2, 4, 0, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2618, Accuracy: 0.5000, Precision: 0.4663, Recall: 0.3472, F1: 0.3874
Epoch 63/70
Train Loss: 0.3483, Accuracy: 0.8768, Precision: 0.7879, Recall: 0.7391, F1: 0.7589
Validation Loss: 0.6981, Accuracy: 0.7784, Precision: 0.6264, Recall: 0.5836, F1: 0.6008
Testing Loss: 0.6668, Accuracy: 0.7952, Precision: 0.5876, Recall: 0.5715, F1: 0.5777
LM Predictions:  [3, 4, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 4, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1481, Accuracy: 0.5714, Precision: 0.5500, Recall: 0.4028, F1: 0.4637
Epoch 64/70
Train Loss: 0.3496, Accuracy: 0.8761, Precision: 0.7990, Recall: 0.7496, F1: 0.7691
Validation Loss: 0.6695, Accuracy: 0.7756, Precision: 0.6163, Recall: 0.5801, F1: 0.5935
Testing Loss: 0.6980, Accuracy: 0.8032, Precision: 0.5878, Recall: 0.5861, F1: 0.5843
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 2, 0, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0387, Accuracy: 0.5000, Precision: 0.5119, Recall: 0.3611, F1: 0.4078
Epoch 65/70
Train Loss: 0.3422, Accuracy: 0.8793, Precision: 0.8112, Recall: 0.7423, F1: 0.7675
Validation Loss: 0.7396, Accuracy: 0.7926, Precision: 0.6678, Recall: 0.5739, F1: 0.6031
Testing Loss: 0.7412, Accuracy: 0.8085, Precision: 0.6135, Recall: 0.5481, F1: 0.5678
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 5, 4, 0, 4, 0, 2, 4, 4, 2, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8772, Accuracy: 0.5714, Precision: 0.5040, Recall: 0.4028, F1: 0.4389
Epoch 66/70
Train Loss: 0.3319, Accuracy: 0.8793, Precision: 0.7945, Recall: 0.7559, F1: 0.7726
Validation Loss: 0.6410, Accuracy: 0.7869, Precision: 0.6030, Recall: 0.5844, F1: 0.5917
Testing Loss: 0.6629, Accuracy: 0.8059, Precision: 0.5669, Recall: 0.5538, F1: 0.5587
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 0, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0196, Accuracy: 0.5357, Precision: 0.5278, Recall: 0.3819, F1: 0.4365
Epoch 67/70
Train Loss: 0.2931, Accuracy: 0.8936, Precision: 0.8143, Recall: 0.7841, F1: 0.7975
Validation Loss: 0.6495, Accuracy: 0.7926, Precision: 0.6065, Recall: 0.6029, F1: 0.6027
Testing Loss: 0.6543, Accuracy: 0.8005, Precision: 0.5610, Recall: 0.5661, F1: 0.5633
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 0, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2389, Accuracy: 0.5357, Precision: 0.5500, Recall: 0.3819, F1: 0.4466
Epoch 68/70
Train Loss: 0.3187, Accuracy: 0.8793, Precision: 0.8149, Recall: 0.7739, F1: 0.7898
Validation Loss: 0.7289, Accuracy: 0.7670, Precision: 0.6204, Recall: 0.6837, F1: 0.6193
Testing Loss: 0.7078, Accuracy: 0.7872, Precision: 0.5994, Recall: 0.5859, F1: 0.5902
LM Predictions:  [3, 4, 2, 3, 4, 3, 5, 2, 3, 3, 3, 5, 0, 4, 0, 0, 3, 0, 4, 0, 0, 4, 0, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1550, Accuracy: 0.4643, Precision: 0.5119, Recall: 0.3125, F1: 0.3838
Epoch 69/70
Train Loss: 0.3116, Accuracy: 0.8873, Precision: 0.8067, Recall: 0.7747, F1: 0.7892
Validation Loss: 0.6652, Accuracy: 0.7983, Precision: 0.6138, Recall: 0.6015, F1: 0.6025
Testing Loss: 0.7097, Accuracy: 0.7979, Precision: 0.5770, Recall: 0.5562, F1: 0.5629
LM Predictions:  [3, 5, 2, 3, 4, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 0, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3330, Accuracy: 0.5357, Precision: 0.5222, Recall: 0.3819, F1: 0.4347
Epoch 70/70
Train Loss: 0.2908, Accuracy: 0.8933, Precision: 0.8239, Recall: 0.7851, F1: 0.8022
Validation Loss: 0.7231, Accuracy: 0.7784, Precision: 0.6410, Recall: 0.5894, F1: 0.6065
Testing Loss: 0.7318, Accuracy: 0.7952, Precision: 0.5839, Recall: 0.5625, F1: 0.5715
LM Predictions:  [3, 2, 2, 3, 4, 3, 5, 3, 0, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 0, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0408, Accuracy: 0.5357, Precision: 0.5278, Recall: 0.3819, F1: 0.4365
For middle layers:  [8, 9, 10, 11, 12, 13, 14, 15]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4281, Accuracy: 0.3772, Precision: 0.1526, Recall: 0.1672, F1: 0.1276
Validation Loss: 1.4044, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4284, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0655, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 2/70
Train Loss: 1.3623, Accuracy: 0.4101, Precision: 0.1378, Recall: 0.1832, F1: 0.1500
Validation Loss: 1.2617, Accuracy: 0.6562, Precision: 0.2485, Recall: 0.3064, F1: 0.2652
Testing Loss: 1.2869, Accuracy: 0.6436, Precision: 0.2445, Recall: 0.2995, F1: 0.2581
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1160, Accuracy: 0.2500, Precision: 0.2444, Recall: 0.2250, F1: 0.1172
Epoch 3/70
Train Loss: 0.8458, Accuracy: 0.7110, Precision: 0.3359, Recall: 0.3534, F1: 0.3383
Validation Loss: 0.7321, Accuracy: 0.7330, Precision: 0.3662, Recall: 0.4128, F1: 0.3802
Testing Loss: 0.7054, Accuracy: 0.7713, Precision: 0.3754, Recall: 0.4345, F1: 0.3964
LM Predictions:  [5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 5, 5, 5, 5, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2625, Accuracy: 0.2143, Precision: 0.1148, Recall: 0.2667, F1: 0.1320
Epoch 4/70
Train Loss: 0.6017, Accuracy: 0.8009, Precision: 0.4890, Recall: 0.4536, F1: 0.4343
Validation Loss: 0.5727, Accuracy: 0.7898, Precision: 0.4499, Recall: 0.4965, F1: 0.4683
Testing Loss: 0.5660, Accuracy: 0.7952, Precision: 0.4758, Recall: 0.4933, F1: 0.4812
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 5, 3, 3, 5, 2, 4, 5, 5, 3, 3, 5, 5, 3, 2, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0655, Accuracy: 0.1429, Precision: 0.2542, Recall: 0.1181, F1: 0.1254
Epoch 5/70
Train Loss: 0.4447, Accuracy: 0.8604, Precision: 0.5911, Recall: 0.5848, F1: 0.5581
Validation Loss: 0.5531, Accuracy: 0.8182, Precision: 0.6522, Recall: 0.5497, F1: 0.5243
Testing Loss: 0.5219, Accuracy: 0.8378, Precision: 0.5929, Recall: 0.5390, F1: 0.5379
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 5, 3, 3, 5, 2, 4, 1, 5, 3, 3, 4, 1, 3, 3, 3, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1874, Accuracy: 0.1786, Precision: 0.2667, Recall: 0.1389, F1: 0.1643
Epoch 6/70
Train Loss: 0.3518, Accuracy: 0.8894, Precision: 0.6248, Recall: 0.6380, F1: 0.6180
Validation Loss: 0.5215, Accuracy: 0.8381, Precision: 0.6474, Recall: 0.6145, F1: 0.6176
Testing Loss: 0.4866, Accuracy: 0.8564, Precision: 0.6068, Recall: 0.6116, F1: 0.6084
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 1, 1, 3, 5, 3, 4, 1, 5, 3, 3, 4, 3, 2, 3, 3, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1862, Accuracy: 0.2143, Precision: 0.3083, Recall: 0.1667, F1: 0.1992
Epoch 7/70
Train Loss: 0.2794, Accuracy: 0.9192, Precision: 0.6736, Recall: 0.7014, F1: 0.6864
Validation Loss: 0.5656, Accuracy: 0.8210, Precision: 0.5995, Recall: 0.5739, F1: 0.5429
Testing Loss: 0.5231, Accuracy: 0.8511, Precision: 0.6085, Recall: 0.6006, F1: 0.6008
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 3, 1, 3, 3, 5, 3, 4, 1, 5, 3, 3, 4, 3, 2, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1385, Accuracy: 0.2500, Precision: 0.3472, Recall: 0.1875, F1: 0.2385
Epoch 8/70
Train Loss: 0.2258, Accuracy: 0.9356, Precision: 0.7068, Recall: 0.7354, F1: 0.7189
Validation Loss: 0.5744, Accuracy: 0.8409, Precision: 0.6416, Recall: 0.6228, F1: 0.6240
Testing Loss: 0.6024, Accuracy: 0.8457, Precision: 0.6427, Recall: 0.6141, F1: 0.6221
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 2, 1, 1, 3, 5, 3, 4, 2, 5, 3, 3, 4, 3, 2, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1280, Accuracy: 0.2857, Precision: 0.3556, Recall: 0.2153, F1: 0.2597
Epoch 9/70
Train Loss: 0.1810, Accuracy: 0.9489, Precision: 0.8993, Recall: 0.7757, F1: 0.7658
Validation Loss: 0.6261, Accuracy: 0.8466, Precision: 0.6536, Recall: 0.6113, F1: 0.6199
Testing Loss: 0.5806, Accuracy: 0.8564, Precision: 0.6506, Recall: 0.6523, F1: 0.6474
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 3, 5, 1, 2, 5, 3, 4, 0, 5, 3, 1, 4, 3, 2, 3, 3, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9877, Accuracy: 0.3571, Precision: 0.5333, Recall: 0.2778, F1: 0.3232
Epoch 10/70
Train Loss: 0.1472, Accuracy: 0.9580, Precision: 0.8570, Recall: 0.8137, F1: 0.8184
Validation Loss: 0.6454, Accuracy: 0.8494, Precision: 0.6551, Recall: 0.6558, F1: 0.6480
Testing Loss: 0.5524, Accuracy: 0.8723, Precision: 0.6565, Recall: 0.6675, F1: 0.6614
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 2, 5, 3, 3, 5, 3, 4, 2, 0, 3, 1, 4, 3, 2, 3, 3, 3, 2, 3, 0, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2725, Accuracy: 0.3214, Precision: 0.4389, Recall: 0.2361, F1: 0.2931
Epoch 11/70
Train Loss: 0.1718, Accuracy: 0.9482, Precision: 0.8115, Recall: 0.7942, F1: 0.7899
Validation Loss: 0.8344, Accuracy: 0.8097, Precision: 0.6339, Recall: 0.5687, F1: 0.5904
Testing Loss: 0.7316, Accuracy: 0.8484, Precision: 0.6814, Recall: 0.6088, F1: 0.6364
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 3, 5, 1, 3, 5, 3, 4, 2, 5, 3, 5, 4, 1, 2, 4, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0206, Accuracy: 0.2857, Precision: 0.3000, Recall: 0.2083, F1: 0.2391
Epoch 12/70
Train Loss: 0.1744, Accuracy: 0.9503, Precision: 0.8748, Recall: 0.8039, F1: 0.8144
Validation Loss: 0.6932, Accuracy: 0.8324, Precision: 0.6661, Recall: 0.6456, F1: 0.6504
Testing Loss: 0.6347, Accuracy: 0.8564, Precision: 0.6660, Recall: 0.6749, F1: 0.6684
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 3, 5, 1, 3, 5, 3, 4, 3, 5, 3, 5, 4, 1, 2, 4, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2972, Accuracy: 0.2857, Precision: 0.3667, Recall: 0.2083, F1: 0.2593
Epoch 13/70
Train Loss: 0.2540, Accuracy: 0.9276, Precision: 0.7950, Recall: 0.7793, F1: 0.7682
Validation Loss: 0.6833, Accuracy: 0.8523, Precision: 0.6855, Recall: 0.6451, F1: 0.6616
Testing Loss: 0.6414, Accuracy: 0.8537, Precision: 0.6340, Recall: 0.6313, F1: 0.6319
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 1, 1, 2, 5, 3, 4, 0, 5, 3, 1, 4, 3, 2, 4, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1068, Accuracy: 0.3214, Precision: 0.5083, Recall: 0.2361, F1: 0.2908
Epoch 14/70
Train Loss: 0.1509, Accuracy: 0.9619, Precision: 0.8879, Recall: 0.8655, F1: 0.8724
Validation Loss: 0.6863, Accuracy: 0.8324, Precision: 0.6712, Recall: 0.6556, F1: 0.6560
Testing Loss: 0.6260, Accuracy: 0.8564, Precision: 0.6369, Recall: 0.6597, F1: 0.6471
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 5, 1, 2, 5, 3, 4, 3, 0, 3, 3, 4, 3, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0087, Accuracy: 0.3571, Precision: 0.5667, Recall: 0.2778, F1: 0.3354
Epoch 15/70
Train Loss: 0.1192, Accuracy: 0.9678, Precision: 0.8976, Recall: 0.8752, F1: 0.8821
Validation Loss: 0.6783, Accuracy: 0.8494, Precision: 0.6876, Recall: 0.6447, F1: 0.6626
Testing Loss: 0.6678, Accuracy: 0.8511, Precision: 0.6481, Recall: 0.6375, F1: 0.6402
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 5, 1, 2, 5, 3, 4, 3, 0, 3, 0, 4, 3, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7782, Accuracy: 0.3929, Precision: 0.5667, Recall: 0.2986, F1: 0.3650
Epoch 16/70
Train Loss: 0.1162, Accuracy: 0.9671, Precision: 0.8901, Recall: 0.8711, F1: 0.8782
Validation Loss: 0.7131, Accuracy: 0.8438, Precision: 0.6779, Recall: 0.6485, F1: 0.6604
Testing Loss: 0.6775, Accuracy: 0.8644, Precision: 0.6742, Recall: 0.6741, F1: 0.6704
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 1, 1, 2, 5, 3, 4, 0, 0, 3, 1, 4, 3, 2, 4, 3, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9025, Accuracy: 0.4286, Precision: 0.5500, Recall: 0.3194, F1: 0.3864
Epoch 17/70
Train Loss: 0.0962, Accuracy: 0.9752, Precision: 0.9217, Recall: 0.9184, F1: 0.9190
Validation Loss: 0.7448, Accuracy: 0.8295, Precision: 0.6606, Recall: 0.6124, F1: 0.6311
Testing Loss: 0.7241, Accuracy: 0.8511, Precision: 0.6650, Recall: 0.6100, F1: 0.6297
LM Predictions:  [3, 5, 2, 3, 3, 3, 0, 2, 0, 4, 2, 5, 3, 4, 3, 0, 3, 0, 4, 3, 2, 4, 3, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7269, Accuracy: 0.5000, Precision: 0.5218, Recall: 0.3681, F1: 0.4179
Epoch 18/70
Train Loss: 0.0762, Accuracy: 0.9822, Precision: 0.9535, Recall: 0.9448, F1: 0.9481
Validation Loss: 0.7880, Accuracy: 0.8466, Precision: 0.6867, Recall: 0.6419, F1: 0.6608
Testing Loss: 0.7718, Accuracy: 0.8590, Precision: 0.6689, Recall: 0.6548, F1: 0.6611
LM Predictions:  [3, 5, 2, 3, 3, 3, 0, 2, 0, 4, 2, 5, 3, 4, 0, 0, 3, 0, 4, 3, 2, 4, 3, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8041, Accuracy: 0.5357, Precision: 0.5417, Recall: 0.3889, F1: 0.4457
Epoch 19/70
Train Loss: 0.0691, Accuracy: 0.9808, Precision: 0.9456, Recall: 0.9350, F1: 0.9391
Validation Loss: 0.8316, Accuracy: 0.8239, Precision: 0.6675, Recall: 0.6238, F1: 0.6378
Testing Loss: 0.7874, Accuracy: 0.8564, Precision: 0.6866, Recall: 0.6421, F1: 0.6602
LM Predictions:  [3, 5, 2, 3, 3, 3, 0, 2, 5, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 3, 2, 4, 3, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5242, Accuracy: 0.5357, Precision: 0.5833, Recall: 0.3889, F1: 0.4540
Epoch 20/70
Train Loss: 0.0637, Accuracy: 0.9853, Precision: 0.9603, Recall: 0.9494, F1: 0.9540
Validation Loss: 0.8103, Accuracy: 0.8352, Precision: 0.6819, Recall: 0.6677, F1: 0.6633
Testing Loss: 0.7849, Accuracy: 0.8431, Precision: 0.6724, Recall: 0.6244, F1: 0.6423
LM Predictions:  [3, 5, 2, 3, 3, 3, 0, 2, 0, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 3, 2, 4, 3, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7142, Accuracy: 0.5357, Precision: 0.5635, Recall: 0.3889, F1: 0.4426
Epoch 21/70
Train Loss: 0.0538, Accuracy: 0.9860, Precision: 0.9636, Recall: 0.9534, F1: 0.9577
Validation Loss: 0.7812, Accuracy: 0.8551, Precision: 0.7498, Recall: 0.7550, F1: 0.7495
Testing Loss: 0.7516, Accuracy: 0.8537, Precision: 0.6776, Recall: 0.6486, F1: 0.6596
LM Predictions:  [3, 1, 2, 3, 3, 3, 0, 2, 3, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 3, 2, 4, 4, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5259, Accuracy: 0.5357, Precision: 0.6250, Recall: 0.3889, F1: 0.4742
Epoch 22/70
Train Loss: 0.0576, Accuracy: 0.9836, Precision: 0.9477, Recall: 0.9439, F1: 0.9453
Validation Loss: 0.7806, Accuracy: 0.8494, Precision: 0.6784, Recall: 0.6548, F1: 0.6657
Testing Loss: 0.7845, Accuracy: 0.8457, Precision: 0.6769, Recall: 0.6534, F1: 0.6600
LM Predictions:  [3, 5, 2, 3, 3, 3, 0, 2, 0, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1937, Accuracy: 0.6071, Precision: 0.6111, Recall: 0.4306, F1: 0.4932
Epoch 23/70
Train Loss: 0.0591, Accuracy: 0.9832, Precision: 0.9522, Recall: 0.9477, F1: 0.9496
Validation Loss: 0.7908, Accuracy: 0.8381, Precision: 0.6564, Recall: 0.6114, F1: 0.6286
Testing Loss: 0.8354, Accuracy: 0.8484, Precision: 0.6712, Recall: 0.6400, F1: 0.6510
LM Predictions:  [3, 5, 2, 3, 4, 3, 0, 2, 3, 4, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2771, Accuracy: 0.6071, Precision: 0.5540, Recall: 0.4375, F1: 0.4799
Epoch 24/70
Train Loss: 0.0695, Accuracy: 0.9804, Precision: 0.9564, Recall: 0.9390, F1: 0.9472
Validation Loss: 0.6717, Accuracy: 0.8551, Precision: 0.6775, Recall: 0.6479, F1: 0.6591
Testing Loss: 0.7345, Accuracy: 0.8484, Precision: 0.6450, Recall: 0.6387, F1: 0.6407
LM Predictions:  [3, 5, 2, 3, 4, 3, 0, 3, 0, 4, 2, 5, 3, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2435, Accuracy: 0.6071, Precision: 0.5302, Recall: 0.4236, F1: 0.4704
Epoch 25/70
Train Loss: 0.0508, Accuracy: 0.9857, Precision: 0.9608, Recall: 0.9515, F1: 0.9554
Validation Loss: 0.7972, Accuracy: 0.8295, Precision: 0.6685, Recall: 0.6019, F1: 0.6231
Testing Loss: 0.8307, Accuracy: 0.8457, Precision: 0.6759, Recall: 0.6047, F1: 0.6299
LM Predictions:  [3, 5, 2, 3, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1552, Accuracy: 0.7143, Precision: 0.7262, Recall: 0.5625, F1: 0.6221
Epoch 26/70
Train Loss: 0.0433, Accuracy: 0.9867, Precision: 0.9626, Recall: 0.9516, F1: 0.9564
Validation Loss: 0.7813, Accuracy: 0.8608, Precision: 0.7448, Recall: 0.6914, F1: 0.7112
Testing Loss: 0.8849, Accuracy: 0.8378, Precision: 0.6510, Recall: 0.6059, F1: 0.6204
LM Predictions:  [3, 1, 2, 3, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1277, Accuracy: 0.7143, Precision: 0.6984, Recall: 0.5625, F1: 0.6102
Epoch 27/70
Train Loss: 0.0495, Accuracy: 0.9867, Precision: 0.9648, Recall: 0.9625, F1: 0.9634
Validation Loss: 0.9196, Accuracy: 0.8381, Precision: 0.6995, Recall: 0.6218, F1: 0.6526
Testing Loss: 1.0304, Accuracy: 0.8191, Precision: 0.6505, Recall: 0.5836, F1: 0.6076
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 5, 4, 2, 5, 1, 4, 0, 0, 4, 3, 4, 0, 2, 4, 2, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1522, Accuracy: 0.6786, Precision: 0.6389, Recall: 0.5625, F1: 0.5813
Epoch 28/70
Train Loss: 0.1006, Accuracy: 0.9717, Precision: 0.9350, Recall: 0.9317, F1: 0.9331
Validation Loss: 0.7457, Accuracy: 0.8097, Precision: 0.6845, Recall: 0.5889, F1: 0.6177
Testing Loss: 0.7716, Accuracy: 0.7899, Precision: 0.6279, Recall: 0.5582, F1: 0.5800
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 5, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 5, 2, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0557, Accuracy: 0.7143, Precision: 0.6111, Recall: 0.5764, F1: 0.5841
Epoch 29/70
Train Loss: 0.0972, Accuracy: 0.9685, Precision: 0.9292, Recall: 0.9111, F1: 0.9187
Validation Loss: 0.7648, Accuracy: 0.8239, Precision: 0.6958, Recall: 0.6065, F1: 0.6404
Testing Loss: 0.8620, Accuracy: 0.8191, Precision: 0.6275, Recall: 0.5697, F1: 0.5891
LM Predictions:  [3, 5, 2, 5, 4, 3, 0, 2, 5, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2106, Accuracy: 0.7143, Precision: 0.7151, Recall: 0.5833, F1: 0.6233
Epoch 30/70
Train Loss: 0.0613, Accuracy: 0.9822, Precision: 0.9523, Recall: 0.9393, F1: 0.9453
Validation Loss: 0.7777, Accuracy: 0.8352, Precision: 0.7034, Recall: 0.6185, F1: 0.6496
Testing Loss: 0.8662, Accuracy: 0.8165, Precision: 0.6208, Recall: 0.5796, F1: 0.5919
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 5, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 2, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0701, Accuracy: 0.7143, Precision: 0.6139, Recall: 0.5764, F1: 0.5865
Epoch 31/70
Train Loss: 0.0493, Accuracy: 0.9874, Precision: 0.9729, Recall: 0.9610, F1: 0.9664
Validation Loss: 0.8080, Accuracy: 0.8381, Precision: 0.6919, Recall: 0.6298, F1: 0.6522
Testing Loss: 0.8831, Accuracy: 0.8298, Precision: 0.6281, Recall: 0.5940, F1: 0.6042
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7241, Accuracy: 0.7500, Precision: 0.6746, Recall: 0.5972, F1: 0.6317
Epoch 32/70
Train Loss: 0.0375, Accuracy: 0.9906, Precision: 0.9635, Recall: 0.9690, F1: 0.9662
Validation Loss: 0.8227, Accuracy: 0.8494, Precision: 0.6917, Recall: 0.6591, F1: 0.6733
Testing Loss: 0.8671, Accuracy: 0.8351, Precision: 0.6299, Recall: 0.6281, F1: 0.6230
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9047, Accuracy: 0.7500, Precision: 0.7024, Recall: 0.5972, F1: 0.6444
Epoch 33/70
Train Loss: 0.0341, Accuracy: 0.9913, Precision: 0.9877, Recall: 0.9803, F1: 0.9839
Validation Loss: 0.7912, Accuracy: 0.8580, Precision: 0.6943, Recall: 0.6722, F1: 0.6809
Testing Loss: 0.8244, Accuracy: 0.8404, Precision: 0.6343, Recall: 0.6371, F1: 0.6243
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7850, Accuracy: 0.7500, Precision: 0.7024, Recall: 0.5972, F1: 0.6444
Epoch 34/70
Train Loss: 0.0396, Accuracy: 0.9902, Precision: 0.9728, Recall: 0.9699, F1: 0.9712
Validation Loss: 0.7528, Accuracy: 0.8438, Precision: 0.6955, Recall: 0.6300, F1: 0.6536
Testing Loss: 0.8052, Accuracy: 0.8404, Precision: 0.6475, Recall: 0.6215, F1: 0.6309
LM Predictions:  [3, 1, 2, 5, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6709, Accuracy: 0.7857, Precision: 0.7222, Recall: 0.6250, F1: 0.6675
Epoch 35/70
Train Loss: 0.0358, Accuracy: 0.9902, Precision: 0.9819, Recall: 0.9678, F1: 0.9745
Validation Loss: 0.8036, Accuracy: 0.8352, Precision: 0.6794, Recall: 0.6306, F1: 0.6508
Testing Loss: 0.8657, Accuracy: 0.8457, Precision: 0.6459, Recall: 0.6210, F1: 0.6279
LM Predictions:  [3, 1, 2, 1, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6835, Accuracy: 0.7500, Precision: 0.7333, Recall: 0.6458, F1: 0.6441
Epoch 36/70
Train Loss: 0.0296, Accuracy: 0.9902, Precision: 0.9714, Recall: 0.9664, F1: 0.9687
Validation Loss: 0.8341, Accuracy: 0.8580, Precision: 0.7030, Recall: 0.6724, F1: 0.6842
Testing Loss: 0.8728, Accuracy: 0.8404, Precision: 0.6396, Recall: 0.6326, F1: 0.6306
LM Predictions:  [3, 1, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4101, Accuracy: 0.8214, Precision: 0.7083, Recall: 0.7083, F1: 0.6996
Epoch 37/70
Train Loss: 0.0356, Accuracy: 0.9895, Precision: 0.9675, Recall: 0.9655, F1: 0.9663
Validation Loss: 0.7486, Accuracy: 0.8352, Precision: 0.7050, Recall: 0.6851, F1: 0.6838
Testing Loss: 0.8200, Accuracy: 0.8457, Precision: 0.6697, Recall: 0.6346, F1: 0.6504
LM Predictions:  [3, 5, 2, 5, 4, 3, 0, 2, 0, 4, 2, 5, 1, 4, 0, 1, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5518, Accuracy: 0.7500, Precision: 0.6567, Recall: 0.6042, F1: 0.6241
Epoch 38/70
Train Loss: 0.0305, Accuracy: 0.9927, Precision: 0.9870, Recall: 0.9824, F1: 0.9846
Validation Loss: 0.7612, Accuracy: 0.8352, Precision: 0.7108, Recall: 0.6769, F1: 0.6825
Testing Loss: 0.8548, Accuracy: 0.8511, Precision: 0.6693, Recall: 0.6408, F1: 0.6510
LM Predictions:  [3, 1, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3463, Accuracy: 0.8929, Precision: 0.7540, Recall: 0.7708, F1: 0.7523
Epoch 39/70
Train Loss: 0.0295, Accuracy: 0.9930, Precision: 0.9808, Recall: 0.9772, F1: 0.9789
Validation Loss: 0.8060, Accuracy: 0.8523, Precision: 0.7021, Recall: 0.6440, F1: 0.6652
Testing Loss: 0.8782, Accuracy: 0.8404, Precision: 0.6648, Recall: 0.6309, F1: 0.6439
LM Predictions:  [3, 1, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 1, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4671, Accuracy: 0.8214, Precision: 0.7024, Recall: 0.7083, F1: 0.6840
Epoch 40/70
Train Loss: 0.0272, Accuracy: 0.9923, Precision: 0.9767, Recall: 0.9708, F1: 0.9735
Validation Loss: 0.8019, Accuracy: 0.8494, Precision: 0.7120, Recall: 0.6980, F1: 0.6903
Testing Loss: 0.9430, Accuracy: 0.8271, Precision: 0.6603, Recall: 0.6251, F1: 0.6332
LM Predictions:  [3, 1, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2247, Accuracy: 0.8571, Precision: 0.7540, Recall: 0.7292, F1: 0.7317
Epoch 41/70
Train Loss: 0.0216, Accuracy: 0.9951, Precision: 0.9886, Recall: 0.9876, F1: 0.9881
Validation Loss: 0.9178, Accuracy: 0.8523, Precision: 0.6931, Recall: 0.6348, F1: 0.6559
Testing Loss: 1.0468, Accuracy: 0.8431, Precision: 0.6511, Recall: 0.6087, F1: 0.6203
LM Predictions:  [3, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3045, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7910
Epoch 42/70
Train Loss: 0.0201, Accuracy: 0.9948, Precision: 0.9863, Recall: 0.9792, F1: 0.9826
Validation Loss: 0.8386, Accuracy: 0.8494, Precision: 0.7126, Recall: 0.6824, F1: 0.6879
Testing Loss: 0.9636, Accuracy: 0.8484, Precision: 0.6629, Recall: 0.6301, F1: 0.6422
LM Predictions:  [3, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1662, Accuracy: 0.8929, Precision: 0.8000, Recall: 0.7708, F1: 0.7799
Epoch 43/70
Train Loss: 0.0154, Accuracy: 0.9969, Precision: 0.9946, Recall: 0.9911, F1: 0.9928
Validation Loss: 0.8410, Accuracy: 0.8523, Precision: 0.6829, Recall: 0.6324, F1: 0.6525
Testing Loss: 0.9832, Accuracy: 0.8431, Precision: 0.6584, Recall: 0.6215, F1: 0.6346
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0726, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 44/70
Train Loss: 0.0208, Accuracy: 0.9941, Precision: 0.9849, Recall: 0.9789, F1: 0.9818
Validation Loss: 0.9980, Accuracy: 0.8267, Precision: 0.6995, Recall: 0.6091, F1: 0.6313
Testing Loss: 1.1059, Accuracy: 0.8378, Precision: 0.6586, Recall: 0.6063, F1: 0.6199
LM Predictions:  [3, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2023, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7910
Epoch 45/70
Train Loss: 0.0150, Accuracy: 0.9965, Precision: 0.9916, Recall: 0.9911, F1: 0.9913
Validation Loss: 0.9105, Accuracy: 0.8352, Precision: 0.6954, Recall: 0.6280, F1: 0.6513
Testing Loss: 1.0401, Accuracy: 0.8378, Precision: 0.6637, Recall: 0.6088, F1: 0.6272
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0665, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 46/70
Train Loss: 0.0303, Accuracy: 0.9916, Precision: 0.9794, Recall: 0.9717, F1: 0.9755
Validation Loss: 0.8766, Accuracy: 0.8466, Precision: 0.6908, Recall: 0.6115, F1: 0.6382
Testing Loss: 1.1301, Accuracy: 0.8298, Precision: 0.6373, Recall: 0.5821, F1: 0.5944
LM Predictions:  [4, 1, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0617, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 47/70
Train Loss: 0.0217, Accuracy: 0.9937, Precision: 0.9858, Recall: 0.9788, F1: 0.9822
Validation Loss: 0.9203, Accuracy: 0.8466, Precision: 0.6887, Recall: 0.6383, F1: 0.6575
Testing Loss: 1.0361, Accuracy: 0.8457, Precision: 0.6756, Recall: 0.6244, F1: 0.6431
LM Predictions:  [3, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1227, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7910
Epoch 48/70
Train Loss: 0.0128, Accuracy: 0.9962, Precision: 0.9900, Recall: 0.9871, F1: 0.9884
Validation Loss: 0.9318, Accuracy: 0.8438, Precision: 0.6956, Recall: 0.6328, F1: 0.6553
Testing Loss: 1.1264, Accuracy: 0.8484, Precision: 0.6733, Recall: 0.6318, F1: 0.6439
LM Predictions:  [3, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0978, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7910
Epoch 49/70
Train Loss: 0.0122, Accuracy: 0.9962, Precision: 0.9923, Recall: 0.9936, F1: 0.9930
Validation Loss: 0.8705, Accuracy: 0.8608, Precision: 0.7182, Recall: 0.7329, F1: 0.7234
Testing Loss: 1.0187, Accuracy: 0.8590, Precision: 0.6707, Recall: 0.6638, F1: 0.6561
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0847, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 50/70
Train Loss: 0.0115, Accuracy: 0.9965, Precision: 0.9871, Recall: 0.9853, F1: 0.9862
Validation Loss: 0.9718, Accuracy: 0.8580, Precision: 0.6903, Recall: 0.6560, F1: 0.6698
Testing Loss: 1.1063, Accuracy: 0.8537, Precision: 0.6643, Recall: 0.6375, F1: 0.6441
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0610, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 51/70
Train Loss: 0.0130, Accuracy: 0.9962, Precision: 0.9891, Recall: 0.9839, F1: 0.9864
Validation Loss: 0.9811, Accuracy: 0.8580, Precision: 0.6791, Recall: 0.6671, F1: 0.6719
Testing Loss: 1.0219, Accuracy: 0.8537, Precision: 0.6567, Recall: 0.6506, F1: 0.6494
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0438, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0093, Accuracy: 0.9965, Precision: 0.9849, Recall: 0.9875, F1: 0.9862
Validation Loss: 0.9756, Accuracy: 0.8580, Precision: 0.6903, Recall: 0.6340, F1: 0.6520
Testing Loss: 1.1813, Accuracy: 0.8298, Precision: 0.6457, Recall: 0.6026, F1: 0.6072
LM Predictions:  [4, 1, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0646, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 53/70
Train Loss: 0.0112, Accuracy: 0.9979, Precision: 0.9951, Recall: 0.9907, F1: 0.9928
Validation Loss: 0.9526, Accuracy: 0.8494, Precision: 0.6876, Recall: 0.6283, F1: 0.6481
Testing Loss: 1.1346, Accuracy: 0.8457, Precision: 0.6553, Recall: 0.6231, F1: 0.6263
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0553, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0060, Accuracy: 0.9990, Precision: 0.9970, Recall: 0.9957, F1: 0.9963
Validation Loss: 0.9739, Accuracy: 0.8523, Precision: 0.7228, Recall: 0.7049, F1: 0.7114
Testing Loss: 1.1416, Accuracy: 0.8457, Precision: 0.6834, Recall: 0.6505, F1: 0.6584
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0547, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 55/70
Train Loss: 0.0074, Accuracy: 0.9976, Precision: 0.9976, Recall: 0.9959, F1: 0.9967
Validation Loss: 0.9832, Accuracy: 0.8494, Precision: 0.7054, Recall: 0.7046, F1: 0.6995
Testing Loss: 1.2024, Accuracy: 0.8457, Precision: 0.6560, Recall: 0.6351, F1: 0.6393
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0178, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0100, Accuracy: 0.9972, Precision: 0.9942, Recall: 0.9914, F1: 0.9928
Validation Loss: 1.0470, Accuracy: 0.8409, Precision: 0.6968, Recall: 0.6891, F1: 0.6851
Testing Loss: 1.1552, Accuracy: 0.8457, Precision: 0.6554, Recall: 0.6363, F1: 0.6404
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0558, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 57/70
Train Loss: 0.0109, Accuracy: 0.9962, Precision: 0.9883, Recall: 0.9895, F1: 0.9889
Validation Loss: 0.9721, Accuracy: 0.8722, Precision: 0.7595, Recall: 0.7150, F1: 0.7234
Testing Loss: 1.2571, Accuracy: 0.8298, Precision: 0.6473, Recall: 0.6062, F1: 0.6112
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0856, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 58/70
Train Loss: 0.0077, Accuracy: 0.9983, Precision: 0.9977, Recall: 0.9910, F1: 0.9943
Validation Loss: 0.9988, Accuracy: 0.8551, Precision: 0.7285, Recall: 0.7098, F1: 0.7040
Testing Loss: 1.2111, Accuracy: 0.8378, Precision: 0.6607, Recall: 0.6149, F1: 0.6243
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0941, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 59/70
Train Loss: 0.0080, Accuracy: 0.9979, Precision: 0.9924, Recall: 0.9907, F1: 0.9915
Validation Loss: 1.0532, Accuracy: 0.8381, Precision: 0.6934, Recall: 0.7063, F1: 0.6894
Testing Loss: 1.1533, Accuracy: 0.8511, Precision: 0.6433, Recall: 0.6601, F1: 0.6459
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0529, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 60/70
Train Loss: 0.0103, Accuracy: 0.9962, Precision: 0.9892, Recall: 0.9885, F1: 0.9888
Validation Loss: 1.0317, Accuracy: 0.8551, Precision: 0.7051, Recall: 0.6871, F1: 0.6842
Testing Loss: 1.1748, Accuracy: 0.8511, Precision: 0.6635, Recall: 0.6363, F1: 0.6431
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0311, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0211, Accuracy: 0.9937, Precision: 0.9878, Recall: 0.9867, F1: 0.9872
Validation Loss: 0.9685, Accuracy: 0.8381, Precision: 0.7017, Recall: 0.6492, F1: 0.6539
Testing Loss: 1.2056, Accuracy: 0.8298, Precision: 0.6596, Recall: 0.6002, F1: 0.6144
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0061, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0095, Accuracy: 0.9965, Precision: 0.9885, Recall: 0.9879, F1: 0.9882
Validation Loss: 1.2462, Accuracy: 0.8011, Precision: 0.6369, Recall: 0.5631, F1: 0.5768
Testing Loss: 1.2466, Accuracy: 0.8165, Precision: 0.6540, Recall: 0.5981, F1: 0.6159
LM Predictions:  [4, 5, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0618, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 63/70
Train Loss: 0.0052, Accuracy: 0.9990, Precision: 0.9979, Recall: 0.9967, F1: 0.9973
Validation Loss: 0.9698, Accuracy: 0.8523, Precision: 0.6671, Recall: 0.6181, F1: 0.6376
Testing Loss: 1.1450, Accuracy: 0.8404, Precision: 0.6672, Recall: 0.6293, F1: 0.6424
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0145, Accuracy: 0.9972, Precision: 0.9890, Recall: 0.9900, F1: 0.9895
Validation Loss: 0.9188, Accuracy: 0.8580, Precision: 0.6764, Recall: 0.6411, F1: 0.6565
Testing Loss: 1.0448, Accuracy: 0.8431, Precision: 0.6702, Recall: 0.6334, F1: 0.6466
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3150, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 65/70
Train Loss: 0.0083, Accuracy: 0.9983, Precision: 0.9934, Recall: 0.9904, F1: 0.9918
Validation Loss: 0.9820, Accuracy: 0.8466, Precision: 0.6750, Recall: 0.6229, F1: 0.6385
Testing Loss: 1.0754, Accuracy: 0.8404, Precision: 0.6683, Recall: 0.6301, F1: 0.6412
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0152, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0129, Accuracy: 0.9955, Precision: 0.9831, Recall: 0.9876, F1: 0.9853
Validation Loss: 0.9051, Accuracy: 0.8523, Precision: 0.6916, Recall: 0.6330, F1: 0.6565
Testing Loss: 1.1495, Accuracy: 0.8431, Precision: 0.6712, Recall: 0.6318, F1: 0.6442
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0363, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0124, Accuracy: 0.9976, Precision: 0.9958, Recall: 0.9919, F1: 0.9938
Validation Loss: 0.9115, Accuracy: 0.8636, Precision: 0.6996, Recall: 0.6445, F1: 0.6587
Testing Loss: 1.0555, Accuracy: 0.8484, Precision: 0.6545, Recall: 0.6309, F1: 0.6372
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0746, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 68/70
Train Loss: 0.0054, Accuracy: 0.9979, Precision: 0.9935, Recall: 0.9950, F1: 0.9943
Validation Loss: 0.9152, Accuracy: 0.8551, Precision: 0.6799, Recall: 0.7035, F1: 0.6810
Testing Loss: 1.0505, Accuracy: 0.8484, Precision: 0.6594, Recall: 0.6822, F1: 0.6655
LM Predictions:  [4, 1, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0356, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 69/70
Train Loss: 0.0105, Accuracy: 0.9965, Precision: 0.9874, Recall: 0.9884, F1: 0.9879
Validation Loss: 0.9076, Accuracy: 0.8523, Precision: 0.6941, Recall: 0.6451, F1: 0.6630
Testing Loss: 1.1834, Accuracy: 0.8351, Precision: 0.6456, Recall: 0.6014, F1: 0.6094
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0125, Accuracy: 0.9958, Precision: 0.9882, Recall: 0.9864, F1: 0.9873
Validation Loss: 0.9619, Accuracy: 0.8466, Precision: 0.7103, Recall: 0.6304, F1: 0.6521
Testing Loss: 1.1198, Accuracy: 0.8404, Precision: 0.6399, Recall: 0.6031, F1: 0.6070
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0250, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
For later layers:  [16, 17, 18, 19, 20, 21, 22, 23]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4313, Accuracy: 0.3915, Precision: 0.1307, Recall: 0.1719, F1: 0.1276
Validation Loss: 1.3979, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4230, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0561, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 2/70
Train Loss: 1.2759, Accuracy: 0.5245, Precision: 0.1836, Recall: 0.2371, F1: 0.2010
Validation Loss: 0.9138, Accuracy: 0.6733, Precision: 0.2495, Recall: 0.3153, F1: 0.2719
Testing Loss: 0.9442, Accuracy: 0.6729, Precision: 0.2486, Recall: 0.3133, F1: 0.2695
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1325, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 3/70
Train Loss: 0.6991, Accuracy: 0.7684, Precision: 0.3723, Recall: 0.4165, F1: 0.3903
Validation Loss: 0.6974, Accuracy: 0.7670, Precision: 0.3680, Recall: 0.4123, F1: 0.3881
Testing Loss: 0.7105, Accuracy: 0.7686, Precision: 0.3645, Recall: 0.4169, F1: 0.3867
LM Predictions:  [5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 5, 5, 5, 4, 5, 5, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2195, Accuracy: 0.2143, Precision: 0.2871, Recall: 0.2417, F1: 0.1560
Epoch 4/70
Train Loss: 0.4892, Accuracy: 0.8362, Precision: 0.5754, Recall: 0.5323, F1: 0.5354
Validation Loss: 0.5337, Accuracy: 0.8239, Precision: 0.6504, Recall: 0.5387, F1: 0.5194
Testing Loss: 0.5474, Accuracy: 0.8431, Precision: 0.6779, Recall: 0.5643, F1: 0.5664
LM Predictions:  [3, 5, 3, 3, 3, 3, 5, 3, 3, 3, 3, 5, 3, 2, 3, 5, 3, 3, 4, 5, 3, 3, 3, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0978, Accuracy: 0.1071, Precision: 0.2417, Recall: 0.0903, F1: 0.1074
Epoch 5/70
Train Loss: 0.3806, Accuracy: 0.8779, Precision: 0.6152, Recall: 0.6308, F1: 0.6153
Validation Loss: 0.5231, Accuracy: 0.8267, Precision: 0.6577, Recall: 0.6069, F1: 0.6107
Testing Loss: 0.5243, Accuracy: 0.8431, Precision: 0.6279, Recall: 0.6288, F1: 0.6237
LM Predictions:  [3, 1, 3, 3, 3, 3, 5, 3, 1, 1, 3, 5, 3, 4, 3, 5, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2908, Accuracy: 0.1429, Precision: 0.3889, Recall: 0.1111, F1: 0.1619
Epoch 6/70
Train Loss: 0.2882, Accuracy: 0.9157, Precision: 0.6774, Recall: 0.7020, F1: 0.6813
Validation Loss: 0.4804, Accuracy: 0.8494, Precision: 0.6432, Recall: 0.6455, F1: 0.6370
Testing Loss: 0.4926, Accuracy: 0.8670, Precision: 0.6463, Recall: 0.6580, F1: 0.6503
LM Predictions:  [3, 1, 3, 3, 3, 3, 5, 3, 1, 1, 3, 5, 3, 4, 3, 5, 3, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1630, Accuracy: 0.1429, Precision: 0.3889, Recall: 0.1111, F1: 0.1619
Epoch 7/70
Train Loss: 0.2251, Accuracy: 0.9353, Precision: 0.8742, Recall: 0.7456, F1: 0.7297
Validation Loss: 0.6008, Accuracy: 0.8182, Precision: 0.6263, Recall: 0.5673, F1: 0.5826
Testing Loss: 0.5481, Accuracy: 0.8644, Precision: 0.7027, Recall: 0.6296, F1: 0.6569
LM Predictions:  [3, 2, 2, 3, 3, 3, 5, 2, 1, 1, 3, 5, 3, 4, 3, 5, 3, 3, 4, 3, 3, 4, 3, 2, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8825, Accuracy: 0.3214, Precision: 0.3500, Recall: 0.2500, F1: 0.2854
Epoch 8/70
Train Loss: 0.1657, Accuracy: 0.9531, Precision: 0.8834, Recall: 0.7913, F1: 0.7880
Validation Loss: 0.5354, Accuracy: 0.8722, Precision: 0.7167, Recall: 0.6851, F1: 0.6900
Testing Loss: 0.5490, Accuracy: 0.8723, Precision: 0.6846, Recall: 0.6863, F1: 0.6836
LM Predictions:  [3, 3, 3, 3, 3, 3, 5, 3, 1, 1, 3, 5, 3, 4, 3, 0, 3, 3, 4, 3, 3, 4, 3, 3, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0804, Accuracy: 0.2857, Precision: 0.6111, Recall: 0.2153, F1: 0.2910
Epoch 9/70
Train Loss: 0.1181, Accuracy: 0.9675, Precision: 0.9058, Recall: 0.8562, F1: 0.8662
Validation Loss: 0.5861, Accuracy: 0.8636, Precision: 0.6899, Recall: 0.6628, F1: 0.6748
Testing Loss: 0.5910, Accuracy: 0.8670, Precision: 0.6854, Recall: 0.6723, F1: 0.6782
LM Predictions:  [3, 2, 2, 3, 3, 3, 5, 2, 1, 1, 3, 5, 3, 4, 3, 0, 3, 3, 4, 3, 3, 4, 3, 2, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9116, Accuracy: 0.3571, Precision: 0.5167, Recall: 0.2708, F1: 0.3224
Epoch 10/70
Train Loss: 0.0930, Accuracy: 0.9755, Precision: 0.9233, Recall: 0.9059, F1: 0.9112
Validation Loss: 0.6031, Accuracy: 0.8636, Precision: 0.6842, Recall: 0.6691, F1: 0.6751
Testing Loss: 0.6317, Accuracy: 0.8670, Precision: 0.6706, Recall: 0.6728, F1: 0.6710
LM Predictions:  [3, 4, 2, 3, 4, 1, 5, 2, 3, 1, 3, 5, 3, 4, 3, 0, 3, 3, 4, 3, 2, 4, 3, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4317, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4444, F1: 0.5214
Epoch 11/70
Train Loss: 0.0719, Accuracy: 0.9794, Precision: 0.9485, Recall: 0.9194, F1: 0.9287
Validation Loss: 0.5882, Accuracy: 0.8608, Precision: 0.7257, Recall: 0.7411, F1: 0.7227
Testing Loss: 0.6175, Accuracy: 0.8856, Precision: 0.6997, Recall: 0.7028, F1: 0.6998
LM Predictions:  [3, 4, 2, 2, 3, 1, 0, 2, 1, 1, 3, 5, 3, 4, 3, 0, 3, 1, 4, 3, 2, 4, 3, 3, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1598, Accuracy: 0.5000, Precision: 0.6750, Recall: 0.4236, F1: 0.4828
Epoch 12/70
Train Loss: 0.0514, Accuracy: 0.9878, Precision: 0.9674, Recall: 0.9589, F1: 0.9623
Validation Loss: 0.6868, Accuracy: 0.8551, Precision: 0.7102, Recall: 0.7102, F1: 0.7005
Testing Loss: 0.7093, Accuracy: 0.8564, Precision: 0.6823, Recall: 0.6600, F1: 0.6667
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 3, 5, 3, 4, 3, 0, 3, 0, 4, 3, 2, 4, 3, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5859, Accuracy: 0.7500, Precision: 0.8333, Recall: 0.6111, F1: 0.6968
Epoch 13/70
Train Loss: 0.0406, Accuracy: 0.9899, Precision: 0.9836, Recall: 0.9777, F1: 0.9804
Validation Loss: 0.7501, Accuracy: 0.8580, Precision: 0.7352, Recall: 0.7777, F1: 0.7321
Testing Loss: 0.7166, Accuracy: 0.8617, Precision: 0.6614, Recall: 0.6839, F1: 0.6699
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 3, 5, 3, 5, 1, 4, 3, 0, 4, 0, 4, 3, 2, 4, 4, 3, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6373, Accuracy: 0.7143, Precision: 0.8125, Recall: 0.6528, F1: 0.7034
Epoch 14/70
Train Loss: 0.0306, Accuracy: 0.9927, Precision: 0.9810, Recall: 0.9795, F1: 0.9799
Validation Loss: 0.7145, Accuracy: 0.8523, Precision: 0.7473, Recall: 0.6968, F1: 0.7114
Testing Loss: 0.7383, Accuracy: 0.8670, Precision: 0.6990, Recall: 0.6580, F1: 0.6658
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 5, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1963, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7926
Epoch 15/70
Train Loss: 0.0167, Accuracy: 0.9951, Precision: 0.9881, Recall: 0.9863, F1: 0.9870
Validation Loss: 0.7577, Accuracy: 0.8608, Precision: 0.6972, Recall: 0.6736, F1: 0.6810
Testing Loss: 0.8476, Accuracy: 0.8644, Precision: 0.6975, Recall: 0.6675, F1: 0.6764
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2567, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7917, F1: 0.8111
Epoch 16/70
Train Loss: 0.0283, Accuracy: 0.9937, Precision: 0.9874, Recall: 0.9812, F1: 0.9840
Validation Loss: 0.7172, Accuracy: 0.8693, Precision: 0.7596, Recall: 0.8410, F1: 0.7634
Testing Loss: 0.7640, Accuracy: 0.8644, Precision: 0.7042, Recall: 0.6870, F1: 0.6933
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1263, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 17/70
Train Loss: 0.0192, Accuracy: 0.9941, Precision: 0.9876, Recall: 0.9844, F1: 0.9858
Validation Loss: 0.7967, Accuracy: 0.8466, Precision: 0.6927, Recall: 0.7097, F1: 0.6897
Testing Loss: 0.8979, Accuracy: 0.8564, Precision: 0.6776, Recall: 0.6682, F1: 0.6682
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1000, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 18/70
Train Loss: 0.0062, Accuracy: 0.9986, Precision: 0.9956, Recall: 0.9952, F1: 0.9954
Validation Loss: 0.7985, Accuracy: 0.8608, Precision: 0.7172, Recall: 0.7347, F1: 0.7146
Testing Loss: 0.9586, Accuracy: 0.8564, Precision: 0.6790, Recall: 0.6727, F1: 0.6703
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0230, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0080, Accuracy: 0.9986, Precision: 0.9953, Recall: 0.9979, F1: 0.9966
Validation Loss: 0.8388, Accuracy: 0.8466, Precision: 0.6998, Recall: 0.7610, F1: 0.7025
Testing Loss: 0.9116, Accuracy: 0.8590, Precision: 0.7031, Recall: 0.7072, F1: 0.7025
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0377, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 20/70
Train Loss: 0.0043, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9998, F1: 0.9992
Validation Loss: 0.8247, Accuracy: 0.8523, Precision: 0.7066, Recall: 0.7720, F1: 0.7158
Testing Loss: 0.9209, Accuracy: 0.8590, Precision: 0.6659, Recall: 0.6658, F1: 0.6607
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0114, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0214, Accuracy: 0.9951, Precision: 0.9902, Recall: 0.9904, F1: 0.9903
Validation Loss: 0.7111, Accuracy: 0.8551, Precision: 0.7297, Recall: 0.7415, F1: 0.7123
Testing Loss: 0.8465, Accuracy: 0.8537, Precision: 0.7201, Recall: 0.6742, F1: 0.6818
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0259, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0060, Accuracy: 0.9986, Precision: 0.9968, Recall: 0.9979, F1: 0.9974
Validation Loss: 0.7454, Accuracy: 0.8636, Precision: 0.7213, Recall: 0.7046, F1: 0.7065
Testing Loss: 0.8620, Accuracy: 0.8697, Precision: 0.7404, Recall: 0.7107, F1: 0.7227
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0119, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 23/70
Train Loss: 0.0055, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9979, F1: 0.9979
Validation Loss: 0.7832, Accuracy: 0.8551, Precision: 0.7033, Recall: 0.7037, F1: 0.6980
Testing Loss: 0.8608, Accuracy: 0.8697, Precision: 0.6854, Recall: 0.6908, F1: 0.6863
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0128, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0060, Accuracy: 0.9990, Precision: 0.9968, Recall: 0.9981, F1: 0.9974
Validation Loss: 0.8164, Accuracy: 0.8551, Precision: 0.7051, Recall: 0.7201, F1: 0.7058
Testing Loss: 0.8688, Accuracy: 0.8644, Precision: 0.6795, Recall: 0.6851, F1: 0.6789
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0093, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0028, Accuracy: 0.9993, Precision: 0.9995, Recall: 0.9995, F1: 0.9995
Validation Loss: 0.8555, Accuracy: 0.8494, Precision: 0.7125, Recall: 0.7477, F1: 0.7155
Testing Loss: 0.9133, Accuracy: 0.8617, Precision: 0.7210, Recall: 0.6874, F1: 0.7015
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0074, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8305, Accuracy: 0.8608, Precision: 0.7186, Recall: 0.7795, F1: 0.7321
Testing Loss: 0.9067, Accuracy: 0.8777, Precision: 0.7373, Recall: 0.7195, F1: 0.7253
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0059, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8966, Accuracy: 0.8466, Precision: 0.7086, Recall: 0.7497, F1: 0.7152
Testing Loss: 0.8884, Accuracy: 0.8803, Precision: 0.7868, Recall: 0.7219, F1: 0.7396
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0051, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0161, Accuracy: 0.9965, Precision: 0.9966, Recall: 0.9952, F1: 0.9959
Validation Loss: 0.8176, Accuracy: 0.8665, Precision: 0.7374, Recall: 0.7139, F1: 0.7187
Testing Loss: 0.8342, Accuracy: 0.8777, Precision: 0.7059, Recall: 0.6806, F1: 0.6884
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0078, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0161, Accuracy: 0.9951, Precision: 0.9974, Recall: 0.9946, F1: 0.9960
Validation Loss: 0.8146, Accuracy: 0.8636, Precision: 0.7455, Recall: 0.7773, F1: 0.7542
Testing Loss: 0.8244, Accuracy: 0.8936, Precision: 0.7657, Recall: 0.7268, F1: 0.7419
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0070, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0113, Accuracy: 0.9976, Precision: 0.9984, Recall: 0.9984, F1: 0.9984
Validation Loss: 0.7581, Accuracy: 0.8636, Precision: 0.7484, Recall: 0.7883, F1: 0.7529
Testing Loss: 0.7912, Accuracy: 0.8936, Precision: 0.7784, Recall: 0.7284, F1: 0.7457
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0071, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0019, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9970, F1: 0.9978
Validation Loss: 0.8273, Accuracy: 0.8636, Precision: 0.7350, Recall: 0.7955, F1: 0.7420
Testing Loss: 0.8049, Accuracy: 0.8883, Precision: 0.7309, Recall: 0.7243, F1: 0.7267
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0027, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8413, Accuracy: 0.8636, Precision: 0.7294, Recall: 0.7900, F1: 0.7349
Testing Loss: 0.8500, Accuracy: 0.8856, Precision: 0.7479, Recall: 0.7264, F1: 0.7331
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0033, Accuracy: 0.9990, Precision: 0.9994, Recall: 0.9992, F1: 0.9993
Validation Loss: 0.8522, Accuracy: 0.8722, Precision: 0.7503, Recall: 0.7828, F1: 0.7519
Testing Loss: 0.8423, Accuracy: 0.8723, Precision: 0.7393, Recall: 0.7034, F1: 0.7165
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0058, Accuracy: 0.9983, Precision: 0.9959, Recall: 0.9948, F1: 0.9953
Validation Loss: 0.8545, Accuracy: 0.8608, Precision: 0.7409, Recall: 0.7669, F1: 0.7390
Testing Loss: 0.9100, Accuracy: 0.8803, Precision: 0.7036, Recall: 0.6974, F1: 0.6981
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0064, Accuracy: 0.9979, Precision: 0.9944, Recall: 0.9944, F1: 0.9944
Validation Loss: 0.9194, Accuracy: 0.8636, Precision: 0.7011, Recall: 0.6449, F1: 0.6642
Testing Loss: 1.0109, Accuracy: 0.8723, Precision: 0.7725, Recall: 0.6890, F1: 0.7122
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 5, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0825, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 36/70
Train Loss: 0.0086, Accuracy: 0.9983, Precision: 0.9988, Recall: 0.9979, F1: 0.9984
Validation Loss: 0.9036, Accuracy: 0.8409, Precision: 0.6696, Recall: 0.6303, F1: 0.6469
Testing Loss: 1.0399, Accuracy: 0.8644, Precision: 0.6914, Recall: 0.6464, F1: 0.6636
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0044, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0145, Accuracy: 0.9962, Precision: 0.9923, Recall: 0.9897, F1: 0.9910
Validation Loss: 0.8347, Accuracy: 0.8608, Precision: 0.7124, Recall: 0.7847, F1: 0.7237
Testing Loss: 0.8660, Accuracy: 0.8750, Precision: 0.7019, Recall: 0.7378, F1: 0.7156
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0201, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0044, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9995, F1: 0.9990
Validation Loss: 0.8591, Accuracy: 0.8636, Precision: 0.7163, Recall: 0.7244, F1: 0.7155
Testing Loss: 0.8422, Accuracy: 0.8883, Precision: 0.7501, Recall: 0.7231, F1: 0.7327
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0028, Accuracy: 0.9993, Precision: 0.9982, Recall: 0.9982, F1: 0.9982
Validation Loss: 0.8913, Accuracy: 0.8608, Precision: 0.7299, Recall: 0.7761, F1: 0.7418
Testing Loss: 0.8418, Accuracy: 0.8803, Precision: 0.7135, Recall: 0.7096, F1: 0.7085
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8957, Accuracy: 0.8665, Precision: 0.7312, Recall: 0.7771, F1: 0.7400
Testing Loss: 0.8543, Accuracy: 0.8830, Precision: 0.7276, Recall: 0.6977, F1: 0.7089
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9164, Accuracy: 0.8693, Precision: 0.7412, Recall: 0.7858, F1: 0.7527
Testing Loss: 0.8958, Accuracy: 0.8830, Precision: 0.7310, Recall: 0.6993, F1: 0.7120
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9306, Accuracy: 0.8722, Precision: 0.7568, Recall: 0.7956, F1: 0.7661
Testing Loss: 0.9138, Accuracy: 0.8803, Precision: 0.7304, Recall: 0.7026, F1: 0.7139
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 1.0149, Accuracy: 0.8636, Precision: 0.7005, Recall: 0.6847, F1: 0.6878
Testing Loss: 1.0694, Accuracy: 0.8750, Precision: 0.7103, Recall: 0.6757, F1: 0.6848
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0176, Accuracy: 0.9951, Precision: 0.9927, Recall: 0.9956, F1: 0.9941
Validation Loss: 1.0298, Accuracy: 0.8466, Precision: 0.7049, Recall: 0.6234, F1: 0.6456
Testing Loss: 1.1079, Accuracy: 0.8537, Precision: 0.6915, Recall: 0.6399, F1: 0.6513
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0113, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0130, Accuracy: 0.9969, Precision: 0.9932, Recall: 0.9925, F1: 0.9928
Validation Loss: 0.9464, Accuracy: 0.8466, Precision: 0.6752, Recall: 0.6445, F1: 0.6557
Testing Loss: 0.9141, Accuracy: 0.8723, Precision: 0.6768, Recall: 0.6885, F1: 0.6815
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0034, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0072, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9981, F1: 0.9981
Validation Loss: 0.8299, Accuracy: 0.8608, Precision: 0.7205, Recall: 0.7025, F1: 0.7058
Testing Loss: 0.8052, Accuracy: 0.8963, Precision: 0.7529, Recall: 0.7236, F1: 0.7348
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8299, Accuracy: 0.8693, Precision: 0.7374, Recall: 0.7273, F1: 0.7301
Testing Loss: 0.8195, Accuracy: 0.9016, Precision: 0.7551, Recall: 0.7305, F1: 0.7400
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9465, Accuracy: 0.8580, Precision: 0.6920, Recall: 0.6576, F1: 0.6701
Testing Loss: 0.9709, Accuracy: 0.8803, Precision: 0.7330, Recall: 0.7194, F1: 0.7235
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8875, Accuracy: 0.8665, Precision: 0.7534, Recall: 0.7270, F1: 0.7374
Testing Loss: 0.9126, Accuracy: 0.8910, Precision: 0.7464, Recall: 0.7272, F1: 0.7311
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8910, Accuracy: 0.8693, Precision: 0.7076, Recall: 0.6755, F1: 0.6879
Testing Loss: 0.9357, Accuracy: 0.8883, Precision: 0.7487, Recall: 0.7259, F1: 0.7312
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8872, Accuracy: 0.8722, Precision: 0.7438, Recall: 0.7297, F1: 0.7331
Testing Loss: 0.9374, Accuracy: 0.8910, Precision: 0.7498, Recall: 0.7272, F1: 0.7324
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0063, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9994, F1: 0.9986
Validation Loss: 0.8211, Accuracy: 0.8608, Precision: 0.7288, Recall: 0.6959, F1: 0.7087
Testing Loss: 0.9169, Accuracy: 0.8910, Precision: 0.7476, Recall: 0.7257, F1: 0.7297
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0041, Accuracy: 0.9986, Precision: 0.9968, Recall: 0.9981, F1: 0.9975
Validation Loss: 0.8574, Accuracy: 0.8693, Precision: 0.7258, Recall: 0.7070, F1: 0.7104
Testing Loss: 0.9434, Accuracy: 0.8883, Precision: 0.7640, Recall: 0.7260, F1: 0.7382
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0061, Accuracy: 0.9983, Precision: 0.9950, Recall: 0.9949, F1: 0.9949
Validation Loss: 0.8833, Accuracy: 0.8494, Precision: 0.7045, Recall: 0.6918, F1: 0.6875
Testing Loss: 1.0255, Accuracy: 0.8723, Precision: 0.7341, Recall: 0.7038, F1: 0.7100
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0091, Accuracy: 0.9976, Precision: 0.9953, Recall: 0.9929, F1: 0.9941
Validation Loss: 0.9034, Accuracy: 0.8523, Precision: 0.6995, Recall: 0.7161, F1: 0.6931
Testing Loss: 0.9453, Accuracy: 0.8750, Precision: 0.7045, Recall: 0.7133, F1: 0.7054
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8716, Accuracy: 0.8636, Precision: 0.7251, Recall: 0.7176, F1: 0.7093
Testing Loss: 0.9460, Accuracy: 0.8856, Precision: 0.7321, Recall: 0.7170, F1: 0.7203
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0157, Accuracy: 0.9958, Precision: 0.9922, Recall: 0.9941, F1: 0.9932
Validation Loss: 0.6705, Accuracy: 0.8722, Precision: 0.7408, Recall: 0.7769, F1: 0.7406
Testing Loss: 0.8592, Accuracy: 0.8750, Precision: 0.7325, Recall: 0.6944, F1: 0.7091
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0052, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0077, Accuracy: 0.9983, Precision: 0.9990, Recall: 0.9990, F1: 0.9990
Validation Loss: 0.7584, Accuracy: 0.8665, Precision: 0.7431, Recall: 0.7715, F1: 0.7357
Testing Loss: 0.9146, Accuracy: 0.8697, Precision: 0.7392, Recall: 0.7010, F1: 0.7139
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0031, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0012, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9997, F1: 0.9997
Validation Loss: 0.7738, Accuracy: 0.8778, Precision: 0.7415, Recall: 0.7880, F1: 0.7444
Testing Loss: 0.8970, Accuracy: 0.8723, Precision: 0.6992, Recall: 0.7092, F1: 0.7011
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7651, Accuracy: 0.8864, Precision: 0.7550, Recall: 0.8051, F1: 0.7637
Testing Loss: 0.9023, Accuracy: 0.8777, Precision: 0.7149, Recall: 0.7100, F1: 0.7099
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7923, Accuracy: 0.8807, Precision: 0.7553, Recall: 0.8041, F1: 0.7660
Testing Loss: 0.9508, Accuracy: 0.8750, Precision: 0.7266, Recall: 0.7105, F1: 0.7109
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8056, Accuracy: 0.8807, Precision: 0.7553, Recall: 0.8041, F1: 0.7660
Testing Loss: 0.9633, Accuracy: 0.8750, Precision: 0.7267, Recall: 0.7105, F1: 0.7110
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8061, Accuracy: 0.8750, Precision: 0.7469, Recall: 0.7986, F1: 0.7558
Testing Loss: 0.9759, Accuracy: 0.8723, Precision: 0.7127, Recall: 0.7092, F1: 0.7066
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8078, Accuracy: 0.8864, Precision: 0.7546, Recall: 0.8037, F1: 0.7625
Testing Loss: 0.9814, Accuracy: 0.8723, Precision: 0.7054, Recall: 0.7031, F1: 0.7005
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8576, Accuracy: 0.8835, Precision: 0.7606, Recall: 0.8132, F1: 0.7697
Testing Loss: 1.0508, Accuracy: 0.8670, Precision: 0.6914, Recall: 0.7023, F1: 0.6937
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8540, Accuracy: 0.8835, Precision: 0.7590, Recall: 0.8103, F1: 0.7681
Testing Loss: 1.0444, Accuracy: 0.8697, Precision: 0.6934, Recall: 0.7035, F1: 0.6954
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8639, Accuracy: 0.8807, Precision: 0.7515, Recall: 0.7894, F1: 0.7557
Testing Loss: 1.0303, Accuracy: 0.8723, Precision: 0.6999, Recall: 0.7047, F1: 0.6988
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8743, Accuracy: 0.8750, Precision: 0.7402, Recall: 0.7854, F1: 0.7423
Testing Loss: 1.0427, Accuracy: 0.8723, Precision: 0.6999, Recall: 0.7047, F1: 0.6988
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8778, Accuracy: 0.8807, Precision: 0.7562, Recall: 0.8028, F1: 0.7624
Testing Loss: 1.0613, Accuracy: 0.8750, Precision: 0.7067, Recall: 0.7105, F1: 0.7053
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8906, Accuracy: 0.8807, Precision: 0.7571, Recall: 0.8028, F1: 0.7629
Testing Loss: 1.0697, Accuracy: 0.8723, Precision: 0.6999, Recall: 0.7047, F1: 0.6988
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



