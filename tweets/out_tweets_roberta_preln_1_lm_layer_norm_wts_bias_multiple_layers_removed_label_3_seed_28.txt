---------------------------------------------------------------------------
Results for seed:  28
Model: andreasmadsen/efficient_mlm_m0.40, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
For early layers:  [0, 1, 2, 3, 4, 5, 6, 7]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4746, Accuracy: 0.3660, Precision: 0.1542, Recall: 0.1678, F1: 0.1418
Validation Loss: 1.4203, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4549, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1133, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 2/70
Train Loss: 1.3761, Accuracy: 0.3954, Precision: 0.1320, Recall: 0.1746, F1: 0.1356
Validation Loss: 1.4297, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4573, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1970, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 3/70
Train Loss: 1.3711, Accuracy: 0.4017, Precision: 0.1391, Recall: 0.1763, F1: 0.1308
Validation Loss: 1.3875, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4159, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0768, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 4/70
Train Loss: 1.3402, Accuracy: 0.4447, Precision: 0.1498, Recall: 0.2000, F1: 0.1665
Validation Loss: 1.3112, Accuracy: 0.5369, Precision: 0.1803, Recall: 0.2519, F1: 0.2100
Testing Loss: 1.3258, Accuracy: 0.5399, Precision: 0.1810, Recall: 0.2515, F1: 0.2103
LM Predictions:  [2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0518, Accuracy: 0.2143, Precision: 0.0600, Recall: 0.1714, F1: 0.0889
Epoch 5/70
Train Loss: 1.2385, Accuracy: 0.5210, Precision: 0.2371, Recall: 0.2454, F1: 0.2217
Validation Loss: 1.1268, Accuracy: 0.5767, Precision: 0.2785, Recall: 0.2907, F1: 0.2713
Testing Loss: 1.0955, Accuracy: 0.6011, Precision: 0.2792, Recall: 0.3045, F1: 0.2851
LM Predictions:  [2, 2, 2, 5, 5, 2, 2, 2, 5, 2, 5, 2, 2, 5, 2, 2, 5, 2, 2, 4, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0389, Accuracy: 0.3571, Precision: 0.1667, Recall: 0.2857, F1: 0.2027
Epoch 6/70
Train Loss: 1.1782, Accuracy: 0.5469, Precision: 0.2544, Recall: 0.2664, F1: 0.2511
Validation Loss: 1.0598, Accuracy: 0.5824, Precision: 0.2807, Recall: 0.3211, F1: 0.2970
Testing Loss: 1.0681, Accuracy: 0.5984, Precision: 0.2891, Recall: 0.3361, F1: 0.3066
LM Predictions:  [5, 2, 2, 5, 5, 2, 5, 5, 5, 5, 5, 4, 2, 5, 4, 5, 5, 5, 2, 4, 5, 5, 5, 5, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9413, Accuracy: 0.3214, Precision: 0.1721, Recall: 0.2686, F1: 0.2015
Epoch 7/70
Train Loss: 1.1190, Accuracy: 0.6036, Precision: 0.2864, Recall: 0.3017, F1: 0.2886
Validation Loss: 1.0568, Accuracy: 0.5852, Precision: 0.2749, Recall: 0.3029, F1: 0.2862
Testing Loss: 1.0273, Accuracy: 0.6250, Precision: 0.3050, Recall: 0.3467, F1: 0.3203
LM Predictions:  [5, 2, 2, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 5, 5, 2, 5, 5, 2, 4, 5, 2, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9244, Accuracy: 0.3571, Precision: 0.1538, Recall: 0.2857, F1: 0.2000
Epoch 8/70
Train Loss: 1.0561, Accuracy: 0.6225, Precision: 0.2970, Recall: 0.3167, F1: 0.3034
Validation Loss: 1.0032, Accuracy: 0.6080, Precision: 0.2962, Recall: 0.3314, F1: 0.3101
Testing Loss: 0.9937, Accuracy: 0.6170, Precision: 0.2989, Recall: 0.3398, F1: 0.3136
LM Predictions:  [5, 2, 2, 5, 5, 2, 2, 5, 5, 5, 5, 2, 2, 5, 4, 5, 5, 5, 2, 4, 5, 5, 5, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9915, Accuracy: 0.3571, Precision: 0.1957, Recall: 0.2971, F1: 0.2293
Epoch 9/70
Train Loss: 1.0080, Accuracy: 0.6445, Precision: 0.3062, Recall: 0.3339, F1: 0.3185
Validation Loss: 0.9395, Accuracy: 0.6364, Precision: 0.3039, Recall: 0.3373, F1: 0.3180
Testing Loss: 0.9708, Accuracy: 0.6543, Precision: 0.3154, Recall: 0.3586, F1: 0.3329
LM Predictions:  [5, 2, 2, 5, 4, 2, 2, 5, 5, 5, 5, 5, 2, 5, 4, 2, 5, 5, 4, 4, 5, 5, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0520, Accuracy: 0.3571, Precision: 0.1969, Recall: 0.2971, F1: 0.2341
Epoch 10/70
Train Loss: 0.9591, Accuracy: 0.6613, Precision: 0.3138, Recall: 0.3424, F1: 0.3265
Validation Loss: 0.9217, Accuracy: 0.6477, Precision: 0.3065, Recall: 0.3391, F1: 0.3207
Testing Loss: 0.9499, Accuracy: 0.6809, Precision: 0.4195, Recall: 0.3857, F1: 0.3840
LM Predictions:  [3, 2, 2, 5, 4, 2, 2, 5, 5, 3, 3, 2, 2, 5, 4, 2, 3, 3, 4, 4, 5, 5, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0984, Accuracy: 0.3214, Precision: 0.1892, Recall: 0.2238, F1: 0.2026
Epoch 11/70
Train Loss: 0.9165, Accuracy: 0.6784, Precision: 0.5328, Recall: 0.3657, F1: 0.3568
Validation Loss: 0.9118, Accuracy: 0.6619, Precision: 0.4809, Recall: 0.3660, F1: 0.3514
Testing Loss: 0.9447, Accuracy: 0.6543, Precision: 0.4309, Recall: 0.3677, F1: 0.3565
LM Predictions:  [2, 2, 2, 5, 4, 5, 2, 5, 5, 5, 3, 4, 3, 5, 4, 2, 5, 5, 4, 5, 5, 5, 5, 3, 4, 2, 4, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0718, Accuracy: 0.2857, Precision: 0.1474, Recall: 0.2000, F1: 0.1649
Epoch 12/70
Train Loss: 0.8837, Accuracy: 0.6847, Precision: 0.5425, Recall: 0.3815, F1: 0.3825
Validation Loss: 0.9302, Accuracy: 0.6477, Precision: 0.4796, Recall: 0.3723, F1: 0.3648
Testing Loss: 0.9404, Accuracy: 0.6622, Precision: 0.4464, Recall: 0.3962, F1: 0.3902
LM Predictions:  [3, 2, 2, 5, 4, 5, 2, 5, 5, 3, 3, 2, 3, 5, 4, 2, 3, 5, 2, 5, 5, 5, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3141, Accuracy: 0.3214, Precision: 0.1889, Recall: 0.2238, F1: 0.1985
Epoch 13/70
Train Loss: 0.8687, Accuracy: 0.6893, Precision: 0.4124, Recall: 0.3912, F1: 0.3901
Validation Loss: 0.9065, Accuracy: 0.6477, Precision: 0.3935, Recall: 0.3946, F1: 0.3754
Testing Loss: 0.9286, Accuracy: 0.6622, Precision: 0.4200, Recall: 0.4190, F1: 0.3970
LM Predictions:  [3, 3, 3, 5, 4, 5, 2, 5, 5, 3, 3, 2, 3, 5, 4, 5, 3, 5, 4, 3, 5, 5, 5, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1922, Accuracy: 0.2500, Precision: 0.1528, Recall: 0.1762, F1: 0.1607
Epoch 14/70
Train Loss: 0.8507, Accuracy: 0.6991, Precision: 0.4614, Recall: 0.4040, F1: 0.4061
Validation Loss: 0.8567, Accuracy: 0.6818, Precision: 0.4853, Recall: 0.3995, F1: 0.4006
Testing Loss: 0.8851, Accuracy: 0.6968, Precision: 0.5052, Recall: 0.4359, F1: 0.4486
LM Predictions:  [3, 2, 2, 5, 4, 5, 2, 5, 5, 3, 3, 2, 3, 5, 4, 2, 3, 5, 4, 4, 2, 5, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2597, Accuracy: 0.2500, Precision: 0.1496, Recall: 0.1762, F1: 0.1593
Epoch 15/70
Train Loss: 0.8421, Accuracy: 0.7005, Precision: 0.5306, Recall: 0.4152, F1: 0.4205
Validation Loss: 0.8377, Accuracy: 0.7017, Precision: 0.5713, Recall: 0.4228, F1: 0.4366
Testing Loss: 0.8911, Accuracy: 0.6915, Precision: 0.4590, Recall: 0.4158, F1: 0.4238
LM Predictions:  [3, 3, 2, 3, 1, 2, 2, 5, 5, 3, 3, 2, 3, 5, 4, 5, 3, 5, 2, 3, 2, 2, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3487, Accuracy: 0.3214, Precision: 0.4722, Recall: 0.2417, F1: 0.2693
Epoch 16/70
Train Loss: 0.8033, Accuracy: 0.7127, Precision: 0.5005, Recall: 0.4361, F1: 0.4446
Validation Loss: 0.8316, Accuracy: 0.7045, Precision: 0.5622, Recall: 0.4185, F1: 0.4141
Testing Loss: 0.8649, Accuracy: 0.7234, Precision: 0.5962, Recall: 0.4502, F1: 0.4545
LM Predictions:  [3, 3, 3, 3, 4, 5, 3, 5, 5, 3, 3, 4, 2, 4, 4, 2, 3, 3, 2, 3, 2, 3, 5, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4662, Accuracy: 0.1786, Precision: 0.1389, Recall: 0.1286, F1: 0.1298
Epoch 17/70
Train Loss: 0.7864, Accuracy: 0.7190, Precision: 0.5312, Recall: 0.4393, F1: 0.4479
Validation Loss: 0.8837, Accuracy: 0.6875, Precision: 0.5378, Recall: 0.4012, F1: 0.4130
Testing Loss: 0.9148, Accuracy: 0.7074, Precision: 0.5711, Recall: 0.4446, F1: 0.4699
LM Predictions:  [3, 3, 2, 3, 4, 5, 2, 5, 5, 3, 3, 4, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 5, 2, 4, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3996, Accuracy: 0.1429, Precision: 0.1269, Recall: 0.1048, F1: 0.1097
Epoch 18/70
Train Loss: 0.7568, Accuracy: 0.7302, Precision: 0.5103, Recall: 0.4514, F1: 0.4588
Validation Loss: 0.7497, Accuracy: 0.7358, Precision: 0.5803, Recall: 0.4507, F1: 0.4590
Testing Loss: 0.8075, Accuracy: 0.7340, Precision: 0.5363, Recall: 0.4633, F1: 0.4656
LM Predictions:  [3, 3, 2, 5, 4, 5, 2, 5, 5, 3, 3, 4, 2, 5, 4, 2, 3, 3, 3, 1, 3, 3, 5, 3, 4, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2787, Accuracy: 0.2500, Precision: 0.2083, Recall: 0.1762, F1: 0.1909
Epoch 19/70
Train Loss: 0.8013, Accuracy: 0.7117, Precision: 0.5274, Recall: 0.4329, F1: 0.4478
Validation Loss: 0.7214, Accuracy: 0.7244, Precision: 0.5921, Recall: 0.4474, F1: 0.4622
Testing Loss: 0.8053, Accuracy: 0.7394, Precision: 0.5747, Recall: 0.4955, F1: 0.5157
LM Predictions:  [3, 3, 3, 5, 1, 3, 2, 5, 5, 3, 3, 4, 2, 5, 4, 2, 3, 3, 4, 3, 3, 3, 5, 3, 4, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3200, Accuracy: 0.2500, Precision: 0.3750, Recall: 0.1940, F1: 0.2426
Epoch 20/70
Train Loss: 0.7543, Accuracy: 0.7376, Precision: 0.5356, Recall: 0.4636, F1: 0.4806
Validation Loss: 0.7301, Accuracy: 0.7131, Precision: 0.5643, Recall: 0.4678, F1: 0.4728
Testing Loss: 0.7723, Accuracy: 0.7261, Precision: 0.6052, Recall: 0.4903, F1: 0.4986
LM Predictions:  [3, 3, 3, 5, 1, 5, 3, 5, 5, 3, 3, 5, 3, 5, 4, 5, 3, 3, 3, 3, 3, 3, 5, 3, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3823, Accuracy: 0.2143, Precision: 0.3333, Recall: 0.1702, F1: 0.2032
Epoch 21/70
Train Loss: 0.7393, Accuracy: 0.7379, Precision: 0.5418, Recall: 0.4785, F1: 0.4895
Validation Loss: 0.7102, Accuracy: 0.7415, Precision: 0.6080, Recall: 0.4856, F1: 0.5068
Testing Loss: 0.7786, Accuracy: 0.7473, Precision: 0.5661, Recall: 0.4784, F1: 0.4879
LM Predictions:  [3, 3, 2, 5, 4, 2, 2, 5, 5, 3, 3, 3, 3, 2, 4, 3, 3, 3, 4, 3, 3, 3, 1, 3, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3579, Accuracy: 0.1429, Precision: 0.1861, Recall: 0.1048, F1: 0.1315
Epoch 22/70
Train Loss: 0.7411, Accuracy: 0.7355, Precision: 0.5209, Recall: 0.4693, F1: 0.4822
Validation Loss: 0.7444, Accuracy: 0.7386, Precision: 0.5581, Recall: 0.4408, F1: 0.4545
Testing Loss: 0.7660, Accuracy: 0.7420, Precision: 0.6030, Recall: 0.4645, F1: 0.4868
LM Predictions:  [3, 3, 2, 5, 4, 2, 3, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 4, 1, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3618, Accuracy: 0.2143, Precision: 0.2222, Recall: 0.1524, F1: 0.1739
Epoch 23/70
Train Loss: 0.7076, Accuracy: 0.7432, Precision: 0.6184, Recall: 0.4842, F1: 0.5010
Validation Loss: 0.6721, Accuracy: 0.7614, Precision: 0.6004, Recall: 0.5297, F1: 0.5429
Testing Loss: 0.7389, Accuracy: 0.7473, Precision: 0.5554, Recall: 0.5012, F1: 0.5170
LM Predictions:  [3, 3, 2, 3, 4, 2, 3, 5, 5, 3, 3, 1, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3142, Accuracy: 0.1786, Precision: 0.2250, Recall: 0.1286, F1: 0.1574
Epoch 24/70
Train Loss: 0.6872, Accuracy: 0.7593, Precision: 0.5471, Recall: 0.5083, F1: 0.5183
Validation Loss: 0.6720, Accuracy: 0.7472, Precision: 0.5447, Recall: 0.5044, F1: 0.5191
Testing Loss: 0.7229, Accuracy: 0.7660, Precision: 0.5721, Recall: 0.5409, F1: 0.5534
LM Predictions:  [3, 3, 3, 3, 4, 2, 3, 5, 5, 3, 3, 2, 2, 4, 4, 3, 3, 3, 4, 1, 3, 3, 1, 3, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3082, Accuracy: 0.1071, Precision: 0.1583, Recall: 0.0810, F1: 0.1007
Epoch 25/70
Train Loss: 0.6872, Accuracy: 0.7607, Precision: 0.7012, Recall: 0.4992, F1: 0.5171
Validation Loss: 0.7036, Accuracy: 0.7557, Precision: 0.5659, Recall: 0.5496, F1: 0.5416
Testing Loss: 0.7552, Accuracy: 0.7394, Precision: 0.5064, Recall: 0.4883, F1: 0.4863
LM Predictions:  [3, 3, 2, 3, 4, 2, 3, 5, 5, 3, 3, 4, 2, 5, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 4, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2401, Accuracy: 0.2143, Precision: 0.2444, Recall: 0.1524, F1: 0.1833
Epoch 26/70
Train Loss: 0.6659, Accuracy: 0.7551, Precision: 0.5897, Recall: 0.5031, F1: 0.5136
Validation Loss: 0.7304, Accuracy: 0.7358, Precision: 0.6254, Recall: 0.4652, F1: 0.4835
Testing Loss: 0.8028, Accuracy: 0.7287, Precision: 0.6073, Recall: 0.4431, F1: 0.4533
LM Predictions:  [3, 3, 2, 3, 4, 3, 4, 5, 5, 3, 3, 5, 2, 4, 4, 5, 3, 3, 3, 3, 3, 3, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1154, Accuracy: 0.2857, Precision: 0.2444, Recall: 0.2095, F1: 0.2248
Epoch 27/70
Train Loss: 0.6773, Accuracy: 0.7579, Precision: 0.6198, Recall: 0.5062, F1: 0.5249
Validation Loss: 0.7254, Accuracy: 0.7358, Precision: 0.5871, Recall: 0.4805, F1: 0.5043
Testing Loss: 0.8118, Accuracy: 0.7314, Precision: 0.4987, Recall: 0.4691, F1: 0.4770
LM Predictions:  [3, 3, 2, 5, 4, 2, 4, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 4, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9341, Accuracy: 0.2500, Precision: 0.2587, Recall: 0.1857, F1: 0.2056
Epoch 28/70
Train Loss: 0.6631, Accuracy: 0.7593, Precision: 0.6062, Recall: 0.5188, F1: 0.5354
Validation Loss: 0.6952, Accuracy: 0.7557, Precision: 0.6238, Recall: 0.4968, F1: 0.5111
Testing Loss: 0.8024, Accuracy: 0.7261, Precision: 0.6114, Recall: 0.4546, F1: 0.4688
LM Predictions:  [3, 3, 2, 3, 5, 3, 4, 5, 5, 3, 3, 5, 2, 5, 3, 2, 3, 3, 3, 5, 3, 3, 5, 2, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9992, Accuracy: 0.2500, Precision: 0.2548, Recall: 0.1762, F1: 0.2024
Epoch 29/70
Train Loss: 0.6531, Accuracy: 0.7635, Precision: 0.6518, Recall: 0.5313, F1: 0.5550
Validation Loss: 0.7184, Accuracy: 0.7557, Precision: 0.6742, Recall: 0.4890, F1: 0.5223
Testing Loss: 0.8119, Accuracy: 0.7287, Precision: 0.6226, Recall: 0.4524, F1: 0.4708
LM Predictions:  [3, 3, 2, 5, 5, 3, 2, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 5, 3, 3, 5, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9555, Accuracy: 0.3571, Precision: 0.2712, Recall: 0.2476, F1: 0.2470
Epoch 30/70
Train Loss: 0.6496, Accuracy: 0.7712, Precision: 0.6838, Recall: 0.5286, F1: 0.5498
Validation Loss: 0.7425, Accuracy: 0.7301, Precision: 0.6465, Recall: 0.4431, F1: 0.4713
Testing Loss: 0.8829, Accuracy: 0.7021, Precision: 0.5294, Recall: 0.4206, F1: 0.4356
LM Predictions:  [3, 3, 2, 2, 4, 2, 3, 5, 5, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 5, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3462, Accuracy: 0.1786, Precision: 0.1389, Recall: 0.1190, F1: 0.1222
Epoch 31/70
Train Loss: 0.6242, Accuracy: 0.7705, Precision: 0.6272, Recall: 0.5418, F1: 0.5620
Validation Loss: 0.6513, Accuracy: 0.7699, Precision: 0.6007, Recall: 0.5382, F1: 0.5553
Testing Loss: 0.7666, Accuracy: 0.7553, Precision: 0.5541, Recall: 0.4994, F1: 0.5179
LM Predictions:  [3, 3, 2, 5, 4, 2, 4, 5, 5, 3, 3, 0, 2, 4, 3, 2, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9465, Accuracy: 0.2500, Precision: 0.4028, Recall: 0.1857, F1: 0.2362
Epoch 32/70
Train Loss: 0.6036, Accuracy: 0.7873, Precision: 0.6786, Recall: 0.5812, F1: 0.6041
Validation Loss: 0.6730, Accuracy: 0.7699, Precision: 0.6265, Recall: 0.5043, F1: 0.5360
Testing Loss: 0.7667, Accuracy: 0.7633, Precision: 0.5971, Recall: 0.5048, F1: 0.5321
LM Predictions:  [2, 3, 2, 0, 1, 2, 2, 5, 5, 3, 3, 0, 2, 4, 4, 2, 3, 3, 3, 1, 3, 3, 1, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8399, Accuracy: 0.3214, Precision: 0.3889, Recall: 0.2512, F1: 0.2779
Epoch 33/70
Train Loss: 0.5872, Accuracy: 0.7883, Precision: 0.6676, Recall: 0.5666, F1: 0.5938
Validation Loss: 0.6951, Accuracy: 0.7500, Precision: 0.6402, Recall: 0.5282, F1: 0.5479
Testing Loss: 0.7852, Accuracy: 0.7766, Precision: 0.6142, Recall: 0.5206, F1: 0.5433
LM Predictions:  [3, 3, 2, 5, 4, 2, 4, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 5, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1590, Accuracy: 0.2500, Precision: 0.2222, Recall: 0.1857, F1: 0.1981
Epoch 34/70
Train Loss: 0.5931, Accuracy: 0.7932, Precision: 0.6924, Recall: 0.5966, F1: 0.6230
Validation Loss: 0.6655, Accuracy: 0.7727, Precision: 0.6342, Recall: 0.5473, F1: 0.5692
Testing Loss: 0.7650, Accuracy: 0.7473, Precision: 0.5704, Recall: 0.4879, F1: 0.5112
LM Predictions:  [3, 3, 2, 2, 4, 2, 3, 5, 5, 3, 3, 3, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0845, Accuracy: 0.2143, Precision: 0.2083, Recall: 0.1524, F1: 0.1630
Epoch 35/70
Train Loss: 0.5763, Accuracy: 0.7929, Precision: 0.7355, Recall: 0.5804, F1: 0.6039
Validation Loss: 0.7016, Accuracy: 0.7614, Precision: 0.6311, Recall: 0.5144, F1: 0.5419
Testing Loss: 0.7730, Accuracy: 0.7527, Precision: 0.6138, Recall: 0.5032, F1: 0.5329
LM Predictions:  [3, 3, 2, 2, 4, 2, 4, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1734, Accuracy: 0.2500, Precision: 0.2222, Recall: 0.1857, F1: 0.1865
Epoch 36/70
Train Loss: 0.5822, Accuracy: 0.7876, Precision: 0.6805, Recall: 0.5812, F1: 0.6087
Validation Loss: 0.7016, Accuracy: 0.7585, Precision: 0.6145, Recall: 0.5553, F1: 0.5643
Testing Loss: 0.7270, Accuracy: 0.7606, Precision: 0.5734, Recall: 0.5365, F1: 0.5490
LM Predictions:  [3, 3, 2, 3, 4, 3, 3, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0788, Accuracy: 0.1786, Precision: 0.2167, Recall: 0.1286, F1: 0.1537
Epoch 37/70
Train Loss: 0.6103, Accuracy: 0.7820, Precision: 0.6921, Recall: 0.5634, F1: 0.5936
Validation Loss: 0.6852, Accuracy: 0.7614, Precision: 0.6318, Recall: 0.5412, F1: 0.5619
Testing Loss: 0.7481, Accuracy: 0.7447, Precision: 0.6084, Recall: 0.5574, F1: 0.5697
LM Predictions:  [3, 3, 2, 0, 1, 3, 3, 5, 5, 3, 3, 0, 2, 5, 4, 3, 3, 3, 3, 1, 3, 3, 1, 2, 4, 2, 2, 0]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8998, Accuracy: 0.2857, Precision: 0.4056, Recall: 0.2274, F1: 0.2869
Epoch 38/70
Train Loss: 0.5887, Accuracy: 0.7915, Precision: 0.6803, Recall: 0.5739, F1: 0.6015
Validation Loss: 0.6743, Accuracy: 0.7841, Precision: 0.6636, Recall: 0.5610, F1: 0.5822
Testing Loss: 0.7186, Accuracy: 0.7527, Precision: 0.5457, Recall: 0.4953, F1: 0.5096
LM Predictions:  [3, 3, 2, 3, 4, 3, 4, 5, 5, 3, 3, 3, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1240, Accuracy: 0.2143, Precision: 0.2500, Recall: 0.1619, F1: 0.1870
Epoch 39/70
Train Loss: 0.5542, Accuracy: 0.7992, Precision: 0.6987, Recall: 0.6111, F1: 0.6391
Validation Loss: 0.7218, Accuracy: 0.7614, Precision: 0.6627, Recall: 0.5347, F1: 0.5606
Testing Loss: 0.7539, Accuracy: 0.7314, Precision: 0.5651, Recall: 0.4761, F1: 0.4941
LM Predictions:  [3, 3, 2, 3, 5, 3, 4, 5, 5, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0277, Accuracy: 0.1786, Precision: 0.2056, Recall: 0.1381, F1: 0.1606
Epoch 40/70
Train Loss: 0.5755, Accuracy: 0.7915, Precision: 0.6760, Recall: 0.5822, F1: 0.6074
Validation Loss: 0.6792, Accuracy: 0.7642, Precision: 0.6129, Recall: 0.5391, F1: 0.5587
Testing Loss: 0.7000, Accuracy: 0.7739, Precision: 0.5532, Recall: 0.5097, F1: 0.5250
LM Predictions:  [3, 3, 2, 3, 5, 3, 4, 5, 5, 3, 3, 4, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0133, Accuracy: 0.1786, Precision: 0.2056, Recall: 0.1381, F1: 0.1606
Epoch 41/70
Train Loss: 0.5542, Accuracy: 0.8009, Precision: 0.6818, Recall: 0.6084, F1: 0.6314
Validation Loss: 0.6933, Accuracy: 0.7500, Precision: 0.5649, Recall: 0.5491, F1: 0.5420
Testing Loss: 0.6952, Accuracy: 0.7713, Precision: 0.5496, Recall: 0.5459, F1: 0.5428
LM Predictions:  [3, 3, 2, 3, 1, 3, 3, 5, 5, 3, 3, 4, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0444, Accuracy: 0.2143, Precision: 0.3083, Recall: 0.1702, F1: 0.2130
Epoch 42/70
Train Loss: 0.5397, Accuracy: 0.8062, Precision: 0.6868, Recall: 0.6130, F1: 0.6349
Validation Loss: 0.6850, Accuracy: 0.7841, Precision: 0.6320, Recall: 0.5672, F1: 0.5768
Testing Loss: 0.6962, Accuracy: 0.7739, Precision: 0.5478, Recall: 0.5327, F1: 0.5388
LM Predictions:  [3, 3, 2, 3, 5, 3, 3, 5, 5, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 5, 2, 4, 2, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1433, Accuracy: 0.1429, Precision: 0.1417, Recall: 0.0952, F1: 0.1136
Epoch 43/70
Train Loss: 0.5148, Accuracy: 0.8160, Precision: 0.6825, Recall: 0.6187, F1: 0.6359
Validation Loss: 0.6392, Accuracy: 0.7841, Precision: 0.6142, Recall: 0.5746, F1: 0.5901
Testing Loss: 0.6974, Accuracy: 0.7739, Precision: 0.5409, Recall: 0.5254, F1: 0.5260
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 5, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 0, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9702, Accuracy: 0.2500, Precision: 0.5139, Recall: 0.2036, F1: 0.2807
Epoch 44/70
Train Loss: 0.5432, Accuracy: 0.8065, Precision: 0.6803, Recall: 0.6175, F1: 0.6390
Validation Loss: 0.6362, Accuracy: 0.7841, Precision: 0.5801, Recall: 0.5851, F1: 0.5757
Testing Loss: 0.6756, Accuracy: 0.7660, Precision: 0.5307, Recall: 0.5433, F1: 0.5311
LM Predictions:  [3, 3, 2, 3, 1, 3, 3, 5, 5, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2, 0, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2020, Accuracy: 0.1786, Precision: 0.4444, Recall: 0.1464, F1: 0.2148
Epoch 45/70
Train Loss: 0.5055, Accuracy: 0.8163, Precision: 0.6947, Recall: 0.6334, F1: 0.6523
Validation Loss: 0.6462, Accuracy: 0.8040, Precision: 0.7004, Recall: 0.5448, F1: 0.5839
Testing Loss: 0.7693, Accuracy: 0.7606, Precision: 0.5663, Recall: 0.4898, F1: 0.5099
LM Predictions:  [2, 3, 2, 3, 1, 3, 2, 5, 2, 3, 3, 3, 2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 1, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8326, Accuracy: 0.2857, Precision: 0.3333, Recall: 0.2083, F1: 0.2025
Epoch 46/70
Train Loss: 0.5188, Accuracy: 0.8076, Precision: 0.6838, Recall: 0.6279, F1: 0.6456
Validation Loss: 0.6252, Accuracy: 0.7812, Precision: 0.6657, Recall: 0.5634, F1: 0.5920
Testing Loss: 0.6917, Accuracy: 0.7899, Precision: 0.5752, Recall: 0.5242, F1: 0.5389
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 3, 2, 4, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9005, Accuracy: 0.2857, Precision: 0.4246, Recall: 0.2179, F1: 0.2579
Epoch 47/70
Train Loss: 0.4924, Accuracy: 0.8195, Precision: 0.6900, Recall: 0.6336, F1: 0.6523
Validation Loss: 0.6356, Accuracy: 0.7926, Precision: 0.6254, Recall: 0.6059, F1: 0.6133
Testing Loss: 0.7043, Accuracy: 0.7926, Precision: 0.5880, Recall: 0.5813, F1: 0.5712
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 3, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0441, Accuracy: 0.3214, Precision: 0.4524, Recall: 0.2512, F1: 0.2903
Epoch 48/70
Train Loss: 0.4943, Accuracy: 0.8237, Precision: 0.7266, Recall: 0.6515, F1: 0.6719
Validation Loss: 0.6295, Accuracy: 0.7841, Precision: 0.6150, Recall: 0.5703, F1: 0.5846
Testing Loss: 0.6817, Accuracy: 0.7979, Precision: 0.5738, Recall: 0.5607, F1: 0.5661
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 5, 3, 3, 3, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9013, Accuracy: 0.3214, Precision: 0.5500, Recall: 0.2607, F1: 0.3333
Epoch 49/70
Train Loss: 0.4864, Accuracy: 0.8240, Precision: 0.7087, Recall: 0.6469, F1: 0.6687
Validation Loss: 0.5821, Accuracy: 0.7955, Precision: 0.6014, Recall: 0.6056, F1: 0.6016
Testing Loss: 0.6862, Accuracy: 0.7819, Precision: 0.5724, Recall: 0.5810, F1: 0.5660
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 0, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8830, Accuracy: 0.3929, Precision: 0.7083, Recall: 0.3179, F1: 0.4028
Epoch 50/70
Train Loss: 0.4693, Accuracy: 0.8355, Precision: 0.7363, Recall: 0.6733, F1: 0.6941
Validation Loss: 0.5945, Accuracy: 0.8068, Precision: 0.6184, Recall: 0.6233, F1: 0.6183
Testing Loss: 0.6791, Accuracy: 0.7793, Precision: 0.5399, Recall: 0.5421, F1: 0.5357
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 4, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8913, Accuracy: 0.4286, Precision: 0.6944, Recall: 0.3512, F1: 0.4444
Epoch 51/70
Train Loss: 0.4463, Accuracy: 0.8345, Precision: 0.7197, Recall: 0.6771, F1: 0.6935
Validation Loss: 0.6042, Accuracy: 0.8040, Precision: 0.6684, Recall: 0.5711, F1: 0.6044
Testing Loss: 0.7287, Accuracy: 0.7819, Precision: 0.5772, Recall: 0.5241, F1: 0.5378
LM Predictions:  [3, 3, 2, 0, 1, 2, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6630, Accuracy: 0.3571, Precision: 0.5370, Recall: 0.2845, F1: 0.3323
Epoch 52/70
Train Loss: 0.4556, Accuracy: 0.8310, Precision: 0.7174, Recall: 0.6694, F1: 0.6860
Validation Loss: 0.5996, Accuracy: 0.8153, Precision: 0.6647, Recall: 0.6289, F1: 0.6427
Testing Loss: 0.7013, Accuracy: 0.7846, Precision: 0.5667, Recall: 0.5441, F1: 0.5472
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5500, Accuracy: 0.4286, Precision: 0.7222, Recall: 0.3512, F1: 0.4457
Epoch 53/70
Train Loss: 0.4408, Accuracy: 0.8401, Precision: 0.7226, Recall: 0.6740, F1: 0.6900
Validation Loss: 0.6114, Accuracy: 0.8210, Precision: 0.6426, Recall: 0.6372, F1: 0.6394
Testing Loss: 0.6930, Accuracy: 0.7846, Precision: 0.5600, Recall: 0.5289, F1: 0.5272
LM Predictions:  [2, 3, 2, 2, 1, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3663, Accuracy: 0.4643, Precision: 0.7222, Recall: 0.3833, F1: 0.4524
Epoch 54/70
Train Loss: 0.4486, Accuracy: 0.8439, Precision: 0.7565, Recall: 0.6750, F1: 0.6993
Validation Loss: 0.6232, Accuracy: 0.7983, Precision: 0.6396, Recall: 0.5955, F1: 0.6104
Testing Loss: 0.7223, Accuracy: 0.7899, Precision: 0.5801, Recall: 0.5307, F1: 0.5496
LM Predictions:  [3, 3, 2, 3, 4, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 4, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4529, Accuracy: 0.3929, Precision: 0.5556, Recall: 0.3095, F1: 0.3689
Epoch 55/70
Train Loss: 0.4363, Accuracy: 0.8397, Precision: 0.7422, Recall: 0.6924, F1: 0.7109
Validation Loss: 0.6380, Accuracy: 0.7841, Precision: 0.6087, Recall: 0.6139, F1: 0.6093
Testing Loss: 0.6811, Accuracy: 0.7819, Precision: 0.5592, Recall: 0.5627, F1: 0.5554
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4414, Accuracy: 0.4643, Precision: 0.7500, Recall: 0.3750, F1: 0.4888
Epoch 56/70
Train Loss: 0.4176, Accuracy: 0.8436, Precision: 0.7493, Recall: 0.7010, F1: 0.7172
Validation Loss: 0.6259, Accuracy: 0.8011, Precision: 0.6388, Recall: 0.6082, F1: 0.6187
Testing Loss: 0.7125, Accuracy: 0.7952, Precision: 0.5641, Recall: 0.5549, F1: 0.5573
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5568, Accuracy: 0.3571, Precision: 0.7024, Recall: 0.2845, F1: 0.3671
Epoch 57/70
Train Loss: 0.4262, Accuracy: 0.8429, Precision: 0.7760, Recall: 0.6989, F1: 0.7229
Validation Loss: 0.6279, Accuracy: 0.7983, Precision: 0.6968, Recall: 0.6406, F1: 0.6491
Testing Loss: 0.7337, Accuracy: 0.7872, Precision: 0.5801, Recall: 0.5330, F1: 0.5478
LM Predictions:  [2, 3, 2, 3, 5, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3699, Accuracy: 0.3929, Precision: 0.5417, Recall: 0.3000, F1: 0.3608
Epoch 58/70
Train Loss: 0.4176, Accuracy: 0.8446, Precision: 0.7529, Recall: 0.7019, F1: 0.7215
Validation Loss: 0.6360, Accuracy: 0.7841, Precision: 0.6407, Recall: 0.5807, F1: 0.5959
Testing Loss: 0.7590, Accuracy: 0.7527, Precision: 0.5405, Recall: 0.5011, F1: 0.5059
LM Predictions:  [2, 3, 2, 5, 5, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 1, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2641, Accuracy: 0.5000, Precision: 0.6706, Recall: 0.3988, F1: 0.4853
Epoch 59/70
Train Loss: 0.4219, Accuracy: 0.8436, Precision: 0.7412, Recall: 0.7012, F1: 0.7162
Validation Loss: 0.6189, Accuracy: 0.8040, Precision: 0.6313, Recall: 0.5922, F1: 0.6063
Testing Loss: 0.7140, Accuracy: 0.8005, Precision: 0.5749, Recall: 0.5303, F1: 0.5459
LM Predictions:  [2, 3, 2, 4, 5, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 1, 3, 3, 3, 1, 2, 4, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3229, Accuracy: 0.4286, Precision: 0.5595, Recall: 0.3417, F1: 0.4048
Epoch 60/70
Train Loss: 0.4107, Accuracy: 0.8488, Precision: 0.7698, Recall: 0.7197, F1: 0.7400
Validation Loss: 0.5601, Accuracy: 0.8153, Precision: 0.6350, Recall: 0.6099, F1: 0.6198
Testing Loss: 0.6653, Accuracy: 0.7926, Precision: 0.5678, Recall: 0.5643, F1: 0.5618
LM Predictions:  [2, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 3, 3, 3, 1, 3, 2, 3, 1, 2, 3, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1946, Accuracy: 0.5000, Precision: 0.7540, Recall: 0.4071, F1: 0.5026
Epoch 61/70
Train Loss: 0.3734, Accuracy: 0.8621, Precision: 0.7762, Recall: 0.7317, F1: 0.7489
Validation Loss: 0.6064, Accuracy: 0.8040, Precision: 0.6402, Recall: 0.6093, F1: 0.6216
Testing Loss: 0.6938, Accuracy: 0.7899, Precision: 0.5800, Recall: 0.5799, F1: 0.5686
LM Predictions:  [2, 0, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 1, 3, 2, 3, 1, 2, 3, 0, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3377, Accuracy: 0.4643, Precision: 0.7302, Recall: 0.3929, F1: 0.4762
Epoch 62/70
Train Loss: 0.3746, Accuracy: 0.8688, Precision: 0.7795, Recall: 0.7460, F1: 0.7596
Validation Loss: 0.6615, Accuracy: 0.7898, Precision: 0.6402, Recall: 0.5703, F1: 0.5955
Testing Loss: 0.7547, Accuracy: 0.7872, Precision: 0.5765, Recall: 0.5562, F1: 0.5589
LM Predictions:  [2, 3, 2, 3, 1, 5, 4, 5, 2, 3, 3, 0, 2, 2, 4, 3, 3, 3, 1, 3, 3, 3, 1, 2, 3, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1626, Accuracy: 0.4643, Precision: 0.6706, Recall: 0.3833, F1: 0.4656
Epoch 63/70
Train Loss: 0.3605, Accuracy: 0.8646, Precision: 0.7811, Recall: 0.7384, F1: 0.7546
Validation Loss: 0.5983, Accuracy: 0.7926, Precision: 0.5983, Recall: 0.5898, F1: 0.5863
Testing Loss: 0.7034, Accuracy: 0.8085, Precision: 0.5870, Recall: 0.5762, F1: 0.5800
LM Predictions:  [2, 3, 2, 3, 1, 3, 4, 5, 2, 3, 3, 3, 2, 2, 4, 3, 3, 3, 1, 3, 3, 3, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3942, Accuracy: 0.4643, Precision: 0.7540, Recall: 0.3833, F1: 0.4702
Epoch 64/70
Train Loss: 0.3620, Accuracy: 0.8674, Precision: 0.7835, Recall: 0.7479, F1: 0.7617
Validation Loss: 0.6194, Accuracy: 0.7898, Precision: 0.6093, Recall: 0.5889, F1: 0.5970
Testing Loss: 0.7416, Accuracy: 0.7872, Precision: 0.5753, Recall: 0.5728, F1: 0.5578
LM Predictions:  [2, 0, 2, 3, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 3, 1, 3, 1, 1, 2, 2, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0403, Accuracy: 0.6071, Precision: 0.7125, Recall: 0.4976, F1: 0.5471
Epoch 65/70
Train Loss: 0.3665, Accuracy: 0.8670, Precision: 0.7907, Recall: 0.7538, F1: 0.7688
Validation Loss: 0.6204, Accuracy: 0.7926, Precision: 0.6112, Recall: 0.5884, F1: 0.5968
Testing Loss: 0.7246, Accuracy: 0.8032, Precision: 0.6051, Recall: 0.5791, F1: 0.5800
LM Predictions:  [2, 0, 2, 5, 1, 3, 4, 5, 2, 3, 3, 0, 2, 2, 4, 5, 4, 3, 1, 5, 2, 2, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8573, Accuracy: 0.7143, Precision: 0.7407, Recall: 0.5786, F1: 0.6354
Epoch 66/70
Train Loss: 0.3354, Accuracy: 0.8754, Precision: 0.7968, Recall: 0.7661, F1: 0.7794
Validation Loss: 0.6629, Accuracy: 0.7983, Precision: 0.6159, Recall: 0.5661, F1: 0.5834
Testing Loss: 0.7562, Accuracy: 0.8005, Precision: 0.5975, Recall: 0.5626, F1: 0.5630
LM Predictions:  [2, 0, 2, 5, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 4, 3, 1, 5, 5, 2, 1, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7229, Accuracy: 0.7857, Precision: 0.7778, Recall: 0.6262, F1: 0.6889
Epoch 67/70
Train Loss: 0.3642, Accuracy: 0.8740, Precision: 0.8042, Recall: 0.7571, F1: 0.7760
Validation Loss: 0.6029, Accuracy: 0.8182, Precision: 0.6496, Recall: 0.5947, F1: 0.6149
Testing Loss: 0.7736, Accuracy: 0.7793, Precision: 0.5631, Recall: 0.5474, F1: 0.5402
LM Predictions:  [2, 0, 2, 5, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 5, 3, 1, 5, 3, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7809, Accuracy: 0.7500, Precision: 0.7639, Recall: 0.6107, F1: 0.6633
Epoch 68/70
Train Loss: 0.3439, Accuracy: 0.8779, Precision: 0.7872, Recall: 0.7499, F1: 0.7652
Validation Loss: 0.8070, Accuracy: 0.7443, Precision: 0.6244, Recall: 0.5605, F1: 0.5686
Testing Loss: 0.8291, Accuracy: 0.7500, Precision: 0.5671, Recall: 0.5532, F1: 0.5203
LM Predictions:  [2, 0, 2, 1, 1, 1, 4, 5, 2, 3, 1, 0, 2, 2, 4, 3, 1, 4, 1, 1, 2, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7970, Accuracy: 0.6429, Precision: 0.6852, Recall: 0.5488, F1: 0.5376
Epoch 69/70
Train Loss: 0.3361, Accuracy: 0.8796, Precision: 0.8121, Recall: 0.7618, F1: 0.7830
Validation Loss: 0.6314, Accuracy: 0.8011, Precision: 0.6088, Recall: 0.6204, F1: 0.6138
Testing Loss: 0.7340, Accuracy: 0.7899, Precision: 0.5708, Recall: 0.5770, F1: 0.5680
LM Predictions:  [2, 0, 2, 1, 1, 3, 4, 5, 2, 3, 3, 0, 2, 5, 4, 3, 4, 3, 1, 1, 3, 3, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9011, Accuracy: 0.6429, Precision: 0.7500, Recall: 0.5488, F1: 0.6011
Epoch 70/70
Train Loss: 0.2976, Accuracy: 0.8898, Precision: 0.8061, Recall: 0.7787, F1: 0.7894
Validation Loss: 0.7070, Accuracy: 0.7869, Precision: 0.6471, Recall: 0.5783, F1: 0.5996
Testing Loss: 0.8429, Accuracy: 0.7846, Precision: 0.5890, Recall: 0.5468, F1: 0.5421
LM Predictions:  [2, 0, 2, 1, 1, 1, 4, 5, 2, 3, 1, 0, 2, 2, 4, 3, 4, 3, 1, 1, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8511, Accuracy: 0.6786, Precision: 0.7083, Recall: 0.5726, F1: 0.5861
For middle layers:  [8, 9, 10, 11, 12, 13, 14, 15]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4419, Accuracy: 0.3597, Precision: 0.1544, Recall: 0.1660, F1: 0.1426
Validation Loss: 1.4108, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4404, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0511, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 2/70
Train Loss: 1.3576, Accuracy: 0.4269, Precision: 0.1480, Recall: 0.1895, F1: 0.1516
Validation Loss: 1.2563, Accuracy: 0.6392, Precision: 0.2474, Recall: 0.2979, F1: 0.2587
Testing Loss: 1.2795, Accuracy: 0.6410, Precision: 0.2452, Recall: 0.2983, F1: 0.2573
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0068, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 3/70
Train Loss: 0.8122, Accuracy: 0.7320, Precision: 0.3489, Recall: 0.3823, F1: 0.3640
Validation Loss: 0.6674, Accuracy: 0.7812, Precision: 0.3838, Recall: 0.4400, F1: 0.4038
Testing Loss: 0.6711, Accuracy: 0.7713, Precision: 0.3708, Recall: 0.4298, F1: 0.3929
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7781, Accuracy: 0.3214, Precision: 0.3560, Recall: 0.2686, F1: 0.1986
Epoch 4/70
Train Loss: 0.5574, Accuracy: 0.8181, Precision: 0.5862, Recall: 0.4578, F1: 0.4346
Validation Loss: 0.6440, Accuracy: 0.7756, Precision: 0.3916, Recall: 0.4101, F1: 0.3958
Testing Loss: 0.6923, Accuracy: 0.7793, Precision: 0.6414, Recall: 0.4388, F1: 0.4473
LM Predictions:  [2, 2, 2, 2, 5, 2, 4, 5, 5, 2, 3, 3, 2, 2, 3, 4, 5, 3, 2, 5, 2, 2, 5, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6985, Accuracy: 0.3214, Precision: 0.2056, Recall: 0.2238, F1: 0.1898
Epoch 5/70
Train Loss: 0.4271, Accuracy: 0.8709, Precision: 0.6231, Recall: 0.5954, F1: 0.5929
Validation Loss: 0.5234, Accuracy: 0.8153, Precision: 0.5359, Recall: 0.5777, F1: 0.5366
Testing Loss: 0.5566, Accuracy: 0.8457, Precision: 0.5888, Recall: 0.6059, F1: 0.5961
LM Predictions:  [3, 3, 3, 3, 5, 3, 4, 5, 1, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9835, Accuracy: 0.1786, Precision: 0.4167, Recall: 0.1286, F1: 0.1926
Epoch 6/70
Train Loss: 0.3499, Accuracy: 0.8950, Precision: 0.6455, Recall: 0.6579, F1: 0.6501
Validation Loss: 0.5510, Accuracy: 0.7955, Precision: 0.5232, Recall: 0.5677, F1: 0.5293
Testing Loss: 0.5802, Accuracy: 0.8351, Precision: 0.5814, Recall: 0.6198, F1: 0.5902
LM Predictions:  [3, 3, 2, 3, 1, 3, 4, 5, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1303, Accuracy: 0.2857, Precision: 0.5833, Recall: 0.2179, F1: 0.2917
Epoch 7/70
Train Loss: 0.2850, Accuracy: 0.9157, Precision: 0.6695, Recall: 0.6902, F1: 0.6786
Validation Loss: 0.5314, Accuracy: 0.8409, Precision: 0.6249, Recall: 0.5890, F1: 0.5837
Testing Loss: 0.5120, Accuracy: 0.8590, Precision: 0.6210, Recall: 0.6309, F1: 0.6257
LM Predictions:  [3, 3, 2, 3, 5, 4, 4, 5, 5, 3, 3, 3, 2, 5, 3, 4, 3, 3, 3, 3, 5, 2, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8938, Accuracy: 0.3571, Precision: 0.3778, Recall: 0.2571, F1: 0.3056
Epoch 8/70
Train Loss: 0.2213, Accuracy: 0.9360, Precision: 0.7151, Recall: 0.7435, F1: 0.7287
Validation Loss: 0.5789, Accuracy: 0.8267, Precision: 0.6057, Recall: 0.6066, F1: 0.5993
Testing Loss: 0.5501, Accuracy: 0.8537, Precision: 0.6019, Recall: 0.6206, F1: 0.6109
LM Predictions:  [3, 3, 2, 3, 1, 4, 4, 5, 5, 3, 3, 3, 2, 5, 3, 5, 3, 3, 3, 3, 5, 2, 1, 2, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6927, Accuracy: 0.4643, Precision: 0.5778, Recall: 0.3643, F1: 0.4405
Epoch 9/70
Train Loss: 0.1750, Accuracy: 0.9517, Precision: 0.8762, Recall: 0.8006, F1: 0.8083
Validation Loss: 0.5688, Accuracy: 0.8466, Precision: 0.6541, Recall: 0.6169, F1: 0.6266
Testing Loss: 0.5481, Accuracy: 0.8457, Precision: 0.6143, Recall: 0.6075, F1: 0.6103
LM Predictions:  [2, 3, 2, 3, 4, 4, 4, 5, 2, 3, 3, 3, 2, 5, 3, 5, 3, 3, 3, 3, 5, 2, 1, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6114, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4036, F1: 0.4823
Epoch 10/70
Train Loss: 0.1295, Accuracy: 0.9668, Precision: 0.9086, Recall: 0.8456, F1: 0.8604
Validation Loss: 0.6249, Accuracy: 0.8523, Precision: 0.7169, Recall: 0.6765, F1: 0.6905
Testing Loss: 0.6009, Accuracy: 0.8378, Precision: 0.6280, Recall: 0.6095, F1: 0.6147
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 3, 3, 5, 2, 1, 2, 3, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4082, Accuracy: 0.5714, Precision: 0.7130, Recall: 0.4369, F1: 0.5131
Epoch 11/70
Train Loss: 0.1071, Accuracy: 0.9689, Precision: 0.8875, Recall: 0.8658, F1: 0.8730
Validation Loss: 0.6642, Accuracy: 0.8438, Precision: 0.7080, Recall: 0.6768, F1: 0.6854
Testing Loss: 0.6638, Accuracy: 0.8511, Precision: 0.6582, Recall: 0.6342, F1: 0.6401
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 1, 5, 2, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0661, Accuracy: 0.6429, Precision: 0.6736, Recall: 0.5119, F1: 0.5684
Epoch 12/70
Train Loss: 0.0777, Accuracy: 0.9801, Precision: 0.9447, Recall: 0.9259, F1: 0.9338
Validation Loss: 0.7167, Accuracy: 0.8466, Precision: 0.6898, Recall: 0.6607, F1: 0.6601
Testing Loss: 0.6715, Accuracy: 0.8511, Precision: 0.6662, Recall: 0.6369, F1: 0.6455
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 5, 4, 5, 3, 4, 3, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7157, Accuracy: 0.8214, Precision: 0.7444, Recall: 0.6595, F1: 0.6972
Epoch 13/70
Train Loss: 0.0498, Accuracy: 0.9885, Precision: 0.9610, Recall: 0.9562, F1: 0.9584
Validation Loss: 0.7434, Accuracy: 0.8551, Precision: 0.6989, Recall: 0.6849, F1: 0.6829
Testing Loss: 0.7570, Accuracy: 0.8564, Precision: 0.6915, Recall: 0.6592, F1: 0.6709
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 5, 4, 5, 3, 4, 4, 5, 5, 2, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8415, Accuracy: 0.7500, Precision: 0.6472, Recall: 0.5929, F1: 0.6119
Epoch 14/70
Train Loss: 0.0456, Accuracy: 0.9871, Precision: 0.9627, Recall: 0.9596, F1: 0.9611
Validation Loss: 0.7907, Accuracy: 0.8438, Precision: 0.6751, Recall: 0.6350, F1: 0.6490
Testing Loss: 0.8513, Accuracy: 0.8431, Precision: 0.6774, Recall: 0.6292, F1: 0.6400
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6204, Accuracy: 0.8571, Precision: 0.7917, Recall: 0.6917, F1: 0.7315
Epoch 15/70
Train Loss: 0.0546, Accuracy: 0.9867, Precision: 0.9572, Recall: 0.9495, F1: 0.9529
Validation Loss: 0.7338, Accuracy: 0.8523, Precision: 0.7186, Recall: 0.6808, F1: 0.6817
Testing Loss: 0.8305, Accuracy: 0.8324, Precision: 0.6551, Recall: 0.6160, F1: 0.6146
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4213, Accuracy: 0.8571, Precision: 0.7708, Recall: 0.6917, F1: 0.7204
Epoch 16/70
Train Loss: 0.0292, Accuracy: 0.9941, Precision: 0.9857, Recall: 0.9851, F1: 0.9854
Validation Loss: 0.7461, Accuracy: 0.8494, Precision: 0.6887, Recall: 0.6286, F1: 0.6498
Testing Loss: 0.8055, Accuracy: 0.8537, Precision: 0.6729, Recall: 0.6166, F1: 0.6335
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3844, Accuracy: 0.8571, Precision: 0.7583, Recall: 0.7000, F1: 0.7026
Epoch 17/70
Train Loss: 0.0265, Accuracy: 0.9927, Precision: 0.9806, Recall: 0.9819, F1: 0.9812
Validation Loss: 0.7805, Accuracy: 0.8551, Precision: 0.6958, Recall: 0.6238, F1: 0.6483
Testing Loss: 0.8676, Accuracy: 0.8378, Precision: 0.6506, Recall: 0.5907, F1: 0.6014
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2519, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 18/70
Train Loss: 0.0210, Accuracy: 0.9937, Precision: 0.9891, Recall: 0.9852, F1: 0.9871
Validation Loss: 0.8100, Accuracy: 0.8580, Precision: 0.7239, Recall: 0.6773, F1: 0.6734
Testing Loss: 0.9080, Accuracy: 0.8378, Precision: 0.6519, Recall: 0.5998, F1: 0.6104
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1896, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7667, F1: 0.7778
Epoch 19/70
Train Loss: 0.0129, Accuracy: 0.9972, Precision: 0.9946, Recall: 0.9910, F1: 0.9927
Validation Loss: 0.7993, Accuracy: 0.8636, Precision: 0.7199, Recall: 0.6755, F1: 0.6902
Testing Loss: 0.9093, Accuracy: 0.8431, Precision: 0.6538, Recall: 0.5899, F1: 0.6065
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3097, Accuracy: 0.8929, Precision: 0.7917, Recall: 0.7333, F1: 0.7397
Epoch 20/70
Train Loss: 0.0097, Accuracy: 0.9979, Precision: 0.9933, Recall: 0.9925, F1: 0.9929
Validation Loss: 0.8427, Accuracy: 0.8466, Precision: 0.6838, Recall: 0.6925, F1: 0.6793
Testing Loss: 0.8794, Accuracy: 0.8457, Precision: 0.6714, Recall: 0.6612, F1: 0.6627
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1636, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 21/70
Train Loss: 0.0326, Accuracy: 0.9916, Precision: 0.9832, Recall: 0.9746, F1: 0.9787
Validation Loss: 0.7807, Accuracy: 0.8523, Precision: 0.7006, Recall: 0.6318, F1: 0.6584
Testing Loss: 0.8957, Accuracy: 0.8484, Precision: 0.6565, Recall: 0.6075, F1: 0.6244
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1383, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 22/70
Train Loss: 0.0154, Accuracy: 0.9965, Precision: 0.9900, Recall: 0.9866, F1: 0.9882
Validation Loss: 0.8693, Accuracy: 0.8324, Precision: 0.6561, Recall: 0.6501, F1: 0.6500
Testing Loss: 0.9563, Accuracy: 0.8298, Precision: 0.6100, Recall: 0.5989, F1: 0.6000
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1030, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 23/70
Train Loss: 0.0173, Accuracy: 0.9941, Precision: 0.9842, Recall: 0.9854, F1: 0.9848
Validation Loss: 0.9136, Accuracy: 0.8523, Precision: 0.6884, Recall: 0.6297, F1: 0.6540
Testing Loss: 0.9798, Accuracy: 0.8378, Precision: 0.6450, Recall: 0.5689, F1: 0.5951
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1133, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 24/70
Train Loss: 0.0126, Accuracy: 0.9958, Precision: 0.9913, Recall: 0.9868, F1: 0.9890
Validation Loss: 0.8803, Accuracy: 0.8494, Precision: 0.7049, Recall: 0.6960, F1: 0.6874
Testing Loss: 1.0017, Accuracy: 0.8404, Precision: 0.6606, Recall: 0.5956, F1: 0.6153
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1027, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 25/70
Train Loss: 0.0058, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9964, F1: 0.9972
Validation Loss: 0.8394, Accuracy: 0.8438, Precision: 0.6738, Recall: 0.6396, F1: 0.6520
Testing Loss: 0.9960, Accuracy: 0.8271, Precision: 0.6395, Recall: 0.5792, F1: 0.6010
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0313, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0105, Accuracy: 0.9969, Precision: 0.9968, Recall: 0.9936, F1: 0.9952
Validation Loss: 0.8748, Accuracy: 0.8551, Precision: 0.7279, Recall: 0.7047, F1: 0.7119
Testing Loss: 0.9814, Accuracy: 0.8511, Precision: 0.6645, Recall: 0.6309, F1: 0.6438
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0136, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0097, Accuracy: 0.9976, Precision: 0.9958, Recall: 0.9916, F1: 0.9937
Validation Loss: 0.9489, Accuracy: 0.8409, Precision: 0.6692, Recall: 0.6378, F1: 0.6503
Testing Loss: 1.0252, Accuracy: 0.8378, Precision: 0.6418, Recall: 0.6203, F1: 0.6276
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1669, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 28/70
Train Loss: 0.0088, Accuracy: 0.9969, Precision: 0.9972, Recall: 0.9950, F1: 0.9961
Validation Loss: 0.9118, Accuracy: 0.8551, Precision: 0.7070, Recall: 0.7044, F1: 0.6938
Testing Loss: 1.0931, Accuracy: 0.8378, Precision: 0.6422, Recall: 0.6075, F1: 0.6161
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0232, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0057, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9965, F1: 0.9973
Validation Loss: 0.8794, Accuracy: 0.8608, Precision: 0.7152, Recall: 0.7062, F1: 0.7035
Testing Loss: 1.0647, Accuracy: 0.8457, Precision: 0.6530, Recall: 0.6215, F1: 0.6316
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0107, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0069, Accuracy: 0.9986, Precision: 0.9992, Recall: 0.9994, F1: 0.9993
Validation Loss: 0.9609, Accuracy: 0.8352, Precision: 0.6838, Recall: 0.6263, F1: 0.6436
Testing Loss: 1.1144, Accuracy: 0.8378, Precision: 0.6598, Recall: 0.5895, F1: 0.6104
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0076, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0099, Accuracy: 0.9976, Precision: 0.9987, Recall: 0.9943, F1: 0.9965
Validation Loss: 1.0888, Accuracy: 0.8125, Precision: 0.6864, Recall: 0.7312, F1: 0.6655
Testing Loss: 1.1854, Accuracy: 0.8059, Precision: 0.6316, Recall: 0.5940, F1: 0.6056
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0068, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0017, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9969, F1: 0.9977
Validation Loss: 0.8755, Accuracy: 0.8494, Precision: 0.7284, Recall: 0.7525, F1: 0.7160
Testing Loss: 1.0576, Accuracy: 0.8404, Precision: 0.6427, Recall: 0.6202, F1: 0.6255
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0033, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0119, Accuracy: 0.9958, Precision: 0.9861, Recall: 0.9875, F1: 0.9868
Validation Loss: 0.8882, Accuracy: 0.8523, Precision: 0.7926, Recall: 0.7325, F1: 0.7500
Testing Loss: 1.1140, Accuracy: 0.8271, Precision: 0.6221, Recall: 0.5928, F1: 0.5975
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0040, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0133, Accuracy: 0.9972, Precision: 0.9934, Recall: 0.9944, F1: 0.9939
Validation Loss: 0.8708, Accuracy: 0.8580, Precision: 0.7379, Recall: 0.7231, F1: 0.7298
Testing Loss: 1.0459, Accuracy: 0.8511, Precision: 0.6530, Recall: 0.6478, F1: 0.6440
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0125, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0035, Accuracy: 0.9986, Precision: 0.9968, Recall: 0.9968, F1: 0.9968
Validation Loss: 0.9218, Accuracy: 0.8580, Precision: 0.7141, Recall: 0.7135, F1: 0.7087
Testing Loss: 1.0501, Accuracy: 0.8537, Precision: 0.6618, Recall: 0.6552, F1: 0.6555
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0033, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0024, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9983, F1: 0.9982
Validation Loss: 0.9727, Accuracy: 0.8438, Precision: 0.7075, Recall: 0.6731, F1: 0.6814
Testing Loss: 1.1263, Accuracy: 0.8457, Precision: 0.6343, Recall: 0.6092, F1: 0.6168
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0028, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9721, Accuracy: 0.8551, Precision: 0.7030, Recall: 0.7068, F1: 0.6995
Testing Loss: 1.1482, Accuracy: 0.8537, Precision: 0.6502, Recall: 0.6490, F1: 0.6429
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9841, Accuracy: 0.8580, Precision: 0.6979, Recall: 0.7018, F1: 0.6944
Testing Loss: 1.1530, Accuracy: 0.8484, Precision: 0.6555, Recall: 0.6404, F1: 0.6419
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9922, Accuracy: 0.8523, Precision: 0.6938, Recall: 0.7056, F1: 0.6946
Testing Loss: 1.1654, Accuracy: 0.8511, Precision: 0.6537, Recall: 0.6433, F1: 0.6432
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0014, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9998, F1: 0.9997
Validation Loss: 1.0504, Accuracy: 0.8494, Precision: 0.7525, Recall: 0.6744, F1: 0.7016
Testing Loss: 1.3504, Accuracy: 0.8378, Precision: 0.6281, Recall: 0.5858, F1: 0.5997
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0137, Accuracy: 0.9969, Precision: 0.9956, Recall: 0.9955, F1: 0.9956
Validation Loss: 1.0485, Accuracy: 0.8295, Precision: 0.7424, Recall: 0.6180, F1: 0.6463
Testing Loss: 1.2674, Accuracy: 0.8378, Precision: 0.6737, Recall: 0.5509, F1: 0.5804
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0237, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0218, Accuracy: 0.9937, Precision: 0.9890, Recall: 0.9907, F1: 0.9899
Validation Loss: 0.7791, Accuracy: 0.8693, Precision: 0.7530, Recall: 0.7742, F1: 0.7558
Testing Loss: 0.9932, Accuracy: 0.8564, Precision: 0.6525, Recall: 0.6416, F1: 0.6445
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0029, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0038, Accuracy: 0.9990, Precision: 0.9958, Recall: 0.9968, F1: 0.9963
Validation Loss: 0.8206, Accuracy: 0.8722, Precision: 0.7434, Recall: 0.7290, F1: 0.7324
Testing Loss: 1.0558, Accuracy: 0.8537, Precision: 0.6486, Recall: 0.6297, F1: 0.6361
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0013, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9986, F1: 0.9991
Validation Loss: 0.8226, Accuracy: 0.8693, Precision: 0.6996, Recall: 0.6719, F1: 0.6837
Testing Loss: 1.0824, Accuracy: 0.8431, Precision: 0.6345, Recall: 0.6051, F1: 0.6152
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0029, Accuracy: 0.9986, Precision: 0.9992, Recall: 0.9994, F1: 0.9993
Validation Loss: 0.8961, Accuracy: 0.8494, Precision: 0.6742, Recall: 0.6507, F1: 0.6604
Testing Loss: 1.1264, Accuracy: 0.8378, Precision: 0.6281, Recall: 0.6051, F1: 0.6140
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9117, Accuracy: 0.8693, Precision: 0.7622, Recall: 0.7199, F1: 0.7360
Testing Loss: 1.1968, Accuracy: 0.8351, Precision: 0.6453, Recall: 0.5874, F1: 0.6091
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0016, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9995, F1: 0.9996
Validation Loss: 0.9624, Accuracy: 0.8580, Precision: 0.6958, Recall: 0.6581, F1: 0.6701
Testing Loss: 1.2615, Accuracy: 0.8484, Precision: 0.6458, Recall: 0.6187, F1: 0.6266
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9575, Accuracy: 0.8636, Precision: 0.6889, Recall: 0.6566, F1: 0.6679
Testing Loss: 1.2260, Accuracy: 0.8431, Precision: 0.6334, Recall: 0.6005, F1: 0.6114
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9579, Accuracy: 0.8665, Precision: 0.7247, Recall: 0.7099, F1: 0.7111
Testing Loss: 1.2360, Accuracy: 0.8511, Precision: 0.6509, Recall: 0.6358, F1: 0.6392
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9669, Accuracy: 0.8580, Precision: 0.7288, Recall: 0.7023, F1: 0.7101
Testing Loss: 1.2695, Accuracy: 0.8378, Precision: 0.6430, Recall: 0.6141, F1: 0.6208
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9982, Accuracy: 0.8636, Precision: 0.7005, Recall: 0.6417, F1: 0.6639
Testing Loss: 1.3308, Accuracy: 0.8378, Precision: 0.6557, Recall: 0.5903, F1: 0.6131
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9976, Accuracy: 0.8580, Precision: 0.6898, Recall: 0.6480, F1: 0.6624
Testing Loss: 1.3382, Accuracy: 0.8431, Precision: 0.6522, Recall: 0.5977, F1: 0.6160
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0021, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9986, F1: 0.9991
Validation Loss: 1.0286, Accuracy: 0.8665, Precision: 0.6783, Recall: 0.6982, F1: 0.6874
Testing Loss: 1.2451, Accuracy: 0.8511, Precision: 0.6226, Recall: 0.6601, F1: 0.6338
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0048, Accuracy: 0.9990, Precision: 0.9979, Recall: 0.9969, F1: 0.9974
Validation Loss: 1.0481, Accuracy: 0.8580, Precision: 0.7302, Recall: 0.6915, F1: 0.6960
Testing Loss: 1.3888, Accuracy: 0.8378, Precision: 0.6366, Recall: 0.5817, F1: 0.6000
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0067, Accuracy: 0.9976, Precision: 0.9982, Recall: 0.9984, F1: 0.9983
Validation Loss: 0.9841, Accuracy: 0.8580, Precision: 0.7321, Recall: 0.6931, F1: 0.7054
Testing Loss: 1.2442, Accuracy: 0.8378, Precision: 0.6289, Recall: 0.5870, F1: 0.6037
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9752, Accuracy: 0.8750, Precision: 0.7074, Recall: 0.6775, F1: 0.6887
Testing Loss: 1.2568, Accuracy: 0.8511, Precision: 0.6436, Recall: 0.6108, F1: 0.6213
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0121, Accuracy: 0.9976, Precision: 0.9952, Recall: 0.9953, F1: 0.9953
Validation Loss: 0.9995, Accuracy: 0.8438, Precision: 0.7502, Recall: 0.7108, F1: 0.7281
Testing Loss: 1.2133, Accuracy: 0.8564, Precision: 0.6567, Recall: 0.6519, F1: 0.6496
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 3, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1596, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 58/70
Train Loss: 0.0088, Accuracy: 0.9986, Precision: 0.9950, Recall: 0.9936, F1: 0.9943
Validation Loss: 0.9093, Accuracy: 0.8580, Precision: 0.6855, Recall: 0.6368, F1: 0.6522
Testing Loss: 1.2379, Accuracy: 0.8298, Precision: 0.6362, Recall: 0.5902, F1: 0.6041
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0110, Accuracy: 0.9972, Precision: 0.9886, Recall: 0.9900, F1: 0.9893
Validation Loss: 1.0527, Accuracy: 0.8352, Precision: 0.6707, Recall: 0.6019, F1: 0.6281
Testing Loss: 1.2772, Accuracy: 0.8511, Precision: 0.6707, Recall: 0.6145, F1: 0.6368
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0067, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0157, Accuracy: 0.9965, Precision: 0.9925, Recall: 0.9910, F1: 0.9917
Validation Loss: 0.9854, Accuracy: 0.8523, Precision: 0.7033, Recall: 0.6922, F1: 0.6867
Testing Loss: 1.2601, Accuracy: 0.8298, Precision: 0.6128, Recall: 0.5938, F1: 0.5977
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0016, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9998, F1: 0.9997
Validation Loss: 1.0365, Accuracy: 0.8494, Precision: 0.7037, Recall: 0.6984, F1: 0.6922
Testing Loss: 1.2499, Accuracy: 0.8564, Precision: 0.6496, Recall: 0.6412, F1: 0.6381
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0042, Accuracy: 0.9990, Precision: 0.9993, Recall: 0.9994, F1: 0.9994
Validation Loss: 0.9984, Accuracy: 0.8466, Precision: 0.6952, Recall: 0.6084, F1: 0.6406
Testing Loss: 1.2549, Accuracy: 0.8431, Precision: 0.6490, Recall: 0.5836, F1: 0.6043
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0038, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9531, Accuracy: 0.8551, Precision: 0.6982, Recall: 0.6497, F1: 0.6702
Testing Loss: 1.1786, Accuracy: 0.8537, Precision: 0.6411, Recall: 0.6112, F1: 0.6213
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0034, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9951, F1: 0.9965
Validation Loss: 1.0040, Accuracy: 0.8295, Precision: 0.6710, Recall: 0.5820, F1: 0.6087
Testing Loss: 1.2943, Accuracy: 0.8564, Precision: 0.6743, Recall: 0.5841, F1: 0.6116
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0048, Accuracy: 0.9990, Precision: 0.9993, Recall: 0.9962, F1: 0.9977
Validation Loss: 0.9093, Accuracy: 0.8665, Precision: 0.7521, Recall: 0.7141, F1: 0.7241
Testing Loss: 1.1691, Accuracy: 0.8511, Precision: 0.6467, Recall: 0.6152, F1: 0.6246
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0283, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 66/70
Train Loss: 0.0069, Accuracy: 0.9979, Precision: 0.9885, Recall: 0.9885, F1: 0.9885
Validation Loss: 0.9293, Accuracy: 0.8750, Precision: 0.7548, Recall: 0.7767, F1: 0.7445
Testing Loss: 1.2431, Accuracy: 0.8271, Precision: 0.6358, Recall: 0.5775, F1: 0.5908
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0075, Accuracy: 0.9976, Precision: 0.9954, Recall: 0.9941, F1: 0.9947
Validation Loss: 0.8653, Accuracy: 0.8636, Precision: 0.7396, Recall: 0.6875, F1: 0.7061
Testing Loss: 1.1602, Accuracy: 0.8431, Precision: 0.6533, Recall: 0.5956, F1: 0.6146
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0028, Accuracy: 0.9986, Precision: 0.9938, Recall: 0.9938, F1: 0.9938
Validation Loss: 0.8585, Accuracy: 0.8750, Precision: 0.7497, Recall: 0.8008, F1: 0.7692
Testing Loss: 1.0997, Accuracy: 0.8644, Precision: 0.6489, Recall: 0.6613, F1: 0.6505
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0026, Accuracy: 0.9993, Precision: 0.9965, Recall: 0.9965, F1: 0.9965
Validation Loss: 0.8228, Accuracy: 0.8835, Precision: 0.7750, Recall: 0.7834, F1: 0.7718
Testing Loss: 1.1153, Accuracy: 0.8617, Precision: 0.6684, Recall: 0.6481, F1: 0.6546
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0010, Accuracy: 0.9993, Precision: 0.9966, Recall: 0.9995, F1: 0.9980
Validation Loss: 0.8354, Accuracy: 0.8750, Precision: 0.7647, Recall: 0.7718, F1: 0.7608
Testing Loss: 1.1316, Accuracy: 0.8670, Precision: 0.6716, Recall: 0.6551, F1: 0.6605
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
For later layers:  [16, 17, 18, 19, 20, 21, 22, 23]
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.weight, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.LayerNorm.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4517, Accuracy: 0.3894, Precision: 0.1302, Recall: 0.1756, F1: 0.1476
Validation Loss: 1.3985, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4315, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9774, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 2/70
Train Loss: 1.1687, Accuracy: 0.5651, Precision: 0.2906, Recall: 0.2659, F1: 0.2465
Validation Loss: 0.7350, Accuracy: 0.7301, Precision: 0.3816, Recall: 0.4254, F1: 0.3839
Testing Loss: 0.7329, Accuracy: 0.7500, Precision: 0.3844, Recall: 0.4379, F1: 0.3915
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8600, Accuracy: 0.2857, Precision: 0.2519, Recall: 0.2286, F1: 0.1324
Epoch 3/70
Train Loss: 0.6249, Accuracy: 0.7943, Precision: 0.3894, Recall: 0.4398, F1: 0.4069
Validation Loss: 0.5245, Accuracy: 0.7983, Precision: 0.3913, Recall: 0.4493, F1: 0.4122
Testing Loss: 0.5631, Accuracy: 0.8005, Precision: 0.3886, Recall: 0.4548, F1: 0.4122
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8338, Accuracy: 0.3214, Precision: 0.2538, Recall: 0.2571, F1: 0.1737
Epoch 4/70
Train Loss: 0.4466, Accuracy: 0.8562, Precision: 0.6187, Recall: 0.5786, F1: 0.5762
Validation Loss: 0.4756, Accuracy: 0.8409, Precision: 0.6380, Recall: 0.5647, F1: 0.5810
Testing Loss: 0.5171, Accuracy: 0.8511, Precision: 0.6535, Recall: 0.6111, F1: 0.6230
LM Predictions:  [3, 3, 3, 5, 5, 5, 5, 5, 5, 3, 3, 3, 2, 5, 3, 5, 3, 3, 3, 3, 5, 5, 5, 2, 1, 3, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8589, Accuracy: 0.2857, Precision: 0.4028, Recall: 0.2083, F1: 0.2285
Epoch 5/70
Train Loss: 0.3467, Accuracy: 0.8936, Precision: 0.6557, Recall: 0.6641, F1: 0.6576
Validation Loss: 0.4422, Accuracy: 0.8580, Precision: 0.6599, Recall: 0.6269, F1: 0.6363
Testing Loss: 0.5303, Accuracy: 0.8511, Precision: 0.6593, Recall: 0.6255, F1: 0.6391
LM Predictions:  [2, 3, 3, 2, 5, 3, 4, 5, 5, 3, 3, 3, 2, 5, 3, 5, 3, 3, 3, 3, 2, 3, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9298, Accuracy: 0.2857, Precision: 0.3778, Recall: 0.2000, F1: 0.2415
Epoch 6/70
Train Loss: 0.2487, Accuracy: 0.9290, Precision: 0.8702, Recall: 0.7321, F1: 0.7217
Validation Loss: 0.4585, Accuracy: 0.8608, Precision: 0.6708, Recall: 0.6401, F1: 0.6517
Testing Loss: 0.5125, Accuracy: 0.8564, Precision: 0.6640, Recall: 0.6370, F1: 0.6483
LM Predictions:  [2, 3, 3, 2, 5, 4, 4, 5, 3, 3, 3, 3, 2, 5, 3, 5, 3, 3, 3, 3, 2, 5, 1, 2, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9678, Accuracy: 0.2857, Precision: 0.3667, Recall: 0.2095, F1: 0.2619
Epoch 7/70
Train Loss: 0.1907, Accuracy: 0.9479, Precision: 0.8728, Recall: 0.7892, F1: 0.7922
Validation Loss: 0.4921, Accuracy: 0.8438, Precision: 0.6454, Recall: 0.6097, F1: 0.6216
Testing Loss: 0.5129, Accuracy: 0.8537, Precision: 0.6528, Recall: 0.6226, F1: 0.6339
LM Predictions:  [2, 3, 3, 2, 3, 4, 4, 5, 3, 3, 3, 3, 2, 5, 4, 5, 3, 3, 3, 3, 2, 2, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8840, Accuracy: 0.3929, Precision: 0.4524, Recall: 0.2905, F1: 0.3440
Epoch 8/70
Train Loss: 0.1432, Accuracy: 0.9598, Precision: 0.9192, Recall: 0.8347, F1: 0.8488
Validation Loss: 0.5199, Accuracy: 0.8636, Precision: 0.6970, Recall: 0.6585, F1: 0.6718
Testing Loss: 0.5604, Accuracy: 0.8777, Precision: 0.6951, Recall: 0.6823, F1: 0.6869
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 4, 3, 3, 3, 2, 5, 4, 5, 3, 3, 3, 3, 5, 2, 5, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5330, Accuracy: 0.5000, Precision: 0.5694, Recall: 0.3798, F1: 0.4342
Epoch 9/70
Train Loss: 0.1027, Accuracy: 0.9706, Precision: 0.9190, Recall: 0.8812, F1: 0.8936
Validation Loss: 0.5649, Accuracy: 0.8494, Precision: 0.6552, Recall: 0.6544, F1: 0.6523
Testing Loss: 0.5589, Accuracy: 0.8723, Precision: 0.6616, Recall: 0.6831, F1: 0.6702
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 2, 5, 2, 5, 2, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3375, Accuracy: 0.6071, Precision: 0.7817, Recall: 0.4786, F1: 0.5627
Epoch 10/70
Train Loss: 0.0833, Accuracy: 0.9780, Precision: 0.9373, Recall: 0.9100, F1: 0.9195
Validation Loss: 0.6115, Accuracy: 0.8693, Precision: 0.7329, Recall: 0.7245, F1: 0.7208
Testing Loss: 0.5643, Accuracy: 0.8830, Precision: 0.6851, Recall: 0.6880, F1: 0.6841
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 4, 3, 5, 5, 2, 5, 2, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0953, Accuracy: 0.6786, Precision: 0.8095, Recall: 0.5357, F1: 0.6115
Epoch 11/70
Train Loss: 0.0567, Accuracy: 0.9871, Precision: 0.9621, Recall: 0.9456, F1: 0.9525
Validation Loss: 0.5948, Accuracy: 0.8636, Precision: 0.7266, Recall: 0.7273, F1: 0.7181
Testing Loss: 0.6039, Accuracy: 0.8803, Precision: 0.6791, Recall: 0.6867, F1: 0.6809
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 4, 3, 5, 5, 2, 5, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9460, Accuracy: 0.6429, Precision: 0.7857, Recall: 0.4940, F1: 0.5561
Epoch 12/70
Train Loss: 0.0346, Accuracy: 0.9927, Precision: 0.9714, Recall: 0.9646, F1: 0.9676
Validation Loss: 0.6697, Accuracy: 0.8523, Precision: 0.7035, Recall: 0.6991, F1: 0.6806
Testing Loss: 0.7257, Accuracy: 0.8404, Precision: 0.6699, Recall: 0.6540, F1: 0.6530
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7021, Accuracy: 0.8571, Precision: 0.7792, Recall: 0.7000, F1: 0.7138
Epoch 13/70
Train Loss: 0.0317, Accuracy: 0.9937, Precision: 0.9754, Recall: 0.9756, F1: 0.9755
Validation Loss: 0.6939, Accuracy: 0.8580, Precision: 0.7242, Recall: 0.7261, F1: 0.7174
Testing Loss: 0.7798, Accuracy: 0.8431, Precision: 0.6476, Recall: 0.6391, F1: 0.6368
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5338, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7333, F1: 0.7620
Epoch 14/70
Train Loss: 0.0214, Accuracy: 0.9944, Precision: 0.9797, Recall: 0.9810, F1: 0.9802
Validation Loss: 0.8113, Accuracy: 0.8608, Precision: 0.7239, Recall: 0.7390, F1: 0.7236
Testing Loss: 0.7539, Accuracy: 0.8590, Precision: 0.6829, Recall: 0.6948, F1: 0.6800
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5053, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7333, F1: 0.7523
Epoch 15/70
Train Loss: 0.0212, Accuracy: 0.9951, Precision: 0.9814, Recall: 0.9826, F1: 0.9820
Validation Loss: 0.7195, Accuracy: 0.8580, Precision: 0.7133, Recall: 0.7216, F1: 0.7079
Testing Loss: 0.7527, Accuracy: 0.8617, Precision: 0.6703, Recall: 0.6612, F1: 0.6580
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3503, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7333, F1: 0.7620
Epoch 16/70
Train Loss: 0.0135, Accuracy: 0.9965, Precision: 0.9888, Recall: 0.9878, F1: 0.9882
Validation Loss: 0.7374, Accuracy: 0.8665, Precision: 0.6810, Recall: 0.6742, F1: 0.6742
Testing Loss: 0.8047, Accuracy: 0.8590, Precision: 0.6779, Recall: 0.6849, F1: 0.6730
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 1, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2864, Accuracy: 0.9286, Precision: 0.9333, Recall: 0.9200, F1: 0.9156
Epoch 17/70
Train Loss: 0.0236, Accuracy: 0.9937, Precision: 0.9911, Recall: 0.9870, F1: 0.9890
Validation Loss: 0.8379, Accuracy: 0.8466, Precision: 0.6817, Recall: 0.6746, F1: 0.6721
Testing Loss: 0.8483, Accuracy: 0.8564, Precision: 0.6704, Recall: 0.6973, F1: 0.6767
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2637, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 18/70
Train Loss: 0.0108, Accuracy: 0.9976, Precision: 0.9952, Recall: 0.9946, F1: 0.9949
Validation Loss: 0.7561, Accuracy: 0.8636, Precision: 0.6829, Recall: 0.6728, F1: 0.6750
Testing Loss: 0.8551, Accuracy: 0.8564, Precision: 0.6882, Recall: 0.6899, F1: 0.6823
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0689, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 19/70
Train Loss: 0.0167, Accuracy: 0.9948, Precision: 0.9901, Recall: 0.9876, F1: 0.9889
Validation Loss: 0.7784, Accuracy: 0.8580, Precision: 0.6866, Recall: 0.7315, F1: 0.6962
Testing Loss: 0.8716, Accuracy: 0.8564, Precision: 0.6384, Recall: 0.6650, F1: 0.6410
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1447, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 20/70
Train Loss: 0.0177, Accuracy: 0.9941, Precision: 0.9941, Recall: 0.9876, F1: 0.9907
Validation Loss: 0.8021, Accuracy: 0.8551, Precision: 0.7262, Recall: 0.7160, F1: 0.7165
Testing Loss: 0.9306, Accuracy: 0.8644, Precision: 0.6930, Recall: 0.6866, F1: 0.6805
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0402, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0153, Accuracy: 0.9962, Precision: 0.9947, Recall: 0.9949, F1: 0.9948
Validation Loss: 0.8251, Accuracy: 0.8523, Precision: 0.6980, Recall: 0.6262, F1: 0.6495
Testing Loss: 0.9413, Accuracy: 0.8644, Precision: 0.7288, Recall: 0.6603, F1: 0.6856
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0222, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0226, Accuracy: 0.9955, Precision: 0.9892, Recall: 0.9949, F1: 0.9920
Validation Loss: 0.7268, Accuracy: 0.8608, Precision: 0.7073, Recall: 0.7197, F1: 0.7062
Testing Loss: 0.8579, Accuracy: 0.8590, Precision: 0.6935, Recall: 0.6833, F1: 0.6814
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0317, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 23/70
Train Loss: 0.0069, Accuracy: 0.9983, Precision: 0.9990, Recall: 0.9989, F1: 0.9989
Validation Loss: 0.7801, Accuracy: 0.8665, Precision: 0.7140, Recall: 0.7215, F1: 0.7076
Testing Loss: 0.9398, Accuracy: 0.8511, Precision: 0.6879, Recall: 0.6722, F1: 0.6672
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0120, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0071, Accuracy: 0.9979, Precision: 0.9938, Recall: 0.9937, F1: 0.9938
Validation Loss: 0.8194, Accuracy: 0.8580, Precision: 0.6846, Recall: 0.6502, F1: 0.6597
Testing Loss: 0.9917, Accuracy: 0.8484, Precision: 0.6937, Recall: 0.6616, F1: 0.6680
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0116, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0023, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9997, F1: 0.9998
Validation Loss: 0.8261, Accuracy: 0.8693, Precision: 0.7073, Recall: 0.6668, F1: 0.6785
Testing Loss: 0.9897, Accuracy: 0.8644, Precision: 0.7001, Recall: 0.6845, F1: 0.6860
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0073, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0121, Accuracy: 0.9972, Precision: 0.9968, Recall: 0.9971, F1: 0.9969
Validation Loss: 0.8113, Accuracy: 0.8665, Precision: 0.6943, Recall: 0.6718, F1: 0.6761
Testing Loss: 0.9263, Accuracy: 0.8670, Precision: 0.7122, Recall: 0.6862, F1: 0.6924
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0060, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8367, Accuracy: 0.8693, Precision: 0.7256, Recall: 0.7258, F1: 0.7162
Testing Loss: 0.9375, Accuracy: 0.8617, Precision: 0.6943, Recall: 0.6850, F1: 0.6835
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0046, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8519, Accuracy: 0.8636, Precision: 0.7218, Recall: 0.7189, F1: 0.7114
Testing Loss: 0.9605, Accuracy: 0.8617, Precision: 0.6959, Recall: 0.6878, F1: 0.6868
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0032, Accuracy: 0.9990, Precision: 0.9966, Recall: 0.9993, F1: 0.9980
Validation Loss: 1.0219, Accuracy: 0.8438, Precision: 0.7007, Recall: 0.6116, F1: 0.6271
Testing Loss: 1.1571, Accuracy: 0.8590, Precision: 0.6952, Recall: 0.6383, F1: 0.6397
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.2391, Accuracy: 0.9286, Precision: 0.9556, Recall: 0.9200, F1: 0.9250
Epoch 30/70
Train Loss: 0.0196, Accuracy: 0.9958, Precision: 0.9895, Recall: 0.9919, F1: 0.9907
Validation Loss: 0.7700, Accuracy: 0.8722, Precision: 0.7205, Recall: 0.7141, F1: 0.7046
Testing Loss: 0.9576, Accuracy: 0.8457, Precision: 0.6846, Recall: 0.6570, F1: 0.6604
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0154, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0051, Accuracy: 0.9986, Precision: 0.9980, Recall: 0.9990, F1: 0.9985
Validation Loss: 0.8099, Accuracy: 0.8580, Precision: 0.6885, Recall: 0.6414, F1: 0.6603
Testing Loss: 0.9903, Accuracy: 0.8537, Precision: 0.7297, Recall: 0.6583, F1: 0.6809
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0051, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0021, Accuracy: 0.9993, Precision: 0.9995, Recall: 0.9997, F1: 0.9996
Validation Loss: 0.7634, Accuracy: 0.8778, Precision: 0.7522, Recall: 0.7653, F1: 0.7513
Testing Loss: 0.9609, Accuracy: 0.8537, Precision: 0.7082, Recall: 0.6730, F1: 0.6829
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0041, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8447, Accuracy: 0.8807, Precision: 0.7233, Recall: 0.7361, F1: 0.7244
Testing Loss: 0.9886, Accuracy: 0.8750, Precision: 0.7223, Recall: 0.6932, F1: 0.6989
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0068, Accuracy: 0.9979, Precision: 0.9989, Recall: 0.9987, F1: 0.9988
Validation Loss: 0.8743, Accuracy: 0.8807, Precision: 0.6999, Recall: 0.6866, F1: 0.6883
Testing Loss: 1.0703, Accuracy: 0.8511, Precision: 0.6773, Recall: 0.6575, F1: 0.6571
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0087, Accuracy: 0.9972, Precision: 0.9940, Recall: 0.9942, F1: 0.9941
Validation Loss: 0.9113, Accuracy: 0.8665, Precision: 0.6780, Recall: 0.6436, F1: 0.6508
Testing Loss: 1.2277, Accuracy: 0.8378, Precision: 0.6420, Recall: 0.6210, F1: 0.6237
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 2, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1081, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 36/70
Train Loss: 0.0271, Accuracy: 0.9920, Precision: 0.9788, Recall: 0.9808, F1: 0.9798
Validation Loss: 0.6350, Accuracy: 0.8835, Precision: 0.6942, Recall: 0.7099, F1: 0.6995
Testing Loss: 0.8672, Accuracy: 0.8378, Precision: 0.6574, Recall: 0.6738, F1: 0.6537
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0104, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0100, Accuracy: 0.9979, Precision: 0.9975, Recall: 0.9985, F1: 0.9980
Validation Loss: 0.6873, Accuracy: 0.8608, Precision: 0.6841, Recall: 0.6473, F1: 0.6611
Testing Loss: 0.9015, Accuracy: 0.8564, Precision: 0.6934, Recall: 0.6796, F1: 0.6805
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0058, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7040, Accuracy: 0.8750, Precision: 0.7358, Recall: 0.7159, F1: 0.7208
Testing Loss: 0.9271, Accuracy: 0.8537, Precision: 0.6772, Recall: 0.6780, F1: 0.6720
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0038, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7356, Accuracy: 0.8722, Precision: 0.7208, Recall: 0.7242, F1: 0.7160
Testing Loss: 0.9675, Accuracy: 0.8484, Precision: 0.6746, Recall: 0.6710, F1: 0.6645
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0029, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7631, Accuracy: 0.8722, Precision: 0.7287, Recall: 0.7156, F1: 0.7118
Testing Loss: 0.9946, Accuracy: 0.8484, Precision: 0.6763, Recall: 0.6693, F1: 0.6650
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7817, Accuracy: 0.8722, Precision: 0.7171, Recall: 0.7189, F1: 0.7067
Testing Loss: 1.0059, Accuracy: 0.8511, Precision: 0.6800, Recall: 0.6734, F1: 0.6670
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0043, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9984, F1: 0.9984
Validation Loss: 0.8023, Accuracy: 0.8778, Precision: 0.7263, Recall: 0.7414, F1: 0.7265
Testing Loss: 1.0435, Accuracy: 0.8537, Precision: 0.6918, Recall: 0.6870, F1: 0.6857
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0038, Accuracy: 0.9990, Precision: 0.9979, Recall: 0.9969, F1: 0.9974
Validation Loss: 0.8124, Accuracy: 0.8722, Precision: 0.6840, Recall: 0.6848, F1: 0.6805
Testing Loss: 1.0282, Accuracy: 0.8457, Precision: 0.6678, Recall: 0.6788, F1: 0.6618
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8192, Accuracy: 0.8778, Precision: 0.6967, Recall: 0.6888, F1: 0.6874
Testing Loss: 1.0770, Accuracy: 0.8404, Precision: 0.6581, Recall: 0.6673, F1: 0.6505
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8310, Accuracy: 0.8807, Precision: 0.7021, Recall: 0.6900, F1: 0.6913
Testing Loss: 1.0974, Accuracy: 0.8378, Precision: 0.6534, Recall: 0.6554, F1: 0.6436
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0137, Accuracy: 0.9962, Precision: 0.9977, Recall: 0.9977, F1: 0.9977
Validation Loss: 0.7691, Accuracy: 0.8693, Precision: 0.6914, Recall: 0.6729, F1: 0.6754
Testing Loss: 1.0170, Accuracy: 0.8351, Precision: 0.6554, Recall: 0.6451, F1: 0.6355
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0125, Accuracy: 0.9976, Precision: 0.9959, Recall: 0.9975, F1: 0.9967
Validation Loss: 0.8394, Accuracy: 0.8494, Precision: 0.6830, Recall: 0.6280, F1: 0.6475
Testing Loss: 1.0551, Accuracy: 0.8324, Precision: 0.6823, Recall: 0.6365, F1: 0.6512
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0029, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0027, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9983, F1: 0.9983
Validation Loss: 0.8507, Accuracy: 0.8523, Precision: 0.6992, Recall: 0.7187, F1: 0.6937
Testing Loss: 1.0313, Accuracy: 0.8351, Precision: 0.6426, Recall: 0.6674, F1: 0.6391
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0053, Accuracy: 0.9983, Precision: 0.9959, Recall: 0.9976, F1: 0.9967
Validation Loss: 0.7520, Accuracy: 0.8665, Precision: 0.6731, Recall: 0.6981, F1: 0.6826
Testing Loss: 1.0030, Accuracy: 0.8484, Precision: 0.6563, Recall: 0.6800, F1: 0.6639
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0053, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9995, F1: 0.9990
Validation Loss: 0.8587, Accuracy: 0.8665, Precision: 0.6952, Recall: 0.7007, F1: 0.6925
Testing Loss: 1.0648, Accuracy: 0.8404, Precision: 0.6599, Recall: 0.6616, F1: 0.6488
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7851, Accuracy: 0.8722, Precision: 0.6987, Recall: 0.6986, F1: 0.6955
Testing Loss: 1.0654, Accuracy: 0.8537, Precision: 0.6823, Recall: 0.6751, F1: 0.6662
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8214, Accuracy: 0.8722, Precision: 0.6987, Recall: 0.6986, F1: 0.6955
Testing Loss: 1.0821, Accuracy: 0.8537, Precision: 0.6750, Recall: 0.6751, F1: 0.6637
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8249, Accuracy: 0.8693, Precision: 0.6935, Recall: 0.6958, F1: 0.6913
Testing Loss: 1.0954, Accuracy: 0.8511, Precision: 0.6803, Recall: 0.6722, F1: 0.6638
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7942, Accuracy: 0.8750, Precision: 0.6967, Recall: 0.6983, F1: 0.6945
Testing Loss: 1.1202, Accuracy: 0.8457, Precision: 0.6772, Recall: 0.6665, F1: 0.6594
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7986, Accuracy: 0.8807, Precision: 0.7033, Recall: 0.7024, F1: 0.7001
Testing Loss: 1.1383, Accuracy: 0.8511, Precision: 0.6803, Recall: 0.6722, F1: 0.6638
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0001, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8141, Accuracy: 0.8807, Precision: 0.7033, Recall: 0.7024, F1: 0.7001
Testing Loss: 1.1488, Accuracy: 0.8511, Precision: 0.6803, Recall: 0.6722, F1: 0.6638
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0025, Accuracy: 0.9993, Precision: 0.9983, Recall: 0.9983, F1: 0.9983
Validation Loss: 1.0064, Accuracy: 0.8636, Precision: 0.7031, Recall: 0.6358, F1: 0.6551
Testing Loss: 1.2470, Accuracy: 0.8564, Precision: 0.7262, Recall: 0.6509, F1: 0.6730
LM Predictions:  [2, 0, 2, 5, 5, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.1925, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 58/70
Train Loss: 0.0081, Accuracy: 0.9972, Precision: 0.9886, Recall: 0.9902, F1: 0.9893
Validation Loss: 0.8588, Accuracy: 0.8608, Precision: 0.6808, Recall: 0.6457, F1: 0.6575
Testing Loss: 1.0993, Accuracy: 0.8431, Precision: 0.6533, Recall: 0.6346, F1: 0.6272
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0061, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0200, Accuracy: 0.9941, Precision: 0.9818, Recall: 0.9826, F1: 0.9822
Validation Loss: 0.8873, Accuracy: 0.8665, Precision: 0.6849, Recall: 0.6732, F1: 0.6771
Testing Loss: 1.0749, Accuracy: 0.8457, Precision: 0.6522, Recall: 0.6523, F1: 0.6465
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0108, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0079, Accuracy: 0.9976, Precision: 0.9962, Recall: 0.9946, F1: 0.9954
Validation Loss: 0.9644, Accuracy: 0.8580, Precision: 0.7356, Recall: 0.7756, F1: 0.7358
Testing Loss: 1.1044, Accuracy: 0.8511, Precision: 0.6990, Recall: 0.6969, F1: 0.6924
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0033, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0040, Accuracy: 0.9986, Precision: 0.9952, Recall: 0.9978, F1: 0.9964
Validation Loss: 0.8961, Accuracy: 0.8608, Precision: 0.7093, Recall: 0.7247, F1: 0.7092
Testing Loss: 1.1261, Accuracy: 0.8378, Precision: 0.6689, Recall: 0.6922, F1: 0.6698
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8834, Accuracy: 0.8693, Precision: 0.7129, Recall: 0.7300, F1: 0.7137
Testing Loss: 1.1252, Accuracy: 0.8404, Precision: 0.6851, Recall: 0.6951, F1: 0.6809
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9241, Accuracy: 0.8580, Precision: 0.6745, Recall: 0.6447, F1: 0.6556
Testing Loss: 1.1604, Accuracy: 0.8404, Precision: 0.6774, Recall: 0.6702, F1: 0.6673
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0007, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9986, F1: 0.9992
Validation Loss: 0.8822, Accuracy: 0.8580, Precision: 0.6954, Recall: 0.7072, F1: 0.6939
Testing Loss: 1.1750, Accuracy: 0.8378, Precision: 0.6852, Recall: 0.6970, F1: 0.6816
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0076, Accuracy: 0.9969, Precision: 0.9941, Recall: 0.9957, F1: 0.9949
Validation Loss: 0.9533, Accuracy: 0.8636, Precision: 0.7440, Recall: 0.7174, F1: 0.7193
Testing Loss: 1.1518, Accuracy: 0.8404, Precision: 0.6635, Recall: 0.6449, F1: 0.6420
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0057, Accuracy: 0.9983, Precision: 0.9992, Recall: 0.9992, F1: 0.9992
Validation Loss: 0.9261, Accuracy: 0.8551, Precision: 0.6888, Recall: 0.6509, F1: 0.6606
Testing Loss: 1.1278, Accuracy: 0.8378, Precision: 0.6566, Recall: 0.6456, F1: 0.6432
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0032, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9969, F1: 0.9982
Validation Loss: 0.9048, Accuracy: 0.8551, Precision: 0.6818, Recall: 0.6448, F1: 0.6570
Testing Loss: 1.0724, Accuracy: 0.8484, Precision: 0.6668, Recall: 0.6552, F1: 0.6564
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9454, Accuracy: 0.8523, Precision: 0.6704, Recall: 0.6645, F1: 0.6638
Testing Loss: 1.1005, Accuracy: 0.8431, Precision: 0.6620, Recall: 0.6511, F1: 0.6515
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9522, Accuracy: 0.8636, Precision: 0.7229, Recall: 0.7240, F1: 0.7162
Testing Loss: 1.1166, Accuracy: 0.8431, Precision: 0.6620, Recall: 0.6511, F1: 0.6515
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0002, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.9418, Accuracy: 0.8608, Precision: 0.7194, Recall: 0.7125, F1: 0.7074
Testing Loss: 1.1217, Accuracy: 0.8431, Precision: 0.6668, Recall: 0.6494, F1: 0.6537
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



