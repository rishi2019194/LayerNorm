---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4182, Accuracy: 0.3663, Precision: 0.1938, Recall: 0.1702, F1: 0.1523
Validation Loss: 1.4299, Accuracy: 0.3324, Precision: 0.0554, Recall: 0.1667, F1: 0.0832
Testing Loss: 1.4471, Accuracy: 0.3537, Precision: 0.0590, Recall: 0.1667, F1: 0.0871
LM Predictions:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8786, Accuracy: 0.1786, Precision: 0.0357, Recall: 0.2000, F1: 0.0606
Epoch 2/70
Train Loss: 1.3835, Accuracy: 0.3793, Precision: 0.1235, Recall: 0.1707, F1: 0.1417
Validation Loss: 1.4208, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.4654, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9587, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 3/70
Train Loss: 1.3563, Accuracy: 0.4440, Precision: 0.1470, Recall: 0.2016, F1: 0.1694
Validation Loss: 1.3256, Accuracy: 0.4716, Precision: 0.1581, Recall: 0.2168, F1: 0.1774
Testing Loss: 1.3354, Accuracy: 0.5027, Precision: 0.1687, Recall: 0.2338, F1: 0.1943
LM Predictions:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9993, Accuracy: 0.2143, Precision: 0.0865, Recall: 0.2171, F1: 0.1167
Epoch 4/70
Train Loss: 1.2311, Accuracy: 0.5371, Precision: 0.2103, Recall: 0.2458, F1: 0.2087
Validation Loss: 1.0843, Accuracy: 0.6165, Precision: 0.2175, Recall: 0.2882, F1: 0.2443
Testing Loss: 1.0295, Accuracy: 0.6436, Precision: 0.2268, Recall: 0.2997, F1: 0.2540
LM Predictions:  [2, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0810, Accuracy: 0.2857, Precision: 0.1322, Recall: 0.2514, F1: 0.1600
Epoch 5/70
Train Loss: 0.9776, Accuracy: 0.6375, Precision: 0.2878, Recall: 0.3169, F1: 0.3002
Validation Loss: 0.9498, Accuracy: 0.6506, Precision: 0.3050, Recall: 0.3161, F1: 0.2850
Testing Loss: 0.9002, Accuracy: 0.6622, Precision: 0.2654, Recall: 0.3152, F1: 0.2783
LM Predictions:  [5, 4, 4, 4, 4, 2, 4, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4, 5, 2, 5, 2, 2, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8772, Accuracy: 0.2857, Precision: 0.1750, Recall: 0.2743, F1: 0.1926
Epoch 6/70
Train Loss: 0.8577, Accuracy: 0.6907, Precision: 0.3322, Recall: 0.3673, F1: 0.3462
Validation Loss: 0.8921, Accuracy: 0.6562, Precision: 0.3080, Recall: 0.3410, F1: 0.3226
Testing Loss: 0.8015, Accuracy: 0.7181, Precision: 0.3359, Recall: 0.3854, F1: 0.3575
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 4, 4, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7114, Accuracy: 0.3571, Precision: 0.3522, Recall: 0.2971, F1: 0.2571
Epoch 7/70
Train Loss: 0.8206, Accuracy: 0.7029, Precision: 0.3407, Recall: 0.3763, F1: 0.3541
Validation Loss: 0.8171, Accuracy: 0.7045, Precision: 0.3375, Recall: 0.3765, F1: 0.3538
Testing Loss: 0.7927, Accuracy: 0.7234, Precision: 0.3435, Recall: 0.3960, F1: 0.3650
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7968, Accuracy: 0.2857, Precision: 0.3480, Recall: 0.2400, F1: 0.1821
Epoch 8/70
Train Loss: 0.7767, Accuracy: 0.7246, Precision: 0.3538, Recall: 0.3943, F1: 0.3680
Validation Loss: 0.8378, Accuracy: 0.7045, Precision: 0.3556, Recall: 0.3960, F1: 0.3650
Testing Loss: 0.7647, Accuracy: 0.7473, Precision: 0.3695, Recall: 0.4284, F1: 0.3864
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9166, Accuracy: 0.2857, Precision: 0.3480, Recall: 0.2400, F1: 0.1821
Epoch 9/70
Train Loss: 0.7084, Accuracy: 0.7449, Precision: 0.3816, Recall: 0.4067, F1: 0.3813
Validation Loss: 0.8697, Accuracy: 0.7188, Precision: 0.3520, Recall: 0.3962, F1: 0.3682
Testing Loss: 0.7986, Accuracy: 0.7500, Precision: 0.3594, Recall: 0.4166, F1: 0.3819
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9202, Accuracy: 0.2857, Precision: 0.3480, Recall: 0.2400, F1: 0.1821
Epoch 10/70
Train Loss: 0.7108, Accuracy: 0.7551, Precision: 0.4626, Recall: 0.4203, F1: 0.4022
Validation Loss: 0.7712, Accuracy: 0.7131, Precision: 0.3556, Recall: 0.4005, F1: 0.3682
Testing Loss: 0.7167, Accuracy: 0.7473, Precision: 0.3653, Recall: 0.4237, F1: 0.3837
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9310, Accuracy: 0.2857, Precision: 0.3480, Recall: 0.2400, F1: 0.1821
Epoch 11/70
Train Loss: 0.6688, Accuracy: 0.7666, Precision: 0.4417, Recall: 0.4653, F1: 0.4495
Validation Loss: 0.7422, Accuracy: 0.7386, Precision: 0.3636, Recall: 0.4116, F1: 0.3802
Testing Loss: 0.6863, Accuracy: 0.7580, Precision: 0.3652, Recall: 0.4253, F1: 0.3874
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8809, Accuracy: 0.3214, Precision: 0.4538, Recall: 0.2686, F1: 0.2015
Epoch 12/70
Train Loss: 0.6178, Accuracy: 0.7803, Precision: 0.4613, Recall: 0.4574, F1: 0.4516
Validation Loss: 0.7925, Accuracy: 0.7415, Precision: 0.3600, Recall: 0.4115, F1: 0.3801
Testing Loss: 0.7287, Accuracy: 0.7660, Precision: 0.3647, Recall: 0.4257, F1: 0.3901
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9653, Accuracy: 0.3214, Precision: 0.4538, Recall: 0.2686, F1: 0.2015
Epoch 13/70
Train Loss: 0.5924, Accuracy: 0.7992, Precision: 0.4599, Recall: 0.5051, F1: 0.4796
Validation Loss: 0.7339, Accuracy: 0.7642, Precision: 0.4523, Recall: 0.5014, F1: 0.4673
Testing Loss: 0.6735, Accuracy: 0.7713, Precision: 0.4479, Recall: 0.5448, F1: 0.4686
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 5, 5, 1, 1]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3289, Accuracy: 0.2143, Precision: 0.4928, Recall: 0.2471, F1: 0.2011
Epoch 14/70
Train Loss: 0.5347, Accuracy: 0.8104, Precision: 0.5217, Recall: 0.5070, F1: 0.5080
Validation Loss: 0.8008, Accuracy: 0.7642, Precision: 0.4456, Recall: 0.5110, F1: 0.4692
Testing Loss: 0.7375, Accuracy: 0.7819, Precision: 0.4518, Recall: 0.4757, F1: 0.4635
LM Predictions:  [3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 5, 5, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 5, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8720, Accuracy: 0.2500, Precision: 0.4074, Recall: 0.1762, F1: 0.2130
Epoch 15/70
Train Loss: 0.5187, Accuracy: 0.8272, Precision: 0.5260, Recall: 0.5513, F1: 0.5156
Validation Loss: 0.7322, Accuracy: 0.7727, Precision: 0.4458, Recall: 0.4914, F1: 0.4654
Testing Loss: 0.7093, Accuracy: 0.7979, Precision: 0.4698, Recall: 0.4934, F1: 0.4785
LM Predictions:  [5, 3, 3, 3, 3, 2, 4, 5, 1, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 5, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7451, Accuracy: 0.3214, Precision: 0.3500, Recall: 0.2333, F1: 0.2675
Epoch 16/70
Train Loss: 0.5109, Accuracy: 0.8237, Precision: 0.5298, Recall: 0.5518, F1: 0.5313
Validation Loss: 0.7766, Accuracy: 0.7557, Precision: 0.4488, Recall: 0.5204, F1: 0.4664
Testing Loss: 0.6316, Accuracy: 0.7926, Precision: 0.4738, Recall: 0.5200, F1: 0.4911
LM Predictions:  [3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 5, 5, 3, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1312, Accuracy: 0.2143, Precision: 0.2063, Recall: 0.1429, F1: 0.1619
Epoch 17/70
Train Loss: 0.4902, Accuracy: 0.8362, Precision: 0.5441, Recall: 0.5586, F1: 0.5390
Validation Loss: 0.7609, Accuracy: 0.7557, Precision: 0.4519, Recall: 0.5010, F1: 0.4623
Testing Loss: 0.7044, Accuracy: 0.7926, Precision: 0.5212, Recall: 0.4893, F1: 0.4893
LM Predictions:  [3, 3, 3, 3, 3, 4, 4, 5, 3, 3, 3, 1, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9299, Accuracy: 0.3214, Precision: 0.3917, Recall: 0.2429, F1: 0.2992
Epoch 18/70
Train Loss: 0.4839, Accuracy: 0.8362, Precision: 0.5305, Recall: 0.5538, F1: 0.5356
Validation Loss: 0.8457, Accuracy: 0.7670, Precision: 0.5406, Recall: 0.5398, F1: 0.5027
Testing Loss: 0.7443, Accuracy: 0.7846, Precision: 0.4925, Recall: 0.4996, F1: 0.4919
LM Predictions:  [3, 3, 3, 3, 3, 4, 4, 5, 1, 3, 3, 3, 5, 2, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 4, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1040, Accuracy: 0.2857, Precision: 0.3361, Recall: 0.2190, F1: 0.2611
Epoch 19/70
Train Loss: 0.4659, Accuracy: 0.8397, Precision: 0.5398, Recall: 0.5574, F1: 0.5453
Validation Loss: 0.7868, Accuracy: 0.7727, Precision: 0.4527, Recall: 0.4902, F1: 0.4694
Testing Loss: 0.7001, Accuracy: 0.7793, Precision: 0.5176, Recall: 0.5057, F1: 0.4745
LM Predictions:  [1, 1, 1, 1, 1, 2, 4, 5, 1, 1, 1, 5, 2, 2, 4, 5, 1, 1, 1, 1, 5, 2, 1, 2, 1, 2, 1, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8113, Accuracy: 0.4286, Precision: 0.4733, Recall: 0.4514, F1: 0.3908
Epoch 20/70
Train Loss: 0.4289, Accuracy: 0.8474, Precision: 0.5400, Recall: 0.5749, F1: 0.5485
Validation Loss: 0.8465, Accuracy: 0.7443, Precision: 0.4510, Recall: 0.4905, F1: 0.4581
Testing Loss: 0.7592, Accuracy: 0.7713, Precision: 0.4669, Recall: 0.4639, F1: 0.4583
LM Predictions:  [3, 3, 4, 3, 3, 4, 4, 5, 3, 3, 3, 3, 2, 4, 4, 3, 4, 4, 4, 3, 5, 2, 3, 2, 4, 2, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9560, Accuracy: 0.3571, Precision: 0.3843, Recall: 0.2857, F1: 0.2840
Epoch 21/70
Train Loss: 0.4177, Accuracy: 0.8527, Precision: 0.5411, Recall: 0.5773, F1: 0.5474
Validation Loss: 0.6982, Accuracy: 0.7926, Precision: 0.5396, Recall: 0.5589, F1: 0.5443
Testing Loss: 0.6545, Accuracy: 0.8005, Precision: 0.5118, Recall: 0.5151, F1: 0.5129
LM Predictions:  [3, 3, 3, 3, 3, 4, 4, 5, 1, 3, 3, 1, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9453, Accuracy: 0.3214, Precision: 0.3917, Recall: 0.2429, F1: 0.2992
Epoch 22/70
Train Loss: 0.4120, Accuracy: 0.8607, Precision: 0.5476, Recall: 0.5757, F1: 0.5541
Validation Loss: 0.7118, Accuracy: 0.7898, Precision: 0.4715, Recall: 0.5263, F1: 0.4958
Testing Loss: 0.7315, Accuracy: 0.8032, Precision: 0.4789, Recall: 0.5061, F1: 0.4916
LM Predictions:  [5, 3, 3, 5, 3, 2, 4, 5, 5, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 3, 2, 3, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8539, Accuracy: 0.2857, Precision: 0.3125, Recall: 0.2095, F1: 0.2388
Epoch 23/70
Train Loss: 0.3963, Accuracy: 0.8642, Precision: 0.5789, Recall: 0.6004, F1: 0.5795
Validation Loss: 0.7175, Accuracy: 0.7670, Precision: 0.5338, Recall: 0.5275, F1: 0.4881
Testing Loss: 0.6400, Accuracy: 0.8138, Precision: 0.5223, Recall: 0.5496, F1: 0.5300
LM Predictions:  [3, 3, 3, 3, 3, 2, 4, 5, 3, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 3, 5, 2, 3, 5, 3, 5, 3, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9815, Accuracy: 0.2857, Precision: 0.3730, Recall: 0.2095, F1: 0.2571
Epoch 24/70
Train Loss: 0.3778, Accuracy: 0.8712, Precision: 0.5768, Recall: 0.6010, F1: 0.5749
Validation Loss: 0.7971, Accuracy: 0.7812, Precision: 0.5178, Recall: 0.5371, F1: 0.4991
Testing Loss: 0.6433, Accuracy: 0.8218, Precision: 0.4946, Recall: 0.5431, F1: 0.5146
LM Predictions:  [3, 3, 3, 3, 3, 4, 4, 5, 3, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0469, Accuracy: 0.3571, Precision: 0.4028, Recall: 0.2667, F1: 0.3185
Epoch 25/70
Train Loss: 0.3697, Accuracy: 0.8779, Precision: 0.5764, Recall: 0.6030, F1: 0.5799
Validation Loss: 0.6996, Accuracy: 0.7756, Precision: 0.4528, Recall: 0.4934, F1: 0.4720
Testing Loss: 0.6916, Accuracy: 0.8112, Precision: 0.4801, Recall: 0.5049, F1: 0.4921
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 5, 2, 4, 4, 5, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7508, Accuracy: 0.2857, Precision: 0.2560, Recall: 0.2190, F1: 0.2357
Epoch 26/70
Train Loss: 0.3554, Accuracy: 0.8786, Precision: 0.5676, Recall: 0.6110, F1: 0.5742
Validation Loss: 0.7363, Accuracy: 0.7869, Precision: 0.5261, Recall: 0.5391, F1: 0.5213
Testing Loss: 0.6439, Accuracy: 0.8059, Precision: 0.5031, Recall: 0.5130, F1: 0.5062
LM Predictions:  [5, 3, 3, 1, 3, 2, 4, 5, 5, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 1, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6542, Accuracy: 0.2500, Precision: 0.2937, Recall: 0.1857, F1: 0.2179
Epoch 27/70
Train Loss: 0.3495, Accuracy: 0.8779, Precision: 0.5758, Recall: 0.5981, F1: 0.5801
Validation Loss: 0.7985, Accuracy: 0.7642, Precision: 0.4728, Recall: 0.5179, F1: 0.4909
Testing Loss: 0.7186, Accuracy: 0.8112, Precision: 0.5668, Recall: 0.5296, F1: 0.5300
LM Predictions:  [5, 3, 3, 5, 3, 4, 4, 5, 5, 3, 3, 5, 2, 4, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 5, 1, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6871, Accuracy: 0.3571, Precision: 0.3657, Recall: 0.2667, F1: 0.2944
Epoch 28/70
Train Loss: 0.3433, Accuracy: 0.8765, Precision: 0.5950, Recall: 0.6117, F1: 0.6000
Validation Loss: 0.6750, Accuracy: 0.7841, Precision: 0.5353, Recall: 0.5510, F1: 0.5273
Testing Loss: 0.6033, Accuracy: 0.8085, Precision: 0.5238, Recall: 0.5362, F1: 0.5287
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8370, Accuracy: 0.3214, Precision: 0.3292, Recall: 0.2429, F1: 0.2750
Epoch 29/70
Train Loss: 0.3229, Accuracy: 0.8887, Precision: 0.6175, Recall: 0.6352, F1: 0.6215
Validation Loss: 0.8016, Accuracy: 0.7898, Precision: 0.5134, Recall: 0.5292, F1: 0.5115
Testing Loss: 0.6814, Accuracy: 0.8165, Precision: 0.5131, Recall: 0.5217, F1: 0.5137
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 1, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7742, Accuracy: 0.3929, Precision: 0.5119, Recall: 0.3083, F1: 0.3638
Epoch 30/70
Train Loss: 0.3162, Accuracy: 0.8926, Precision: 0.6252, Recall: 0.6514, F1: 0.6367
Validation Loss: 0.7785, Accuracy: 0.7727, Precision: 0.4835, Recall: 0.5028, F1: 0.4920
Testing Loss: 0.6819, Accuracy: 0.8059, Precision: 0.5445, Recall: 0.5414, F1: 0.5424
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 1, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 3, 2, 3, 2, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9923, Accuracy: 0.3214, Precision: 0.3333, Recall: 0.2429, F1: 0.2745
Epoch 31/70
Train Loss: 0.3003, Accuracy: 0.8954, Precision: 0.7251, Recall: 0.6450, F1: 0.6355
Validation Loss: 0.8806, Accuracy: 0.7727, Precision: 0.4880, Recall: 0.5078, F1: 0.4927
Testing Loss: 0.8594, Accuracy: 0.7846, Precision: 0.5621, Recall: 0.5235, F1: 0.5336
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 1, 2, 2, 4, 2, 3, 3, 3, 3, 2, 2, 1, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0010, Accuracy: 0.2857, Precision: 0.2963, Recall: 0.2190, F1: 0.2417
Epoch 32/70
Train Loss: 0.2868, Accuracy: 0.9006, Precision: 0.6738, Recall: 0.6542, F1: 0.6458
Validation Loss: 0.8670, Accuracy: 0.7812, Precision: 0.4797, Recall: 0.5318, F1: 0.5013
Testing Loss: 0.7505, Accuracy: 0.8165, Precision: 0.5350, Recall: 0.5435, F1: 0.5369
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 3, 5, 2, 3, 2, 3, 2, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0318, Accuracy: 0.3571, Precision: 0.3500, Recall: 0.2667, F1: 0.2972
Epoch 33/70
Train Loss: 0.2680, Accuracy: 0.9090, Precision: 0.6537, Recall: 0.6771, F1: 0.6641
Validation Loss: 0.8557, Accuracy: 0.7699, Precision: 0.5121, Recall: 0.5373, F1: 0.5203
Testing Loss: 0.7540, Accuracy: 0.8059, Precision: 0.5453, Recall: 0.5559, F1: 0.5480
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 1, 2, 2, 4, 5, 3, 3, 3, 3, 1, 2, 1, 2, 3, 3, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3584, Accuracy: 0.2857, Precision: 0.3333, Recall: 0.2190, F1: 0.2639
Epoch 34/70
Train Loss: 0.2717, Accuracy: 0.9132, Precision: 0.8340, Recall: 0.6929, F1: 0.6835
Validation Loss: 0.9691, Accuracy: 0.7926, Precision: 0.6016, Recall: 0.5161, F1: 0.5410
Testing Loss: 0.9377, Accuracy: 0.7979, Precision: 0.5862, Recall: 0.5441, F1: 0.5456
LM Predictions:  [5, 3, 3, 2, 1, 4, 2, 5, 5, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 1, 2, 2, 1, 2, 1, 2, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.6025, Accuracy: 0.3571, Precision: 0.3741, Recall: 0.2929, F1: 0.3082
Epoch 35/70
Train Loss: 0.2894, Accuracy: 0.9020, Precision: 0.6941, Recall: 0.6629, F1: 0.6586
Validation Loss: 0.8419, Accuracy: 0.7869, Precision: 0.5105, Recall: 0.5301, F1: 0.5124
Testing Loss: 0.7393, Accuracy: 0.8112, Precision: 0.5768, Recall: 0.5661, F1: 0.5669
LM Predictions:  [0, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 2, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 5, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2543, Accuracy: 0.3214, Precision: 0.3241, Recall: 0.2429, F1: 0.2689
Epoch 36/70
Train Loss: 0.2554, Accuracy: 0.9136, Precision: 0.7254, Recall: 0.7101, F1: 0.7081
Validation Loss: 0.8925, Accuracy: 0.7955, Precision: 0.5419, Recall: 0.5494, F1: 0.5403
Testing Loss: 0.7968, Accuracy: 0.8059, Precision: 0.5500, Recall: 0.5344, F1: 0.5357
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 2, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3220, Accuracy: 0.3214, Precision: 0.3333, Recall: 0.2429, F1: 0.2745
Epoch 37/70
Train Loss: 0.2303, Accuracy: 0.9213, Precision: 0.7504, Recall: 0.7256, F1: 0.7339
Validation Loss: 0.8372, Accuracy: 0.7983, Precision: 0.5621, Recall: 0.5713, F1: 0.5652
Testing Loss: 0.7604, Accuracy: 0.8245, Precision: 0.5781, Recall: 0.5903, F1: 0.5816
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 1, 3, 3, 2, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 1, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5286, Accuracy: 0.3214, Precision: 0.3611, Recall: 0.2429, F1: 0.2806
Epoch 38/70
Train Loss: 0.2379, Accuracy: 0.9241, Precision: 0.7568, Recall: 0.7279, F1: 0.7246
Validation Loss: 0.8819, Accuracy: 0.7955, Precision: 0.6068, Recall: 0.5472, F1: 0.5610
Testing Loss: 0.8241, Accuracy: 0.8191, Precision: 0.5820, Recall: 0.5697, F1: 0.5707
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 2, 2, 5, 4, 5, 3, 5, 3, 3, 5, 2, 5, 2, 3, 5, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8469, Accuracy: 0.3929, Precision: 0.3519, Recall: 0.2905, F1: 0.3109
Epoch 39/70
Train Loss: 0.2305, Accuracy: 0.9265, Precision: 0.7812, Recall: 0.7562, F1: 0.7622
Validation Loss: 0.8155, Accuracy: 0.8210, Precision: 0.6461, Recall: 0.5767, F1: 0.5945
Testing Loss: 0.8125, Accuracy: 0.8351, Precision: 0.6227, Recall: 0.5948, F1: 0.6005
LM Predictions:  [5, 3, 3, 2, 3, 4, 4, 5, 5, 3, 3, 2, 2, 2, 4, 5, 3, 3, 3, 1, 2, 2, 1, 2, 3, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0790, Accuracy: 0.3214, Precision: 0.3167, Recall: 0.2429, F1: 0.2694
Epoch 40/70
Train Loss: 0.2086, Accuracy: 0.9374, Precision: 0.8175, Recall: 0.8019, F1: 0.8076
Validation Loss: 0.7736, Accuracy: 0.8040, Precision: 0.6058, Recall: 0.6011, F1: 0.5757
Testing Loss: 0.7356, Accuracy: 0.8324, Precision: 0.6102, Recall: 0.5722, F1: 0.5791
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 1, 3, 3, 0, 2, 0, 4, 5, 3, 3, 3, 1, 2, 2, 1, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1221, Accuracy: 0.3929, Precision: 0.5333, Recall: 0.3179, F1: 0.3803
Epoch 41/70
Train Loss: 0.1985, Accuracy: 0.9332, Precision: 0.8089, Recall: 0.8063, F1: 0.8060
Validation Loss: 0.8823, Accuracy: 0.8011, Precision: 0.6163, Recall: 0.5696, F1: 0.5858
Testing Loss: 0.8612, Accuracy: 0.8298, Precision: 0.6125, Recall: 0.6064, F1: 0.6079
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 2, 2, 1, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0312, Accuracy: 0.4643, Precision: 0.5500, Recall: 0.3655, F1: 0.4339
Epoch 42/70
Train Loss: 0.1989, Accuracy: 0.9423, Precision: 0.8326, Recall: 0.8298, F1: 0.8300
Validation Loss: 0.9765, Accuracy: 0.7983, Precision: 0.6093, Recall: 0.5368, F1: 0.5596
Testing Loss: 1.0201, Accuracy: 0.8059, Precision: 0.5920, Recall: 0.5590, F1: 0.5698
LM Predictions:  [0, 3, 3, 2, 3, 4, 4, 5, 3, 3, 3, 5, 2, 2, 4, 5, 3, 3, 3, 3, 2, 2, 1, 2, 3, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1524, Accuracy: 0.3214, Precision: 0.3333, Recall: 0.2429, F1: 0.2745
Epoch 43/70
Train Loss: 0.1958, Accuracy: 0.9356, Precision: 0.8042, Recall: 0.8129, F1: 0.8083
Validation Loss: 0.8877, Accuracy: 0.7898, Precision: 0.5700, Recall: 0.6043, F1: 0.5697
Testing Loss: 0.7936, Accuracy: 0.8085, Precision: 0.5911, Recall: 0.5788, F1: 0.5793
LM Predictions:  [0, 3, 3, 5, 3, 4, 4, 5, 3, 3, 3, 0, 2, 0, 4, 5, 3, 3, 3, 1, 2, 2, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2656, Accuracy: 0.3929, Precision: 0.5222, Recall: 0.3000, F1: 0.3778
Epoch 44/70
Train Loss: 0.2189, Accuracy: 0.9335, Precision: 0.8038, Recall: 0.8168, F1: 0.8084
Validation Loss: 0.9974, Accuracy: 0.7642, Precision: 0.5789, Recall: 0.6432, F1: 0.5972
Testing Loss: 0.7811, Accuracy: 0.8191, Precision: 0.6069, Recall: 0.6428, F1: 0.6206
LM Predictions:  [0, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 3, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7990, Accuracy: 0.4643, Precision: 0.5278, Recall: 0.3560, F1: 0.4056
Epoch 45/70
Train Loss: 0.2192, Accuracy: 0.9293, Precision: 0.8147, Recall: 0.8069, F1: 0.8060
Validation Loss: 0.8596, Accuracy: 0.7955, Precision: 0.6235, Recall: 0.5812, F1: 0.5884
Testing Loss: 0.7452, Accuracy: 0.8298, Precision: 0.6259, Recall: 0.6162, F1: 0.6187
LM Predictions:  [5, 3, 3, 0, 3, 4, 4, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7342, Accuracy: 0.4286, Precision: 0.4259, Recall: 0.3143, F1: 0.3504
Epoch 46/70
Train Loss: 0.2028, Accuracy: 0.9360, Precision: 0.7985, Recall: 0.7992, F1: 0.7949
Validation Loss: 1.0970, Accuracy: 0.7557, Precision: 0.5307, Recall: 0.4944, F1: 0.5005
Testing Loss: 0.9604, Accuracy: 0.7819, Precision: 0.5746, Recall: 0.5358, F1: 0.5280
LM Predictions:  [5, 4, 4, 0, 1, 4, 4, 5, 3, 4, 3, 2, 2, 0, 4, 5, 4, 4, 4, 1, 2, 2, 1, 2, 4, 4, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9478, Accuracy: 0.4286, Precision: 0.3396, Recall: 0.3512, F1: 0.3151
Epoch 47/70
Train Loss: 0.2333, Accuracy: 0.9342, Precision: 0.8266, Recall: 0.8294, F1: 0.8267
Validation Loss: 0.7814, Accuracy: 0.7528, Precision: 0.5491, Recall: 0.5281, F1: 0.5282
Testing Loss: 0.6926, Accuracy: 0.7872, Precision: 0.5213, Recall: 0.4921, F1: 0.4988
LM Predictions:  [5, 3, 3, 3, 5, 2, 4, 5, 3, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 3, 3, 2, 5, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2200, Accuracy: 0.3214, Precision: 0.3714, Recall: 0.2333, F1: 0.2778
Epoch 48/70
Train Loss: 0.3746, Accuracy: 0.8936, Precision: 0.6830, Recall: 0.7054, F1: 0.6905
Validation Loss: 1.0766, Accuracy: 0.7301, Precision: 0.5909, Recall: 0.5345, F1: 0.5369
Testing Loss: 1.0046, Accuracy: 0.7660, Precision: 0.5675, Recall: 0.5521, F1: 0.5278
LM Predictions:  [5, 3, 3, 5, 1, 5, 3, 5, 5, 5, 3, 5, 2, 5, 3, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1279, Accuracy: 0.3571, Precision: 0.3214, Recall: 0.2560, F1: 0.2508
Epoch 49/70
Train Loss: 0.2657, Accuracy: 0.9153, Precision: 0.7597, Recall: 0.7541, F1: 0.7497
Validation Loss: 0.9455, Accuracy: 0.8125, Precision: 0.6295, Recall: 0.5759, F1: 0.5947
Testing Loss: 0.9927, Accuracy: 0.8032, Precision: 0.5944, Recall: 0.5910, F1: 0.5833
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 2, 2, 5, 3, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3815, Accuracy: 0.4643, Precision: 0.4944, Recall: 0.3464, F1: 0.3869
Epoch 50/70
Train Loss: 0.2080, Accuracy: 0.9416, Precision: 0.8123, Recall: 0.8234, F1: 0.8161
Validation Loss: 0.8468, Accuracy: 0.8068, Precision: 0.6486, Recall: 0.6372, F1: 0.6211
Testing Loss: 0.8266, Accuracy: 0.8298, Precision: 0.6168, Recall: 0.6014, F1: 0.6053
LM Predictions:  [0, 3, 3, 3, 1, 4, 4, 5, 5, 3, 3, 2, 2, 0, 4, 5, 3, 3, 3, 3, 5, 2, 1, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9595, Accuracy: 0.4286, Precision: 0.5639, Recall: 0.3417, F1: 0.4242
Epoch 51/70
Train Loss: 0.1589, Accuracy: 0.9559, Precision: 0.8673, Recall: 0.8685, F1: 0.8678
Validation Loss: 0.8440, Accuracy: 0.8040, Precision: 0.6262, Recall: 0.6214, F1: 0.6038
Testing Loss: 0.7902, Accuracy: 0.8324, Precision: 0.6526, Recall: 0.6153, F1: 0.6318
LM Predictions:  [0, 3, 3, 0, 3, 4, 4, 5, 5, 3, 3, 2, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7853, Accuracy: 0.4286, Precision: 0.4190, Recall: 0.3143, F1: 0.3552
Epoch 52/70
Train Loss: 0.1490, Accuracy: 0.9563, Precision: 0.8580, Recall: 0.8678, F1: 0.8617
Validation Loss: 0.8663, Accuracy: 0.8239, Precision: 0.6410, Recall: 0.5817, F1: 0.5996
Testing Loss: 0.8113, Accuracy: 0.8431, Precision: 0.6541, Recall: 0.6239, F1: 0.6355
LM Predictions:  [5, 3, 3, 3, 1, 4, 4, 5, 3, 3, 3, 2, 2, 2, 4, 5, 3, 3, 3, 3, 5, 2, 1, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9447, Accuracy: 0.3929, Precision: 0.4702, Recall: 0.3083, F1: 0.3667
Epoch 53/70
Train Loss: 0.1542, Accuracy: 0.9559, Precision: 0.8711, Recall: 0.8688, F1: 0.8685
Validation Loss: 0.9266, Accuracy: 0.7983, Precision: 0.6265, Recall: 0.6036, F1: 0.5891
Testing Loss: 0.9215, Accuracy: 0.8218, Precision: 0.6190, Recall: 0.5878, F1: 0.5987
LM Predictions:  [0, 3, 3, 5, 3, 4, 4, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8041, Accuracy: 0.4643, Precision: 0.4111, Recall: 0.3381, F1: 0.3611
Epoch 54/70
Train Loss: 0.2000, Accuracy: 0.9486, Precision: 0.8559, Recall: 0.8579, F1: 0.8566
Validation Loss: 0.8627, Accuracy: 0.8040, Precision: 0.6532, Recall: 0.6678, F1: 0.6198
Testing Loss: 0.8504, Accuracy: 0.8218, Precision: 0.6297, Recall: 0.6014, F1: 0.6127
LM Predictions:  [0, 3, 3, 0, 3, 4, 4, 5, 3, 3, 3, 5, 2, 0, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0101, Accuracy: 0.3929, Precision: 0.4111, Recall: 0.2905, F1: 0.3387
Epoch 55/70
Train Loss: 0.1375, Accuracy: 0.9626, Precision: 0.8804, Recall: 0.8942, F1: 0.8866
Validation Loss: 0.9661, Accuracy: 0.8182, Precision: 0.6977, Recall: 0.6239, F1: 0.6301
Testing Loss: 1.0572, Accuracy: 0.8112, Precision: 0.6145, Recall: 0.5779, F1: 0.5894
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 2, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2275, Accuracy: 0.5000, Precision: 0.4702, Recall: 0.3798, F1: 0.4091
Epoch 56/70
Train Loss: 0.1334, Accuracy: 0.9650, Precision: 0.8914, Recall: 0.9016, F1: 0.8961
Validation Loss: 0.9084, Accuracy: 0.8153, Precision: 0.6571, Recall: 0.5847, F1: 0.6058
Testing Loss: 0.8919, Accuracy: 0.8271, Precision: 0.6228, Recall: 0.6022, F1: 0.6087
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 3, 3, 3, 2, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7739, Accuracy: 0.5000, Precision: 0.4881, Recall: 0.3798, F1: 0.4187
Epoch 57/70
Train Loss: 0.1438, Accuracy: 0.9615, Precision: 0.8835, Recall: 0.8824, F1: 0.8825
Validation Loss: 0.8628, Accuracy: 0.8097, Precision: 0.6186, Recall: 0.6092, F1: 0.6047
Testing Loss: 0.8050, Accuracy: 0.8298, Precision: 0.6349, Recall: 0.6261, F1: 0.6282
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 3, 3, 3, 5, 2, 3, 4, 5, 3, 3, 3, 3, 5, 2, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9260, Accuracy: 0.4643, Precision: 0.5278, Recall: 0.3738, F1: 0.4303
Epoch 58/70
Train Loss: 0.1337, Accuracy: 0.9626, Precision: 0.8879, Recall: 0.8944, F1: 0.8910
Validation Loss: 0.8941, Accuracy: 0.8011, Precision: 0.6832, Recall: 0.6400, F1: 0.6343
Testing Loss: 0.9038, Accuracy: 0.7979, Precision: 0.6197, Recall: 0.6057, F1: 0.6003
LM Predictions:  [0, 3, 3, 0, 1, 4, 4, 5, 0, 3, 3, 0, 2, 0, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7256, Accuracy: 0.5714, Precision: 0.6825, Recall: 0.4738, F1: 0.5460
Epoch 59/70
Train Loss: 0.1368, Accuracy: 0.9608, Precision: 0.8663, Recall: 0.8780, F1: 0.8716
Validation Loss: 0.9758, Accuracy: 0.7955, Precision: 0.5975, Recall: 0.5803, F1: 0.5604
Testing Loss: 0.9122, Accuracy: 0.8298, Precision: 0.6043, Recall: 0.6239, F1: 0.6103
LM Predictions:  [5, 3, 3, 3, 3, 4, 4, 5, 3, 3, 3, 3, 2, 3, 3, 5, 3, 3, 3, 3, 5, 2, 1, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2241, Accuracy: 0.3214, Precision: 0.4250, Recall: 0.2333, F1: 0.2973
Epoch 60/70
Train Loss: 0.1260, Accuracy: 0.9661, Precision: 0.8988, Recall: 0.8954, F1: 0.8960
Validation Loss: 0.9060, Accuracy: 0.8097, Precision: 0.6395, Recall: 0.6239, F1: 0.6246
Testing Loss: 0.7831, Accuracy: 0.8271, Precision: 0.6354, Recall: 0.6322, F1: 0.6326
LM Predictions:  [5, 3, 3, 0, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 1, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6778, Accuracy: 0.5000, Precision: 0.5774, Recall: 0.3893, F1: 0.4545
Epoch 61/70
Train Loss: 0.1166, Accuracy: 0.9682, Precision: 0.8927, Recall: 0.9102, F1: 0.9008
Validation Loss: 0.9680, Accuracy: 0.8068, Precision: 0.6151, Recall: 0.6085, F1: 0.6084
Testing Loss: 0.9279, Accuracy: 0.8271, Precision: 0.6107, Recall: 0.6350, F1: 0.6202
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 1, 2, 1, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9269, Accuracy: 0.4286, Precision: 0.4667, Recall: 0.3321, F1: 0.3843
Epoch 62/70
Train Loss: 0.1223, Accuracy: 0.9696, Precision: 0.9009, Recall: 0.9146, F1: 0.9072
Validation Loss: 0.9796, Accuracy: 0.8097, Precision: 0.6461, Recall: 0.6065, F1: 0.6243
Testing Loss: 1.0348, Accuracy: 0.8138, Precision: 0.5953, Recall: 0.5943, F1: 0.5902
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 0, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 1, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7544, Accuracy: 0.5357, Precision: 0.6056, Recall: 0.4226, F1: 0.4967
Epoch 63/70
Train Loss: 0.1191, Accuracy: 0.9675, Precision: 0.8986, Recall: 0.9095, F1: 0.9038
Validation Loss: 0.8920, Accuracy: 0.8040, Precision: 0.6349, Recall: 0.6553, F1: 0.6291
Testing Loss: 0.8868, Accuracy: 0.8245, Precision: 0.6098, Recall: 0.6322, F1: 0.6084
LM Predictions:  [0, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 0, 2, 3, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6962, Accuracy: 0.5357, Precision: 0.6278, Recall: 0.4405, F1: 0.5139
Epoch 64/70
Train Loss: 0.1190, Accuracy: 0.9689, Precision: 0.9029, Recall: 0.9213, F1: 0.9114
Validation Loss: 0.9592, Accuracy: 0.8011, Precision: 0.6534, Recall: 0.6759, F1: 0.6439
Testing Loss: 0.8834, Accuracy: 0.8059, Precision: 0.5956, Recall: 0.6226, F1: 0.6035
LM Predictions:  [0, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6315, Accuracy: 0.6071, Precision: 0.6984, Recall: 0.4881, F1: 0.5676
Epoch 65/70
Train Loss: 0.1532, Accuracy: 0.9633, Precision: 0.8790, Recall: 0.9077, F1: 0.8918
Validation Loss: 0.8948, Accuracy: 0.7926, Precision: 0.6199, Recall: 0.6918, F1: 0.6115
Testing Loss: 0.8708, Accuracy: 0.8059, Precision: 0.5811, Recall: 0.5866, F1: 0.5825
LM Predictions:  [0, 3, 3, 0, 3, 4, 4, 5, 3, 3, 3, 0, 2, 3, 4, 5, 3, 3, 3, 3, 5, 2, 1, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8302, Accuracy: 0.4286, Precision: 0.5833, Recall: 0.3333, F1: 0.4203
Epoch 66/70
Train Loss: 0.1264, Accuracy: 0.9668, Precision: 0.8946, Recall: 0.9057, F1: 0.8999
Validation Loss: 1.0052, Accuracy: 0.7869, Precision: 0.6150, Recall: 0.6017, F1: 0.6041
Testing Loss: 1.0286, Accuracy: 0.8138, Precision: 0.6102, Recall: 0.6212, F1: 0.6140
LM Predictions:  [0, 3, 3, 0, 1, 4, 4, 5, 0, 3, 3, 0, 2, 0, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 3, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7561, Accuracy: 0.5000, Precision: 0.6056, Recall: 0.3988, F1: 0.4735
Epoch 67/70
Train Loss: 0.1195, Accuracy: 0.9692, Precision: 0.9123, Recall: 0.9221, F1: 0.9168
Validation Loss: 0.8662, Accuracy: 0.8125, Precision: 0.6670, Recall: 0.5900, F1: 0.6119
Testing Loss: 0.8959, Accuracy: 0.8245, Precision: 0.6193, Recall: 0.5854, F1: 0.5980
LM Predictions:  [0, 3, 3, 0, 1, 4, 4, 5, 0, 3, 3, 0, 2, 0, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 4, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7802, Accuracy: 0.5357, Precision: 0.6250, Recall: 0.4405, F1: 0.5094
Epoch 68/70
Train Loss: 0.1068, Accuracy: 0.9699, Precision: 0.9053, Recall: 0.9113, F1: 0.9081
Validation Loss: 0.8740, Accuracy: 0.8182, Precision: 0.6419, Recall: 0.6390, F1: 0.6393
Testing Loss: 0.8638, Accuracy: 0.8245, Precision: 0.6026, Recall: 0.6297, F1: 0.6130
LM Predictions:  [0, 3, 3, 0, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5164, Accuracy: 0.5714, Precision: 0.6468, Recall: 0.4643, F1: 0.5346
Epoch 69/70
Train Loss: 0.1044, Accuracy: 0.9717, Precision: 0.9225, Recall: 0.9249, F1: 0.9229
Validation Loss: 0.8909, Accuracy: 0.8210, Precision: 0.6533, Recall: 0.6200, F1: 0.6334
Testing Loss: 0.9201, Accuracy: 0.8191, Precision: 0.6134, Recall: 0.6234, F1: 0.6063
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 5, 2, 5, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6023, Accuracy: 0.5357, Precision: 0.5111, Recall: 0.4214, F1: 0.4490
Epoch 70/70
Train Loss: 0.1049, Accuracy: 0.9724, Precision: 0.9117, Recall: 0.9206, F1: 0.9159
Validation Loss: 0.9948, Accuracy: 0.7955, Precision: 0.6064, Recall: 0.6089, F1: 0.6033
Testing Loss: 0.9308, Accuracy: 0.8191, Precision: 0.5973, Recall: 0.6350, F1: 0.6063
LM Predictions:  [5, 3, 3, 5, 1, 4, 4, 5, 5, 3, 3, 0, 2, 3, 4, 5, 3, 3, 3, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6379, Accuracy: 0.5714, Precision: 0.7302, Recall: 0.4643, F1: 0.5557
Label Memorization Analysis: 
LM Loss: 1.6379, Accuracy: 0.5714, Precision: 0.7302, Recall: 0.4643, F1: 0.5557
---------------------------------------------------------------------------



