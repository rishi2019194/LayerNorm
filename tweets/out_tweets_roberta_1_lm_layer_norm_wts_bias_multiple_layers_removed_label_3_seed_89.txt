---------------------------------------------------------------------------
Results for seed:  89
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 974
  Label 2: 1105
  Label 5: 490
  Label 1: 117
  Label 0: 56
  Label 3: 116
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1498, Accuracy: 0.5766, Precision: 0.2939, Recall: 0.2937, F1: 0.2850
Validation Loss: 0.9175, Accuracy: 0.6875, Precision: 0.3238, Recall: 0.3694, F1: 0.3446
Testing Loss: 0.9406, Accuracy: 0.7021, Precision: 0.3297, Recall: 0.3765, F1: 0.3511
LM Predictions:  [5, 5, 2, 5, 5, 2, 2, 5, 5, 4, 5, 5, 2, 4, 2, 5, 5, 5, 4, 5, 4, 2, 5, 4, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3770, Accuracy: 0.2143, Precision: 0.1567, Recall: 0.2167, F1: 0.1608
Epoch 2/70
Train Loss: 0.7753, Accuracy: 0.7313, Precision: 0.5515, Recall: 0.3994, F1: 0.3824
Validation Loss: 0.6918, Accuracy: 0.7500, Precision: 0.4132, Recall: 0.4455, F1: 0.4233
Testing Loss: 0.6865, Accuracy: 0.7713, Precision: 0.4638, Recall: 0.4765, F1: 0.4630
LM Predictions:  [3, 5, 2, 5, 3, 3, 5, 5, 5, 5, 3, 5, 2, 4, 5, 5, 3, 5, 5, 5, 5, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8520, Accuracy: 0.2143, Precision: 0.2535, Recall: 0.2014, F1: 0.1426
Epoch 3/70
Train Loss: 0.6313, Accuracy: 0.7792, Precision: 0.4776, Recall: 0.4896, F1: 0.4730
Validation Loss: 0.6110, Accuracy: 0.8068, Precision: 0.6336, Recall: 0.5249, F1: 0.5007
Testing Loss: 0.6487, Accuracy: 0.7872, Precision: 0.5093, Recall: 0.5281, F1: 0.5138
LM Predictions:  [3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 3, 5, 3, 3, 5, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0559, Accuracy: 0.0714, Precision: 0.0889, Recall: 0.0694, F1: 0.0741
Epoch 4/70
Train Loss: 0.5801, Accuracy: 0.8072, Precision: 0.5031, Recall: 0.5278, F1: 0.5039
Validation Loss: 0.5999, Accuracy: 0.7955, Precision: 0.5552, Recall: 0.5605, F1: 0.5498
Testing Loss: 0.6103, Accuracy: 0.8059, Precision: 0.5442, Recall: 0.5676, F1: 0.5541
LM Predictions:  [3, 2, 2, 1, 3, 3, 5, 3, 1, 1, 3, 5, 3, 4, 3, 5, 3, 1, 4, 3, 3, 2, 1, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1355, Accuracy: 0.1786, Precision: 0.2889, Recall: 0.1389, F1: 0.1749
Epoch 5/70
Train Loss: 0.4990, Accuracy: 0.8275, Precision: 0.6059, Recall: 0.5609, F1: 0.5536
Validation Loss: 0.5777, Accuracy: 0.8097, Precision: 0.5998, Recall: 0.5919, F1: 0.5937
Testing Loss: 0.5574, Accuracy: 0.7952, Precision: 0.5638, Recall: 0.5620, F1: 0.5431
LM Predictions:  [3, 1, 1, 5, 3, 3, 5, 1, 1, 1, 3, 5, 3, 4, 1, 5, 1, 1, 4, 1, 1, 1, 1, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2764, Accuracy: 0.2143, Precision: 0.4333, Recall: 0.1944, F1: 0.2254
Epoch 6/70
Train Loss: 0.4429, Accuracy: 0.8527, Precision: 0.6080, Recall: 0.6035, F1: 0.5984
Validation Loss: 0.6897, Accuracy: 0.7841, Precision: 0.5752, Recall: 0.5062, F1: 0.5029
Testing Loss: 0.5999, Accuracy: 0.8298, Precision: 0.6222, Recall: 0.5803, F1: 0.5838
LM Predictions:  [3, 4, 2, 5, 3, 3, 5, 3, 0, 1, 3, 5, 3, 4, 0, 0, 3, 0, 4, 5, 5, 4, 3, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4995, Accuracy: 0.4643, Precision: 0.5278, Recall: 0.3472, F1: 0.3963
Epoch 7/70
Train Loss: 0.3980, Accuracy: 0.8555, Precision: 0.6402, Recall: 0.6156, F1: 0.6186
Validation Loss: 0.5421, Accuracy: 0.8040, Precision: 0.6258, Recall: 0.5498, F1: 0.5700
Testing Loss: 0.6000, Accuracy: 0.8005, Precision: 0.5937, Recall: 0.5443, F1: 0.5439
LM Predictions:  [3, 2, 2, 4, 2, 3, 5, 2, 1, 1, 2, 1, 3, 4, 1, 5, 3, 3, 4, 3, 2, 2, 1, 2, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1517, Accuracy: 0.2500, Precision: 0.1869, Recall: 0.1806, F1: 0.1586
Epoch 8/70
Train Loss: 0.3773, Accuracy: 0.8782, Precision: 0.7110, Recall: 0.6791, F1: 0.6884
Validation Loss: 0.5338, Accuracy: 0.8153, Precision: 0.6216, Recall: 0.6064, F1: 0.6001
Testing Loss: 0.5272, Accuracy: 0.8298, Precision: 0.6135, Recall: 0.6252, F1: 0.6154
LM Predictions:  [3, 1, 2, 5, 3, 3, 5, 3, 3, 1, 3, 1, 3, 4, 3, 5, 3, 3, 4, 5, 3, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1227, Accuracy: 0.2500, Precision: 0.4000, Recall: 0.2014, F1: 0.2483
Epoch 9/70
Train Loss: 0.3222, Accuracy: 0.8950, Precision: 0.7703, Recall: 0.7262, F1: 0.7386
Validation Loss: 0.5377, Accuracy: 0.8068, Precision: 0.6180, Recall: 0.5607, F1: 0.5747
Testing Loss: 0.5707, Accuracy: 0.8059, Precision: 0.6196, Recall: 0.6085, F1: 0.5890
LM Predictions:  [3, 1, 2, 5, 3, 3, 0, 3, 0, 1, 3, 5, 3, 4, 1, 0, 3, 0, 4, 0, 2, 2, 0, 2, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5939, Accuracy: 0.4286, Precision: 0.5437, Recall: 0.3125, F1: 0.3737
Epoch 10/70
Train Loss: 0.2803, Accuracy: 0.9129, Precision: 0.7990, Recall: 0.7818, F1: 0.7871
Validation Loss: 0.6326, Accuracy: 0.8324, Precision: 0.6218, Recall: 0.6272, F1: 0.6239
Testing Loss: 0.6134, Accuracy: 0.8378, Precision: 0.6485, Recall: 0.6517, F1: 0.6474
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 3, 0, 1, 3, 5, 3, 4, 3, 0, 3, 0, 4, 0, 3, 4, 3, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5565, Accuracy: 0.4643, Precision: 0.6250, Recall: 0.3472, F1: 0.4274
Epoch 11/70
Train Loss: 0.2584, Accuracy: 0.9195, Precision: 0.8176, Recall: 0.8082, F1: 0.8110
Validation Loss: 0.5777, Accuracy: 0.8239, Precision: 0.6154, Recall: 0.6032, F1: 0.6076
Testing Loss: 0.6136, Accuracy: 0.8085, Precision: 0.6166, Recall: 0.6077, F1: 0.5924
LM Predictions:  [3, 1, 2, 5, 3, 3, 5, 3, 0, 1, 3, 5, 3, 4, 1, 0, 3, 0, 4, 0, 0, 1, 0, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.9156, Accuracy: 0.3929, Precision: 0.5694, Recall: 0.3056, F1: 0.3702
Epoch 12/70
Train Loss: 0.2378, Accuracy: 0.9300, Precision: 0.8193, Recall: 0.8068, F1: 0.8096
Validation Loss: 0.7931, Accuracy: 0.8097, Precision: 0.6198, Recall: 0.5886, F1: 0.5700
Testing Loss: 0.7420, Accuracy: 0.8218, Precision: 0.6355, Recall: 0.6269, F1: 0.6109
LM Predictions:  [3, 3, 4, 3, 3, 3, 0, 3, 0, 3, 3, 5, 3, 4, 3, 0, 3, 3, 4, 3, 3, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4405, Accuracy: 0.3214, Precision: 0.6250, Recall: 0.2361, F1: 0.3330
Epoch 13/70
Train Loss: 0.2226, Accuracy: 0.9360, Precision: 0.8305, Recall: 0.8236, F1: 0.8252
Validation Loss: 0.6168, Accuracy: 0.8608, Precision: 0.6725, Recall: 0.6509, F1: 0.6605
Testing Loss: 0.6168, Accuracy: 0.8378, Precision: 0.6180, Recall: 0.6178, F1: 0.6144
LM Predictions:  [3, 2, 2, 5, 3, 3, 5, 2, 5, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 3, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3155, Accuracy: 0.5357, Precision: 0.5524, Recall: 0.4097, F1: 0.4413
Epoch 14/70
Train Loss: 0.1938, Accuracy: 0.9461, Precision: 0.8666, Recall: 0.8703, F1: 0.8683
Validation Loss: 0.7776, Accuracy: 0.8153, Precision: 0.6513, Recall: 0.6025, F1: 0.6207
Testing Loss: 0.7134, Accuracy: 0.8324, Precision: 0.6625, Recall: 0.6791, F1: 0.6631
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 3, 0, 3, 3, 5, 3, 4, 0, 0, 3, 3, 4, 3, 3, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8310, Accuracy: 0.4286, Precision: 0.6667, Recall: 0.3264, F1: 0.4282
Epoch 15/70
Train Loss: 0.1730, Accuracy: 0.9507, Precision: 0.8642, Recall: 0.8723, F1: 0.8677
Validation Loss: 0.6214, Accuracy: 0.8409, Precision: 0.6520, Recall: 0.6213, F1: 0.6346
Testing Loss: 0.6483, Accuracy: 0.8324, Precision: 0.6126, Recall: 0.6276, F1: 0.6074
LM Predictions:  [3, 1, 2, 5, 3, 3, 0, 2, 5, 1, 2, 5, 3, 4, 0, 0, 3, 3, 4, 0, 2, 4, 0, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6579, Accuracy: 0.5357, Precision: 0.5917, Recall: 0.4097, F1: 0.4700
Epoch 16/70
Train Loss: 0.1569, Accuracy: 0.9556, Precision: 0.8865, Recall: 0.8816, F1: 0.8837
Validation Loss: 0.7461, Accuracy: 0.8295, Precision: 0.6736, Recall: 0.6314, F1: 0.6491
Testing Loss: 0.7286, Accuracy: 0.8484, Precision: 0.6562, Recall: 0.6670, F1: 0.6564
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5863, Accuracy: 0.5714, Precision: 0.6333, Recall: 0.4236, F1: 0.4978
Epoch 17/70
Train Loss: 0.1616, Accuracy: 0.9552, Precision: 0.8760, Recall: 0.8932, F1: 0.8841
Validation Loss: 0.7439, Accuracy: 0.8267, Precision: 0.6510, Recall: 0.5570, F1: 0.5801
Testing Loss: 0.8875, Accuracy: 0.8431, Precision: 0.6773, Recall: 0.5985, F1: 0.6191
LM Predictions:  [3, 5, 2, 5, 3, 3, 5, 2, 5, 1, 2, 3, 3, 4, 5, 5, 3, 0, 4, 5, 2, 4, 3, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0173, Accuracy: 0.3929, Precision: 0.5093, Recall: 0.3056, F1: 0.3181
Epoch 18/70
Train Loss: 0.1570, Accuracy: 0.9549, Precision: 0.8921, Recall: 0.8769, F1: 0.8834
Validation Loss: 0.6991, Accuracy: 0.8409, Precision: 0.6609, Recall: 0.6190, F1: 0.6317
Testing Loss: 0.7569, Accuracy: 0.8404, Precision: 0.6328, Recall: 0.6170, F1: 0.6240
LM Predictions:  [3, 5, 2, 5, 3, 3, 0, 3, 0, 3, 2, 3, 3, 4, 0, 0, 3, 3, 4, 0, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0222, Accuracy: 0.5000, Precision: 0.6111, Recall: 0.3611, F1: 0.4477
Epoch 19/70
Train Loss: 0.1272, Accuracy: 0.9668, Precision: 0.9052, Recall: 0.9120, F1: 0.9080
Validation Loss: 0.8013, Accuracy: 0.8352, Precision: 0.6788, Recall: 0.6905, F1: 0.6708
Testing Loss: 0.8509, Accuracy: 0.8378, Precision: 0.6821, Recall: 0.6910, F1: 0.6807
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 3, 4, 3, 2, 4, 3, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5503, Accuracy: 0.5357, Precision: 0.6333, Recall: 0.4028, F1: 0.4863
Epoch 20/70
Train Loss: 0.1155, Accuracy: 0.9696, Precision: 0.9099, Recall: 0.9211, F1: 0.9150
Validation Loss: 0.6699, Accuracy: 0.8352, Precision: 0.6428, Recall: 0.6244, F1: 0.6290
Testing Loss: 0.7032, Accuracy: 0.8457, Precision: 0.7818, Recall: 0.6502, F1: 0.6655
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 2, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3938, Accuracy: 0.6786, Precision: 0.6429, Recall: 0.5000, F1: 0.5507
Epoch 21/70
Train Loss: 0.1342, Accuracy: 0.9608, Precision: 0.9005, Recall: 0.9059, F1: 0.9022
Validation Loss: 0.7967, Accuracy: 0.8210, Precision: 0.6294, Recall: 0.6214, F1: 0.6148
Testing Loss: 0.8387, Accuracy: 0.8218, Precision: 0.6585, Recall: 0.6557, F1: 0.6486
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 2, 0, 3, 2, 2, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4244, Accuracy: 0.6071, Precision: 0.6389, Recall: 0.4306, F1: 0.5040
Epoch 22/70
Train Loss: 0.1213, Accuracy: 0.9643, Precision: 0.8974, Recall: 0.8994, F1: 0.8969
Validation Loss: 0.7442, Accuracy: 0.8409, Precision: 0.6403, Recall: 0.6564, F1: 0.6461
Testing Loss: 0.8264, Accuracy: 0.8378, Precision: 0.6482, Recall: 0.6763, F1: 0.6538
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 3, 0, 3, 2, 3, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6775, Accuracy: 0.5714, Precision: 0.6667, Recall: 0.4028, F1: 0.4984
Epoch 23/70
Train Loss: 0.1001, Accuracy: 0.9734, Precision: 0.9258, Recall: 0.9408, F1: 0.9330
Validation Loss: 0.6692, Accuracy: 0.8494, Precision: 0.6736, Recall: 0.6450, F1: 0.6563
Testing Loss: 0.7157, Accuracy: 0.8457, Precision: 0.6341, Recall: 0.6400, F1: 0.6326
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 2, 3, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 3, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2930, Accuracy: 0.6071, Precision: 0.6667, Recall: 0.4583, F1: 0.5317
Epoch 24/70
Train Loss: 0.1004, Accuracy: 0.9741, Precision: 0.9415, Recall: 0.9408, F1: 0.9411
Validation Loss: 0.7456, Accuracy: 0.8438, Precision: 0.6651, Recall: 0.6359, F1: 0.6486
Testing Loss: 0.7824, Accuracy: 0.8431, Precision: 0.6736, Recall: 0.6578, F1: 0.6537
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2971, Accuracy: 0.6429, Precision: 0.6250, Recall: 0.4653, F1: 0.5250
Epoch 25/70
Train Loss: 0.0902, Accuracy: 0.9776, Precision: 0.9329, Recall: 0.9385, F1: 0.9354
Validation Loss: 0.8071, Accuracy: 0.8381, Precision: 0.6546, Recall: 0.6167, F1: 0.6325
Testing Loss: 0.7553, Accuracy: 0.8404, Precision: 0.6779, Recall: 0.6750, F1: 0.6710
LM Predictions:  [3, 4, 2, 5, 3, 3, 0, 2, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 4, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2225, Accuracy: 0.6429, Precision: 0.5917, Recall: 0.4722, F1: 0.5219
Epoch 26/70
Train Loss: 0.0858, Accuracy: 0.9766, Precision: 0.9393, Recall: 0.9352, F1: 0.9368
Validation Loss: 0.8077, Accuracy: 0.8381, Precision: 0.6445, Recall: 0.6464, F1: 0.6442
Testing Loss: 0.8262, Accuracy: 0.8404, Precision: 0.6201, Recall: 0.6301, F1: 0.6159
LM Predictions:  [3, 4, 2, 5, 3, 5, 0, 3, 5, 3, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1704, Accuracy: 0.6071, Precision: 0.5714, Recall: 0.4514, F1: 0.4817
Epoch 27/70
Train Loss: 0.0813, Accuracy: 0.9748, Precision: 0.9307, Recall: 0.9211, F1: 0.9251
Validation Loss: 0.8952, Accuracy: 0.8182, Precision: 0.6405, Recall: 0.5859, F1: 0.5993
Testing Loss: 0.8898, Accuracy: 0.8431, Precision: 0.6477, Recall: 0.6228, F1: 0.6251
LM Predictions:  [1, 4, 2, 5, 2, 5, 0, 3, 0, 3, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2173, Accuracy: 0.6786, Precision: 0.5514, Recall: 0.4931, F1: 0.5069
Epoch 28/70
Train Loss: 0.0852, Accuracy: 0.9780, Precision: 0.9411, Recall: 0.9426, F1: 0.9416
Validation Loss: 0.8761, Accuracy: 0.8438, Precision: 0.7149, Recall: 0.6942, F1: 0.6917
Testing Loss: 1.0428, Accuracy: 0.8138, Precision: 0.6519, Recall: 0.6715, F1: 0.6409
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7925, Accuracy: 0.7500, Precision: 0.8333, Recall: 0.6042, F1: 0.6873
Epoch 29/70
Train Loss: 0.1072, Accuracy: 0.9738, Precision: 0.9321, Recall: 0.9186, F1: 0.9250
Validation Loss: 0.8678, Accuracy: 0.8040, Precision: 0.6056, Recall: 0.5992, F1: 0.5892
Testing Loss: 0.7777, Accuracy: 0.8271, Precision: 0.6471, Recall: 0.6409, F1: 0.6375
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 3, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9925, Accuracy: 0.7500, Precision: 0.7917, Recall: 0.6042, F1: 0.6694
Epoch 30/70
Train Loss: 0.0871, Accuracy: 0.9755, Precision: 0.9378, Recall: 0.9314, F1: 0.9340
Validation Loss: 0.9019, Accuracy: 0.8239, Precision: 0.6405, Recall: 0.6305, F1: 0.6344
Testing Loss: 0.9048, Accuracy: 0.8484, Precision: 0.6829, Recall: 0.6633, F1: 0.6638
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8758, Accuracy: 0.7500, Precision: 0.6875, Recall: 0.6042, F1: 0.6319
Epoch 31/70
Train Loss: 0.0665, Accuracy: 0.9836, Precision: 0.9584, Recall: 0.9550, F1: 0.9565
Validation Loss: 0.9669, Accuracy: 0.8153, Precision: 0.6408, Recall: 0.5954, F1: 0.5871
Testing Loss: 0.9861, Accuracy: 0.8245, Precision: 0.6681, Recall: 0.5985, F1: 0.6105
LM Predictions:  [3, 4, 2, 5, 3, 5, 0, 2, 0, 3, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8328, Accuracy: 0.7143, Precision: 0.5833, Recall: 0.5208, F1: 0.5333
Epoch 32/70
Train Loss: 0.0753, Accuracy: 0.9790, Precision: 0.9378, Recall: 0.9305, F1: 0.9339
Validation Loss: 0.9427, Accuracy: 0.8352, Precision: 0.6401, Recall: 0.6840, F1: 0.6517
Testing Loss: 1.0062, Accuracy: 0.8457, Precision: 0.6728, Recall: 0.6911, F1: 0.6782
LM Predictions:  [3, 4, 2, 5, 3, 0, 0, 2, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8874, Accuracy: 0.7500, Precision: 0.6333, Recall: 0.5417, F1: 0.5688
Epoch 33/70
Train Loss: 0.0671, Accuracy: 0.9839, Precision: 0.9465, Recall: 0.9552, F1: 0.9507
Validation Loss: 0.8481, Accuracy: 0.8466, Precision: 0.6915, Recall: 0.6185, F1: 0.6458
Testing Loss: 0.9489, Accuracy: 0.8324, Precision: 0.6748, Recall: 0.6308, F1: 0.6434
LM Predictions:  [3, 4, 2, 5, 3, 5, 0, 2, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9936, Accuracy: 0.7143, Precision: 0.6042, Recall: 0.5208, F1: 0.5486
Epoch 34/70
Train Loss: 0.0545, Accuracy: 0.9888, Precision: 0.9752, Recall: 0.9676, F1: 0.9713
Validation Loss: 0.8989, Accuracy: 0.8381, Precision: 0.6479, Recall: 0.6507, F1: 0.6484
Testing Loss: 0.9556, Accuracy: 0.8457, Precision: 0.6337, Recall: 0.6425, F1: 0.6349
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 3, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7191, Accuracy: 0.7500, Precision: 0.7500, Recall: 0.6042, F1: 0.6595
Epoch 35/70
Train Loss: 0.0654, Accuracy: 0.9853, Precision: 0.9608, Recall: 0.9470, F1: 0.9537
Validation Loss: 0.8137, Accuracy: 0.8210, Precision: 0.6310, Recall: 0.6209, F1: 0.6238
Testing Loss: 0.9450, Accuracy: 0.8245, Precision: 0.6564, Recall: 0.6510, F1: 0.6410
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 4, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0525, Accuracy: 0.7143, Precision: 0.6958, Recall: 0.5764, F1: 0.6261
Epoch 36/70
Train Loss: 0.0526, Accuracy: 0.9878, Precision: 0.9687, Recall: 0.9698, F1: 0.9692
Validation Loss: 1.0004, Accuracy: 0.8409, Precision: 0.6656, Recall: 0.6164, F1: 0.6370
Testing Loss: 0.9806, Accuracy: 0.8431, Precision: 0.7284, Recall: 0.6652, F1: 0.6820
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 5, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1284, Accuracy: 0.7143, Precision: 0.6625, Recall: 0.5764, F1: 0.6029
Epoch 37/70
Train Loss: 0.0578, Accuracy: 0.9857, Precision: 0.9537, Recall: 0.9650, F1: 0.9592
Validation Loss: 0.8991, Accuracy: 0.7983, Precision: 0.6706, Recall: 0.6442, F1: 0.6520
Testing Loss: 0.9071, Accuracy: 0.8298, Precision: 0.6566, Recall: 0.6899, F1: 0.6668
LM Predictions:  [3, 4, 3, 5, 3, 5, 0, 3, 0, 1, 2, 5, 0, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1551, Accuracy: 0.6786, Precision: 0.6065, Recall: 0.4861, F1: 0.5263
Epoch 38/70
Train Loss: 0.0693, Accuracy: 0.9843, Precision: 0.9687, Recall: 0.9577, F1: 0.9629
Validation Loss: 0.9817, Accuracy: 0.8125, Precision: 0.5934, Recall: 0.6170, F1: 0.6011
Testing Loss: 1.0394, Accuracy: 0.8431, Precision: 0.6149, Recall: 0.6470, F1: 0.6271
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 3, 0, 1, 2, 5, 1, 3, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7111, Accuracy: 0.7500, Precision: 0.7778, Recall: 0.6597, F1: 0.6853
Epoch 39/70
Train Loss: 0.0523, Accuracy: 0.9867, Precision: 0.9606, Recall: 0.9629, F1: 0.9615
Validation Loss: 1.0031, Accuracy: 0.8239, Precision: 0.6304, Recall: 0.6126, F1: 0.6195
Testing Loss: 0.9694, Accuracy: 0.8484, Precision: 0.6411, Recall: 0.6560, F1: 0.6452
LM Predictions:  [3, 4, 2, 5, 5, 4, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0661, Accuracy: 0.7857, Precision: 0.6750, Recall: 0.6250, F1: 0.6442
Epoch 40/70
Train Loss: 0.0475, Accuracy: 0.9867, Precision: 0.9641, Recall: 0.9684, F1: 0.9663
Validation Loss: 1.0119, Accuracy: 0.8352, Precision: 0.6477, Recall: 0.6193, F1: 0.6300
Testing Loss: 0.9788, Accuracy: 0.8564, Precision: 0.6944, Recall: 0.6735, F1: 0.6783
LM Predictions:  [3, 4, 2, 5, 5, 4, 0, 2, 0, 1, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9854, Accuracy: 0.7500, Precision: 0.5667, Recall: 0.5417, F1: 0.5470
Epoch 41/70
Train Loss: 0.0318, Accuracy: 0.9937, Precision: 0.9811, Recall: 0.9848, F1: 0.9829
Validation Loss: 1.0296, Accuracy: 0.8381, Precision: 0.6400, Recall: 0.6142, F1: 0.6250
Testing Loss: 0.9127, Accuracy: 0.8644, Precision: 0.7127, Recall: 0.6830, F1: 0.6928
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7643, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 42/70
Train Loss: 0.1050, Accuracy: 0.9727, Precision: 0.9427, Recall: 0.9459, F1: 0.9441
Validation Loss: 0.8318, Accuracy: 0.8352, Precision: 0.6452, Recall: 0.6460, F1: 0.6453
Testing Loss: 0.8510, Accuracy: 0.8511, Precision: 0.6932, Recall: 0.7074, F1: 0.6986
LM Predictions:  [3, 4, 2, 3, 3, 1, 0, 3, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8944, Accuracy: 0.7500, Precision: 0.7778, Recall: 0.6389, F1: 0.6737
Epoch 43/70
Train Loss: 0.0472, Accuracy: 0.9888, Precision: 0.9665, Recall: 0.9729, F1: 0.9697
Validation Loss: 1.0009, Accuracy: 0.8381, Precision: 0.6624, Recall: 0.6246, F1: 0.6390
Testing Loss: 1.0298, Accuracy: 0.8537, Precision: 0.6795, Recall: 0.6470, F1: 0.6603
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8930, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 44/70
Train Loss: 0.1393, Accuracy: 0.9580, Precision: 0.9440, Recall: 0.9470, F1: 0.9454
Validation Loss: 0.9241, Accuracy: 0.8381, Precision: 0.6847, Recall: 0.7096, F1: 0.6802
Testing Loss: 1.0600, Accuracy: 0.8165, Precision: 0.6344, Recall: 0.6602, F1: 0.6371
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7680, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 45/70
Train Loss: 0.0688, Accuracy: 0.9850, Precision: 0.9647, Recall: 0.9642, F1: 0.9641
Validation Loss: 0.8573, Accuracy: 0.8352, Precision: 0.6459, Recall: 0.6264, F1: 0.6319
Testing Loss: 0.8236, Accuracy: 0.8484, Precision: 0.6389, Recall: 0.6597, F1: 0.6476
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 5, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7991, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6806, F1: 0.6876
Epoch 46/70
Train Loss: 0.0443, Accuracy: 0.9899, Precision: 0.9760, Recall: 0.9765, F1: 0.9762
Validation Loss: 0.9693, Accuracy: 0.8125, Precision: 0.6464, Recall: 0.7045, F1: 0.6448
Testing Loss: 0.9755, Accuracy: 0.8324, Precision: 0.6788, Recall: 0.7281, F1: 0.6910
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6918, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 47/70
Train Loss: 0.0368, Accuracy: 0.9923, Precision: 0.9754, Recall: 0.9858, F1: 0.9806
Validation Loss: 1.0415, Accuracy: 0.8324, Precision: 0.6445, Recall: 0.6406, F1: 0.6416
Testing Loss: 0.9481, Accuracy: 0.8404, Precision: 0.6571, Recall: 0.6607, F1: 0.6534
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7594, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 48/70
Train Loss: 0.0511, Accuracy: 0.9885, Precision: 0.9799, Recall: 0.9758, F1: 0.9779
Validation Loss: 0.8915, Accuracy: 0.8352, Precision: 0.6535, Recall: 0.6134, F1: 0.6304
Testing Loss: 0.8844, Accuracy: 0.8670, Precision: 0.7432, Recall: 0.7010, F1: 0.7025
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6444, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 49/70
Train Loss: 0.0242, Accuracy: 0.9948, Precision: 0.9880, Recall: 0.9861, F1: 0.9870
Validation Loss: 1.0983, Accuracy: 0.8153, Precision: 0.6248, Recall: 0.5938, F1: 0.6069
Testing Loss: 0.9365, Accuracy: 0.8537, Precision: 0.7020, Recall: 0.7120, F1: 0.7054
LM Predictions:  [1, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9319, Accuracy: 0.7857, Precision: 0.6806, Recall: 0.6250, F1: 0.6361
Epoch 50/70
Train Loss: 0.0431, Accuracy: 0.9899, Precision: 0.9748, Recall: 0.9597, F1: 0.9663
Validation Loss: 1.0001, Accuracy: 0.8295, Precision: 0.6655, Recall: 0.6146, F1: 0.6370
Testing Loss: 0.9339, Accuracy: 0.8484, Precision: 0.6960, Recall: 0.6971, F1: 0.6909
LM Predictions:  [1, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8397, Accuracy: 0.8214, Precision: 0.7500, Recall: 0.7083, F1: 0.6984
Epoch 51/70
Train Loss: 0.0481, Accuracy: 0.9888, Precision: 0.9616, Recall: 0.9634, F1: 0.9622
Validation Loss: 0.8131, Accuracy: 0.8295, Precision: 0.6241, Recall: 0.6293, F1: 0.6265
Testing Loss: 0.8173, Accuracy: 0.8457, Precision: 0.7034, Recall: 0.6694, F1: 0.6668
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 5, 4, 0, 0, 3, 3, 4, 3, 2, 4, 2, 0, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9954, Accuracy: 0.6786, Precision: 0.7083, Recall: 0.5625, F1: 0.6143
Epoch 52/70
Train Loss: 0.0474, Accuracy: 0.9888, Precision: 0.9686, Recall: 0.9705, F1: 0.9690
Validation Loss: 0.9859, Accuracy: 0.8182, Precision: 0.6376, Recall: 0.6059, F1: 0.6193
Testing Loss: 0.9018, Accuracy: 0.8537, Precision: 0.6950, Recall: 0.6854, F1: 0.6847
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6580, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 53/70
Train Loss: 0.0199, Accuracy: 0.9955, Precision: 0.9867, Recall: 0.9863, F1: 0.9863
Validation Loss: 1.1531, Accuracy: 0.8267, Precision: 0.6482, Recall: 0.6096, F1: 0.6244
Testing Loss: 1.0758, Accuracy: 0.8457, Precision: 0.6734, Recall: 0.6837, F1: 0.6768
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7964, Accuracy: 0.7857, Precision: 0.7083, Recall: 0.6250, F1: 0.6528
Epoch 54/70
Train Loss: 0.0392, Accuracy: 0.9920, Precision: 0.9801, Recall: 0.9838, F1: 0.9819
Validation Loss: 1.0792, Accuracy: 0.8239, Precision: 0.6267, Recall: 0.6251, F1: 0.6247
Testing Loss: 1.0532, Accuracy: 0.8351, Precision: 0.6257, Recall: 0.6499, F1: 0.6272
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6599, Accuracy: 0.8214, Precision: 0.7361, Recall: 0.7083, F1: 0.7028
Epoch 55/70
Train Loss: 0.0315, Accuracy: 0.9927, Precision: 0.9790, Recall: 0.9835, F1: 0.9811
Validation Loss: 1.1102, Accuracy: 0.8295, Precision: 0.6473, Recall: 0.6035, F1: 0.6220
Testing Loss: 1.0992, Accuracy: 0.8298, Precision: 0.6244, Recall: 0.6234, F1: 0.6164
LM Predictions:  [3, 1, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 5, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8905, Accuracy: 0.7500, Precision: 0.6806, Recall: 0.6042, F1: 0.6159
Epoch 56/70
Train Loss: 0.0504, Accuracy: 0.9881, Precision: 0.9641, Recall: 0.9753, F1: 0.9696
Validation Loss: 1.1311, Accuracy: 0.8381, Precision: 0.6898, Recall: 0.6151, F1: 0.6423
Testing Loss: 1.1542, Accuracy: 0.8298, Precision: 0.6233, Recall: 0.6099, F1: 0.6086
LM Predictions:  [2, 1, 2, 5, 1, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 2, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6545, Accuracy: 0.7857, Precision: 0.7800, Recall: 0.8250, F1: 0.7448
Epoch 57/70
Train Loss: 0.0979, Accuracy: 0.9783, Precision: 0.9553, Recall: 0.9597, F1: 0.9574
Validation Loss: 0.9684, Accuracy: 0.8324, Precision: 0.6646, Recall: 0.6249, F1: 0.6396
Testing Loss: 0.9390, Accuracy: 0.8511, Precision: 0.6374, Recall: 0.6548, F1: 0.6419
LM Predictions:  [3, 4, 2, 5, 1, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6610, Accuracy: 0.8214, Precision: 0.7083, Recall: 0.7083, F1: 0.6806
Epoch 58/70
Train Loss: 0.0389, Accuracy: 0.9909, Precision: 0.9723, Recall: 0.9728, F1: 0.9725
Validation Loss: 0.9839, Accuracy: 0.8239, Precision: 0.6450, Recall: 0.6281, F1: 0.6337
Testing Loss: 0.9798, Accuracy: 0.8404, Precision: 0.6247, Recall: 0.6630, F1: 0.6379
LM Predictions:  [4, 4, 2, 5, 1, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4685, Accuracy: 0.8214, Precision: 0.6833, Recall: 0.7083, F1: 0.6726
Epoch 59/70
Train Loss: 0.0337, Accuracy: 0.9902, Precision: 0.9765, Recall: 0.9678, F1: 0.9721
Validation Loss: 0.9341, Accuracy: 0.8438, Precision: 0.7092, Recall: 0.6124, F1: 0.6482
Testing Loss: 1.0937, Accuracy: 0.8165, Precision: 0.7050, Recall: 0.6307, F1: 0.6434
LM Predictions:  [2, 4, 2, 5, 1, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 1, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6191, Accuracy: 0.8214, Precision: 0.8014, Recall: 0.8500, F1: 0.7822
Epoch 60/70
Train Loss: 0.0358, Accuracy: 0.9927, Precision: 0.9799, Recall: 0.9768, F1: 0.9783
Validation Loss: 1.0097, Accuracy: 0.8381, Precision: 0.6661, Recall: 0.6081, F1: 0.6321
Testing Loss: 1.1143, Accuracy: 0.8351, Precision: 0.7185, Recall: 0.6578, F1: 0.6705
LM Predictions:  [2, 4, 2, 5, 3, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6333, Accuracy: 0.8214, Precision: 0.7679, Recall: 0.7083, F1: 0.7233
Epoch 61/70
Train Loss: 0.0232, Accuracy: 0.9941, Precision: 0.9846, Recall: 0.9869, F1: 0.9856
Validation Loss: 1.0478, Accuracy: 0.8153, Precision: 0.6633, Recall: 0.6785, F1: 0.6604
Testing Loss: 1.0840, Accuracy: 0.8351, Precision: 0.6579, Recall: 0.6551, F1: 0.6542
LM Predictions:  [4, 4, 2, 5, 5, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5108, Accuracy: 0.9286, Precision: 0.7917, Recall: 0.7708, F1: 0.7806
Epoch 62/70
Train Loss: 0.0267, Accuracy: 0.9920, Precision: 0.9809, Recall: 0.9733, F1: 0.9770
Validation Loss: 1.1828, Accuracy: 0.8267, Precision: 0.6474, Recall: 0.6574, F1: 0.6389
Testing Loss: 1.1743, Accuracy: 0.8271, Precision: 0.6234, Recall: 0.6108, F1: 0.6165
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4344, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7083, F1: 0.7206
Epoch 63/70
Train Loss: 0.0644, Accuracy: 0.9850, Precision: 0.9754, Recall: 0.9745, F1: 0.9746
Validation Loss: 1.2219, Accuracy: 0.7784, Precision: 0.6233, Recall: 0.6745, F1: 0.6346
Testing Loss: 1.1162, Accuracy: 0.8005, Precision: 0.6109, Recall: 0.6359, F1: 0.6148
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 1, 5, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5819, Accuracy: 0.8929, Precision: 0.7361, Recall: 0.7431, F1: 0.7321
Epoch 64/70
Train Loss: 0.0298, Accuracy: 0.9930, Precision: 0.9795, Recall: 0.9773, F1: 0.9782
Validation Loss: 1.2944, Accuracy: 0.8153, Precision: 0.6431, Recall: 0.6242, F1: 0.6320
Testing Loss: 1.0245, Accuracy: 0.8590, Precision: 0.6899, Recall: 0.6928, F1: 0.6885
LM Predictions:  [1, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4074, Accuracy: 0.8571, Precision: 0.7500, Recall: 0.7292, F1: 0.7155
Epoch 65/70
Train Loss: 0.0290, Accuracy: 0.9951, Precision: 0.9815, Recall: 0.9878, F1: 0.9846
Validation Loss: 1.2464, Accuracy: 0.8182, Precision: 0.6566, Recall: 0.6639, F1: 0.6478
Testing Loss: 1.0758, Accuracy: 0.8404, Precision: 0.6743, Recall: 0.6500, F1: 0.6611
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2851, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9500, F1: 0.9314
Epoch 66/70
Train Loss: 0.0443, Accuracy: 0.9892, Precision: 0.9742, Recall: 0.9680, F1: 0.9710
Validation Loss: 1.1933, Accuracy: 0.8210, Precision: 0.6389, Recall: 0.6185, F1: 0.6259
Testing Loss: 1.1069, Accuracy: 0.8484, Precision: 0.6491, Recall: 0.6551, F1: 0.6514
LM Predictions:  [5, 4, 2, 5, 3, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5372, Accuracy: 0.8571, Precision: 0.7111, Recall: 0.7292, F1: 0.7060
Epoch 67/70
Train Loss: 0.0327, Accuracy: 0.9951, Precision: 0.9872, Recall: 0.9838, F1: 0.9853
Validation Loss: 1.1465, Accuracy: 0.8239, Precision: 0.6785, Recall: 0.6066, F1: 0.6339
Testing Loss: 1.1310, Accuracy: 0.8378, Precision: 0.6960, Recall: 0.6386, F1: 0.6610
LM Predictions:  [5, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6731, Accuracy: 0.8214, Precision: 0.8190, Recall: 0.8500, F1: 0.8024
Epoch 68/70
Train Loss: 0.0193, Accuracy: 0.9955, Precision: 0.9847, Recall: 0.9829, F1: 0.9837
Validation Loss: 1.1338, Accuracy: 0.8409, Precision: 0.7009, Recall: 0.6960, F1: 0.6854
Testing Loss: 1.1436, Accuracy: 0.8351, Precision: 0.6655, Recall: 0.6525, F1: 0.6537
LM Predictions:  [1, 4, 2, 5, 1, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4110, Accuracy: 0.8929, Precision: 0.8800, Recall: 0.9000, F1: 0.8571
Epoch 69/70
Train Loss: 0.0202, Accuracy: 0.9965, Precision: 0.9893, Recall: 0.9873, F1: 0.9882
Validation Loss: 1.1651, Accuracy: 0.8267, Precision: 0.6816, Recall: 0.6851, F1: 0.6693
Testing Loss: 1.2738, Accuracy: 0.8191, Precision: 0.6352, Recall: 0.6382, F1: 0.6253
LM Predictions:  [5, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8669, Accuracy: 0.8214, Precision: 0.6944, Recall: 0.7083, F1: 0.6778
Epoch 70/70
Train Loss: 0.0199, Accuracy: 0.9958, Precision: 0.9890, Recall: 0.9918, F1: 0.9903
Validation Loss: 1.1231, Accuracy: 0.8438, Precision: 0.6765, Recall: 0.6429, F1: 0.6574
Testing Loss: 1.1479, Accuracy: 0.8324, Precision: 0.6334, Recall: 0.6309, F1: 0.6276
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 1, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5730, Accuracy: 0.8929, Precision: 0.8500, Recall: 0.9000, F1: 0.8548
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1422, Accuracy: 0.5682, Precision: 0.2725, Recall: 0.2858, F1: 0.2738
Validation Loss: 0.6671, Accuracy: 0.7670, Precision: 0.3896, Recall: 0.4427, F1: 0.4006
Testing Loss: 0.6599, Accuracy: 0.7793, Precision: 0.3829, Recall: 0.4466, F1: 0.4026
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3438, Accuracy: 0.1429, Precision: 0.1240, Recall: 0.1833, F1: 0.0914
Epoch 2/70
Train Loss: 0.5853, Accuracy: 0.8062, Precision: 0.4949, Recall: 0.4789, F1: 0.4714
Validation Loss: 0.5547, Accuracy: 0.8068, Precision: 0.4737, Recall: 0.5416, F1: 0.4983
Testing Loss: 0.5140, Accuracy: 0.8298, Precision: 0.4940, Recall: 0.5290, F1: 0.5104
LM Predictions:  [3, 5, 3, 3, 3, 3, 5, 3, 5, 3, 3, 5, 3, 5, 3, 5, 3, 3, 5, 5, 3, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.6289, Accuracy: 0.0714, Precision: 0.1042, Recall: 0.0694, F1: 0.0694
Epoch 3/70
Train Loss: 0.4420, Accuracy: 0.8537, Precision: 0.5865, Recall: 0.5994, F1: 0.5860
Validation Loss: 0.4764, Accuracy: 0.8494, Precision: 0.6593, Recall: 0.6211, F1: 0.6355
Testing Loss: 0.5639, Accuracy: 0.8457, Precision: 0.6114, Recall: 0.6073, F1: 0.6028
LM Predictions:  [3, 5, 1, 1, 5, 3, 5, 1, 1, 1, 3, 5, 3, 3, 1, 5, 3, 3, 1, 5, 1, 1, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.7627, Accuracy: 0.0714, Precision: 0.1111, Recall: 0.0694, F1: 0.0750
Epoch 4/70
Train Loss: 0.3678, Accuracy: 0.8870, Precision: 0.6648, Recall: 0.6776, F1: 0.6639
Validation Loss: 0.4561, Accuracy: 0.8551, Precision: 0.6584, Recall: 0.6485, F1: 0.6457
Testing Loss: 0.4205, Accuracy: 0.8697, Precision: 0.6455, Recall: 0.6597, F1: 0.6496
LM Predictions:  [3, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 5, 3, 3, 3, 3, 3, 3, 4, 5, 3, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4743, Accuracy: 0.1071, Precision: 0.3750, Recall: 0.0903, F1: 0.1263
Epoch 5/70
Train Loss: 0.2904, Accuracy: 0.9101, Precision: 0.7626, Recall: 0.7415, F1: 0.7406
Validation Loss: 0.5365, Accuracy: 0.8239, Precision: 0.6944, Recall: 0.6931, F1: 0.6556
Testing Loss: 0.5030, Accuracy: 0.8404, Precision: 0.6603, Recall: 0.6260, F1: 0.6378
LM Predictions:  [3, 3, 3, 3, 0, 3, 5, 3, 5, 3, 3, 5, 0, 3, 3, 0, 3, 3, 4, 0, 3, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2723, Accuracy: 0.1786, Precision: 0.4722, Recall: 0.1319, F1: 0.1878
Epoch 6/70
Train Loss: 0.2345, Accuracy: 0.9314, Precision: 0.8169, Recall: 0.8046, F1: 0.8041
Validation Loss: 0.5462, Accuracy: 0.8494, Precision: 0.6771, Recall: 0.6287, F1: 0.6393
Testing Loss: 0.5683, Accuracy: 0.8564, Precision: 0.6861, Recall: 0.6507, F1: 0.6595
LM Predictions:  [3, 3, 3, 3, 5, 3, 5, 3, 5, 3, 2, 5, 0, 4, 5, 5, 3, 0, 4, 0, 3, 3, 3, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0163, Accuracy: 0.2857, Precision: 0.4306, Recall: 0.2222, F1: 0.2569
Epoch 7/70
Train Loss: 0.1850, Accuracy: 0.9458, Precision: 0.8376, Recall: 0.8457, F1: 0.8404
Validation Loss: 0.5422, Accuracy: 0.8580, Precision: 0.6836, Recall: 0.7235, F1: 0.6988
Testing Loss: 0.5680, Accuracy: 0.8484, Precision: 0.6103, Recall: 0.6527, F1: 0.6236
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 0, 1, 3, 3, 3, 4, 3, 1, 3, 3, 4, 0, 3, 3, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4728, Accuracy: 0.2143, Precision: 0.5000, Recall: 0.1389, F1: 0.2167
Epoch 8/70
Train Loss: 0.1458, Accuracy: 0.9545, Precision: 0.8767, Recall: 0.8782, F1: 0.8766
Validation Loss: 0.5430, Accuracy: 0.8551, Precision: 0.7032, Recall: 0.7083, F1: 0.6886
Testing Loss: 0.5462, Accuracy: 0.8697, Precision: 0.6898, Recall: 0.6891, F1: 0.6858
LM Predictions:  [3, 3, 2, 3, 3, 3, 0, 3, 0, 3, 3, 5, 1, 4, 1, 0, 3, 3, 4, 3, 3, 2, 3, 3, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6796, Accuracy: 0.3929, Precision: 0.6944, Recall: 0.3472, F1: 0.4503
Epoch 9/70
Train Loss: 0.1297, Accuracy: 0.9643, Precision: 0.8998, Recall: 0.9065, F1: 0.9025
Validation Loss: 0.5720, Accuracy: 0.8580, Precision: 0.6652, Recall: 0.6724, F1: 0.6673
Testing Loss: 0.5879, Accuracy: 0.8617, Precision: 0.6484, Recall: 0.6687, F1: 0.6556
LM Predictions:  [3, 3, 1, 3, 3, 3, 5, 3, 0, 1, 2, 5, 0, 4, 3, 0, 3, 0, 4, 0, 3, 4, 3, 0, 2, 3, 3, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5853, Accuracy: 0.4286, Precision: 0.5556, Recall: 0.2847, F1: 0.3690
Epoch 10/70
Train Loss: 0.1100, Accuracy: 0.9689, Precision: 0.9102, Recall: 0.9187, F1: 0.9142
Validation Loss: 0.7496, Accuracy: 0.8438, Precision: 0.6706, Recall: 0.6341, F1: 0.6432
Testing Loss: 0.6825, Accuracy: 0.8670, Precision: 0.6685, Recall: 0.6618, F1: 0.6547
LM Predictions:  [3, 3, 2, 5, 4, 3, 0, 2, 5, 5, 2, 5, 1, 4, 5, 1, 4, 0, 4, 5, 3, 2, 3, 0, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2094, Accuracy: 0.6071, Precision: 0.6452, Recall: 0.5278, F1: 0.5449
Epoch 11/70
Train Loss: 0.1043, Accuracy: 0.9717, Precision: 0.9209, Recall: 0.9150, F1: 0.9172
Validation Loss: 0.5821, Accuracy: 0.8693, Precision: 0.6955, Recall: 0.6632, F1: 0.6771
Testing Loss: 0.5682, Accuracy: 0.8777, Precision: 0.6681, Recall: 0.6703, F1: 0.6674
LM Predictions:  [3, 3, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 3, 0, 4, 0, 4, 5, 3, 3, 4, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9026, Accuracy: 0.7143, Precision: 0.7083, Recall: 0.6319, F1: 0.6536
Epoch 12/70
Train Loss: 0.0765, Accuracy: 0.9783, Precision: 0.9371, Recall: 0.9541, F1: 0.9450
Validation Loss: 0.7183, Accuracy: 0.8580, Precision: 0.7008, Recall: 0.6257, F1: 0.6518
Testing Loss: 0.7074, Accuracy: 0.8670, Precision: 0.6986, Recall: 0.6506, F1: 0.6662
LM Predictions:  [3, 4, 2, 5, 4, 2, 0, 5, 0, 1, 2, 5, 1, 4, 5, 0, 4, 0, 4, 5, 3, 3, 4, 0, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0900, Accuracy: 0.6429, Precision: 0.5893, Recall: 0.5208, F1: 0.5358
Epoch 13/70
Train Loss: 0.0985, Accuracy: 0.9706, Precision: 0.9216, Recall: 0.9252, F1: 0.9234
Validation Loss: 0.6617, Accuracy: 0.8665, Precision: 0.7055, Recall: 0.6533, F1: 0.6753
Testing Loss: 0.7057, Accuracy: 0.8803, Precision: 0.7046, Recall: 0.6678, F1: 0.6820
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 3, 0, 4, 0, 4, 0, 3, 4, 4, 0, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5983, Accuracy: 0.7857, Precision: 0.7153, Recall: 0.6736, F1: 0.6804
Epoch 14/70
Train Loss: 0.0762, Accuracy: 0.9801, Precision: 0.9379, Recall: 0.9409, F1: 0.9394
Validation Loss: 0.5834, Accuracy: 0.8466, Precision: 0.6586, Recall: 0.6208, F1: 0.6357
Testing Loss: 0.6206, Accuracy: 0.8590, Precision: 0.6483, Recall: 0.6363, F1: 0.6394
LM Predictions:  [3, 4, 2, 3, 4, 3, 0, 2, 0, 3, 3, 5, 1, 4, 3, 0, 4, 0, 4, 0, 3, 4, 4, 0, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2081, Accuracy: 0.6786, Precision: 0.7569, Recall: 0.5208, F1: 0.6062
Epoch 15/70
Train Loss: 0.0618, Accuracy: 0.9839, Precision: 0.9555, Recall: 0.9591, F1: 0.9573
Validation Loss: 0.5766, Accuracy: 0.8665, Precision: 0.6820, Recall: 0.6620, F1: 0.6715
Testing Loss: 0.6387, Accuracy: 0.8697, Precision: 0.6631, Recall: 0.6842, F1: 0.6710
LM Predictions:  [3, 3, 2, 3, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 2, 0, 4, 0, 3, 3, 2, 0, 2, 0, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8429, Accuracy: 0.7500, Precision: 0.7857, Recall: 0.6597, F1: 0.6953
Epoch 16/70
Train Loss: 0.0511, Accuracy: 0.9888, Precision: 0.9681, Recall: 0.9776, F1: 0.9727
Validation Loss: 0.6763, Accuracy: 0.8580, Precision: 0.6770, Recall: 0.6181, F1: 0.6397
Testing Loss: 0.6601, Accuracy: 0.8644, Precision: 0.6409, Recall: 0.6531, F1: 0.6357
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2480, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 17/70
Train Loss: 0.0453, Accuracy: 0.9881, Precision: 0.9786, Recall: 0.9798, F1: 0.9792
Validation Loss: 0.7525, Accuracy: 0.8665, Precision: 0.6820, Recall: 0.6597, F1: 0.6697
Testing Loss: 0.8155, Accuracy: 0.8697, Precision: 0.6670, Recall: 0.6843, F1: 0.6734
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1924, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 18/70
Train Loss: 0.0377, Accuracy: 0.9885, Precision: 0.9729, Recall: 0.9731, F1: 0.9729
Validation Loss: 0.7516, Accuracy: 0.8636, Precision: 0.7060, Recall: 0.7003, F1: 0.6892
Testing Loss: 0.6698, Accuracy: 0.8723, Precision: 0.6799, Recall: 0.6724, F1: 0.6733
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0624, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0452, Accuracy: 0.9902, Precision: 0.9801, Recall: 0.9800, F1: 0.9801
Validation Loss: 0.7927, Accuracy: 0.8494, Precision: 0.6773, Recall: 0.7039, F1: 0.6734
Testing Loss: 0.6582, Accuracy: 0.8750, Precision: 0.6442, Recall: 0.6605, F1: 0.6494
LM Predictions:  [4, 3, 2, 5, 4, 3, 0, 2, 0, 3, 2, 5, 1, 3, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4324, Accuracy: 0.8214, Precision: 0.8333, Recall: 0.6458, F1: 0.7190
Epoch 20/70
Train Loss: 0.0482, Accuracy: 0.9874, Precision: 0.9650, Recall: 0.9706, F1: 0.9677
Validation Loss: 0.7226, Accuracy: 0.8438, Precision: 0.6917, Recall: 0.6953, F1: 0.6826
Testing Loss: 0.6372, Accuracy: 0.8697, Precision: 0.6661, Recall: 0.6576, F1: 0.6569
LM Predictions:  [4, 2, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1099, Accuracy: 0.9643, Precision: 0.9714, Recall: 0.9750, F1: 0.9713
Epoch 21/70
Train Loss: 0.0229, Accuracy: 0.9934, Precision: 0.9830, Recall: 0.9873, F1: 0.9851
Validation Loss: 0.7572, Accuracy: 0.8580, Precision: 0.6672, Recall: 0.6606, F1: 0.6635
Testing Loss: 0.7222, Accuracy: 0.8750, Precision: 0.6711, Recall: 0.6691, F1: 0.6679
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0128, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0190, Accuracy: 0.9958, Precision: 0.9909, Recall: 0.9909, F1: 0.9909
Validation Loss: 0.8565, Accuracy: 0.8636, Precision: 0.6568, Recall: 0.6638, F1: 0.6576
Testing Loss: 0.6849, Accuracy: 0.8830, Precision: 0.6752, Recall: 0.6769, F1: 0.6752
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0370, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 23/70
Train Loss: 0.0327, Accuracy: 0.9923, Precision: 0.9765, Recall: 0.9821, F1: 0.9793
Validation Loss: 0.7182, Accuracy: 0.8722, Precision: 0.7622, Recall: 0.7786, F1: 0.7426
Testing Loss: 0.8384, Accuracy: 0.8511, Precision: 0.6921, Recall: 0.6702, F1: 0.6772
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0188, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0425, Accuracy: 0.9892, Precision: 0.9715, Recall: 0.9694, F1: 0.9704
Validation Loss: 0.8453, Accuracy: 0.8409, Precision: 0.6870, Recall: 0.6904, F1: 0.6665
Testing Loss: 0.8319, Accuracy: 0.8511, Precision: 0.6339, Recall: 0.6292, F1: 0.6292
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0380, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0478, Accuracy: 0.9881, Precision: 0.9682, Recall: 0.9649, F1: 0.9665
Validation Loss: 0.7365, Accuracy: 0.8580, Precision: 0.7068, Recall: 0.7244, F1: 0.7030
Testing Loss: 0.7866, Accuracy: 0.8697, Precision: 0.6339, Recall: 0.6465, F1: 0.6394
LM Predictions:  [4, 4, 2, 5, 4, 3, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3959, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7292, F1: 0.7667
Epoch 26/70
Train Loss: 0.0392, Accuracy: 0.9902, Precision: 0.9665, Recall: 0.9774, F1: 0.9718
Validation Loss: 0.7528, Accuracy: 0.8494, Precision: 0.6992, Recall: 0.6893, F1: 0.6778
Testing Loss: 0.6650, Accuracy: 0.8564, Precision: 0.6779, Recall: 0.6694, F1: 0.6702
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0227, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0300, Accuracy: 0.9927, Precision: 0.9860, Recall: 0.9808, F1: 0.9833
Validation Loss: 0.8759, Accuracy: 0.8580, Precision: 0.7030, Recall: 0.6448, F1: 0.6694
Testing Loss: 0.7514, Accuracy: 0.8750, Precision: 0.6713, Recall: 0.6642, F1: 0.6666
LM Predictions:  [4, 4, 2, 5, 4, 1, 4, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0668, Accuracy: 0.9643, Precision: 0.9778, Recall: 0.9750, F1: 0.9749
Epoch 28/70
Train Loss: 0.0162, Accuracy: 0.9962, Precision: 0.9887, Recall: 0.9913, F1: 0.9899
Validation Loss: 0.8628, Accuracy: 0.8466, Precision: 0.7035, Recall: 0.7313, F1: 0.7048
Testing Loss: 0.8626, Accuracy: 0.8644, Precision: 0.6566, Recall: 0.6749, F1: 0.6636
LM Predictions:  [4, 4, 5, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0637, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9667, F1: 0.9596
Epoch 29/70
Train Loss: 0.0234, Accuracy: 0.9948, Precision: 0.9884, Recall: 0.9931, F1: 0.9907
Validation Loss: 0.8226, Accuracy: 0.8693, Precision: 0.7588, Recall: 0.7474, F1: 0.7225
Testing Loss: 0.7607, Accuracy: 0.8777, Precision: 0.7008, Recall: 0.6687, F1: 0.6827
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0029, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0327, Accuracy: 0.9909, Precision: 0.9750, Recall: 0.9781, F1: 0.9763
Validation Loss: 0.7675, Accuracy: 0.8778, Precision: 0.7403, Recall: 0.7444, F1: 0.7403
Testing Loss: 0.7440, Accuracy: 0.8723, Precision: 0.6479, Recall: 0.6859, F1: 0.6570
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 3, 4, 0, 4, 0, 2, 4, 2, 1, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1201, Accuracy: 0.9286, Precision: 0.7778, Recall: 0.7917, F1: 0.7762
Epoch 31/70
Train Loss: 0.0304, Accuracy: 0.9927, Precision: 0.9799, Recall: 0.9756, F1: 0.9777
Validation Loss: 0.8222, Accuracy: 0.8409, Precision: 0.7062, Recall: 0.7383, F1: 0.6795
Testing Loss: 0.8573, Accuracy: 0.8484, Precision: 0.6686, Recall: 0.6387, F1: 0.6462
LM Predictions:  [0, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 0, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4117, Accuracy: 0.9286, Precision: 0.9600, Recall: 0.9500, F1: 0.9492
Epoch 32/70
Train Loss: 0.0417, Accuracy: 0.9909, Precision: 0.9731, Recall: 0.9777, F1: 0.9753
Validation Loss: 0.8456, Accuracy: 0.8636, Precision: 0.7224, Recall: 0.6890, F1: 0.6817
Testing Loss: 0.9110, Accuracy: 0.8564, Precision: 0.6801, Recall: 0.6289, F1: 0.6391
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0063, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0337, Accuracy: 0.9934, Precision: 0.9884, Recall: 0.9815, F1: 0.9849
Validation Loss: 0.9843, Accuracy: 0.8409, Precision: 0.6770, Recall: 0.6407, F1: 0.6283
Testing Loss: 0.8860, Accuracy: 0.8537, Precision: 0.6669, Recall: 0.6231, F1: 0.6312
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2747, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 34/70
Train Loss: 0.0486, Accuracy: 0.9902, Precision: 0.9873, Recall: 0.9873, F1: 0.9873
Validation Loss: 0.8087, Accuracy: 0.8523, Precision: 0.6719, Recall: 0.6914, F1: 0.6731
Testing Loss: 0.7258, Accuracy: 0.8670, Precision: 0.6657, Recall: 0.6457, F1: 0.6516
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0048, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0314, Accuracy: 0.9920, Precision: 0.9888, Recall: 0.9867, F1: 0.9878
Validation Loss: 0.8251, Accuracy: 0.8523, Precision: 0.7076, Recall: 0.6862, F1: 0.6900
Testing Loss: 0.7450, Accuracy: 0.8777, Precision: 0.6777, Recall: 0.6687, F1: 0.6721
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0132, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0218, Accuracy: 0.9962, Precision: 0.9915, Recall: 0.9925, F1: 0.9920
Validation Loss: 0.8760, Accuracy: 0.8636, Precision: 0.6706, Recall: 0.6561, F1: 0.6628
Testing Loss: 0.8350, Accuracy: 0.8777, Precision: 0.6543, Recall: 0.6719, F1: 0.6608
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0085, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9990, F1: 0.9985
Validation Loss: 0.9873, Accuracy: 0.8523, Precision: 0.6646, Recall: 0.6393, F1: 0.6511
Testing Loss: 0.8709, Accuracy: 0.8723, Precision: 0.6477, Recall: 0.6481, F1: 0.6460
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0614, Accuracy: 0.9850, Precision: 0.9578, Recall: 0.9606, F1: 0.9589
Validation Loss: 0.8882, Accuracy: 0.8466, Precision: 0.6939, Recall: 0.6674, F1: 0.6742
Testing Loss: 0.7896, Accuracy: 0.8670, Precision: 0.6712, Recall: 0.6592, F1: 0.6627
LM Predictions:  [4, 4, 2, 3, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1072, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 39/70
Train Loss: 0.0297, Accuracy: 0.9934, Precision: 0.9710, Recall: 0.9800, F1: 0.9752
Validation Loss: 0.8320, Accuracy: 0.8665, Precision: 0.7176, Recall: 0.7137, F1: 0.7110
Testing Loss: 0.7824, Accuracy: 0.8644, Precision: 0.6452, Recall: 0.6621, F1: 0.6398
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0318, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0337, Accuracy: 0.9913, Precision: 0.9803, Recall: 0.9833, F1: 0.9817
Validation Loss: 0.9199, Accuracy: 0.8466, Precision: 0.6560, Recall: 0.6302, F1: 0.6353
Testing Loss: 0.7889, Accuracy: 0.8697, Precision: 0.6926, Recall: 0.6699, F1: 0.6774
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 5, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0860, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 41/70
Train Loss: 0.0231, Accuracy: 0.9944, Precision: 0.9938, Recall: 0.9925, F1: 0.9931
Validation Loss: 0.8701, Accuracy: 0.8551, Precision: 0.6873, Recall: 0.6562, F1: 0.6698
Testing Loss: 0.7935, Accuracy: 0.8697, Precision: 0.6682, Recall: 0.6748, F1: 0.6669
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0089, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0558, Accuracy: 0.9867, Precision: 0.9836, Recall: 0.9738, F1: 0.9786
Validation Loss: 0.7768, Accuracy: 0.8381, Precision: 0.6587, Recall: 0.6301, F1: 0.6429
Testing Loss: 0.8134, Accuracy: 0.8644, Precision: 0.6668, Recall: 0.6526, F1: 0.6579
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0277, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0107, Accuracy: 0.9972, Precision: 0.9905, Recall: 0.9930, F1: 0.9917
Validation Loss: 1.0839, Accuracy: 0.8438, Precision: 0.6637, Recall: 0.6645, F1: 0.6620
Testing Loss: 0.9628, Accuracy: 0.8777, Precision: 0.6694, Recall: 0.6847, F1: 0.6677
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0031, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0113, Accuracy: 0.9972, Precision: 0.9939, Recall: 0.9917, F1: 0.9928
Validation Loss: 0.9055, Accuracy: 0.8580, Precision: 0.7082, Recall: 0.7100, F1: 0.6949
Testing Loss: 0.8805, Accuracy: 0.8723, Precision: 0.6645, Recall: 0.6612, F1: 0.6530
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0184, Accuracy: 0.9969, Precision: 0.9904, Recall: 0.9915, F1: 0.9910
Validation Loss: 0.8241, Accuracy: 0.8665, Precision: 0.7318, Recall: 0.7546, F1: 0.7164
Testing Loss: 0.8327, Accuracy: 0.8803, Precision: 0.7094, Recall: 0.6625, F1: 0.6817
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0093, Accuracy: 0.9976, Precision: 0.9932, Recall: 0.9916, F1: 0.9924
Validation Loss: 0.9991, Accuracy: 0.8693, Precision: 0.7185, Recall: 0.6416, F1: 0.6589
Testing Loss: 0.9446, Accuracy: 0.8777, Precision: 0.6778, Recall: 0.6736, F1: 0.6686
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0028, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0290, Accuracy: 0.9941, Precision: 0.9916, Recall: 0.9904, F1: 0.9909
Validation Loss: 0.8996, Accuracy: 0.8494, Precision: 0.6613, Recall: 0.6219, F1: 0.6373
Testing Loss: 0.9013, Accuracy: 0.8537, Precision: 0.6512, Recall: 0.6555, F1: 0.6476
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2929, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 48/70
Train Loss: 0.0402, Accuracy: 0.9916, Precision: 0.9798, Recall: 0.9797, F1: 0.9797
Validation Loss: 0.8174, Accuracy: 0.8551, Precision: 0.6819, Recall: 0.7292, F1: 0.6919
Testing Loss: 0.8468, Accuracy: 0.8777, Precision: 0.6816, Recall: 0.6654, F1: 0.6727
LM Predictions:  [3, 3, 2, 5, 3, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 3, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2318, Accuracy: 0.7857, Precision: 0.8333, Recall: 0.7083, F1: 0.7333
Epoch 49/70
Train Loss: 0.0589, Accuracy: 0.9874, Precision: 0.9751, Recall: 0.9761, F1: 0.9755
Validation Loss: 0.9695, Accuracy: 0.8523, Precision: 0.7277, Recall: 0.6993, F1: 0.7038
Testing Loss: 0.9391, Accuracy: 0.8537, Precision: 0.6711, Recall: 0.6338, F1: 0.6497
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0035, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0252, Accuracy: 0.9948, Precision: 0.9853, Recall: 0.9913, F1: 0.9882
Validation Loss: 0.9073, Accuracy: 0.8352, Precision: 0.6736, Recall: 0.6924, F1: 0.6612
Testing Loss: 0.9370, Accuracy: 0.8537, Precision: 0.6557, Recall: 0.6771, F1: 0.6544
LM Predictions:  [4, 4, 2, 5, 3, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2592, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 51/70
Train Loss: 0.0283, Accuracy: 0.9948, Precision: 0.9891, Recall: 0.9931, F1: 0.9911
Validation Loss: 0.8108, Accuracy: 0.8551, Precision: 0.7528, Recall: 0.6149, F1: 0.6540
Testing Loss: 0.7968, Accuracy: 0.8723, Precision: 0.6734, Recall: 0.6527, F1: 0.6530
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0055, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0141, Accuracy: 0.9976, Precision: 0.9947, Recall: 0.9947, F1: 0.9947
Validation Loss: 0.9211, Accuracy: 0.8494, Precision: 0.6871, Recall: 0.6480, F1: 0.6653
Testing Loss: 0.7996, Accuracy: 0.8777, Precision: 0.6592, Recall: 0.6756, F1: 0.6660
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0044, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0040, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.9593, Accuracy: 0.8494, Precision: 0.6826, Recall: 0.6410, F1: 0.6589
Testing Loss: 0.8420, Accuracy: 0.8910, Precision: 0.6799, Recall: 0.6843, F1: 0.6815
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0076, Accuracy: 0.9990, Precision: 0.9982, Recall: 0.9982, F1: 0.9982
Validation Loss: 0.9973, Accuracy: 0.8551, Precision: 0.6886, Recall: 0.6290, F1: 0.6497
Testing Loss: 0.8790, Accuracy: 0.8856, Precision: 0.6599, Recall: 0.6699, F1: 0.6625
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0038, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 1.0226, Accuracy: 0.8608, Precision: 0.6771, Recall: 0.6775, F1: 0.6766
Testing Loss: 0.9175, Accuracy: 0.8856, Precision: 0.6735, Recall: 0.6847, F1: 0.6788
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0071, Accuracy: 0.9993, Precision: 0.9995, Recall: 0.9997, F1: 0.9996
Validation Loss: 1.0041, Accuracy: 0.8750, Precision: 0.7087, Recall: 0.6737, F1: 0.6893
Testing Loss: 0.8961, Accuracy: 0.8963, Precision: 0.6874, Recall: 0.6912, F1: 0.6892
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0374, Accuracy: 0.9906, Precision: 0.9706, Recall: 0.9696, F1: 0.9700
Validation Loss: 0.8645, Accuracy: 0.8693, Precision: 0.7747, Recall: 0.7291, F1: 0.7415
Testing Loss: 0.9849, Accuracy: 0.8750, Precision: 0.6708, Recall: 0.6711, F1: 0.6692
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 3, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0510, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 58/70
Train Loss: 0.0286, Accuracy: 0.9948, Precision: 0.9889, Recall: 0.9820, F1: 0.9853
Validation Loss: 0.7657, Accuracy: 0.8665, Precision: 0.7379, Recall: 0.7523, F1: 0.7197
Testing Loss: 0.7756, Accuracy: 0.8723, Precision: 0.6869, Recall: 0.6670, F1: 0.6755
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0137, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0253, Accuracy: 0.9948, Precision: 0.9957, Recall: 0.9939, F1: 0.9948
Validation Loss: 0.7197, Accuracy: 0.8807, Precision: 0.7605, Recall: 0.7623, F1: 0.7429
Testing Loss: 0.8108, Accuracy: 0.8617, Precision: 0.6645, Recall: 0.6608, F1: 0.6593
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0042, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.7978, Accuracy: 0.8665, Precision: 0.7552, Recall: 0.7451, F1: 0.7341
Testing Loss: 0.8675, Accuracy: 0.8803, Precision: 0.6934, Recall: 0.6855, F1: 0.6872
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0063, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9954, F1: 0.9967
Validation Loss: 0.8110, Accuracy: 0.8807, Precision: 0.7369, Recall: 0.7307, F1: 0.7241
Testing Loss: 0.9126, Accuracy: 0.8697, Precision: 0.6834, Recall: 0.6773, F1: 0.6783
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0034, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.8143, Accuracy: 0.8835, Precision: 0.7893, Recall: 0.7236, F1: 0.7489
Testing Loss: 0.9022, Accuracy: 0.8750, Precision: 0.6815, Recall: 0.6797, F1: 0.6786
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0089, Accuracy: 0.9976, Precision: 0.9945, Recall: 0.9989, F1: 0.9967
Validation Loss: 0.9504, Accuracy: 0.8750, Precision: 0.7999, Recall: 0.6951, F1: 0.7349
Testing Loss: 0.9620, Accuracy: 0.8830, Precision: 0.6883, Recall: 0.6621, F1: 0.6733
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0461, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0336, Accuracy: 0.9916, Precision: 0.9838, Recall: 0.9847, F1: 0.9842
Validation Loss: 0.8677, Accuracy: 0.8807, Precision: 0.7591, Recall: 0.7447, F1: 0.7486
Testing Loss: 1.0064, Accuracy: 0.8537, Precision: 0.6227, Recall: 0.6662, F1: 0.6400
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 4, 0, 4, 5, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2011, Accuracy: 0.9286, Precision: 0.7917, Recall: 0.7708, F1: 0.7806
Epoch 65/70
Train Loss: 0.0614, Accuracy: 0.9874, Precision: 0.9865, Recall: 0.9845, F1: 0.9855
Validation Loss: 0.9032, Accuracy: 0.8693, Precision: 0.6806, Recall: 0.6543, F1: 0.6655
Testing Loss: 0.8986, Accuracy: 0.8777, Precision: 0.6731, Recall: 0.6765, F1: 0.6706
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0308, Accuracy: 0.9934, Precision: 0.9910, Recall: 0.9911, F1: 0.9911
Validation Loss: 0.8814, Accuracy: 0.8580, Precision: 0.7594, Recall: 0.7403, F1: 0.7327
Testing Loss: 0.9448, Accuracy: 0.8644, Precision: 0.6910, Recall: 0.6542, F1: 0.6692
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0040, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0282, Accuracy: 0.9930, Precision: 0.9948, Recall: 0.9935, F1: 0.9942
Validation Loss: 0.8522, Accuracy: 0.8722, Precision: 0.7280, Recall: 0.7213, F1: 0.7227
Testing Loss: 0.8670, Accuracy: 0.8856, Precision: 0.6803, Recall: 0.6937, F1: 0.6863
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0227, Accuracy: 0.9955, Precision: 0.9933, Recall: 0.9922, F1: 0.9927
Validation Loss: 1.0604, Accuracy: 0.8352, Precision: 0.7026, Recall: 0.6019, F1: 0.6293
Testing Loss: 0.9170, Accuracy: 0.8644, Precision: 0.7021, Recall: 0.6527, F1: 0.6655
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0049, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0263, Accuracy: 0.9962, Precision: 0.9978, Recall: 0.9925, F1: 0.9952
Validation Loss: 0.9450, Accuracy: 0.8693, Precision: 0.7180, Recall: 0.7747, F1: 0.7295
Testing Loss: 0.9830, Accuracy: 0.8670, Precision: 0.6555, Recall: 0.6872, F1: 0.6642
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0285, Accuracy: 0.9951, Precision: 0.9816, Recall: 0.9832, F1: 0.9823
Validation Loss: 0.9036, Accuracy: 0.8665, Precision: 0.7484, Recall: 0.6937, F1: 0.7099
Testing Loss: 0.7885, Accuracy: 0.8777, Precision: 0.6761, Recall: 0.6806, F1: 0.6726
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0087, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.8234, Accuracy: 0.7134, Precision: 0.5082, Recall: 0.4290, F1: 0.4469
Validation Loss: 0.4084, Accuracy: 0.8807, Precision: 0.6771, Recall: 0.6699, F1: 0.6718
Testing Loss: 0.4491, Accuracy: 0.8697, Precision: 0.6399, Recall: 0.6531, F1: 0.6417
LM Predictions:  [3, 1, 4, 1, 3, 3, 5, 1, 1, 1, 3, 3, 3, 3, 2, 5, 3, 3, 1, 3, 1, 1, 3, 3, 5, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4972, Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000
Epoch 2/70
Train Loss: 0.4102, Accuracy: 0.8730, Precision: 0.8125, Recall: 0.6720, F1: 0.6671
Validation Loss: 0.4347, Accuracy: 0.8665, Precision: 0.6776, Recall: 0.6751, F1: 0.6715
Testing Loss: 0.4314, Accuracy: 0.8750, Precision: 0.6644, Recall: 0.6793, F1: 0.6657
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.8802, Accuracy: 0.0714, Precision: 0.1111, Recall: 0.0556, F1: 0.0741
Epoch 3/70
Train Loss: 0.2625, Accuracy: 0.9178, Precision: 0.8139, Recall: 0.7864, F1: 0.7869
Validation Loss: 0.4155, Accuracy: 0.8722, Precision: 0.7248, Recall: 0.6596, F1: 0.6822
Testing Loss: 0.4297, Accuracy: 0.8564, Precision: 0.6692, Recall: 0.6573, F1: 0.6545
LM Predictions:  [3, 4, 4, 3, 3, 3, 5, 3, 5, 3, 3, 3, 3, 4, 4, 5, 3, 1, 4, 3, 3, 4, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2541, Accuracy: 0.1786, Precision: 0.1944, Recall: 0.1111, F1: 0.1369
Epoch 4/70
Train Loss: 0.1772, Accuracy: 0.9482, Precision: 0.8861, Recall: 0.8573, F1: 0.8630
Validation Loss: 0.5469, Accuracy: 0.8438, Precision: 0.7174, Recall: 0.6258, F1: 0.6574
Testing Loss: 0.4940, Accuracy: 0.8537, Precision: 0.6780, Recall: 0.6347, F1: 0.6474
LM Predictions:  [3, 4, 2, 3, 3, 3, 0, 3, 0, 3, 2, 2, 3, 4, 4, 4, 3, 3, 4, 3, 3, 4, 3, 4, 2, 3, 2, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8201, Accuracy: 0.3571, Precision: 0.3708, Recall: 0.2292, F1: 0.2617
Epoch 5/70
Train Loss: 0.1377, Accuracy: 0.9584, Precision: 0.9022, Recall: 0.9060, F1: 0.9026
Validation Loss: 0.3418, Accuracy: 0.9006, Precision: 0.7664, Recall: 0.7485, F1: 0.7485
Testing Loss: 0.4528, Accuracy: 0.8803, Precision: 0.6710, Recall: 0.6822, F1: 0.6730
LM Predictions:  [3, 4, 2, 3, 3, 3, 5, 2, 5, 3, 2, 5, 1, 4, 2, 0, 3, 3, 4, 5, 3, 4, 3, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2416, Accuracy: 0.4286, Precision: 0.6619, Recall: 0.3819, F1: 0.4359
Epoch 6/70
Train Loss: 0.0865, Accuracy: 0.9762, Precision: 0.9406, Recall: 0.9396, F1: 0.9391
Validation Loss: 0.5378, Accuracy: 0.8636, Precision: 0.7179, Recall: 0.7344, F1: 0.7186
Testing Loss: 0.5760, Accuracy: 0.8511, Precision: 0.6448, Recall: 0.6778, F1: 0.6500
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 1, 4, 0, 3, 4, 4, 3, 2, 1, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8118, Accuracy: 0.7143, Precision: 0.7262, Recall: 0.6319, F1: 0.6488
Epoch 7/70
Train Loss: 0.0686, Accuracy: 0.9825, Precision: 0.9547, Recall: 0.9584, F1: 0.9565
Validation Loss: 0.5401, Accuracy: 0.8636, Precision: 0.7517, Recall: 0.6570, F1: 0.6719
Testing Loss: 0.6189, Accuracy: 0.8617, Precision: 0.6623, Recall: 0.6222, F1: 0.6363
LM Predictions:  [3, 4, 2, 5, 2, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 3, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4230, Accuracy: 0.8214, Precision: 0.8095, Recall: 0.7083, F1: 0.7471
Epoch 8/70
Train Loss: 0.0596, Accuracy: 0.9825, Precision: 0.9649, Recall: 0.9689, F1: 0.9668
Validation Loss: 0.4163, Accuracy: 0.8750, Precision: 0.7308, Recall: 0.7484, F1: 0.7314
Testing Loss: 0.5470, Accuracy: 0.8830, Precision: 0.6692, Recall: 0.6814, F1: 0.6747
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 3, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3005, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7639, F1: 0.7944
Epoch 9/70
Train Loss: 0.0445, Accuracy: 0.9878, Precision: 0.9768, Recall: 0.9800, F1: 0.9783
Validation Loss: 0.4884, Accuracy: 0.8892, Precision: 0.7332, Recall: 0.6670, F1: 0.6926
Testing Loss: 0.5740, Accuracy: 0.8803, Precision: 0.6782, Recall: 0.6424, F1: 0.6551
LM Predictions:  [3, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1654, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 10/70
Train Loss: 0.0490, Accuracy: 0.9853, Precision: 0.9579, Recall: 0.9622, F1: 0.9599
Validation Loss: 0.5426, Accuracy: 0.8807, Precision: 0.7643, Recall: 0.7282, F1: 0.7354
Testing Loss: 0.5788, Accuracy: 0.8856, Precision: 0.7052, Recall: 0.7063, F1: 0.7012
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0361, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 11/70
Train Loss: 0.0231, Accuracy: 0.9934, Precision: 0.9923, Recall: 0.9886, F1: 0.9904
Validation Loss: 0.5056, Accuracy: 0.8977, Precision: 0.7984, Recall: 0.7521, F1: 0.7709
Testing Loss: 0.6903, Accuracy: 0.8723, Precision: 0.6762, Recall: 0.6764, F1: 0.6721
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0410, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 12/70
Train Loss: 0.0171, Accuracy: 0.9951, Precision: 0.9855, Recall: 0.9881, F1: 0.9868
Validation Loss: 0.6034, Accuracy: 0.8778, Precision: 0.7262, Recall: 0.7468, F1: 0.7262
Testing Loss: 0.6454, Accuracy: 0.8856, Precision: 0.7077, Recall: 0.6715, F1: 0.6811
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 5, 2, 5, 3, 4, 0, 0, 4, 0, 4, 0, 2, 3, 2, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3869, Accuracy: 0.8214, Precision: 0.8333, Recall: 0.6667, F1: 0.7282
Epoch 13/70
Train Loss: 0.0263, Accuracy: 0.9916, Precision: 0.9810, Recall: 0.9835, F1: 0.9822
Validation Loss: 0.5791, Accuracy: 0.8665, Precision: 0.7005, Recall: 0.7691, F1: 0.7008
Testing Loss: 0.7698, Accuracy: 0.8617, Precision: 0.6583, Recall: 0.6839, F1: 0.6583
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0503, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 14/70
Train Loss: 0.0437, Accuracy: 0.9881, Precision: 0.9686, Recall: 0.9747, F1: 0.9716
Validation Loss: 0.6768, Accuracy: 0.8665, Precision: 0.7607, Recall: 0.6723, F1: 0.6734
Testing Loss: 0.7142, Accuracy: 0.8564, Precision: 0.6536, Recall: 0.6244, F1: 0.6295
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 3, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1780, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 15/70
Train Loss: 0.0339, Accuracy: 0.9885, Precision: 0.9767, Recall: 0.9762, F1: 0.9764
Validation Loss: 0.5699, Accuracy: 0.8807, Precision: 0.7490, Recall: 0.7556, F1: 0.7470
Testing Loss: 0.6724, Accuracy: 0.8803, Precision: 0.6712, Recall: 0.6883, F1: 0.6773
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0070, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 16/70
Train Loss: 0.0141, Accuracy: 0.9958, Precision: 0.9952, Recall: 0.9970, F1: 0.9961
Validation Loss: 0.6017, Accuracy: 0.8722, Precision: 0.7374, Recall: 0.7014, F1: 0.7119
Testing Loss: 0.7853, Accuracy: 0.8723, Precision: 0.6917, Recall: 0.6768, F1: 0.6813
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0090, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 17/70
Train Loss: 0.0238, Accuracy: 0.9923, Precision: 0.9877, Recall: 0.9861, F1: 0.9869
Validation Loss: 0.6251, Accuracy: 0.8608, Precision: 0.7095, Recall: 0.6627, F1: 0.6821
Testing Loss: 0.7964, Accuracy: 0.8644, Precision: 0.6457, Recall: 0.6699, F1: 0.6547
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0086, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 18/70
Train Loss: 0.0384, Accuracy: 0.9885, Precision: 0.9744, Recall: 0.9645, F1: 0.9693
Validation Loss: 0.7134, Accuracy: 0.8580, Precision: 0.7448, Recall: 0.6819, F1: 0.6981
Testing Loss: 0.7653, Accuracy: 0.8697, Precision: 0.6660, Recall: 0.6678, F1: 0.6641
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0037, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0307, Accuracy: 0.9913, Precision: 0.9842, Recall: 0.9833, F1: 0.9837
Validation Loss: 0.6894, Accuracy: 0.8778, Precision: 0.7468, Recall: 0.7518, F1: 0.7455
Testing Loss: 0.7021, Accuracy: 0.8803, Precision: 0.6726, Recall: 0.7016, F1: 0.6828
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0107, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 20/70
Train Loss: 0.0212, Accuracy: 0.9944, Precision: 0.9940, Recall: 0.9942, F1: 0.9941
Validation Loss: 0.6627, Accuracy: 0.8693, Precision: 0.7133, Recall: 0.7181, F1: 0.7092
Testing Loss: 0.7766, Accuracy: 0.8617, Precision: 0.6436, Recall: 0.6613, F1: 0.6496
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0164, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0176, Accuracy: 0.9958, Precision: 0.9941, Recall: 0.9911, F1: 0.9926
Validation Loss: 0.7094, Accuracy: 0.8608, Precision: 0.7355, Recall: 0.7816, F1: 0.7411
Testing Loss: 0.6636, Accuracy: 0.8777, Precision: 0.6784, Recall: 0.6864, F1: 0.6807
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0225, Accuracy: 0.9951, Precision: 0.9903, Recall: 0.9856, F1: 0.9879
Validation Loss: 0.8538, Accuracy: 0.8381, Precision: 0.6826, Recall: 0.7149, F1: 0.6853
Testing Loss: 0.7508, Accuracy: 0.8723, Precision: 0.6764, Recall: 0.6918, F1: 0.6820
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 3, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1471, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8056, F1: 0.8182
Epoch 23/70
Train Loss: 0.0193, Accuracy: 0.9944, Precision: 0.9861, Recall: 0.9873, F1: 0.9867
Validation Loss: 0.5886, Accuracy: 0.8864, Precision: 0.7701, Recall: 0.7380, F1: 0.7458
Testing Loss: 0.7345, Accuracy: 0.8644, Precision: 0.6659, Recall: 0.6457, F1: 0.6523
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0052, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0041, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9983, F1: 0.9982
Validation Loss: 0.6234, Accuracy: 0.8920, Precision: 0.7651, Recall: 0.7338, F1: 0.7370
Testing Loss: 0.8050, Accuracy: 0.8564, Precision: 0.6447, Recall: 0.6567, F1: 0.6464
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0396, Accuracy: 0.9881, Precision: 0.9683, Recall: 0.9697, F1: 0.9690
Validation Loss: 0.6051, Accuracy: 0.8807, Precision: 0.7665, Recall: 0.7745, F1: 0.7368
Testing Loss: 0.6774, Accuracy: 0.8697, Precision: 0.7003, Recall: 0.6601, F1: 0.6783
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0054, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0105, Accuracy: 0.9983, Precision: 0.9976, Recall: 0.9934, F1: 0.9954
Validation Loss: 0.8007, Accuracy: 0.8636, Precision: 0.7825, Recall: 0.6803, F1: 0.6527
Testing Loss: 0.8161, Accuracy: 0.8511, Precision: 0.6975, Recall: 0.5939, F1: 0.6272
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0018, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 27/70
Train Loss: 0.0050, Accuracy: 0.9983, Precision: 0.9960, Recall: 0.9939, F1: 0.9949
Validation Loss: 0.6776, Accuracy: 0.8920, Precision: 0.7951, Recall: 0.7844, F1: 0.7727
Testing Loss: 0.7555, Accuracy: 0.8830, Precision: 0.6887, Recall: 0.6790, F1: 0.6831
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0156, Accuracy: 0.9965, Precision: 0.9916, Recall: 0.9929, F1: 0.9923
Validation Loss: 0.5536, Accuracy: 0.8920, Precision: 0.7530, Recall: 0.7636, F1: 0.7547
Testing Loss: 0.7551, Accuracy: 0.8830, Precision: 0.6684, Recall: 0.6802, F1: 0.6736
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0063, Accuracy: 0.9983, Precision: 0.9976, Recall: 0.9978, F1: 0.9977
Validation Loss: 0.6259, Accuracy: 0.8807, Precision: 0.7456, Recall: 0.7328, F1: 0.7262
Testing Loss: 0.7884, Accuracy: 0.8723, Precision: 0.6793, Recall: 0.6609, F1: 0.6690
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0342, Accuracy: 0.9909, Precision: 0.9890, Recall: 0.9872, F1: 0.9881
Validation Loss: 0.6432, Accuracy: 0.8835, Precision: 0.7525, Recall: 0.7241, F1: 0.7252
Testing Loss: 0.7385, Accuracy: 0.8697, Precision: 0.6802, Recall: 0.6699, F1: 0.6713
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 5, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0599, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 31/70
Train Loss: 0.0217, Accuracy: 0.9944, Precision: 0.9907, Recall: 0.9896, F1: 0.9902
Validation Loss: 0.6131, Accuracy: 0.8835, Precision: 0.7575, Recall: 0.7840, F1: 0.7459
Testing Loss: 0.6087, Accuracy: 0.8856, Precision: 0.7219, Recall: 0.7277, F1: 0.7238
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1465, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 32/70
Train Loss: 0.0028, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9998, F1: 0.9992
Validation Loss: 0.6632, Accuracy: 0.8920, Precision: 0.7738, Recall: 0.7179, F1: 0.7301
Testing Loss: 0.6558, Accuracy: 0.8989, Precision: 0.7089, Recall: 0.7089, F1: 0.7088
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0180, Accuracy: 0.9955, Precision: 0.9862, Recall: 0.9900, F1: 0.9881
Validation Loss: 0.5825, Accuracy: 0.8977, Precision: 0.8143, Recall: 0.7603, F1: 0.7825
Testing Loss: 0.6681, Accuracy: 0.8910, Precision: 0.6890, Recall: 0.7142, F1: 0.7004
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0188, Accuracy: 0.9941, Precision: 0.9871, Recall: 0.9883, F1: 0.9877
Validation Loss: 0.6012, Accuracy: 0.8778, Precision: 0.7461, Recall: 0.7231, F1: 0.7274
Testing Loss: 0.7047, Accuracy: 0.8830, Precision: 0.6925, Recall: 0.7035, F1: 0.6951
LM Predictions:  [4, 4, 2, 5, 4, 1, 3, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0847, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 35/70
Train Loss: 0.0112, Accuracy: 0.9969, Precision: 0.9908, Recall: 0.9957, F1: 0.9932
Validation Loss: 0.8077, Accuracy: 0.8778, Precision: 0.7647, Recall: 0.6926, F1: 0.7010
Testing Loss: 0.7911, Accuracy: 0.8750, Precision: 0.6697, Recall: 0.6781, F1: 0.6628
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0082, Accuracy: 0.9979, Precision: 0.9976, Recall: 0.9963, F1: 0.9969
Validation Loss: 0.7343, Accuracy: 0.8807, Precision: 0.7568, Recall: 0.7299, F1: 0.7332
Testing Loss: 0.8525, Accuracy: 0.8750, Precision: 0.6613, Recall: 0.6826, F1: 0.6672
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0334, Accuracy: 0.9892, Precision: 0.9676, Recall: 0.9737, F1: 0.9706
Validation Loss: 0.8621, Accuracy: 0.8551, Precision: 0.7459, Recall: 0.6991, F1: 0.6989
Testing Loss: 0.7007, Accuracy: 0.8697, Precision: 0.6803, Recall: 0.6794, F1: 0.6734
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 5, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0567, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9667, F1: 0.9596
Epoch 38/70
Train Loss: 0.0184, Accuracy: 0.9941, Precision: 0.9910, Recall: 0.9897, F1: 0.9903
Validation Loss: 0.6760, Accuracy: 0.8722, Precision: 0.7621, Recall: 0.7631, F1: 0.7266
Testing Loss: 0.6991, Accuracy: 0.8723, Precision: 0.7187, Recall: 0.6936, F1: 0.7033
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0083, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0033, Accuracy: 0.9986, Precision: 0.9990, Recall: 0.9990, F1: 0.9990
Validation Loss: 0.6391, Accuracy: 0.8892, Precision: 0.7434, Recall: 0.7357, F1: 0.7294
Testing Loss: 0.7181, Accuracy: 0.8910, Precision: 0.6830, Recall: 0.6929, F1: 0.6865
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.6950, Accuracy: 0.8835, Precision: 0.7356, Recall: 0.7582, F1: 0.7337
Testing Loss: 0.7635, Accuracy: 0.8936, Precision: 0.6750, Recall: 0.7110, F1: 0.6902
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0110, Accuracy: 0.9969, Precision: 0.9982, Recall: 0.9979, F1: 0.9980
Validation Loss: 0.6352, Accuracy: 0.8693, Precision: 0.7300, Recall: 0.7229, F1: 0.7140
Testing Loss: 0.7584, Accuracy: 0.8723, Precision: 0.6874, Recall: 0.6960, F1: 0.6893
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0056, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0370, Accuracy: 0.9906, Precision: 0.9800, Recall: 0.9818, F1: 0.9809
Validation Loss: 0.8369, Accuracy: 0.8750, Precision: 0.7530, Recall: 0.6808, F1: 0.6991
Testing Loss: 0.8225, Accuracy: 0.8590, Precision: 0.6549, Recall: 0.6494, F1: 0.6400
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0405, Accuracy: 0.9913, Precision: 0.9806, Recall: 0.9817, F1: 0.9811
Validation Loss: 0.7836, Accuracy: 0.8835, Precision: 0.7209, Recall: 0.6781, F1: 0.6898
Testing Loss: 0.7907, Accuracy: 0.8856, Precision: 0.6856, Recall: 0.7031, F1: 0.6924
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1718, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 44/70
Train Loss: 0.0264, Accuracy: 0.9941, Precision: 0.9882, Recall: 0.9833, F1: 0.9858
Validation Loss: 0.7375, Accuracy: 0.8665, Precision: 0.7080, Recall: 0.6695, F1: 0.6820
Testing Loss: 0.7619, Accuracy: 0.8617, Precision: 0.6761, Recall: 0.7038, F1: 0.6839
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0240, Accuracy: 0.9934, Precision: 0.9802, Recall: 0.9802, F1: 0.9802
Validation Loss: 0.7291, Accuracy: 0.8722, Precision: 0.6771, Recall: 0.6552, F1: 0.6598
Testing Loss: 0.8430, Accuracy: 0.8617, Precision: 0.6408, Recall: 0.6667, F1: 0.6410
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 3, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 3, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1814, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7569, F1: 0.7889
Epoch 46/70
Train Loss: 0.0128, Accuracy: 0.9962, Precision: 0.9884, Recall: 0.9885, F1: 0.9884
Validation Loss: 0.6805, Accuracy: 0.8920, Precision: 0.7253, Recall: 0.6991, F1: 0.7109
Testing Loss: 0.8127, Accuracy: 0.8777, Precision: 0.6622, Recall: 0.6818, F1: 0.6713
LM Predictions:  [4, 4, 2, 5, 3, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3379, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7917, F1: 0.8111
Epoch 47/70
Train Loss: 0.0069, Accuracy: 0.9983, Precision: 0.9949, Recall: 0.9936, F1: 0.9943
Validation Loss: 0.7396, Accuracy: 0.8892, Precision: 0.7233, Recall: 0.6830, F1: 0.6980
Testing Loss: 0.8076, Accuracy: 0.8803, Precision: 0.6700, Recall: 0.6749, F1: 0.6699
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0048, Accuracy: 0.9986, Precision: 0.9977, Recall: 0.9992, F1: 0.9984
Validation Loss: 0.8207, Accuracy: 0.8750, Precision: 0.7619, Recall: 0.7052, F1: 0.7254
Testing Loss: 0.8431, Accuracy: 0.8750, Precision: 0.6783, Recall: 0.6551, F1: 0.6623
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0021, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7995, Accuracy: 0.8807, Precision: 0.8042, Recall: 0.7162, F1: 0.7522
Testing Loss: 0.8217, Accuracy: 0.8803, Precision: 0.6786, Recall: 0.6740, F1: 0.6752
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0219, Accuracy: 0.9955, Precision: 0.9932, Recall: 0.9892, F1: 0.9912
Validation Loss: 0.6713, Accuracy: 0.8665, Precision: 0.7154, Recall: 0.6684, F1: 0.6880
Testing Loss: 0.7784, Accuracy: 0.8590, Precision: 0.6498, Recall: 0.6671, F1: 0.6560
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0028, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0026, Accuracy: 0.9990, Precision: 0.9995, Recall: 0.9995, F1: 0.9995
Validation Loss: 0.7240, Accuracy: 0.8693, Precision: 0.7120, Recall: 0.6793, F1: 0.6937
Testing Loss: 0.8269, Accuracy: 0.8670, Precision: 0.6623, Recall: 0.6802, F1: 0.6699
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0120, Accuracy: 0.9958, Precision: 0.9943, Recall: 0.9909, F1: 0.9926
Validation Loss: 0.8926, Accuracy: 0.8580, Precision: 0.6984, Recall: 0.6377, F1: 0.6612
Testing Loss: 0.7796, Accuracy: 0.8723, Precision: 0.6659, Recall: 0.6588, F1: 0.6583
LM Predictions:  [4, 4, 2, 5, 5, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3087, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 53/70
Train Loss: 0.0175, Accuracy: 0.9962, Precision: 0.9939, Recall: 0.9937, F1: 0.9938
Validation Loss: 0.7181, Accuracy: 0.8636, Precision: 0.7041, Recall: 0.6769, F1: 0.6866
Testing Loss: 0.6998, Accuracy: 0.8856, Precision: 0.6849, Recall: 0.7110, F1: 0.6955
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0062, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0158, Accuracy: 0.9965, Precision: 0.9911, Recall: 0.9940, F1: 0.9926
Validation Loss: 0.8435, Accuracy: 0.8750, Precision: 0.7194, Recall: 0.6603, F1: 0.6854
Testing Loss: 0.7431, Accuracy: 0.8750, Precision: 0.6648, Recall: 0.6547, F1: 0.6580
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0021, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0081, Accuracy: 0.9986, Precision: 0.9977, Recall: 0.9966, F1: 0.9972
Validation Loss: 0.7281, Accuracy: 0.8835, Precision: 0.7361, Recall: 0.6934, F1: 0.7106
Testing Loss: 0.8128, Accuracy: 0.8830, Precision: 0.6710, Recall: 0.6855, F1: 0.6726
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0066, Accuracy: 0.9972, Precision: 0.9965, Recall: 0.9935, F1: 0.9950
Validation Loss: 0.9756, Accuracy: 0.8608, Precision: 0.6622, Recall: 0.6612, F1: 0.6560
Testing Loss: 0.9496, Accuracy: 0.8777, Precision: 0.6546, Recall: 0.6970, F1: 0.6628
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0036, Accuracy: 0.9990, Precision: 0.9969, Recall: 0.9954, F1: 0.9961
Validation Loss: 0.9600, Accuracy: 0.8608, Precision: 0.6985, Recall: 0.6528, F1: 0.6712
Testing Loss: 0.9161, Accuracy: 0.8750, Precision: 0.6708, Recall: 0.6928, F1: 0.6811
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0088, Accuracy: 0.9969, Precision: 0.9927, Recall: 0.9927, F1: 0.9927
Validation Loss: 0.6965, Accuracy: 0.8892, Precision: 0.7765, Recall: 0.8060, F1: 0.7681
Testing Loss: 0.8636, Accuracy: 0.8697, Precision: 0.6651, Recall: 0.6888, F1: 0.6689
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0120, Accuracy: 0.9972, Precision: 0.9904, Recall: 0.9982, F1: 0.9942
Validation Loss: 0.7367, Accuracy: 0.8778, Precision: 0.7423, Recall: 0.6648, F1: 0.6951
Testing Loss: 0.8362, Accuracy: 0.8803, Precision: 0.6932, Recall: 0.6818, F1: 0.6841
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0172, Accuracy: 0.9972, Precision: 0.9961, Recall: 0.9895, F1: 0.9928
Validation Loss: 0.7947, Accuracy: 0.8693, Precision: 0.7165, Recall: 0.7252, F1: 0.7165
Testing Loss: 0.7942, Accuracy: 0.8936, Precision: 0.6928, Recall: 0.7027, F1: 0.6910
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0025, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9995, F1: 0.9996
Validation Loss: 0.7851, Accuracy: 0.8835, Precision: 0.7894, Recall: 0.7656, F1: 0.7712
Testing Loss: 0.8404, Accuracy: 0.8910, Precision: 0.6954, Recall: 0.7068, F1: 0.6973
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0088, Accuracy: 0.9983, Precision: 0.9967, Recall: 0.9966, F1: 0.9966
Validation Loss: 0.8248, Accuracy: 0.8892, Precision: 0.7882, Recall: 0.7659, F1: 0.7695
Testing Loss: 0.8417, Accuracy: 0.8936, Precision: 0.6973, Recall: 0.6995, F1: 0.6925
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0573, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 63/70
Train Loss: 0.0110, Accuracy: 0.9983, Precision: 0.9979, Recall: 0.9979, F1: 0.9979
Validation Loss: 0.8804, Accuracy: 0.8608, Precision: 0.6940, Recall: 0.7671, F1: 0.7129
Testing Loss: 0.8702, Accuracy: 0.8670, Precision: 0.6439, Recall: 0.6912, F1: 0.6623
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1371, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 64/70
Train Loss: 0.0165, Accuracy: 0.9965, Precision: 0.9933, Recall: 0.9944, F1: 0.9938
Validation Loss: 0.8857, Accuracy: 0.8750, Precision: 0.7851, Recall: 0.6927, F1: 0.7181
Testing Loss: 0.9250, Accuracy: 0.8723, Precision: 0.6911, Recall: 0.6756, F1: 0.6701
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0016, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.8133, Accuracy: 0.8807, Precision: 0.6975, Recall: 0.6916, F1: 0.6941
Testing Loss: 0.8339, Accuracy: 0.8936, Precision: 0.6915, Recall: 0.7097, F1: 0.6999
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0502, Accuracy: 0.9899, Precision: 0.9675, Recall: 0.9639, F1: 0.9657
Validation Loss: 0.6984, Accuracy: 0.8693, Precision: 0.7097, Recall: 0.6744, F1: 0.6764
Testing Loss: 0.7185, Accuracy: 0.8564, Precision: 0.6626, Recall: 0.6596, F1: 0.6345
LM Predictions:  [4, 4, 2, 5, 4, 1, 5, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1685, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 67/70
Train Loss: 0.0395, Accuracy: 0.9909, Precision: 0.9735, Recall: 0.9654, F1: 0.9692
Validation Loss: 0.8049, Accuracy: 0.8693, Precision: 0.7324, Recall: 0.7006, F1: 0.7005
Testing Loss: 0.7755, Accuracy: 0.8750, Precision: 0.6582, Recall: 0.6695, F1: 0.6518
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0039, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0053, Accuracy: 0.9990, Precision: 0.9942, Recall: 0.9970, F1: 0.9956
Validation Loss: 0.8419, Accuracy: 0.8750, Precision: 0.7183, Recall: 0.6550, F1: 0.6807
Testing Loss: 0.8041, Accuracy: 0.8803, Precision: 0.7013, Recall: 0.6572, F1: 0.6664
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0137, Accuracy: 0.9955, Precision: 0.9843, Recall: 0.9829, F1: 0.9836
Validation Loss: 1.0517, Accuracy: 0.8267, Precision: 0.7465, Recall: 0.6850, F1: 0.6693
Testing Loss: 0.8455, Accuracy: 0.8484, Precision: 0.6846, Recall: 0.6080, F1: 0.6342
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0037, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0132, Accuracy: 0.9965, Precision: 0.9977, Recall: 0.9966, F1: 0.9972
Validation Loss: 0.7989, Accuracy: 0.8665, Precision: 0.7309, Recall: 0.7133, F1: 0.7069
Testing Loss: 0.7766, Accuracy: 0.8590, Precision: 0.6505, Recall: 0.6670, F1: 0.6484
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



