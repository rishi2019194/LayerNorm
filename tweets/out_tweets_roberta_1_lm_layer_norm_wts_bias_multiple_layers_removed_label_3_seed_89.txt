---------------------------------------------------------------------------
Results for seed:  89
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 974
  Label 2: 1105
  Label 5: 490
  Label 1: 117
  Label 0: 56
  Label 3: 116
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1468, Accuracy: 0.5609, Precision: 0.2630, Recall: 0.2895, F1: 0.2749
Validation Loss: 0.8937, Accuracy: 0.7159, Precision: 0.3603, Recall: 0.4024, F1: 0.3714
Testing Loss: 0.8776, Accuracy: 0.7048, Precision: 0.3587, Recall: 0.4100, F1: 0.3677
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4229, Accuracy: 0.1786, Precision: 0.1308, Recall: 0.2333, F1: 0.1033
Epoch 2/70
Train Loss: 0.7611, Accuracy: 0.7414, Precision: 0.4442, Recall: 0.4062, F1: 0.3804
Validation Loss: 0.6580, Accuracy: 0.7614, Precision: 0.4476, Recall: 0.4812, F1: 0.4638
Testing Loss: 0.6911, Accuracy: 0.7766, Precision: 0.4648, Recall: 0.4794, F1: 0.4707
LM Predictions:  [3, 5, 5, 5, 3, 3, 5, 5, 3, 5, 5, 5, 5, 4, 5, 4, 3, 3, 5, 3, 5, 5, 3, 4, 2, 5, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0246, Accuracy: 0.1786, Precision: 0.1746, Recall: 0.1736, F1: 0.1275
Epoch 3/70
Train Loss: 0.6243, Accuracy: 0.7775, Precision: 0.4796, Recall: 0.4729, F1: 0.4598
Validation Loss: 0.6377, Accuracy: 0.7756, Precision: 0.5064, Recall: 0.5242, F1: 0.4994
Testing Loss: 0.6336, Accuracy: 0.7819, Precision: 0.5045, Recall: 0.5172, F1: 0.5042
LM Predictions:  [3, 3, 3, 3, 3, 3, 5, 3, 3, 1, 3, 1, 3, 4, 3, 3, 3, 3, 1, 5, 3, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5366, Accuracy: 0.0714, Precision: 0.2500, Recall: 0.0486, F1: 0.0787
Epoch 4/70
Train Loss: 0.5407, Accuracy: 0.8184, Precision: 0.5245, Recall: 0.5424, F1: 0.5217
Validation Loss: 0.6395, Accuracy: 0.7898, Precision: 0.5739, Recall: 0.5392, F1: 0.5367
Testing Loss: 0.6659, Accuracy: 0.7979, Precision: 0.5645, Recall: 0.5424, F1: 0.5473
LM Predictions:  [3, 5, 4, 3, 3, 3, 5, 3, 3, 1, 3, 5, 3, 4, 4, 5, 3, 3, 5, 5, 1, 4, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3185, Accuracy: 0.1429, Precision: 0.1944, Recall: 0.1111, F1: 0.1306
Epoch 5/70
Train Loss: 0.4804, Accuracy: 0.8387, Precision: 0.5913, Recall: 0.5767, F1: 0.5713
Validation Loss: 0.6221, Accuracy: 0.7898, Precision: 0.4788, Recall: 0.5429, F1: 0.4952
Testing Loss: 0.6395, Accuracy: 0.8085, Precision: 0.4975, Recall: 0.5329, F1: 0.5057
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 3, 3, 3, 5, 3, 3, 1, 1, 3, 3, 3, 5, 3, 1, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.7313, Accuracy: 0.1071, Precision: 0.1667, Recall: 0.0972, F1: 0.1217
Epoch 6/70
Train Loss: 0.4412, Accuracy: 0.8485, Precision: 0.6265, Recall: 0.6068, F1: 0.6021
Validation Loss: 0.5769, Accuracy: 0.8267, Precision: 0.6148, Recall: 0.5775, F1: 0.5758
Testing Loss: 0.5675, Accuracy: 0.8245, Precision: 0.5741, Recall: 0.5845, F1: 0.5770
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 3, 1, 3, 5, 3, 4, 1, 5, 3, 3, 4, 5, 3, 4, 5, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1489, Accuracy: 0.2500, Precision: 0.3750, Recall: 0.2014, F1: 0.2298
Epoch 7/70
Train Loss: 0.3858, Accuracy: 0.8723, Precision: 0.6982, Recall: 0.6595, F1: 0.6694
Validation Loss: 0.5198, Accuracy: 0.8381, Precision: 0.6100, Recall: 0.5782, F1: 0.5831
Testing Loss: 0.6023, Accuracy: 0.8271, Precision: 0.6176, Recall: 0.6058, F1: 0.6036
LM Predictions:  [3, 1, 2, 1, 3, 3, 5, 1, 3, 1, 3, 5, 3, 4, 1, 5, 3, 3, 4, 5, 5, 2, 5, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2630, Accuracy: 0.2143, Precision: 0.3148, Recall: 0.1806, F1: 0.1920
Epoch 8/70
Train Loss: 0.3603, Accuracy: 0.8887, Precision: 0.7469, Recall: 0.6905, F1: 0.6950
Validation Loss: 0.5989, Accuracy: 0.8409, Precision: 0.6526, Recall: 0.6175, F1: 0.6248
Testing Loss: 0.5968, Accuracy: 0.8431, Precision: 0.6443, Recall: 0.6374, F1: 0.6389
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 5, 1, 3, 5, 3, 4, 0, 5, 3, 3, 4, 5, 3, 2, 5, 5, 2, 5, 0, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1165, Accuracy: 0.2143, Precision: 0.3819, Recall: 0.1597, F1: 0.2019
Epoch 9/70
Train Loss: 0.3269, Accuracy: 0.9034, Precision: 0.7777, Recall: 0.7324, F1: 0.7386
Validation Loss: 0.5560, Accuracy: 0.8295, Precision: 0.6132, Recall: 0.5975, F1: 0.6048
Testing Loss: 0.5826, Accuracy: 0.8378, Precision: 0.6182, Recall: 0.6330, F1: 0.6221
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 3, 1, 2, 5, 3, 4, 5, 5, 3, 3, 4, 5, 3, 2, 2, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1768, Accuracy: 0.2500, Precision: 0.3333, Recall: 0.1944, F1: 0.2249
Epoch 10/70
Train Loss: 0.2826, Accuracy: 0.9164, Precision: 0.7976, Recall: 0.7550, F1: 0.7638
Validation Loss: 0.5765, Accuracy: 0.8352, Precision: 0.6303, Recall: 0.6000, F1: 0.6045
Testing Loss: 0.5820, Accuracy: 0.8404, Precision: 0.6165, Recall: 0.6276, F1: 0.6209
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 0, 3, 2, 5, 3, 4, 0, 0, 3, 3, 4, 3, 3, 2, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8981, Accuracy: 0.3214, Precision: 0.5417, Recall: 0.2292, F1: 0.3131
Epoch 11/70
Train Loss: 0.2362, Accuracy: 0.9293, Precision: 0.8266, Recall: 0.8011, F1: 0.8108
Validation Loss: 0.6598, Accuracy: 0.8438, Precision: 0.6640, Recall: 0.6045, F1: 0.6263
Testing Loss: 0.8353, Accuracy: 0.8191, Precision: 0.6686, Recall: 0.6275, F1: 0.6352
LM Predictions:  [3, 1, 2, 3, 3, 3, 5, 3, 0, 1, 2, 5, 1, 4, 0, 0, 3, 3, 4, 5, 3, 4, 2, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0864, Accuracy: 0.4286, Precision: 0.5556, Recall: 0.3611, F1: 0.4072
Epoch 12/70
Train Loss: 0.2428, Accuracy: 0.9307, Precision: 0.8177, Recall: 0.8051, F1: 0.8101
Validation Loss: 0.5589, Accuracy: 0.8381, Precision: 0.6724, Recall: 0.6185, F1: 0.6392
Testing Loss: 0.5990, Accuracy: 0.8351, Precision: 0.6358, Recall: 0.6285, F1: 0.6283
LM Predictions:  [3, 2, 2, 3, 3, 3, 0, 3, 5, 1, 3, 5, 1, 4, 0, 0, 3, 3, 4, 5, 2, 2, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8399, Accuracy: 0.4286, Precision: 0.5952, Recall: 0.3819, F1: 0.4268
Epoch 13/70
Train Loss: 0.1932, Accuracy: 0.9447, Precision: 0.8462, Recall: 0.8407, F1: 0.8419
Validation Loss: 0.5422, Accuracy: 0.8239, Precision: 0.6094, Recall: 0.6339, F1: 0.6179
Testing Loss: 0.5486, Accuracy: 0.8245, Precision: 0.5989, Recall: 0.6212, F1: 0.6044
LM Predictions:  [3, 1, 2, 5, 3, 3, 5, 3, 5, 1, 3, 5, 1, 4, 0, 0, 3, 3, 4, 5, 5, 4, 2, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.5000, Accuracy: 0.4286, Precision: 0.6111, Recall: 0.3958, F1: 0.4123
Epoch 14/70
Train Loss: 0.1787, Accuracy: 0.9472, Precision: 0.8538, Recall: 0.8545, F1: 0.8535
Validation Loss: 0.5766, Accuracy: 0.8551, Precision: 0.6323, Recall: 0.6616, F1: 0.6451
Testing Loss: 0.6091, Accuracy: 0.8511, Precision: 0.6156, Recall: 0.6424, F1: 0.6270
LM Predictions:  [3, 2, 2, 5, 3, 3, 5, 3, 5, 1, 3, 5, 1, 4, 5, 0, 3, 3, 4, 5, 2, 2, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8890, Accuracy: 0.3929, Precision: 0.5992, Recall: 0.3819, F1: 0.3891
Epoch 15/70
Train Loss: 0.2005, Accuracy: 0.9398, Precision: 0.8471, Recall: 0.8273, F1: 0.8331
Validation Loss: 0.6508, Accuracy: 0.8324, Precision: 0.6277, Recall: 0.6224, F1: 0.6239
Testing Loss: 0.6548, Accuracy: 0.8378, Precision: 0.6335, Recall: 0.6462, F1: 0.6348
LM Predictions:  [3, 2, 2, 5, 3, 3, 0, 3, 0, 1, 3, 5, 3, 4, 0, 0, 3, 3, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6972, Accuracy: 0.5357, Precision: 0.6333, Recall: 0.4028, F1: 0.4832
Epoch 16/70
Train Loss: 0.1550, Accuracy: 0.9524, Precision: 0.8787, Recall: 0.8669, F1: 0.8710
Validation Loss: 0.6410, Accuracy: 0.8352, Precision: 0.7078, Recall: 0.5768, F1: 0.6168
Testing Loss: 0.6705, Accuracy: 0.8245, Precision: 0.6740, Recall: 0.6458, F1: 0.6502
LM Predictions:  [3, 2, 2, 5, 2, 3, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 3, 4, 0, 2, 4, 2, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3876, Accuracy: 0.6429, Precision: 0.6667, Recall: 0.5417, F1: 0.5703
Epoch 17/70
Train Loss: 0.1624, Accuracy: 0.9535, Precision: 0.8906, Recall: 0.8678, F1: 0.8774
Validation Loss: 0.7074, Accuracy: 0.8097, Precision: 0.6143, Recall: 0.6600, F1: 0.6211
Testing Loss: 0.6647, Accuracy: 0.8404, Precision: 0.6505, Recall: 0.6785, F1: 0.6511
LM Predictions:  [3, 1, 2, 5, 2, 3, 0, 3, 0, 1, 3, 5, 1, 4, 0, 0, 3, 3, 4, 0, 2, 4, 2, 3, 2, 3, 0, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4136, Accuracy: 0.5357, Precision: 0.6611, Recall: 0.4444, F1: 0.5089
Epoch 18/70
Train Loss: 0.1357, Accuracy: 0.9657, Precision: 0.9156, Recall: 0.9095, F1: 0.9117
Validation Loss: 0.7120, Accuracy: 0.8239, Precision: 0.6284, Recall: 0.5941, F1: 0.6067
Testing Loss: 0.7401, Accuracy: 0.8617, Precision: 0.6867, Recall: 0.6714, F1: 0.6745
LM Predictions:  [3, 2, 2, 5, 2, 3, 0, 3, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1552, Accuracy: 0.6429, Precision: 0.6607, Recall: 0.5347, F1: 0.5703
Epoch 19/70
Train Loss: 0.1334, Accuracy: 0.9622, Precision: 0.8959, Recall: 0.8868, F1: 0.8902
Validation Loss: 0.7272, Accuracy: 0.8125, Precision: 0.5825, Recall: 0.5953, F1: 0.5864
Testing Loss: 0.7196, Accuracy: 0.8404, Precision: 0.6179, Recall: 0.6400, F1: 0.6212
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 3, 0, 1, 2, 5, 1, 4, 3, 0, 3, 3, 4, 3, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3551, Accuracy: 0.5714, Precision: 0.7222, Recall: 0.4931, F1: 0.5671
Epoch 20/70
Train Loss: 0.1137, Accuracy: 0.9696, Precision: 0.9170, Recall: 0.9134, F1: 0.9148
Validation Loss: 0.7212, Accuracy: 0.8239, Precision: 0.6363, Recall: 0.6060, F1: 0.6110
Testing Loss: 0.7722, Accuracy: 0.8271, Precision: 0.6028, Recall: 0.6170, F1: 0.5966
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 3, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0990, Accuracy: 0.6786, Precision: 0.7024, Recall: 0.5556, F1: 0.6084
Epoch 21/70
Train Loss: 0.1120, Accuracy: 0.9696, Precision: 0.9191, Recall: 0.9220, F1: 0.9197
Validation Loss: 0.6227, Accuracy: 0.8523, Precision: 0.6565, Recall: 0.6169, F1: 0.6340
Testing Loss: 0.7087, Accuracy: 0.8431, Precision: 0.6252, Recall: 0.6309, F1: 0.6222
LM Predictions:  [3, 2, 2, 5, 2, 3, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1094, Accuracy: 0.6786, Precision: 0.6667, Recall: 0.5625, F1: 0.5850
Epoch 22/70
Train Loss: 0.1112, Accuracy: 0.9685, Precision: 0.9232, Recall: 0.9216, F1: 0.9221
Validation Loss: 0.6224, Accuracy: 0.8409, Precision: 0.6376, Recall: 0.6557, F1: 0.6442
Testing Loss: 0.7611, Accuracy: 0.8245, Precision: 0.5923, Recall: 0.6182, F1: 0.5975
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1625, Accuracy: 0.7143, Precision: 0.8095, Recall: 0.5833, F1: 0.6618
Epoch 23/70
Train Loss: 0.0864, Accuracy: 0.9783, Precision: 0.9302, Recall: 0.9465, F1: 0.9380
Validation Loss: 0.7943, Accuracy: 0.8352, Precision: 0.6378, Recall: 0.6195, F1: 0.6266
Testing Loss: 0.8368, Accuracy: 0.8351, Precision: 0.5997, Recall: 0.6146, F1: 0.6063
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 2, 3, 3, 2, 5, 1, 4, 3, 0, 3, 3, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2488, Accuracy: 0.6071, Precision: 0.8095, Recall: 0.5208, F1: 0.6098
Epoch 24/70
Train Loss: 0.0909, Accuracy: 0.9755, Precision: 0.9347, Recall: 0.9207, F1: 0.9268
Validation Loss: 0.7645, Accuracy: 0.8409, Precision: 0.6490, Recall: 0.6339, F1: 0.6383
Testing Loss: 0.8560, Accuracy: 0.8404, Precision: 0.6309, Recall: 0.6346, F1: 0.6253
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 5, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0560, Accuracy: 0.7143, Precision: 0.7679, Recall: 0.5833, F1: 0.6439
Epoch 25/70
Train Loss: 0.0846, Accuracy: 0.9766, Precision: 0.9363, Recall: 0.9443, F1: 0.9400
Validation Loss: 0.8361, Accuracy: 0.8267, Precision: 0.6127, Recall: 0.6291, F1: 0.6189
Testing Loss: 0.7886, Accuracy: 0.8564, Precision: 0.6267, Recall: 0.6630, F1: 0.6408
LM Predictions:  [3, 4, 2, 5, 1, 3, 0, 3, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2691, Accuracy: 0.6786, Precision: 0.7500, Recall: 0.5556, F1: 0.6317
Epoch 26/70
Train Loss: 0.0771, Accuracy: 0.9804, Precision: 0.9512, Recall: 0.9472, F1: 0.9483
Validation Loss: 0.7988, Accuracy: 0.8551, Precision: 0.6771, Recall: 0.6416, F1: 0.6568
Testing Loss: 0.8438, Accuracy: 0.8404, Precision: 0.6303, Recall: 0.6297, F1: 0.6255
LM Predictions:  [3, 4, 2, 5, 2, 3, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3109, Accuracy: 0.7143, Precision: 0.7917, Recall: 0.5833, F1: 0.6508
Epoch 27/70
Train Loss: 0.0961, Accuracy: 0.9773, Precision: 0.9455, Recall: 0.9348, F1: 0.9391
Validation Loss: 0.7312, Accuracy: 0.8693, Precision: 0.6886, Recall: 0.6743, F1: 0.6800
Testing Loss: 0.8346, Accuracy: 0.8590, Precision: 0.6417, Recall: 0.6642, F1: 0.6485
LM Predictions:  [3, 4, 2, 5, 1, 3, 0, 2, 0, 3, 3, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 3, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4767, Accuracy: 0.6071, Precision: 0.7500, Recall: 0.5069, F1: 0.5933
Epoch 28/70
Train Loss: 0.0826, Accuracy: 0.9801, Precision: 0.9423, Recall: 0.9545, F1: 0.9481
Validation Loss: 0.7808, Accuracy: 0.8409, Precision: 0.6662, Recall: 0.6382, F1: 0.6500
Testing Loss: 0.8634, Accuracy: 0.8484, Precision: 0.6348, Recall: 0.6499, F1: 0.6369
LM Predictions:  [3, 4, 2, 5, 1, 3, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 3, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2263, Accuracy: 0.6786, Precision: 0.7262, Recall: 0.5625, F1: 0.6138
Epoch 29/70
Train Loss: 0.0997, Accuracy: 0.9706, Precision: 0.9461, Recall: 0.9352, F1: 0.9402
Validation Loss: 0.8106, Accuracy: 0.8466, Precision: 0.6447, Recall: 0.6554, F1: 0.6484
Testing Loss: 0.9173, Accuracy: 0.8351, Precision: 0.6083, Recall: 0.6523, F1: 0.6185
LM Predictions:  [3, 4, 2, 5, 1, 3, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6034, Accuracy: 0.6786, Precision: 0.7222, Recall: 0.5556, F1: 0.6150
Epoch 30/70
Train Loss: 0.0581, Accuracy: 0.9832, Precision: 0.9510, Recall: 0.9484, F1: 0.9494
Validation Loss: 0.8290, Accuracy: 0.8466, Precision: 0.6698, Recall: 0.6770, F1: 0.6577
Testing Loss: 0.9696, Accuracy: 0.8245, Precision: 0.6313, Recall: 0.6329, F1: 0.6171
LM Predictions:  [3, 4, 2, 5, 4, 3, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8970, Accuracy: 0.7500, Precision: 0.7262, Recall: 0.6042, F1: 0.6511
Epoch 31/70
Train Loss: 0.0845, Accuracy: 0.9783, Precision: 0.9435, Recall: 0.9403, F1: 0.9414
Validation Loss: 0.9002, Accuracy: 0.8409, Precision: 0.6423, Recall: 0.6285, F1: 0.6349
Testing Loss: 0.9204, Accuracy: 0.8351, Precision: 0.6277, Recall: 0.6343, F1: 0.6221
LM Predictions:  [3, 4, 2, 5, 1, 3, 0, 2, 5, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2869, Accuracy: 0.6786, Precision: 0.6806, Recall: 0.5625, F1: 0.5976
Epoch 32/70
Train Loss: 0.0699, Accuracy: 0.9790, Precision: 0.9442, Recall: 0.9435, F1: 0.9436
Validation Loss: 0.8839, Accuracy: 0.8324, Precision: 0.6502, Recall: 0.6012, F1: 0.6218
Testing Loss: 0.9081, Accuracy: 0.8298, Precision: 0.6294, Recall: 0.6190, F1: 0.6171
LM Predictions:  [3, 4, 2, 5, 4, 3, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8767, Accuracy: 0.7857, Precision: 0.8095, Recall: 0.6250, F1: 0.6916
Epoch 33/70
Train Loss: 0.0642, Accuracy: 0.9829, Precision: 0.9562, Recall: 0.9538, F1: 0.9548
Validation Loss: 0.8739, Accuracy: 0.8381, Precision: 0.6120, Recall: 0.6036, F1: 0.6074
Testing Loss: 0.9106, Accuracy: 0.8457, Precision: 0.6279, Recall: 0.6383, F1: 0.6286
LM Predictions:  [1, 4, 2, 5, 1, 1, 0, 3, 5, 1, 2, 5, 1, 4, 0, 0, 1, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.1997, Accuracy: 0.6786, Precision: 0.6556, Recall: 0.6181, F1: 0.5853
Epoch 34/70
Train Loss: 0.0637, Accuracy: 0.9839, Precision: 0.9502, Recall: 0.9555, F1: 0.9524
Validation Loss: 0.7683, Accuracy: 0.8182, Precision: 0.5938, Recall: 0.6204, F1: 0.6028
Testing Loss: 0.8100, Accuracy: 0.8457, Precision: 0.6057, Recall: 0.6318, F1: 0.6170
LM Predictions:  [3, 4, 2, 5, 4, 3, 0, 3, 0, 3, 3, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 3, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0984, Accuracy: 0.6429, Precision: 0.7917, Recall: 0.5278, F1: 0.6234
Epoch 35/70
Train Loss: 0.0528, Accuracy: 0.9850, Precision: 0.9476, Recall: 0.9544, F1: 0.9509
Validation Loss: 0.8809, Accuracy: 0.8409, Precision: 0.6693, Recall: 0.6049, F1: 0.6261
Testing Loss: 0.9540, Accuracy: 0.8537, Precision: 0.6487, Recall: 0.6441, F1: 0.6423
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9039, Accuracy: 0.7857, Precision: 0.7540, Recall: 0.6875, F1: 0.7011
Epoch 36/70
Train Loss: 0.0503, Accuracy: 0.9895, Precision: 0.9823, Recall: 0.9673, F1: 0.9744
Validation Loss: 0.9960, Accuracy: 0.8210, Precision: 0.6017, Recall: 0.6237, F1: 0.6102
Testing Loss: 0.9803, Accuracy: 0.8351, Precision: 0.6056, Recall: 0.6429, F1: 0.6163
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7633, Accuracy: 0.7857, Precision: 0.7917, Recall: 0.6875, F1: 0.7294
Epoch 37/70
Train Loss: 0.0545, Accuracy: 0.9881, Precision: 0.9699, Recall: 0.9748, F1: 0.9722
Validation Loss: 0.8777, Accuracy: 0.8381, Precision: 0.6821, Recall: 0.6323, F1: 0.6470
Testing Loss: 0.8892, Accuracy: 0.8537, Precision: 0.6515, Recall: 0.6502, F1: 0.6460
LM Predictions:  [3, 4, 2, 5, 2, 5, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8495, Accuracy: 0.7143, Precision: 0.6417, Recall: 0.5833, F1: 0.5913
Epoch 38/70
Train Loss: 0.0677, Accuracy: 0.9832, Precision: 0.9734, Recall: 0.9562, F1: 0.9643
Validation Loss: 0.9885, Accuracy: 0.8182, Precision: 0.6212, Recall: 0.6191, F1: 0.6165
Testing Loss: 0.8560, Accuracy: 0.8457, Precision: 0.6235, Recall: 0.6613, F1: 0.6356
LM Predictions:  [3, 4, 2, 5, 1, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9140, Accuracy: 0.7500, Precision: 0.7361, Recall: 0.6667, F1: 0.6790
Epoch 39/70
Train Loss: 0.0600, Accuracy: 0.9843, Precision: 0.9684, Recall: 0.9697, F1: 0.9689
Validation Loss: 0.8581, Accuracy: 0.8324, Precision: 0.6303, Recall: 0.6303, F1: 0.6295
Testing Loss: 0.8632, Accuracy: 0.8404, Precision: 0.6514, Recall: 0.6612, F1: 0.6536
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7739, Accuracy: 0.7857, Precision: 0.8333, Recall: 0.6875, F1: 0.7473
Epoch 40/70
Train Loss: 0.0412, Accuracy: 0.9902, Precision: 0.9722, Recall: 0.9733, F1: 0.9726
Validation Loss: 0.9694, Accuracy: 0.8466, Precision: 0.6780, Recall: 0.6054, F1: 0.6312
Testing Loss: 1.0373, Accuracy: 0.8404, Precision: 0.6489, Recall: 0.6169, F1: 0.6263
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9204, Accuracy: 0.7857, Precision: 0.7111, Recall: 0.6875, F1: 0.6822
Epoch 41/70
Train Loss: 0.0830, Accuracy: 0.9825, Precision: 0.9697, Recall: 0.9613, F1: 0.9653
Validation Loss: 0.8633, Accuracy: 0.8523, Precision: 0.7014, Recall: 0.6302, F1: 0.6537
Testing Loss: 0.9000, Accuracy: 0.8511, Precision: 0.6516, Recall: 0.6338, F1: 0.6387
LM Predictions:  [3, 4, 2, 5, 2, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7597, Accuracy: 0.7500, Precision: 0.7429, Recall: 0.6667, F1: 0.6856
Epoch 42/70
Train Loss: 0.0626, Accuracy: 0.9843, Precision: 0.9630, Recall: 0.9581, F1: 0.9604
Validation Loss: 0.8871, Accuracy: 0.8381, Precision: 0.6305, Recall: 0.6419, F1: 0.6353
Testing Loss: 0.8058, Accuracy: 0.8537, Precision: 0.6382, Recall: 0.6543, F1: 0.6455
LM Predictions:  [3, 4, 2, 5, 2, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9105, Accuracy: 0.7500, Precision: 0.8095, Recall: 0.6667, F1: 0.7173
Epoch 43/70
Train Loss: 0.0345, Accuracy: 0.9934, Precision: 0.9814, Recall: 0.9849, F1: 0.9830
Validation Loss: 0.9094, Accuracy: 0.8551, Precision: 0.6660, Recall: 0.6433, F1: 0.6535
Testing Loss: 0.8912, Accuracy: 0.8564, Precision: 0.6816, Recall: 0.6702, F1: 0.6737
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8151, Accuracy: 0.7500, Precision: 0.7917, Recall: 0.6667, F1: 0.7123
Epoch 44/70
Train Loss: 0.0558, Accuracy: 0.9888, Precision: 0.9657, Recall: 0.9759, F1: 0.9707
Validation Loss: 0.9487, Accuracy: 0.8409, Precision: 0.6362, Recall: 0.6324, F1: 0.6340
Testing Loss: 0.9116, Accuracy: 0.8404, Precision: 0.6712, Recall: 0.6632, F1: 0.6651
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9396, Accuracy: 0.7500, Precision: 0.7583, Recall: 0.6667, F1: 0.7038
Epoch 45/70
Train Loss: 0.0485, Accuracy: 0.9888, Precision: 0.9711, Recall: 0.9718, F1: 0.9713
Validation Loss: 1.0441, Accuracy: 0.8324, Precision: 0.6416, Recall: 0.5899, F1: 0.6094
Testing Loss: 0.9649, Accuracy: 0.8324, Precision: 0.6507, Recall: 0.6289, F1: 0.6351
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9285, Accuracy: 0.7857, Precision: 0.7639, Recall: 0.6875, F1: 0.7202
Epoch 46/70
Train Loss: 0.0542, Accuracy: 0.9867, Precision: 0.9651, Recall: 0.9652, F1: 0.9648
Validation Loss: 0.9185, Accuracy: 0.8381, Precision: 0.6518, Recall: 0.6583, F1: 0.6536
Testing Loss: 0.8692, Accuracy: 0.8378, Precision: 0.6082, Recall: 0.6429, F1: 0.6222
LM Predictions:  [3, 4, 2, 3, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 1, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9764, Accuracy: 0.7500, Precision: 0.7778, Recall: 0.6458, F1: 0.6822
Epoch 47/70
Train Loss: 0.0359, Accuracy: 0.9899, Precision: 0.9777, Recall: 0.9761, F1: 0.9767
Validation Loss: 1.0316, Accuracy: 0.8523, Precision: 0.6543, Recall: 0.6335, F1: 0.6432
Testing Loss: 0.8837, Accuracy: 0.8378, Precision: 0.6602, Recall: 0.6620, F1: 0.6595
LM Predictions:  [3, 4, 2, 5, 1, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 1, 0, 4, 0, 2, 2, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2561, Accuracy: 0.7143, Precision: 0.6845, Recall: 0.6458, F1: 0.6340
Epoch 48/70
Train Loss: 0.0253, Accuracy: 0.9944, Precision: 0.9849, Recall: 0.9870, F1: 0.9859
Validation Loss: 1.0662, Accuracy: 0.8352, Precision: 0.6254, Recall: 0.6270, F1: 0.6219
Testing Loss: 0.8588, Accuracy: 0.8617, Precision: 0.6691, Recall: 0.6552, F1: 0.6593
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7253, Accuracy: 0.7500, Precision: 0.7667, Recall: 0.6667, F1: 0.6984
Epoch 49/70
Train Loss: 0.0272, Accuracy: 0.9930, Precision: 0.9868, Recall: 0.9830, F1: 0.9848
Validation Loss: 1.0400, Accuracy: 0.8295, Precision: 0.6466, Recall: 0.5859, F1: 0.6071
Testing Loss: 1.0399, Accuracy: 0.8298, Precision: 0.8139, Recall: 0.6432, F1: 0.6753
LM Predictions:  [5, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5814, Accuracy: 0.7857, Precision: 0.6944, Recall: 0.6875, F1: 0.6711
Epoch 50/70
Train Loss: 0.0419, Accuracy: 0.9913, Precision: 0.9839, Recall: 0.9790, F1: 0.9813
Validation Loss: 1.0151, Accuracy: 0.8239, Precision: 0.6717, Recall: 0.6162, F1: 0.6363
Testing Loss: 0.8097, Accuracy: 0.8537, Precision: 0.6830, Recall: 0.6714, F1: 0.6757
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8256, Accuracy: 0.7500, Precision: 0.7667, Recall: 0.6667, F1: 0.6984
Epoch 51/70
Train Loss: 0.0449, Accuracy: 0.9899, Precision: 0.9761, Recall: 0.9685, F1: 0.9719
Validation Loss: 0.9550, Accuracy: 0.8352, Precision: 0.6523, Recall: 0.6435, F1: 0.6427
Testing Loss: 0.9486, Accuracy: 0.8564, Precision: 0.7493, Recall: 0.6736, F1: 0.6932
LM Predictions:  [3, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.8145, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6875, F1: 0.6961
Epoch 52/70
Train Loss: 0.0202, Accuracy: 0.9951, Precision: 0.9815, Recall: 0.9815, F1: 0.9815
Validation Loss: 0.9836, Accuracy: 0.8580, Precision: 0.6945, Recall: 0.6332, F1: 0.6534
Testing Loss: 0.9219, Accuracy: 0.8670, Precision: 0.8521, Recall: 0.6706, F1: 0.7056
LM Predictions:  [5, 4, 2, 5, 5, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7110, Accuracy: 0.7857, Precision: 0.7111, Recall: 0.6875, F1: 0.6822
Epoch 53/70
Train Loss: 0.0366, Accuracy: 0.9909, Precision: 0.9800, Recall: 0.9782, F1: 0.9790
Validation Loss: 1.0382, Accuracy: 0.8267, Precision: 0.6595, Recall: 0.6325, F1: 0.6383
Testing Loss: 0.9863, Accuracy: 0.8431, Precision: 0.6387, Recall: 0.6449, F1: 0.6374
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5889, Accuracy: 0.8571, Precision: 0.7262, Recall: 0.7292, F1: 0.7062
Epoch 54/70
Train Loss: 0.0268, Accuracy: 0.9916, Precision: 0.9741, Recall: 0.9749, F1: 0.9744
Validation Loss: 1.1244, Accuracy: 0.8352, Precision: 0.6487, Recall: 0.6244, F1: 0.6303
Testing Loss: 1.0799, Accuracy: 0.8457, Precision: 0.6488, Recall: 0.6494, F1: 0.6421
LM Predictions:  [1, 4, 2, 5, 2, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6682, Accuracy: 0.8214, Precision: 0.7083, Recall: 0.7083, F1: 0.6806
Epoch 55/70
Train Loss: 0.0522, Accuracy: 0.9895, Precision: 0.9777, Recall: 0.9760, F1: 0.9768
Validation Loss: 1.0532, Accuracy: 0.8352, Precision: 0.6662, Recall: 0.6922, F1: 0.6693
Testing Loss: 0.9508, Accuracy: 0.8537, Precision: 0.6833, Recall: 0.6945, F1: 0.6854
LM Predictions:  [1, 4, 2, 5, 2, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7542, Accuracy: 0.7857, Precision: 0.7262, Recall: 0.6875, F1: 0.6789
Epoch 56/70
Train Loss: 0.0513, Accuracy: 0.9888, Precision: 0.9836, Recall: 0.9752, F1: 0.9793
Validation Loss: 0.9858, Accuracy: 0.8324, Precision: 0.6335, Recall: 0.6087, F1: 0.6199
Testing Loss: 0.8412, Accuracy: 0.8457, Precision: 0.7043, Recall: 0.6649, F1: 0.6781
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7669, Accuracy: 0.8214, Precision: 0.7540, Recall: 0.7083, F1: 0.7190
Epoch 57/70
Train Loss: 0.0356, Accuracy: 0.9913, Precision: 0.9781, Recall: 0.9849, F1: 0.9814
Validation Loss: 1.0564, Accuracy: 0.8267, Precision: 0.6126, Recall: 0.5973, F1: 0.6028
Testing Loss: 1.0104, Accuracy: 0.8484, Precision: 0.6434, Recall: 0.6532, F1: 0.6451
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9798, Accuracy: 0.8214, Precision: 0.6873, Recall: 0.7083, F1: 0.6873
Epoch 58/70
Train Loss: 0.0241, Accuracy: 0.9948, Precision: 0.9879, Recall: 0.9793, F1: 0.9834
Validation Loss: 1.1020, Accuracy: 0.8409, Precision: 0.6317, Recall: 0.6337, F1: 0.6300
Testing Loss: 0.9439, Accuracy: 0.8564, Precision: 0.6468, Recall: 0.6613, F1: 0.6497
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7413, Accuracy: 0.8571, Precision: 0.7361, Recall: 0.7292, F1: 0.7234
Epoch 59/70
Train Loss: 0.0362, Accuracy: 0.9930, Precision: 0.9782, Recall: 0.9733, F1: 0.9756
Validation Loss: 0.9857, Accuracy: 0.8409, Precision: 0.6645, Recall: 0.7022, F1: 0.6740
Testing Loss: 0.8445, Accuracy: 0.8378, Precision: 0.6482, Recall: 0.6665, F1: 0.6551
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 4, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4031, Accuracy: 0.8571, Precision: 0.7708, Recall: 0.7292, F1: 0.7470
Epoch 60/70
Train Loss: 0.0236, Accuracy: 0.9937, Precision: 0.9908, Recall: 0.9866, F1: 0.9886
Validation Loss: 1.0538, Accuracy: 0.8409, Precision: 0.6664, Recall: 0.6328, F1: 0.6440
Testing Loss: 0.9686, Accuracy: 0.8511, Precision: 0.7058, Recall: 0.6694, F1: 0.6794
LM Predictions:  [5, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6677, Accuracy: 0.8571, Precision: 0.7111, Recall: 0.7292, F1: 0.7095
Epoch 61/70
Train Loss: 0.0216, Accuracy: 0.9955, Precision: 0.9902, Recall: 0.9906, F1: 0.9904
Validation Loss: 1.0801, Accuracy: 0.8523, Precision: 0.6867, Recall: 0.6887, F1: 0.6828
Testing Loss: 1.1602, Accuracy: 0.8324, Precision: 0.6550, Recall: 0.6427, F1: 0.6343
LM Predictions:  [5, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 1, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6944, Accuracy: 0.8214, Precision: 0.7083, Recall: 0.7083, F1: 0.6885
Epoch 62/70
Train Loss: 0.0302, Accuracy: 0.9916, Precision: 0.9772, Recall: 0.9793, F1: 0.9782
Validation Loss: 0.9854, Accuracy: 0.8352, Precision: 0.6530, Recall: 0.6013, F1: 0.6198
Testing Loss: 1.0240, Accuracy: 0.8457, Precision: 0.6380, Recall: 0.6383, F1: 0.6348
LM Predictions:  [5, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5801, Accuracy: 0.8571, Precision: 0.7619, Recall: 0.7500, F1: 0.7403
Epoch 63/70
Train Loss: 0.0249, Accuracy: 0.9937, Precision: 0.9908, Recall: 0.9827, F1: 0.9866
Validation Loss: 1.0525, Accuracy: 0.8438, Precision: 0.6448, Recall: 0.6528, F1: 0.6465
Testing Loss: 0.9564, Accuracy: 0.8537, Precision: 0.6477, Recall: 0.6531, F1: 0.6491
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 1, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2853, Accuracy: 0.9286, Precision: 0.7778, Recall: 0.7708, F1: 0.7651
Epoch 64/70
Train Loss: 0.0141, Accuracy: 0.9979, Precision: 0.9951, Recall: 0.9974, F1: 0.9962
Validation Loss: 1.0822, Accuracy: 0.8523, Precision: 0.6839, Recall: 0.6133, F1: 0.6375
Testing Loss: 1.0826, Accuracy: 0.8511, Precision: 0.6505, Recall: 0.6380, F1: 0.6391
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5333, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7708, F1: 0.7651
Epoch 65/70
Train Loss: 0.0298, Accuracy: 0.9944, Precision: 0.9872, Recall: 0.9877, F1: 0.9874
Validation Loss: 1.1694, Accuracy: 0.8381, Precision: 0.6240, Recall: 0.6101, F1: 0.6162
Testing Loss: 1.0978, Accuracy: 0.8564, Precision: 0.6662, Recall: 0.6646, F1: 0.6650
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2324, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7917, F1: 0.8111
Epoch 66/70
Train Loss: 0.0547, Accuracy: 0.9871, Precision: 0.9825, Recall: 0.9711, F1: 0.9764
Validation Loss: 0.9574, Accuracy: 0.8210, Precision: 0.6650, Recall: 0.5735, F1: 0.5973
Testing Loss: 0.9701, Accuracy: 0.8191, Precision: 0.6641, Recall: 0.6255, F1: 0.6271
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2106, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 67/70
Train Loss: 0.0557, Accuracy: 0.9881, Precision: 0.9708, Recall: 0.9566, F1: 0.9633
Validation Loss: 0.9568, Accuracy: 0.8466, Precision: 0.6508, Recall: 0.6219, F1: 0.6332
Testing Loss: 1.0578, Accuracy: 0.8324, Precision: 0.6147, Recall: 0.6063, F1: 0.6080
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3064, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7926
Epoch 68/70
Train Loss: 0.0540, Accuracy: 0.9857, Precision: 0.9766, Recall: 0.9652, F1: 0.9707
Validation Loss: 0.9906, Accuracy: 0.8125, Precision: 0.6116, Recall: 0.5873, F1: 0.5956
Testing Loss: 1.0245, Accuracy: 0.8165, Precision: 0.6515, Recall: 0.6074, F1: 0.6174
LM Predictions:  [4, 4, 2, 1, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 2, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3565, Accuracy: 0.8929, Precision: 0.7540, Recall: 0.7500, F1: 0.7411
Epoch 69/70
Train Loss: 0.0233, Accuracy: 0.9955, Precision: 0.9931, Recall: 0.9854, F1: 0.9892
Validation Loss: 1.0590, Accuracy: 0.8239, Precision: 0.6388, Recall: 0.5991, F1: 0.6140
Testing Loss: 0.9957, Accuracy: 0.8511, Precision: 0.7035, Recall: 0.6423, F1: 0.6621
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2496, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7926
Epoch 70/70
Train Loss: 0.0127, Accuracy: 0.9969, Precision: 0.9940, Recall: 0.9955, F1: 0.9947
Validation Loss: 1.1690, Accuracy: 0.8409, Precision: 0.6737, Recall: 0.5987, F1: 0.6250
Testing Loss: 1.1509, Accuracy: 0.8351, Precision: 0.6565, Recall: 0.5924, F1: 0.6133
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.5019, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7708, F1: 0.7651
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1620, Accuracy: 0.5623, Precision: 0.2805, Recall: 0.2675, F1: 0.2486
Validation Loss: 0.7929, Accuracy: 0.7472, Precision: 0.3732, Recall: 0.4294, F1: 0.3884
Testing Loss: 0.7357, Accuracy: 0.7766, Precision: 0.3832, Recall: 0.4503, F1: 0.4034
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1711, Accuracy: 0.1429, Precision: 0.0750, Recall: 0.1833, F1: 0.0829
Epoch 2/70
Train Loss: 0.6511, Accuracy: 0.7915, Precision: 0.4524, Recall: 0.4467, F1: 0.4315
Validation Loss: 0.5659, Accuracy: 0.8068, Precision: 0.4714, Recall: 0.5100, F1: 0.4879
Testing Loss: 0.5598, Accuracy: 0.8324, Precision: 0.5131, Recall: 0.5287, F1: 0.5196
LM Predictions:  [3, 2, 2, 3, 2, 3, 5, 3, 3, 3, 3, 5, 2, 5, 2, 5, 3, 3, 5, 3, 3, 3, 3, 3, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3366, Accuracy: 0.1071, Precision: 0.0750, Recall: 0.0972, F1: 0.0847
Epoch 3/70
Train Loss: 0.4889, Accuracy: 0.8380, Precision: 0.5532, Recall: 0.5624, F1: 0.5543
Validation Loss: 0.5382, Accuracy: 0.8210, Precision: 0.6052, Recall: 0.5398, F1: 0.5325
Testing Loss: 0.4894, Accuracy: 0.8670, Precision: 0.7062, Recall: 0.5768, F1: 0.5872
LM Predictions:  [3, 5, 2, 5, 2, 3, 5, 3, 5, 5, 3, 5, 3, 4, 2, 2, 3, 3, 4, 5, 3, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2963, Accuracy: 0.2500, Precision: 0.2937, Recall: 0.2222, F1: 0.2131
Epoch 4/70
Train Loss: 0.4118, Accuracy: 0.8677, Precision: 0.6084, Recall: 0.6259, F1: 0.6162
Validation Loss: 0.4393, Accuracy: 0.8494, Precision: 0.6119, Recall: 0.5973, F1: 0.5970
Testing Loss: 0.4446, Accuracy: 0.8644, Precision: 0.6258, Recall: 0.6026, F1: 0.6044
LM Predictions:  [3, 1, 2, 1, 5, 3, 5, 3, 5, 1, 3, 5, 3, 4, 5, 5, 3, 3, 4, 5, 3, 3, 3, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0664, Accuracy: 0.2143, Precision: 0.3750, Recall: 0.1806, F1: 0.2056
Epoch 5/70
Train Loss: 0.3285, Accuracy: 0.8950, Precision: 0.6468, Recall: 0.6760, F1: 0.6606
Validation Loss: 0.4462, Accuracy: 0.8750, Precision: 0.6850, Recall: 0.6427, F1: 0.6552
Testing Loss: 0.4503, Accuracy: 0.8590, Precision: 0.6494, Recall: 0.6493, F1: 0.6482
LM Predictions:  [3, 1, 2, 3, 2, 3, 5, 3, 1, 1, 3, 5, 3, 4, 2, 5, 3, 3, 4, 3, 3, 2, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1777, Accuracy: 0.1786, Precision: 0.2778, Recall: 0.1389, F1: 0.1698
Epoch 6/70
Train Loss: 0.2676, Accuracy: 0.9174, Precision: 0.7497, Recall: 0.7405, F1: 0.7388
Validation Loss: 0.4252, Accuracy: 0.8722, Precision: 0.6788, Recall: 0.6584, F1: 0.6673
Testing Loss: 0.4459, Accuracy: 0.8803, Precision: 0.6811, Recall: 0.6744, F1: 0.6767
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 3, 1, 3, 5, 3, 4, 2, 5, 3, 3, 4, 3, 3, 2, 3, 3, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0437, Accuracy: 0.1786, Precision: 0.3056, Recall: 0.1389, F1: 0.1810
Epoch 7/70
Train Loss: 0.2033, Accuracy: 0.9367, Precision: 0.8256, Recall: 0.8011, F1: 0.8023
Validation Loss: 0.4843, Accuracy: 0.8580, Precision: 0.8631, Recall: 0.6746, F1: 0.7130
Testing Loss: 0.4545, Accuracy: 0.8883, Precision: 0.7185, Recall: 0.6823, F1: 0.6912
LM Predictions:  [3, 3, 2, 5, 3, 3, 5, 2, 5, 5, 3, 5, 3, 5, 3, 5, 3, 3, 4, 5, 2, 3, 5, 3, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.7703, Accuracy: 0.3214, Precision: 0.4000, Recall: 0.2986, F1: 0.2656
Epoch 8/70
Train Loss: 0.1883, Accuracy: 0.9412, Precision: 0.8389, Recall: 0.8248, F1: 0.8295
Validation Loss: 0.5003, Accuracy: 0.8636, Precision: 0.6878, Recall: 0.6338, F1: 0.6530
Testing Loss: 0.5014, Accuracy: 0.8777, Precision: 0.6756, Recall: 0.6543, F1: 0.6636
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 0, 3, 3, 5, 3, 4, 3, 0, 3, 3, 4, 0, 2, 3, 4, 2, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1750, Accuracy: 0.3214, Precision: 0.4611, Recall: 0.2292, F1: 0.2980
Epoch 9/70
Train Loss: 0.1324, Accuracy: 0.9636, Precision: 0.8965, Recall: 0.8936, F1: 0.8937
Validation Loss: 0.5344, Accuracy: 0.8750, Precision: 0.7595, Recall: 0.7670, F1: 0.7274
Testing Loss: 0.5696, Accuracy: 0.8537, Precision: 0.6774, Recall: 0.6231, F1: 0.6462
LM Predictions:  [3, 3, 2, 5, 2, 3, 0, 2, 0, 3, 2, 5, 3, 4, 2, 0, 3, 3, 4, 3, 2, 3, 2, 0, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3559, Accuracy: 0.5357, Precision: 0.5833, Recall: 0.4167, F1: 0.4456
Epoch 10/70
Train Loss: 0.1389, Accuracy: 0.9591, Precision: 0.8755, Recall: 0.8792, F1: 0.8771
Validation Loss: 0.5286, Accuracy: 0.8580, Precision: 0.7161, Recall: 0.6951, F1: 0.6929
Testing Loss: 0.4923, Accuracy: 0.8723, Precision: 0.6756, Recall: 0.6618, F1: 0.6659
LM Predictions:  [3, 3, 4, 3, 2, 3, 0, 2, 3, 3, 3, 5, 3, 4, 3, 0, 3, 3, 4, 3, 2, 3, 4, 4, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.6186, Accuracy: 0.3214, Precision: 0.5250, Recall: 0.2500, F1: 0.3291
Epoch 11/70
Train Loss: 0.1033, Accuracy: 0.9682, Precision: 0.9161, Recall: 0.9163, F1: 0.9155
Validation Loss: 0.4547, Accuracy: 0.8864, Precision: 0.7184, Recall: 0.6526, F1: 0.6724
Testing Loss: 0.5848, Accuracy: 0.8803, Precision: 0.6878, Recall: 0.6863, F1: 0.6869
LM Predictions:  [3, 2, 2, 5, 3, 3, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 3, 3, 4, 0, 2, 3, 4, 0, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2236, Accuracy: 0.6429, Precision: 0.7167, Recall: 0.5556, F1: 0.6016
Epoch 12/70
Train Loss: 0.0796, Accuracy: 0.9762, Precision: 0.9295, Recall: 0.9268, F1: 0.9272
Validation Loss: 0.4812, Accuracy: 0.8750, Precision: 0.7058, Recall: 0.7183, F1: 0.7005
Testing Loss: 0.5352, Accuracy: 0.8777, Precision: 0.6923, Recall: 0.6862, F1: 0.6856
LM Predictions:  [3, 3, 2, 5, 5, 3, 0, 2, 0, 5, 2, 5, 0, 4, 0, 0, 3, 3, 4, 0, 2, 3, 2, 0, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9650, Accuracy: 0.6429, Precision: 0.6095, Recall: 0.5000, F1: 0.5148
Epoch 13/70
Train Loss: 0.0652, Accuracy: 0.9825, Precision: 0.9488, Recall: 0.9554, F1: 0.9519
Validation Loss: 0.6056, Accuracy: 0.8494, Precision: 0.7078, Recall: 0.6912, F1: 0.6871
Testing Loss: 0.6728, Accuracy: 0.8644, Precision: 0.6655, Recall: 0.6658, F1: 0.6621
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 2, 0, 5, 2, 5, 1, 4, 0, 1, 3, 3, 4, 0, 2, 3, 4, 0, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.3055, Accuracy: 0.6071, Precision: 0.6944, Recall: 0.5347, F1: 0.5903
Epoch 14/70
Train Loss: 0.0643, Accuracy: 0.9836, Precision: 0.9546, Recall: 0.9585, F1: 0.9562
Validation Loss: 0.5940, Accuracy: 0.8580, Precision: 0.7111, Recall: 0.7699, F1: 0.7100
Testing Loss: 0.6229, Accuracy: 0.8617, Precision: 0.6583, Recall: 0.6847, F1: 0.6687
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 2, 0, 5, 2, 5, 1, 4, 3, 0, 3, 0, 4, 3, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7757, Accuracy: 0.7143, Precision: 0.8333, Recall: 0.6042, F1: 0.6782
Epoch 15/70
Train Loss: 0.0655, Accuracy: 0.9808, Precision: 0.9457, Recall: 0.9471, F1: 0.9463
Validation Loss: 0.6207, Accuracy: 0.8494, Precision: 0.7080, Recall: 0.7516, F1: 0.7249
Testing Loss: 0.6746, Accuracy: 0.8644, Precision: 0.6406, Recall: 0.6926, F1: 0.6584
LM Predictions:  [3, 3, 2, 5, 3, 3, 0, 2, 0, 5, 2, 5, 1, 4, 5, 1, 3, 0, 4, 0, 2, 2, 2, 3, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2739, Accuracy: 0.6429, Precision: 0.6929, Recall: 0.5625, F1: 0.5802
Epoch 16/70
Train Loss: 0.0618, Accuracy: 0.9836, Precision: 0.9489, Recall: 0.9599, F1: 0.9542
Validation Loss: 0.5648, Accuracy: 0.8636, Precision: 0.7007, Recall: 0.6641, F1: 0.6780
Testing Loss: 0.6383, Accuracy: 0.8723, Precision: 0.6683, Recall: 0.6810, F1: 0.6711
LM Predictions:  [3, 3, 2, 5, 2, 2, 0, 2, 0, 5, 2, 5, 1, 4, 2, 1, 3, 5, 4, 0, 2, 4, 2, 0, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.9320, Accuracy: 0.6429, Precision: 0.6500, Recall: 0.5625, F1: 0.5585
Epoch 17/70
Train Loss: 0.0481, Accuracy: 0.9878, Precision: 0.9691, Recall: 0.9711, F1: 0.9700
Validation Loss: 0.6400, Accuracy: 0.8722, Precision: 0.7408, Recall: 0.7711, F1: 0.7439
Testing Loss: 0.5482, Accuracy: 0.8856, Precision: 0.6982, Recall: 0.7019, F1: 0.6981
LM Predictions:  [3, 3, 2, 5, 4, 3, 0, 2, 0, 5, 2, 5, 1, 4, 3, 0, 3, 0, 4, 3, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7998, Accuracy: 0.7500, Precision: 0.8333, Recall: 0.6250, F1: 0.6984
Epoch 18/70
Train Loss: 0.0502, Accuracy: 0.9867, Precision: 0.9697, Recall: 0.9698, F1: 0.9697
Validation Loss: 0.6536, Accuracy: 0.8665, Precision: 0.7450, Recall: 0.7494, F1: 0.7385
Testing Loss: 0.7115, Accuracy: 0.8697, Precision: 0.6753, Recall: 0.6732, F1: 0.6735
LM Predictions:  [3, 4, 2, 5, 4, 0, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 4, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2469, Accuracy: 0.8929, Precision: 0.7940, Recall: 0.7014, F1: 0.7320
Epoch 19/70
Train Loss: 0.0405, Accuracy: 0.9920, Precision: 0.9841, Recall: 0.9835, F1: 0.9838
Validation Loss: 0.7221, Accuracy: 0.8722, Precision: 0.8048, Recall: 0.7497, F1: 0.7590
Testing Loss: 0.7999, Accuracy: 0.8644, Precision: 0.7618, Recall: 0.7000, F1: 0.7150
LM Predictions:  [3, 2, 2, 5, 5, 3, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3396, Accuracy: 0.8214, Precision: 0.7762, Recall: 0.6667, F1: 0.6909
Epoch 20/70
Train Loss: 0.0387, Accuracy: 0.9902, Precision: 0.9803, Recall: 0.9813, F1: 0.9808
Validation Loss: 0.7792, Accuracy: 0.8608, Precision: 0.7093, Recall: 0.6960, F1: 0.6850
Testing Loss: 0.7372, Accuracy: 0.8590, Precision: 0.6569, Recall: 0.6527, F1: 0.6374
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1467, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 21/70
Train Loss: 0.0390, Accuracy: 0.9892, Precision: 0.9677, Recall: 0.9708, F1: 0.9692
Validation Loss: 0.5451, Accuracy: 0.8693, Precision: 0.7966, Recall: 0.6748, F1: 0.7150
Testing Loss: 0.6826, Accuracy: 0.8697, Precision: 0.6827, Recall: 0.6728, F1: 0.6675
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0621, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0274, Accuracy: 0.9934, Precision: 0.9836, Recall: 0.9850, F1: 0.9843
Validation Loss: 0.7510, Accuracy: 0.8466, Precision: 0.7387, Recall: 0.6584, F1: 0.6756
Testing Loss: 0.7778, Accuracy: 0.8564, Precision: 0.7026, Recall: 0.6662, F1: 0.6784
LM Predictions:  [4, 4, 2, 4, 4, 1, 0, 2, 0, 5, 2, 4, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1490, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7292, F1: 0.7484
Epoch 23/70
Train Loss: 0.0566, Accuracy: 0.9836, Precision: 0.9598, Recall: 0.9534, F1: 0.9565
Validation Loss: 0.6029, Accuracy: 0.8778, Precision: 0.8541, Recall: 0.7380, F1: 0.7648
Testing Loss: 0.5709, Accuracy: 0.8750, Precision: 0.6626, Recall: 0.6944, F1: 0.6719
LM Predictions:  [5, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1100, Accuracy: 0.9286, Precision: 0.9333, Recall: 0.9500, F1: 0.9314
Epoch 24/70
Train Loss: 0.0306, Accuracy: 0.9899, Precision: 0.9671, Recall: 0.9647, F1: 0.9659
Validation Loss: 0.7138, Accuracy: 0.8807, Precision: 0.6797, Recall: 0.7107, F1: 0.6928
Testing Loss: 0.7523, Accuracy: 0.8670, Precision: 0.6387, Recall: 0.7003, F1: 0.6626
LM Predictions:  [4, 3, 2, 5, 4, 1, 5, 2, 5, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6864, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7708, F1: 0.7651
Epoch 25/70
Train Loss: 0.0264, Accuracy: 0.9944, Precision: 0.9883, Recall: 0.9791, F1: 0.9834
Validation Loss: 0.6601, Accuracy: 0.8807, Precision: 0.7613, Recall: 0.7858, F1: 0.7518
Testing Loss: 0.7050, Accuracy: 0.8777, Precision: 0.6631, Recall: 0.6941, F1: 0.6697
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2814, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7708, F1: 0.7984
Epoch 26/70
Train Loss: 0.0285, Accuracy: 0.9909, Precision: 0.9793, Recall: 0.9807, F1: 0.9800
Validation Loss: 0.9076, Accuracy: 0.8551, Precision: 0.7566, Recall: 0.6784, F1: 0.7041
Testing Loss: 0.6607, Accuracy: 0.8936, Precision: 0.6997, Recall: 0.7085, F1: 0.7033
LM Predictions:  [3, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 1, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3077, Accuracy: 0.8929, Precision: 0.7778, Recall: 0.7708, F1: 0.7651
Epoch 27/70
Train Loss: 0.0486, Accuracy: 0.9850, Precision: 0.9730, Recall: 0.9716, F1: 0.9722
Validation Loss: 0.9019, Accuracy: 0.8551, Precision: 0.7203, Recall: 0.5891, F1: 0.6250
Testing Loss: 0.8137, Accuracy: 0.8590, Precision: 0.7213, Recall: 0.6223, F1: 0.6544
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0151, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0231, Accuracy: 0.9944, Precision: 0.9942, Recall: 0.9923, F1: 0.9932
Validation Loss: 0.6804, Accuracy: 0.8722, Precision: 0.7388, Recall: 0.7412, F1: 0.7221
Testing Loss: 0.6946, Accuracy: 0.8803, Precision: 0.6800, Recall: 0.6806, F1: 0.6762
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0529, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0176, Accuracy: 0.9965, Precision: 0.9953, Recall: 0.9969, F1: 0.9961
Validation Loss: 0.6194, Accuracy: 0.8807, Precision: 0.7724, Recall: 0.7220, F1: 0.7434
Testing Loss: 0.7440, Accuracy: 0.8856, Precision: 0.6808, Recall: 0.6961, F1: 0.6873
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0097, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0067, Accuracy: 0.9986, Precision: 0.9968, Recall: 0.9968, F1: 0.9968
Validation Loss: 0.6377, Accuracy: 0.8920, Precision: 0.7649, Recall: 0.7321, F1: 0.7435
Testing Loss: 0.7042, Accuracy: 0.8856, Precision: 0.6801, Recall: 0.6933, F1: 0.6852
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0069, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0358, Accuracy: 0.9902, Precision: 0.9771, Recall: 0.9670, F1: 0.9720
Validation Loss: 0.6197, Accuracy: 0.8835, Precision: 0.7118, Recall: 0.6880, F1: 0.6982
Testing Loss: 0.7354, Accuracy: 0.8750, Precision: 0.6802, Recall: 0.6699, F1: 0.6706
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.6183, Accuracy: 0.8571, Precision: 0.7778, Recall: 0.7500, F1: 0.7504
Epoch 32/70
Train Loss: 0.0366, Accuracy: 0.9909, Precision: 0.9709, Recall: 0.9782, F1: 0.9745
Validation Loss: 0.7972, Accuracy: 0.8608, Precision: 0.7015, Recall: 0.6183, F1: 0.6481
Testing Loss: 0.8168, Accuracy: 0.8723, Precision: 0.6704, Recall: 0.6596, F1: 0.6616
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0116, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0302, Accuracy: 0.9937, Precision: 0.9845, Recall: 0.9915, F1: 0.9880
Validation Loss: 0.6558, Accuracy: 0.8750, Precision: 0.7713, Recall: 0.6821, F1: 0.7090
Testing Loss: 0.8399, Accuracy: 0.8511, Precision: 0.6530, Recall: 0.6354, F1: 0.6322
LM Predictions:  [4, 2, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0696, Accuracy: 0.9643, Precision: 0.9714, Recall: 0.9750, F1: 0.9713
Epoch 34/70
Train Loss: 0.0312, Accuracy: 0.9920, Precision: 0.9882, Recall: 0.9861, F1: 0.9871
Validation Loss: 0.6015, Accuracy: 0.8949, Precision: 0.7821, Recall: 0.7349, F1: 0.7534
Testing Loss: 0.7155, Accuracy: 0.8777, Precision: 0.6762, Recall: 0.6941, F1: 0.6774
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0103, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0164, Accuracy: 0.9962, Precision: 0.9907, Recall: 0.9938, F1: 0.9923
Validation Loss: 0.6733, Accuracy: 0.8864, Precision: 0.8836, Recall: 0.7225, F1: 0.7715
Testing Loss: 0.6893, Accuracy: 0.8803, Precision: 0.6949, Recall: 0.6830, F1: 0.6878
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0102, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0193, Accuracy: 0.9972, Precision: 0.9866, Recall: 0.9956, F1: 0.9910
Validation Loss: 0.6886, Accuracy: 0.8949, Precision: 0.7334, Recall: 0.6796, F1: 0.7012
Testing Loss: 0.7809, Accuracy: 0.8830, Precision: 0.6840, Recall: 0.6941, F1: 0.6878
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0058, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0235, Accuracy: 0.9951, Precision: 0.9878, Recall: 0.9876, F1: 0.9877
Validation Loss: 0.5826, Accuracy: 0.9006, Precision: 0.8180, Recall: 0.7593, F1: 0.7834
Testing Loss: 0.8172, Accuracy: 0.8777, Precision: 0.6861, Recall: 0.6966, F1: 0.6897
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0121, Accuracy: 0.9983, Precision: 0.9960, Recall: 0.9960, F1: 0.9960
Validation Loss: 0.6068, Accuracy: 0.8949, Precision: 0.7377, Recall: 0.7549, F1: 0.7394
Testing Loss: 0.8771, Accuracy: 0.8803, Precision: 0.6707, Recall: 0.6954, F1: 0.6785
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 0, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2726, Accuracy: 0.9643, Precision: 0.9778, Recall: 0.9667, F1: 0.9701
Epoch 39/70
Train Loss: 0.0046, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9970, F1: 0.9984
Validation Loss: 0.6513, Accuracy: 0.9062, Precision: 0.7889, Recall: 0.7618, F1: 0.7740
Testing Loss: 0.9245, Accuracy: 0.8723, Precision: 0.6681, Recall: 0.6929, F1: 0.6782
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0034, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0375, Accuracy: 0.9906, Precision: 0.9817, Recall: 0.9777, F1: 0.9796
Validation Loss: 0.7407, Accuracy: 0.8807, Precision: 0.7312, Recall: 0.7537, F1: 0.7342
Testing Loss: 0.9112, Accuracy: 0.8617, Precision: 0.7234, Recall: 0.6788, F1: 0.6943
LM Predictions:  [3, 3, 2, 5, 3, 1, 0, 2, 3, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 0, 4, 2, 0, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.0427, Accuracy: 0.7143, Precision: 0.7762, Recall: 0.6597, F1: 0.6906
Epoch 41/70
Train Loss: 0.0204, Accuracy: 0.9951, Precision: 0.9790, Recall: 0.9899, F1: 0.9843
Validation Loss: 0.7105, Accuracy: 0.8722, Precision: 0.7370, Recall: 0.7126, F1: 0.7104
Testing Loss: 0.8725, Accuracy: 0.8697, Precision: 0.6595, Recall: 0.6798, F1: 0.6626
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0079, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0166, Accuracy: 0.9969, Precision: 0.9825, Recall: 0.9897, F1: 0.9860
Validation Loss: 0.6876, Accuracy: 0.8807, Precision: 0.7413, Recall: 0.7405, F1: 0.7288
Testing Loss: 0.8240, Accuracy: 0.8750, Precision: 0.6847, Recall: 0.6711, F1: 0.6770
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0164, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0064, Accuracy: 0.9983, Precision: 0.9987, Recall: 0.9989, F1: 0.9988
Validation Loss: 0.7897, Accuracy: 0.8608, Precision: 0.7273, Recall: 0.7465, F1: 0.7103
Testing Loss: 0.7628, Accuracy: 0.8803, Precision: 0.7050, Recall: 0.6744, F1: 0.6870
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0065, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0298, Accuracy: 0.9934, Precision: 0.9846, Recall: 0.9832, F1: 0.9839
Validation Loss: 0.6957, Accuracy: 0.8807, Precision: 0.8584, Recall: 0.7625, F1: 0.7817
Testing Loss: 0.8218, Accuracy: 0.8697, Precision: 0.6605, Recall: 0.6794, F1: 0.6693
LM Predictions:  [3, 4, 2, 5, 3, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 3, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1542, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7708, F1: 0.7984
Epoch 45/70
Train Loss: 0.0211, Accuracy: 0.9948, Precision: 0.9892, Recall: 0.9899, F1: 0.9896
Validation Loss: 0.7756, Accuracy: 0.8807, Precision: 0.7598, Recall: 0.7611, F1: 0.7553
Testing Loss: 0.8471, Accuracy: 0.8830, Precision: 0.6696, Recall: 0.7020, F1: 0.6799
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0054, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0097, Accuracy: 0.9979, Precision: 0.9910, Recall: 0.9937, F1: 0.9923
Validation Loss: 0.8935, Accuracy: 0.8807, Precision: 0.7702, Recall: 0.7382, F1: 0.7451
Testing Loss: 0.9423, Accuracy: 0.8803, Precision: 0.6835, Recall: 0.6798, F1: 0.6767
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 5, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0479, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9667, F1: 0.9596
Epoch 47/70
Train Loss: 0.0474, Accuracy: 0.9885, Precision: 0.9812, Recall: 0.9812, F1: 0.9812
Validation Loss: 0.6329, Accuracy: 0.8608, Precision: 0.7267, Recall: 0.7264, F1: 0.7263
Testing Loss: 0.7045, Accuracy: 0.8750, Precision: 0.6796, Recall: 0.7003, F1: 0.6866
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0162, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0266, Accuracy: 0.9923, Precision: 0.9909, Recall: 0.9880, F1: 0.9895
Validation Loss: 0.6251, Accuracy: 0.8835, Precision: 0.8701, Recall: 0.7333, F1: 0.7725
Testing Loss: 0.8839, Accuracy: 0.8723, Precision: 0.6813, Recall: 0.6843, F1: 0.6827
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0023, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0165, Accuracy: 0.9958, Precision: 0.9960, Recall: 0.9958, F1: 0.9959
Validation Loss: 0.8479, Accuracy: 0.8807, Precision: 0.7418, Recall: 0.6776, F1: 0.7030
Testing Loss: 0.9959, Accuracy: 0.8670, Precision: 0.6990, Recall: 0.6498, F1: 0.6695
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0575, Accuracy: 0.9860, Precision: 0.9761, Recall: 0.9735, F1: 0.9748
Validation Loss: 0.6935, Accuracy: 0.8778, Precision: 0.7293, Recall: 0.7512, F1: 0.7365
Testing Loss: 0.8368, Accuracy: 0.8697, Precision: 0.6643, Recall: 0.6933, F1: 0.6767
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0182, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0280, Accuracy: 0.9941, Precision: 0.9937, Recall: 0.9928, F1: 0.9932
Validation Loss: 0.6699, Accuracy: 0.8892, Precision: 0.7526, Recall: 0.7556, F1: 0.7453
Testing Loss: 0.8722, Accuracy: 0.8670, Precision: 0.6751, Recall: 0.6974, F1: 0.6826
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0127, Accuracy: 0.9976, Precision: 0.9935, Recall: 0.9949, F1: 0.9942
Validation Loss: 0.7016, Accuracy: 0.8750, Precision: 0.7667, Recall: 0.7010, F1: 0.7216
Testing Loss: 0.8645, Accuracy: 0.8803, Precision: 0.7166, Recall: 0.6818, F1: 0.6966
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0081, Accuracy: 0.9979, Precision: 0.9985, Recall: 0.9987, F1: 0.9986
Validation Loss: 0.8651, Accuracy: 0.8750, Precision: 0.7258, Recall: 0.7049, F1: 0.6963
Testing Loss: 0.8993, Accuracy: 0.8750, Precision: 0.7249, Recall: 0.7075, F1: 0.7096
LM Predictions:  [5, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7583, Accuracy: 0.8929, Precision: 0.9143, Recall: 0.9250, F1: 0.8993
Epoch 54/70
Train Loss: 0.0205, Accuracy: 0.9965, Precision: 0.9938, Recall: 0.9969, F1: 0.9953
Validation Loss: 0.7636, Accuracy: 0.8835, Precision: 0.7392, Recall: 0.7227, F1: 0.7249
Testing Loss: 0.9273, Accuracy: 0.8830, Precision: 0.7232, Recall: 0.6892, F1: 0.7033
LM Predictions:  [4, 3, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2739, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7917, F1: 0.7910
Epoch 55/70
Train Loss: 0.0617, Accuracy: 0.9808, Precision: 0.9387, Recall: 0.9444, F1: 0.9415
Validation Loss: 0.6293, Accuracy: 0.8551, Precision: 0.7276, Recall: 0.7096, F1: 0.7066
Testing Loss: 0.8096, Accuracy: 0.8324, Precision: 0.6174, Recall: 0.6289, F1: 0.6145
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 1, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 1, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.4296, Accuracy: 0.9286, Precision: 0.9000, Recall: 0.9500, F1: 0.9048
Epoch 56/70
Train Loss: 0.0321, Accuracy: 0.9923, Precision: 0.9744, Recall: 0.9800, F1: 0.9772
Validation Loss: 0.6507, Accuracy: 0.8949, Precision: 0.7721, Recall: 0.7408, F1: 0.7445
Testing Loss: 0.9329, Accuracy: 0.8617, Precision: 0.6773, Recall: 0.6794, F1: 0.6774
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0128, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0202, Accuracy: 0.9955, Precision: 0.9924, Recall: 0.9938, F1: 0.9931
Validation Loss: 0.6130, Accuracy: 0.8892, Precision: 0.7447, Recall: 0.7401, F1: 0.7306
Testing Loss: 0.9843, Accuracy: 0.8644, Precision: 0.6570, Recall: 0.6761, F1: 0.6596
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0229, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0077, Accuracy: 0.9986, Precision: 0.9979, Recall: 0.9990, F1: 0.9985
Validation Loss: 0.7102, Accuracy: 0.8892, Precision: 0.7664, Recall: 0.7409, F1: 0.7480
Testing Loss: 0.9643, Accuracy: 0.8644, Precision: 0.6720, Recall: 0.6885, F1: 0.6769
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0034, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0071, Accuracy: 0.9979, Precision: 0.9964, Recall: 0.9948, F1: 0.9955
Validation Loss: 0.8619, Accuracy: 0.8807, Precision: 0.7760, Recall: 0.7635, F1: 0.7443
Testing Loss: 1.0409, Accuracy: 0.8723, Precision: 0.7025, Recall: 0.6764, F1: 0.6824
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0090, Accuracy: 0.9990, Precision: 0.9941, Recall: 0.9982, F1: 0.9961
Validation Loss: 0.8187, Accuracy: 0.8949, Precision: 0.8005, Recall: 0.7457, F1: 0.7689
Testing Loss: 0.9805, Accuracy: 0.8750, Precision: 0.6846, Recall: 0.6962, F1: 0.6894
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0066, Accuracy: 0.9990, Precision: 0.9981, Recall: 0.9983, F1: 0.9982
Validation Loss: 0.8558, Accuracy: 0.8864, Precision: 0.7663, Recall: 0.7824, F1: 0.7541
Testing Loss: 1.0334, Accuracy: 0.8750, Precision: 0.6671, Recall: 0.6884, F1: 0.6714
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0390, Accuracy: 0.9930, Precision: 0.9882, Recall: 0.9854, F1: 0.9868
Validation Loss: 0.7021, Accuracy: 0.8864, Precision: 0.7822, Recall: 0.7327, F1: 0.7508
Testing Loss: 0.9014, Accuracy: 0.8750, Precision: 0.6818, Recall: 0.6933, F1: 0.6863
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0151, Accuracy: 0.9976, Precision: 0.9987, Recall: 0.9987, F1: 0.9987
Validation Loss: 0.7610, Accuracy: 0.8750, Precision: 0.7280, Recall: 0.7161, F1: 0.7196
Testing Loss: 0.9199, Accuracy: 0.8777, Precision: 0.6911, Recall: 0.6925, F1: 0.6910
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0043, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0344, Accuracy: 0.9930, Precision: 0.9896, Recall: 0.9877, F1: 0.9887
Validation Loss: 1.0691, Accuracy: 0.8267, Precision: 0.7573, Recall: 0.7394, F1: 0.7211
Testing Loss: 1.0007, Accuracy: 0.8271, Precision: 0.6840, Recall: 0.6651, F1: 0.6694
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0180, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0263, Accuracy: 0.9955, Precision: 0.9854, Recall: 0.9928, F1: 0.9890
Validation Loss: 0.7427, Accuracy: 0.8835, Precision: 0.7974, Recall: 0.7782, F1: 0.7691
Testing Loss: 0.8888, Accuracy: 0.8803, Precision: 0.7097, Recall: 0.7069, F1: 0.7061
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0252, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0509, Accuracy: 0.9899, Precision: 0.9768, Recall: 0.9858, F1: 0.9812
Validation Loss: 0.7371, Accuracy: 0.8551, Precision: 0.7613, Recall: 0.7318, F1: 0.7305
Testing Loss: 0.6862, Accuracy: 0.8910, Precision: 0.7221, Recall: 0.7118, F1: 0.7154
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 3, 2, 3, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2032, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 67/70
Train Loss: 0.0377, Accuracy: 0.9906, Precision: 0.9785, Recall: 0.9814, F1: 0.9799
Validation Loss: 0.7590, Accuracy: 0.8807, Precision: 0.7713, Recall: 0.8004, F1: 0.7786
Testing Loss: 0.8623, Accuracy: 0.8670, Precision: 0.6577, Recall: 0.6875, F1: 0.6717
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 5, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2865, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 68/70
Train Loss: 0.0280, Accuracy: 0.9937, Precision: 0.9860, Recall: 0.9911, F1: 0.9886
Validation Loss: 0.8667, Accuracy: 0.8438, Precision: 0.7103, Recall: 0.7260, F1: 0.6841
Testing Loss: 1.0390, Accuracy: 0.8351, Precision: 0.6531, Recall: 0.6391, F1: 0.6364
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 4, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1330, Accuracy: 0.9643, Precision: 0.9778, Recall: 0.9500, F1: 0.9597
Epoch 69/70
Train Loss: 0.0358, Accuracy: 0.9930, Precision: 0.9866, Recall: 0.9838, F1: 0.9852
Validation Loss: 0.8300, Accuracy: 0.8750, Precision: 0.7615, Recall: 0.7714, F1: 0.7549
Testing Loss: 0.8124, Accuracy: 0.8777, Precision: 0.6824, Recall: 0.6937, F1: 0.6867
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0093, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0068, Accuracy: 0.9986, Precision: 0.9981, Recall: 0.9990, F1: 0.9985
Validation Loss: 0.7648, Accuracy: 0.8920, Precision: 0.7832, Recall: 0.8134, F1: 0.7916
Testing Loss: 0.8581, Accuracy: 0.8803, Precision: 0.6672, Recall: 0.6991, F1: 0.6797
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.7461, Accuracy: 0.7309, Precision: 0.5205, Recall: 0.4502, F1: 0.4660
Validation Loss: 0.4043, Accuracy: 0.8750, Precision: 0.7000, Recall: 0.6623, F1: 0.6782
Testing Loss: 0.4874, Accuracy: 0.8537, Precision: 0.6497, Recall: 0.6484, F1: 0.6405
LM Predictions:  [3, 5, 2, 3, 3, 3, 5, 3, 5, 1, 3, 1, 3, 3, 2, 5, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.6917, Accuracy: 0.0714, Precision: 0.0833, Recall: 0.0556, F1: 0.0667
Epoch 2/70
Train Loss: 0.3763, Accuracy: 0.8870, Precision: 0.7666, Recall: 0.6934, F1: 0.6916
Validation Loss: 0.3931, Accuracy: 0.8835, Precision: 0.7155, Recall: 0.6264, F1: 0.6385
Testing Loss: 0.5116, Accuracy: 0.8511, Precision: 0.6775, Recall: 0.6136, F1: 0.6223
LM Predictions:  [3, 5, 2, 3, 5, 3, 5, 3, 5, 1, 3, 5, 0, 3, 2, 5, 3, 1, 3, 3, 3, 2, 3, 5, 5, 5, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5328, Accuracy: 0.0714, Precision: 0.0602, Recall: 0.0694, F1: 0.0590
Epoch 3/70
Train Loss: 0.2491, Accuracy: 0.9230, Precision: 0.8217, Recall: 0.7972, F1: 0.8014
Validation Loss: 0.3420, Accuracy: 0.8864, Precision: 0.6998, Recall: 0.7238, F1: 0.7099
Testing Loss: 0.4175, Accuracy: 0.9016, Precision: 0.6733, Recall: 0.7176, F1: 0.6922
LM Predictions:  [3, 3, 2, 3, 3, 3, 5, 3, 1, 3, 3, 5, 3, 4, 3, 5, 3, 3, 3, 3, 3, 4, 3, 3, 2, 3, 2, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0689, Accuracy: 0.2143, Precision: 0.3333, Recall: 0.1597, F1: 0.2126
Epoch 4/70
Train Loss: 0.1804, Accuracy: 0.9423, Precision: 0.8773, Recall: 0.8603, F1: 0.8657
Validation Loss: 0.3901, Accuracy: 0.8807, Precision: 0.7417, Recall: 0.7579, F1: 0.7338
Testing Loss: 0.4641, Accuracy: 0.8590, Precision: 0.6666, Recall: 0.6698, F1: 0.6649
LM Predictions:  [3, 3, 2, 3, 3, 3, 0, 3, 0, 3, 3, 5, 0, 3, 0, 0, 3, 3, 4, 0, 3, 3, 3, 3, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.4427, Accuracy: 0.3929, Precision: 0.5833, Recall: 0.2847, F1: 0.3643
Epoch 5/70
Train Loss: 0.1222, Accuracy: 0.9619, Precision: 0.9060, Recall: 0.9198, F1: 0.9124
Validation Loss: 0.3791, Accuracy: 0.8864, Precision: 0.7408, Recall: 0.7638, F1: 0.7408
Testing Loss: 0.4427, Accuracy: 0.8856, Precision: 0.6765, Recall: 0.6888, F1: 0.6823
LM Predictions:  [3, 3, 2, 3, 3, 3, 0, 3, 0, 3, 3, 5, 1, 4, 1, 0, 3, 3, 4, 0, 3, 4, 3, 3, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.2871, Accuracy: 0.5000, Precision: 0.7500, Recall: 0.4097, F1: 0.5171
Epoch 6/70
Train Loss: 0.0867, Accuracy: 0.9755, Precision: 0.9394, Recall: 0.9418, F1: 0.9402
Validation Loss: 0.4486, Accuracy: 0.8977, Precision: 0.7938, Recall: 0.7734, F1: 0.7782
Testing Loss: 0.4977, Accuracy: 0.8830, Precision: 0.6867, Recall: 0.6807, F1: 0.6818
LM Predictions:  [3, 3, 2, 5, 4, 0, 0, 2, 0, 3, 2, 5, 1, 4, 3, 0, 3, 3, 4, 0, 2, 4, 3, 0, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.7639, Accuracy: 0.6786, Precision: 0.7639, Recall: 0.5556, F1: 0.6349
Epoch 7/70
Train Loss: 0.0792, Accuracy: 0.9766, Precision: 0.9340, Recall: 0.9423, F1: 0.9378
Validation Loss: 0.3758, Accuracy: 0.9062, Precision: 0.8478, Recall: 0.7534, F1: 0.7892
Testing Loss: 0.5142, Accuracy: 0.8856, Precision: 0.7061, Recall: 0.6744, F1: 0.6840
LM Predictions:  [3, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 5, 5, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3983, Accuracy: 0.8571, Precision: 0.7778, Recall: 0.7500, F1: 0.7524
Epoch 8/70
Train Loss: 0.0444, Accuracy: 0.9857, Precision: 0.9703, Recall: 0.9766, F1: 0.9734
Validation Loss: 0.4842, Accuracy: 0.8778, Precision: 0.7618, Recall: 0.7384, F1: 0.7325
Testing Loss: 0.5820, Accuracy: 0.8830, Precision: 0.6756, Recall: 0.6868, F1: 0.6705
LM Predictions:  [3, 3, 2, 5, 4, 1, 0, 3, 0, 5, 2, 5, 1, 4, 1, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.3524, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.7222, F1: 0.7353
Epoch 9/70
Train Loss: 0.0294, Accuracy: 0.9916, Precision: 0.9855, Recall: 0.9883, F1: 0.9868
Validation Loss: 0.4934, Accuracy: 0.8892, Precision: 0.7614, Recall: 0.7431, F1: 0.7378
Testing Loss: 0.6573, Accuracy: 0.8564, Precision: 0.6663, Recall: 0.6486, F1: 0.6506
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 1, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1071, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 10/70
Train Loss: 0.0320, Accuracy: 0.9916, Precision: 0.9803, Recall: 0.9856, F1: 0.9829
Validation Loss: 0.4827, Accuracy: 0.8949, Precision: 0.7624, Recall: 0.7697, F1: 0.7607
Testing Loss: 0.6503, Accuracy: 0.8803, Precision: 0.6684, Recall: 0.6638, F1: 0.6639
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0728, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 11/70
Train Loss: 0.0289, Accuracy: 0.9923, Precision: 0.9913, Recall: 0.9902, F1: 0.9908
Validation Loss: 0.4748, Accuracy: 0.8807, Precision: 0.7551, Recall: 0.7380, F1: 0.7357
Testing Loss: 0.5770, Accuracy: 0.8830, Precision: 0.6691, Recall: 0.6753, F1: 0.6674
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 1, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0786, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 12/70
Train Loss: 0.0339, Accuracy: 0.9902, Precision: 0.9861, Recall: 0.9854, F1: 0.9857
Validation Loss: 0.4733, Accuracy: 0.8864, Precision: 0.7185, Recall: 0.7308, F1: 0.7139
Testing Loss: 0.5985, Accuracy: 0.8856, Precision: 0.6793, Recall: 0.6601, F1: 0.6685
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0271, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 13/70
Train Loss: 0.0303, Accuracy: 0.9916, Precision: 0.9837, Recall: 0.9830, F1: 0.9833
Validation Loss: 0.6746, Accuracy: 0.8665, Precision: 0.7490, Recall: 0.7106, F1: 0.7113
Testing Loss: 0.7838, Accuracy: 0.8431, Precision: 0.6518, Recall: 0.6235, F1: 0.6308
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0211, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 14/70
Train Loss: 0.0303, Accuracy: 0.9934, Precision: 0.9856, Recall: 0.9864, F1: 0.9860
Validation Loss: 0.4631, Accuracy: 0.9006, Precision: 0.7866, Recall: 0.7537, F1: 0.7556
Testing Loss: 0.6858, Accuracy: 0.8697, Precision: 0.6891, Recall: 0.6542, F1: 0.6622
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0056, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 15/70
Train Loss: 0.0324, Accuracy: 0.9899, Precision: 0.9775, Recall: 0.9730, F1: 0.9753
Validation Loss: 0.5795, Accuracy: 0.8608, Precision: 0.7076, Recall: 0.7923, F1: 0.7114
Testing Loss: 0.6551, Accuracy: 0.8564, Precision: 0.6764, Recall: 0.6846, F1: 0.6721
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0425, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 16/70
Train Loss: 0.0179, Accuracy: 0.9958, Precision: 0.9897, Recall: 0.9903, F1: 0.9900
Validation Loss: 0.5644, Accuracy: 0.8807, Precision: 0.7782, Recall: 0.7332, F1: 0.7470
Testing Loss: 0.7390, Accuracy: 0.8617, Precision: 0.6857, Recall: 0.6477, F1: 0.6627
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0270, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 17/70
Train Loss: 0.0077, Accuracy: 0.9983, Precision: 0.9990, Recall: 0.9978, F1: 0.9984
Validation Loss: 0.5944, Accuracy: 0.8920, Precision: 0.7521, Recall: 0.7820, F1: 0.7586
Testing Loss: 0.7269, Accuracy: 0.8723, Precision: 0.6571, Recall: 0.6642, F1: 0.6571
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0061, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 18/70
Train Loss: 0.0255, Accuracy: 0.9923, Precision: 0.9891, Recall: 0.9914, F1: 0.9903
Validation Loss: 0.5807, Accuracy: 0.8807, Precision: 0.7646, Recall: 0.7273, F1: 0.7371
Testing Loss: 0.6790, Accuracy: 0.8670, Precision: 0.6618, Recall: 0.6227, F1: 0.6368
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0054, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0445, Accuracy: 0.9850, Precision: 0.9592, Recall: 0.9615, F1: 0.9603
Validation Loss: 0.5357, Accuracy: 0.8551, Precision: 0.7393, Recall: 0.6896, F1: 0.6803
Testing Loss: 0.7390, Accuracy: 0.8511, Precision: 0.6941, Recall: 0.6400, F1: 0.6608
LM Predictions:  [4, 4, 2, 5, 4, 1, 5, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2962, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 20/70
Train Loss: 0.0263, Accuracy: 0.9923, Precision: 0.9825, Recall: 0.9837, F1: 0.9831
Validation Loss: 0.5910, Accuracy: 0.8750, Precision: 0.7280, Recall: 0.6475, F1: 0.6757
Testing Loss: 0.7202, Accuracy: 0.8670, Precision: 0.6902, Recall: 0.6502, F1: 0.6656
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0094, Accuracy: 0.9969, Precision: 0.9954, Recall: 0.9945, F1: 0.9950
Validation Loss: 0.4880, Accuracy: 0.9034, Precision: 0.7475, Recall: 0.6999, F1: 0.7200
Testing Loss: 0.6657, Accuracy: 0.8803, Precision: 0.6871, Recall: 0.6609, F1: 0.6713
LM Predictions:  [0, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0676, Accuracy: 0.9643, Precision: 0.9778, Recall: 0.9750, F1: 0.9749
Epoch 22/70
Train Loss: 0.0341, Accuracy: 0.9902, Precision: 0.9818, Recall: 0.9782, F1: 0.9800
Validation Loss: 0.6880, Accuracy: 0.8665, Precision: 0.7327, Recall: 0.7247, F1: 0.7098
Testing Loss: 0.7740, Accuracy: 0.8644, Precision: 0.6855, Recall: 0.6412, F1: 0.6571
LM Predictions:  [0, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0653, Accuracy: 0.9643, Precision: 0.9778, Recall: 0.9750, F1: 0.9749
Epoch 23/70
Train Loss: 0.0180, Accuracy: 0.9962, Precision: 0.9905, Recall: 0.9930, F1: 0.9917
Validation Loss: 0.5636, Accuracy: 0.8949, Precision: 0.7393, Recall: 0.6860, F1: 0.7040
Testing Loss: 0.6940, Accuracy: 0.8777, Precision: 0.6717, Recall: 0.6748, F1: 0.6682
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0027, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0132, Accuracy: 0.9958, Precision: 0.9951, Recall: 0.9961, F1: 0.9956
Validation Loss: 0.7816, Accuracy: 0.8551, Precision: 0.7207, Recall: 0.7097, F1: 0.7008
Testing Loss: 0.8039, Accuracy: 0.8511, Precision: 0.6292, Recall: 0.6528, F1: 0.6355
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0044, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0064, Accuracy: 0.9983, Precision: 0.9963, Recall: 0.9978, F1: 0.9970
Validation Loss: 0.6934, Accuracy: 0.8722, Precision: 0.7139, Recall: 0.7478, F1: 0.7171
Testing Loss: 0.8135, Accuracy: 0.8617, Precision: 0.6363, Recall: 0.6741, F1: 0.6469
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0045, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9984, F1: 0.9990
Validation Loss: 0.8039, Accuracy: 0.8778, Precision: 0.7152, Recall: 0.7866, F1: 0.7394
Testing Loss: 0.9094, Accuracy: 0.8670, Precision: 0.6468, Recall: 0.7016, F1: 0.6674
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 3, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 3, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1889, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7778, F1: 0.8000
Epoch 27/70
Train Loss: 0.0286, Accuracy: 0.9899, Precision: 0.9803, Recall: 0.9750, F1: 0.9776
Validation Loss: 0.5856, Accuracy: 0.9006, Precision: 0.7644, Recall: 0.7831, F1: 0.7716
Testing Loss: 0.8602, Accuracy: 0.8537, Precision: 0.6163, Recall: 0.6502, F1: 0.6282
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 3, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.2978, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 28/70
Train Loss: 0.0310, Accuracy: 0.9909, Precision: 0.9691, Recall: 0.9704, F1: 0.9697
Validation Loss: 0.5478, Accuracy: 0.8949, Precision: 0.7641, Recall: 0.7532, F1: 0.7536
Testing Loss: 0.6815, Accuracy: 0.8750, Precision: 0.6784, Recall: 0.6609, F1: 0.6689
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0201, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0100, Accuracy: 0.9976, Precision: 0.9943, Recall: 0.9943, F1: 0.9943
Validation Loss: 0.5854, Accuracy: 0.8835, Precision: 0.7191, Recall: 0.7038, F1: 0.7083
Testing Loss: 0.8024, Accuracy: 0.8723, Precision: 0.6468, Recall: 0.6703, F1: 0.6542
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0069, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0043, Accuracy: 0.9986, Precision: 0.9926, Recall: 0.9964, F1: 0.9945
Validation Loss: 0.7017, Accuracy: 0.8722, Precision: 0.7225, Recall: 0.6658, F1: 0.6872
Testing Loss: 0.8700, Accuracy: 0.8697, Precision: 0.6550, Recall: 0.6600, F1: 0.6568
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0033, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0013, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9970, F1: 0.9983
Validation Loss: 0.7981, Accuracy: 0.8580, Precision: 0.7496, Recall: 0.6916, F1: 0.6936
Testing Loss: 0.9539, Accuracy: 0.8537, Precision: 0.6488, Recall: 0.6370, F1: 0.6412
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7786, Accuracy: 0.8750, Precision: 0.7096, Recall: 0.6592, F1: 0.6794
Testing Loss: 0.8656, Accuracy: 0.8723, Precision: 0.6669, Recall: 0.6556, F1: 0.6607
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.8523, Accuracy: 0.8750, Precision: 0.7451, Recall: 0.6850, F1: 0.6934
Testing Loss: 0.9290, Accuracy: 0.8617, Precision: 0.6568, Recall: 0.6322, F1: 0.6406
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0102, Accuracy: 0.9979, Precision: 0.9948, Recall: 0.9976, F1: 0.9962
Validation Loss: 0.7167, Accuracy: 0.8864, Precision: 0.7316, Recall: 0.6522, F1: 0.6805
Testing Loss: 1.0030, Accuracy: 0.8511, Precision: 0.6848, Recall: 0.6050, F1: 0.6282
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 5, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.1733, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9750, F1: 0.9644
Epoch 35/70
Train Loss: 0.0498, Accuracy: 0.9874, Precision: 0.9746, Recall: 0.9771, F1: 0.9758
Validation Loss: 0.6424, Accuracy: 0.8693, Precision: 0.8121, Recall: 0.7083, F1: 0.7477
Testing Loss: 0.8166, Accuracy: 0.8484, Precision: 0.6456, Recall: 0.6215, F1: 0.6312
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0133, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0133, Accuracy: 0.9944, Precision: 0.9938, Recall: 0.9894, F1: 0.9916
Validation Loss: 0.7808, Accuracy: 0.8665, Precision: 0.7403, Recall: 0.7066, F1: 0.7110
Testing Loss: 0.8802, Accuracy: 0.8617, Precision: 0.6681, Recall: 0.6699, F1: 0.6646
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 3, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0488, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 37/70
Train Loss: 0.0219, Accuracy: 0.9951, Precision: 0.9930, Recall: 0.9919, F1: 0.9924
Validation Loss: 0.6096, Accuracy: 0.8807, Precision: 0.7730, Recall: 0.7854, F1: 0.7513
Testing Loss: 0.8820, Accuracy: 0.8617, Precision: 0.6687, Recall: 0.6502, F1: 0.6581
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0013, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9997, F1: 0.9991
Validation Loss: 0.7615, Accuracy: 0.8807, Precision: 0.7639, Recall: 0.7139, F1: 0.7320
Testing Loss: 0.9882, Accuracy: 0.8564, Precision: 0.6654, Recall: 0.6210, F1: 0.6384
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0167, Accuracy: 0.9958, Precision: 0.9929, Recall: 0.9932, F1: 0.9931
Validation Loss: 0.6917, Accuracy: 0.8864, Precision: 0.7492, Recall: 0.8054, F1: 0.7477
Testing Loss: 0.9788, Accuracy: 0.8644, Precision: 0.6603, Recall: 0.6695, F1: 0.6600
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0139, Accuracy: 0.9962, Precision: 0.9934, Recall: 0.9935, F1: 0.9935
Validation Loss: 0.7187, Accuracy: 0.8920, Precision: 0.7821, Recall: 0.8177, F1: 0.7833
Testing Loss: 0.9630, Accuracy: 0.8617, Precision: 0.6867, Recall: 0.7001, F1: 0.6804
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0376, Accuracy: 0.9920, Precision: 0.9876, Recall: 0.9816, F1: 0.9845
Validation Loss: 0.6490, Accuracy: 0.8778, Precision: 0.7443, Recall: 0.7989, F1: 0.7551
Testing Loss: 0.8444, Accuracy: 0.8670, Precision: 0.6529, Recall: 0.6609, F1: 0.6548
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0126, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 42/70
Train Loss: 0.0349, Accuracy: 0.9902, Precision: 0.9799, Recall: 0.9732, F1: 0.9765
Validation Loss: 0.6785, Accuracy: 0.8864, Precision: 0.7621, Recall: 0.7292, F1: 0.7338
Testing Loss: 0.7058, Accuracy: 0.8830, Precision: 0.6749, Recall: 0.6847, F1: 0.6756
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0039, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0203, Accuracy: 0.9937, Precision: 0.9763, Recall: 0.9774, F1: 0.9768
Validation Loss: 0.6103, Accuracy: 0.8778, Precision: 0.7356, Recall: 0.7600, F1: 0.7397
Testing Loss: 0.7851, Accuracy: 0.8697, Precision: 0.7009, Recall: 0.7051, F1: 0.7011
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0143, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0154, Accuracy: 0.9944, Precision: 0.9901, Recall: 0.9901, F1: 0.9901
Validation Loss: 0.6547, Accuracy: 0.8864, Precision: 0.7829, Recall: 0.7986, F1: 0.7740
Testing Loss: 0.7623, Accuracy: 0.8777, Precision: 0.6733, Recall: 0.6990, F1: 0.6809
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0058, Accuracy: 0.9986, Precision: 0.9953, Recall: 0.9962, F1: 0.9957
Validation Loss: 0.7490, Accuracy: 0.8722, Precision: 0.7574, Recall: 0.7629, F1: 0.7456
Testing Loss: 0.8067, Accuracy: 0.8750, Precision: 0.6889, Recall: 0.6555, F1: 0.6619
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0057, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0187, Accuracy: 0.9941, Precision: 0.9869, Recall: 0.9832, F1: 0.9850
Validation Loss: 0.7099, Accuracy: 0.8722, Precision: 0.7000, Recall: 0.6542, F1: 0.6618
Testing Loss: 0.8689, Accuracy: 0.8670, Precision: 0.6480, Recall: 0.6756, F1: 0.6528
LM Predictions:  [1, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0427, Accuracy: 0.9643, Precision: 0.9333, Recall: 0.9750, F1: 0.9467
Epoch 47/70
Train Loss: 0.0154, Accuracy: 0.9955, Precision: 0.9869, Recall: 0.9867, F1: 0.9868
Validation Loss: 0.8186, Accuracy: 0.8778, Precision: 0.7408, Recall: 0.6528, F1: 0.6803
Testing Loss: 0.9126, Accuracy: 0.8723, Precision: 0.7149, Recall: 0.6629, F1: 0.6819
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0018, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7310, Accuracy: 0.8864, Precision: 0.7706, Recall: 0.7163, F1: 0.7276
Testing Loss: 0.9503, Accuracy: 0.8723, Precision: 0.6861, Recall: 0.6461, F1: 0.6614
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0339, Accuracy: 0.9927, Precision: 0.9849, Recall: 0.9854, F1: 0.9852
Validation Loss: 0.6166, Accuracy: 0.8920, Precision: 0.7404, Recall: 0.6709, F1: 0.6848
Testing Loss: 0.8388, Accuracy: 0.8697, Precision: 0.6730, Recall: 0.6682, F1: 0.6601
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0286, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0087, Accuracy: 0.9983, Precision: 0.9959, Recall: 0.9963, F1: 0.9961
Validation Loss: 0.7011, Accuracy: 0.8892, Precision: 0.7500, Recall: 0.6723, F1: 0.6947
Testing Loss: 0.8867, Accuracy: 0.8723, Precision: 0.6839, Recall: 0.6674, F1: 0.6739
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0015, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0255, Accuracy: 0.9944, Precision: 0.9921, Recall: 0.9840, F1: 0.9880
Validation Loss: 0.6263, Accuracy: 0.8580, Precision: 0.7220, Recall: 0.7788, F1: 0.7420
Testing Loss: 0.8287, Accuracy: 0.8484, Precision: 0.6417, Recall: 0.6975, F1: 0.6634
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0121, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0109, Accuracy: 0.9976, Precision: 0.9974, Recall: 0.9985, F1: 0.9980
Validation Loss: 0.5808, Accuracy: 0.8807, Precision: 0.7530, Recall: 0.7172, F1: 0.7287
Testing Loss: 0.7752, Accuracy: 0.8750, Precision: 0.6737, Recall: 0.6678, F1: 0.6683
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0198, Accuracy: 0.9962, Precision: 0.9939, Recall: 0.9917, F1: 0.9928
Validation Loss: 0.5875, Accuracy: 0.8778, Precision: 0.7409, Recall: 0.8052, F1: 0.7582
Testing Loss: 0.7429, Accuracy: 0.8670, Precision: 0.6504, Recall: 0.6859, F1: 0.6643
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0102, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0071, Accuracy: 0.9979, Precision: 0.9924, Recall: 0.9922, F1: 0.9923
Validation Loss: 0.7176, Accuracy: 0.8665, Precision: 0.7647, Recall: 0.7470, F1: 0.7380
Testing Loss: 0.7606, Accuracy: 0.8670, Precision: 0.6651, Recall: 0.6560, F1: 0.6589
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0040, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0014, Accuracy: 0.9997, Precision: 0.9997, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.7146, Accuracy: 0.8665, Precision: 0.7413, Recall: 0.7784, F1: 0.7498
Testing Loss: 0.8115, Accuracy: 0.8590, Precision: 0.6455, Recall: 0.6658, F1: 0.6537
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7393, Accuracy: 0.8693, Precision: 0.7517, Recall: 0.7962, F1: 0.7538
Testing Loss: 0.8852, Accuracy: 0.8617, Precision: 0.6488, Recall: 0.6798, F1: 0.6589
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0046, Accuracy: 0.9990, Precision: 0.9966, Recall: 0.9991, F1: 0.9978
Validation Loss: 0.6871, Accuracy: 0.8693, Precision: 0.7003, Recall: 0.6638, F1: 0.6773
Testing Loss: 0.7961, Accuracy: 0.8777, Precision: 0.6853, Recall: 0.6814, F1: 0.6798
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0040, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0082, Accuracy: 0.9979, Precision: 0.9985, Recall: 0.9946, F1: 0.9966
Validation Loss: 0.7404, Accuracy: 0.8807, Precision: 0.7095, Recall: 0.6954, F1: 0.7020
Testing Loss: 0.8436, Accuracy: 0.8856, Precision: 0.6793, Recall: 0.6995, F1: 0.6874
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0043, Accuracy: 0.9986, Precision: 0.9990, Recall: 0.9990, F1: 0.9990
Validation Loss: 0.7041, Accuracy: 0.8750, Precision: 0.6920, Recall: 0.6621, F1: 0.6756
Testing Loss: 0.7582, Accuracy: 0.8803, Precision: 0.6899, Recall: 0.6732, F1: 0.6780
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0084, Accuracy: 0.9979, Precision: 0.9965, Recall: 0.9976, F1: 0.9971
Validation Loss: 0.6962, Accuracy: 0.8693, Precision: 0.7034, Recall: 0.6646, F1: 0.6820
Testing Loss: 0.7775, Accuracy: 0.8723, Precision: 0.6690, Recall: 0.6826, F1: 0.6724
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0015, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.9470, Accuracy: 0.8750, Precision: 0.7582, Recall: 0.7129, F1: 0.7200
Testing Loss: 1.0217, Accuracy: 0.8670, Precision: 0.6725, Recall: 0.6823, F1: 0.6705
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0394, Accuracy: 0.9923, Precision: 0.9858, Recall: 0.9851, F1: 0.9854
Validation Loss: 0.7553, Accuracy: 0.8608, Precision: 0.7255, Recall: 0.6125, F1: 0.6523
Testing Loss: 0.7158, Accuracy: 0.8830, Precision: 0.6908, Recall: 0.6707, F1: 0.6795
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0107, Accuracy: 0.9969, Precision: 0.9929, Recall: 0.9927, F1: 0.9928
Validation Loss: 0.6933, Accuracy: 0.8750, Precision: 0.7043, Recall: 0.6602, F1: 0.6780
Testing Loss: 0.8494, Accuracy: 0.8723, Precision: 0.6821, Recall: 0.6904, F1: 0.6852
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0009, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9997, F1: 0.9998
Validation Loss: 0.8058, Accuracy: 0.8750, Precision: 0.7253, Recall: 0.6565, F1: 0.6762
Testing Loss: 0.9143, Accuracy: 0.8644, Precision: 0.6895, Recall: 0.6707, F1: 0.6751
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0179, Accuracy: 0.9948, Precision: 0.9885, Recall: 0.9875, F1: 0.9880
Validation Loss: 0.6215, Accuracy: 0.8864, Precision: 0.7308, Recall: 0.6893, F1: 0.7058
Testing Loss: 0.8338, Accuracy: 0.8644, Precision: 0.6626, Recall: 0.6641, F1: 0.6580
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0056, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0049, Accuracy: 0.9983, Precision: 0.9965, Recall: 0.9976, F1: 0.9970
Validation Loss: 0.6386, Accuracy: 0.8920, Precision: 0.7144, Recall: 0.7350, F1: 0.7231
Testing Loss: 0.8952, Accuracy: 0.8644, Precision: 0.6498, Recall: 0.6851, F1: 0.6643
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0101, Accuracy: 0.9969, Precision: 0.9953, Recall: 0.9966, F1: 0.9960
Validation Loss: 0.6780, Accuracy: 0.8665, Precision: 0.7053, Recall: 0.6590, F1: 0.6747
Testing Loss: 0.8060, Accuracy: 0.8564, Precision: 0.6511, Recall: 0.6535, F1: 0.6441
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0091, Accuracy: 0.9976, Precision: 0.9967, Recall: 0.9947, F1: 0.9957
Validation Loss: 0.6599, Accuracy: 0.8807, Precision: 0.7158, Recall: 0.7026, F1: 0.7076
Testing Loss: 0.7896, Accuracy: 0.8777, Precision: 0.6690, Recall: 0.6925, F1: 0.6780
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0026, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9984, F1: 0.9984
Validation Loss: 0.6734, Accuracy: 0.8835, Precision: 0.7126, Recall: 0.7117, F1: 0.7111
Testing Loss: 0.7956, Accuracy: 0.8856, Precision: 0.6849, Recall: 0.7147, F1: 0.6958
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0005, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0042, Accuracy: 0.9993, Precision: 0.9969, Recall: 0.9995, F1: 0.9982
Validation Loss: 0.7874, Accuracy: 0.8778, Precision: 0.7484, Recall: 0.7966, F1: 0.7519
Testing Loss: 0.8616, Accuracy: 0.8830, Precision: 0.6717, Recall: 0.6950, F1: 0.6821
LM Predictions:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 0.0003, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



