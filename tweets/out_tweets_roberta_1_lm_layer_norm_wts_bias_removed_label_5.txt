---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 5
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 3: 486
  Label 1: 115
  Label 5: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 3: 60
  Label 0: 3
  Label 5: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 5: 29
  Label 3: 58
28
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 4: 973
  Label 2: 1106
  Label 3: 491
  Label 1: 119
  Label 5: 116
  Label 0: 53
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4106, Accuracy: 0.3768, Precision: 0.1517, Recall: 0.1709, F1: 0.1450
Validation Loss: 1.4452, Accuracy: 0.3352, Precision: 0.1390, Recall: 0.1694, F1: 0.0889
Testing Loss: 1.4605, Accuracy: 0.3537, Precision: 0.1008, Recall: 0.1683, F1: 0.0925
LM Predictions:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8654, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 2/70
Train Loss: 1.3564, Accuracy: 0.4237, Precision: 0.1603, Recall: 0.1924, F1: 0.1628
Validation Loss: 1.2363, Accuracy: 0.5852, Precision: 0.2070, Recall: 0.2731, F1: 0.2313
Testing Loss: 1.2642, Accuracy: 0.5931, Precision: 0.2063, Recall: 0.2762, F1: 0.2331
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1807, Accuracy: 0.3214, Precision: 0.1583, Recall: 0.2571, F1: 0.1630
Epoch 3/70
Train Loss: 1.1200, Accuracy: 0.5994, Precision: 0.2201, Recall: 0.2727, F1: 0.2364
Validation Loss: 1.0054, Accuracy: 0.6449, Precision: 0.2454, Recall: 0.3009, F1: 0.2605
Testing Loss: 1.0058, Accuracy: 0.6543, Precision: 0.2448, Recall: 0.3046, F1: 0.2621
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0530, Accuracy: 0.2500, Precision: 0.0519, Recall: 0.2000, F1: 0.0824
Epoch 4/70
Train Loss: 0.9599, Accuracy: 0.6554, Precision: 0.2871, Recall: 0.3034, F1: 0.2724
Validation Loss: 0.9478, Accuracy: 0.6477, Precision: 0.2456, Recall: 0.3025, F1: 0.2619
Testing Loss: 0.9661, Accuracy: 0.6489, Precision: 0.2437, Recall: 0.3021, F1: 0.2602
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0040, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 5/70
Train Loss: 0.8560, Accuracy: 0.6795, Precision: 0.3226, Recall: 0.3528, F1: 0.3359
Validation Loss: 0.8537, Accuracy: 0.6875, Precision: 0.3326, Recall: 0.3790, F1: 0.3498
Testing Loss: 0.7885, Accuracy: 0.7101, Precision: 0.3408, Recall: 0.3932, F1: 0.3603
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2478, Accuracy: 0.1786, Precision: 0.2308, Recall: 0.1886, F1: 0.1016
Epoch 6/70
Train Loss: 0.7834, Accuracy: 0.7187, Precision: 0.3483, Recall: 0.3875, F1: 0.3637
Validation Loss: 0.8212, Accuracy: 0.7244, Precision: 0.3414, Recall: 0.3845, F1: 0.3613
Testing Loss: 0.8009, Accuracy: 0.7420, Precision: 0.3517, Recall: 0.3980, F1: 0.3721
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3118, Accuracy: 0.2143, Precision: 0.1781, Recall: 0.2171, F1: 0.1393
Epoch 7/70
Train Loss: 0.7274, Accuracy: 0.7460, Precision: 0.4123, Recall: 0.4178, F1: 0.4000
Validation Loss: 0.7765, Accuracy: 0.7159, Precision: 0.3408, Recall: 0.3912, F1: 0.3636
Testing Loss: 0.7679, Accuracy: 0.7527, Precision: 0.3543, Recall: 0.4097, F1: 0.3798
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 4, 3, 3, 3, 4, 2, 2, 3, 3, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8834, Accuracy: 0.2500, Precision: 0.2181, Recall: 0.2457, F1: 0.1726
Epoch 8/70
Train Loss: 0.6742, Accuracy: 0.7698, Precision: 0.4251, Recall: 0.4340, F1: 0.4179
Validation Loss: 0.7849, Accuracy: 0.7500, Precision: 0.4477, Recall: 0.4258, F1: 0.4023
Testing Loss: 0.6973, Accuracy: 0.7473, Precision: 0.3600, Recall: 0.4137, F1: 0.3810
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0601, Accuracy: 0.1429, Precision: 0.0256, Recall: 0.1333, F1: 0.0430
Epoch 9/70
Train Loss: 0.6667, Accuracy: 0.7743, Precision: 0.4647, Recall: 0.4511, F1: 0.4419
Validation Loss: 0.8063, Accuracy: 0.7557, Precision: 0.4337, Recall: 0.4806, F1: 0.4540
Testing Loss: 0.6669, Accuracy: 0.7846, Precision: 0.4574, Recall: 0.4864, F1: 0.4709
LM Predictions:  [3, 5, 5, 5, 5, 3, 3, 3, 3, 5, 5, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 3, 5, 2, 3, 3, 5, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9858, Accuracy: 0.1429, Precision: 0.3667, Recall: 0.1143, F1: 0.1278
Epoch 10/70
Train Loss: 0.6272, Accuracy: 0.7943, Precision: 0.5138, Recall: 0.4947, F1: 0.4781
Validation Loss: 0.8413, Accuracy: 0.7386, Precision: 0.3558, Recall: 0.4072, F1: 0.3786
Testing Loss: 0.8138, Accuracy: 0.7606, Precision: 0.3611, Recall: 0.4182, F1: 0.3865
LM Predictions:  [3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 2, 4, 3, 3, 2, 3, 3, 2, 3, 2, 2, 2, 3, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0287, Accuracy: 0.2857, Precision: 0.3153, Recall: 0.2629, F1: 0.1987
Epoch 11/70
Train Loss: 0.5986, Accuracy: 0.8041, Precision: 0.5454, Recall: 0.5033, F1: 0.4828
Validation Loss: 0.7355, Accuracy: 0.7699, Precision: 0.4512, Recall: 0.5192, F1: 0.4762
Testing Loss: 0.6736, Accuracy: 0.7979, Precision: 0.4747, Recall: 0.5106, F1: 0.4914
LM Predictions:  [5, 5, 5, 5, 5, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 3, 5, 3, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2359, Accuracy: 0.1429, Precision: 0.3056, Recall: 0.1143, F1: 0.1393
Epoch 12/70
Train Loss: 0.5457, Accuracy: 0.8198, Precision: 0.5143, Recall: 0.5296, F1: 0.5085
Validation Loss: 0.7398, Accuracy: 0.7614, Precision: 0.4479, Recall: 0.5267, F1: 0.4716
Testing Loss: 0.6678, Accuracy: 0.7979, Precision: 0.4760, Recall: 0.5258, F1: 0.4963
LM Predictions:  [5, 5, 5, 5, 5, 3, 5, 3, 3, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 3, 5, 3, 5, 3, 5, 3]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3593, Accuracy: 0.0714, Precision: 0.1905, Recall: 0.0571, F1: 0.0694
Epoch 13/70
Train Loss: 0.5495, Accuracy: 0.8188, Precision: 0.5249, Recall: 0.5442, F1: 0.5144
Validation Loss: 0.7336, Accuracy: 0.7670, Precision: 0.4503, Recall: 0.5230, F1: 0.4766
Testing Loss: 0.7193, Accuracy: 0.7926, Precision: 0.4698, Recall: 0.5037, F1: 0.4853
LM Predictions:  [3, 5, 5, 5, 5, 3, 5, 3, 3, 5, 5, 3, 5, 5, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2737, Accuracy: 0.1786, Precision: 0.3250, Recall: 0.1286, F1: 0.1659
Epoch 14/70
Train Loss: 0.5141, Accuracy: 0.8244, Precision: 0.5341, Recall: 0.5548, F1: 0.5361
Validation Loss: 0.7322, Accuracy: 0.7784, Precision: 0.4699, Recall: 0.5159, F1: 0.4876
Testing Loss: 0.6918, Accuracy: 0.8032, Precision: 0.5569, Recall: 0.5369, F1: 0.5433
LM Predictions:  [3, 5, 5, 5, 5, 3, 1, 3, 3, 5, 5, 1, 5, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3955, Accuracy: 0.2143, Precision: 0.3333, Recall: 0.1524, F1: 0.2020
Epoch 15/70
Train Loss: 0.5011, Accuracy: 0.8355, Precision: 0.5462, Recall: 0.5645, F1: 0.5481
Validation Loss: 0.7128, Accuracy: 0.7727, Precision: 0.4540, Recall: 0.5064, F1: 0.4763
Testing Loss: 0.6713, Accuracy: 0.8032, Precision: 0.4897, Recall: 0.5148, F1: 0.5016
LM Predictions:  [5, 5, 5, 5, 5, 3, 4, 3, 3, 5, 5, 5, 4, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0945, Accuracy: 0.2143, Precision: 0.2639, Recall: 0.1524, F1: 0.1932
Epoch 16/70
Train Loss: 0.4806, Accuracy: 0.8436, Precision: 0.5505, Recall: 0.5693, F1: 0.5517
Validation Loss: 0.8086, Accuracy: 0.7727, Precision: 0.4536, Recall: 0.4850, F1: 0.4679
Testing Loss: 0.7875, Accuracy: 0.7952, Precision: 0.5343, Recall: 0.4984, F1: 0.5011
LM Predictions:  [3, 5, 5, 5, 4, 2, 2, 3, 3, 5, 5, 3, 3, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0574, Accuracy: 0.1786, Precision: 0.1944, Recall: 0.1190, F1: 0.1436
Epoch 17/70
Train Loss: 0.4373, Accuracy: 0.8579, Precision: 0.5909, Recall: 0.6014, F1: 0.5850
Validation Loss: 0.7582, Accuracy: 0.7841, Precision: 0.4946, Recall: 0.4916, F1: 0.4880
Testing Loss: 0.7045, Accuracy: 0.7926, Precision: 0.5158, Recall: 0.5011, F1: 0.5040
LM Predictions:  [3, 5, 5, 5, 1, 3, 2, 3, 3, 5, 5, 1, 3, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3987, Accuracy: 0.2500, Precision: 0.3556, Recall: 0.1940, F1: 0.2384
Epoch 18/70
Train Loss: 0.4474, Accuracy: 0.8537, Precision: 0.5702, Recall: 0.5994, F1: 0.5813
Validation Loss: 0.7630, Accuracy: 0.7670, Precision: 0.5300, Recall: 0.5106, F1: 0.5060
Testing Loss: 0.7014, Accuracy: 0.8059, Precision: 0.5590, Recall: 0.5422, F1: 0.5462
LM Predictions:  [3, 5, 5, 1, 5, 3, 2, 3, 3, 5, 5, 3, 3, 4, 5, 4, 5, 5, 5, 1, 5, 2, 5, 2, 5, 2, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8384, Accuracy: 0.2143, Precision: 0.2944, Recall: 0.1524, F1: 0.1877
Epoch 19/70
Train Loss: 0.4110, Accuracy: 0.8625, Precision: 0.5901, Recall: 0.6066, F1: 0.5893
Validation Loss: 0.6689, Accuracy: 0.7841, Precision: 0.4983, Recall: 0.5283, F1: 0.5107
Testing Loss: 0.6646, Accuracy: 0.8138, Precision: 0.5390, Recall: 0.5455, F1: 0.5414
LM Predictions:  [1, 5, 5, 5, 5, 2, 3, 3, 3, 5, 5, 1, 3, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 3, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1797, Accuracy: 0.2143, Precision: 0.3250, Recall: 0.1524, F1: 0.1983
Epoch 20/70
Train Loss: 0.3876, Accuracy: 0.8740, Precision: 0.6012, Recall: 0.6243, F1: 0.6100
Validation Loss: 0.7472, Accuracy: 0.7727, Precision: 0.5133, Recall: 0.5365, F1: 0.5074
Testing Loss: 0.6806, Accuracy: 0.7926, Precision: 0.5049, Recall: 0.5180, F1: 0.5057
LM Predictions:  [1, 5, 5, 5, 5, 3, 2, 4, 1, 5, 5, 5, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 4, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1446, Accuracy: 0.2857, Precision: 0.4250, Recall: 0.2000, F1: 0.2576
Epoch 21/70
Train Loss: 0.3866, Accuracy: 0.8733, Precision: 0.6034, Recall: 0.6240, F1: 0.6116
Validation Loss: 0.8645, Accuracy: 0.7727, Precision: 0.4691, Recall: 0.5134, F1: 0.4883
Testing Loss: 0.8798, Accuracy: 0.7952, Precision: 0.5190, Recall: 0.5151, F1: 0.5113
LM Predictions:  [1, 5, 5, 4, 5, 2, 2, 2, 3, 5, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 2, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1455, Accuracy: 0.2500, Precision: 0.2500, Recall: 0.1667, F1: 0.1889
Epoch 22/70
Train Loss: 0.3742, Accuracy: 0.8821, Precision: 0.6083, Recall: 0.6277, F1: 0.6158
Validation Loss: 0.7734, Accuracy: 0.7756, Precision: 0.5214, Recall: 0.5097, F1: 0.5086
Testing Loss: 0.7569, Accuracy: 0.8059, Precision: 0.5450, Recall: 0.5398, F1: 0.5363
LM Predictions:  [3, 5, 1, 2, 5, 2, 3, 4, 3, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 3, 5, 2, 1, 2, 5, 3, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8831, Accuracy: 0.2857, Precision: 0.3056, Recall: 0.2000, F1: 0.2329
Epoch 23/70
Train Loss: 0.3413, Accuracy: 0.8831, Precision: 0.6641, Recall: 0.6393, F1: 0.6268
Validation Loss: 0.7725, Accuracy: 0.7812, Precision: 0.5365, Recall: 0.5420, F1: 0.5329
Testing Loss: 0.7445, Accuracy: 0.7793, Precision: 0.5101, Recall: 0.5024, F1: 0.5022
LM Predictions:  [3, 5, 5, 5, 5, 2, 2, 4, 1, 5, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 5, 3, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0120, Accuracy: 0.2500, Precision: 0.2778, Recall: 0.1667, F1: 0.2026
Epoch 24/70
Train Loss: 0.3262, Accuracy: 0.8870, Precision: 0.6239, Recall: 0.6471, F1: 0.6349
Validation Loss: 1.0268, Accuracy: 0.7699, Precision: 0.4942, Recall: 0.5184, F1: 0.4877
Testing Loss: 0.9582, Accuracy: 0.7686, Precision: 0.4727, Recall: 0.4815, F1: 0.4672
LM Predictions:  [1, 5, 5, 4, 5, 2, 2, 4, 5, 5, 5, 5, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 1, 4, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4404, Accuracy: 0.2857, Precision: 0.2444, Recall: 0.1905, F1: 0.2137
Epoch 25/70
Train Loss: 0.3340, Accuracy: 0.8856, Precision: 0.6062, Recall: 0.6396, F1: 0.6154
Validation Loss: 0.6936, Accuracy: 0.7869, Precision: 0.5415, Recall: 0.5382, F1: 0.5393
Testing Loss: 0.6931, Accuracy: 0.8191, Precision: 0.5720, Recall: 0.5771, F1: 0.5731
LM Predictions:  [3, 5, 5, 2, 5, 3, 2, 4, 1, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 1, 5, 2, 1, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7419, Accuracy: 0.2857, Precision: 0.3036, Recall: 0.2000, F1: 0.2323
Epoch 26/70
Train Loss: 0.3032, Accuracy: 0.8996, Precision: 0.6398, Recall: 0.6598, F1: 0.6485
Validation Loss: 0.7809, Accuracy: 0.7898, Precision: 0.5521, Recall: 0.5339, F1: 0.5374
Testing Loss: 0.8596, Accuracy: 0.7819, Precision: 0.5273, Recall: 0.5037, F1: 0.5080
LM Predictions:  [1, 5, 5, 2, 4, 2, 2, 4, 1, 5, 5, 1, 2, 4, 4, 4, 5, 5, 5, 5, 5, 2, 1, 2, 5, 1, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1095, Accuracy: 0.2500, Precision: 0.1833, Recall: 0.1667, F1: 0.1722
Epoch 27/70
Train Loss: 0.2905, Accuracy: 0.9017, Precision: 0.6958, Recall: 0.6736, F1: 0.6603
Validation Loss: 0.8255, Accuracy: 0.7955, Precision: 0.5757, Recall: 0.5618, F1: 0.5673
Testing Loss: 0.7737, Accuracy: 0.8112, Precision: 0.5648, Recall: 0.5615, F1: 0.5579
LM Predictions:  [3, 5, 5, 2, 5, 3, 3, 4, 1, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 1, 5, 2, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9099, Accuracy: 0.3571, Precision: 0.3861, Recall: 0.2750, F1: 0.3109
Epoch 28/70
Train Loss: 0.2939, Accuracy: 0.9041, Precision: 0.6497, Recall: 0.6765, F1: 0.6622
Validation Loss: 0.7104, Accuracy: 0.8040, Precision: 0.5766, Recall: 0.5710, F1: 0.5709
Testing Loss: 0.7493, Accuracy: 0.8218, Precision: 0.5850, Recall: 0.5710, F1: 0.5757
LM Predictions:  [3, 5, 5, 2, 1, 3, 3, 4, 1, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 3, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6459, Accuracy: 0.3929, Precision: 0.4444, Recall: 0.3167, F1: 0.3584
Epoch 29/70
Train Loss: 0.2944, Accuracy: 0.9052, Precision: 0.6544, Recall: 0.6732, F1: 0.6631
Validation Loss: 0.8384, Accuracy: 0.7812, Precision: 0.5419, Recall: 0.5334, F1: 0.5288
Testing Loss: 0.8106, Accuracy: 0.8059, Precision: 0.5537, Recall: 0.5287, F1: 0.5349
LM Predictions:  [1, 5, 5, 2, 5, 3, 2, 4, 1, 5, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 5, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2325, Accuracy: 0.2857, Precision: 0.3452, Recall: 0.2000, F1: 0.2429
Epoch 30/70
Train Loss: 0.2720, Accuracy: 0.9122, Precision: 0.7469, Recall: 0.7047, F1: 0.7048
Validation Loss: 0.9998, Accuracy: 0.7841, Precision: 0.5766, Recall: 0.5368, F1: 0.5437
Testing Loss: 0.9785, Accuracy: 0.8085, Precision: 0.5662, Recall: 0.5431, F1: 0.5517
LM Predictions:  [3, 5, 5, 2, 5, 3, 3, 4, 3, 5, 5, 3, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9340, Accuracy: 0.3571, Precision: 0.3000, Recall: 0.2571, F1: 0.2743
Epoch 31/70
Train Loss: 0.2501, Accuracy: 0.9195, Precision: 0.7413, Recall: 0.7219, F1: 0.7215
Validation Loss: 0.9033, Accuracy: 0.7670, Precision: 0.5123, Recall: 0.5476, F1: 0.5232
Testing Loss: 0.8872, Accuracy: 0.7952, Precision: 0.5437, Recall: 0.5703, F1: 0.5522
LM Predictions:  [1, 5, 5, 4, 5, 3, 3, 4, 1, 5, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 5, 1, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0775, Accuracy: 0.3571, Precision: 0.5000, Recall: 0.2571, F1: 0.3377
Epoch 32/70
Train Loss: 0.2540, Accuracy: 0.9143, Precision: 0.7383, Recall: 0.7305, F1: 0.7328
Validation Loss: 0.7700, Accuracy: 0.8097, Precision: 0.6086, Recall: 0.5905, F1: 0.5979
Testing Loss: 0.8047, Accuracy: 0.8085, Precision: 0.5586, Recall: 0.5561, F1: 0.5538
LM Predictions:  [1, 5, 5, 2, 1, 3, 2, 4, 1, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1813, Accuracy: 0.3214, Precision: 0.3591, Recall: 0.2417, F1: 0.2786
Epoch 33/70
Train Loss: 0.2361, Accuracy: 0.9181, Precision: 0.7230, Recall: 0.7192, F1: 0.7089
Validation Loss: 0.8902, Accuracy: 0.8068, Precision: 0.6277, Recall: 0.5905, F1: 0.6015
Testing Loss: 0.8615, Accuracy: 0.8138, Precision: 0.5689, Recall: 0.5660, F1: 0.5636
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 1, 5, 3, 2, 4, 5, 4, 5, 5, 5, 1, 4, 2, 3, 2, 5, 3, 3, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2359, Accuracy: 0.4286, Precision: 0.4587, Recall: 0.3405, F1: 0.3831
Epoch 34/70
Train Loss: 0.2274, Accuracy: 0.9209, Precision: 0.7581, Recall: 0.7413, F1: 0.7432
Validation Loss: 0.7587, Accuracy: 0.8040, Precision: 0.6274, Recall: 0.6870, F1: 0.6220
Testing Loss: 0.7718, Accuracy: 0.8059, Precision: 0.5733, Recall: 0.5632, F1: 0.5681
LM Predictions:  [1, 5, 5, 2, 5, 3, 3, 4, 1, 1, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 3, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0413, Accuracy: 0.4286, Precision: 0.6111, Recall: 0.3321, F1: 0.4082
Epoch 35/70
Train Loss: 0.2221, Accuracy: 0.9332, Precision: 0.7726, Recall: 0.7743, F1: 0.7700
Validation Loss: 0.7380, Accuracy: 0.8324, Precision: 0.6355, Recall: 0.6080, F1: 0.6184
Testing Loss: 0.8278, Accuracy: 0.8112, Precision: 0.5846, Recall: 0.5866, F1: 0.5825
LM Predictions:  [1, 5, 5, 2, 1, 3, 3, 4, 1, 5, 5, 1, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 1, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1973, Accuracy: 0.3929, Precision: 0.4722, Recall: 0.2988, F1: 0.3523
Epoch 36/70
Train Loss: 0.2156, Accuracy: 0.9272, Precision: 0.7943, Recall: 0.7788, F1: 0.7828
Validation Loss: 0.8699, Accuracy: 0.7756, Precision: 0.5653, Recall: 0.5823, F1: 0.5466
Testing Loss: 0.8760, Accuracy: 0.7926, Precision: 0.5792, Recall: 0.5435, F1: 0.5557
LM Predictions:  [5, 5, 5, 2, 5, 3, 2, 4, 1, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 0, 2, 5, 1, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1240, Accuracy: 0.3571, Precision: 0.6111, Recall: 0.2667, F1: 0.3534
Epoch 37/70
Train Loss: 0.1887, Accuracy: 0.9405, Precision: 0.8106, Recall: 0.8184, F1: 0.8138
Validation Loss: 1.0280, Accuracy: 0.7898, Precision: 0.6072, Recall: 0.5349, F1: 0.5347
Testing Loss: 1.0407, Accuracy: 0.8059, Precision: 0.5851, Recall: 0.5554, F1: 0.5663
LM Predictions:  [3, 5, 5, 2, 5, 3, 3, 4, 3, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 3, 2, 5, 1, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4180, Accuracy: 0.3571, Precision: 0.3333, Recall: 0.2571, F1: 0.2844
Epoch 38/70
Train Loss: 0.2848, Accuracy: 0.9216, Precision: 0.7711, Recall: 0.7390, F1: 0.7523
Validation Loss: 0.8453, Accuracy: 0.7955, Precision: 0.7009, Recall: 0.5536, F1: 0.5431
Testing Loss: 0.8829, Accuracy: 0.7766, Precision: 0.6347, Recall: 0.4836, F1: 0.4809
LM Predictions:  [1, 4, 4, 4, 4, 3, 3, 4, 3, 1, 4, 2, 2, 4, 4, 4, 5, 4, 4, 1, 4, 2, 1, 2, 4, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9753, Accuracy: 0.4643, Precision: 0.3130, Recall: 0.3464, F1: 0.3183
Epoch 39/70
Train Loss: 0.2428, Accuracy: 0.9265, Precision: 0.7671, Recall: 0.7303, F1: 0.7388
Validation Loss: 0.8340, Accuracy: 0.7955, Precision: 0.5935, Recall: 0.5632, F1: 0.5659
Testing Loss: 0.8054, Accuracy: 0.8165, Precision: 0.6130, Recall: 0.5940, F1: 0.6026
LM Predictions:  [0, 5, 5, 2, 5, 3, 3, 4, 3, 1, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0743, Accuracy: 0.4286, Precision: 0.5278, Recall: 0.3321, F1: 0.4010
Epoch 40/70
Train Loss: 0.1983, Accuracy: 0.9360, Precision: 0.7956, Recall: 0.7861, F1: 0.7900
Validation Loss: 0.9582, Accuracy: 0.7983, Precision: 0.6138, Recall: 0.5705, F1: 0.5885
Testing Loss: 0.9668, Accuracy: 0.8059, Precision: 0.6140, Recall: 0.5973, F1: 0.6025
LM Predictions:  [0, 5, 5, 4, 1, 3, 2, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0126, Accuracy: 0.4286, Precision: 0.5119, Recall: 0.3226, F1: 0.3849
Epoch 41/70
Train Loss: 0.1896, Accuracy: 0.9454, Precision: 0.8220, Recall: 0.8265, F1: 0.8237
Validation Loss: 1.0327, Accuracy: 0.7955, Precision: 0.6488, Recall: 0.6156, F1: 0.6154
Testing Loss: 0.9893, Accuracy: 0.8112, Precision: 0.6106, Recall: 0.5920, F1: 0.5913
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 1, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4930, Accuracy: 0.3929, Precision: 0.3667, Recall: 0.2988, F1: 0.3280
Epoch 42/70
Train Loss: 0.1921, Accuracy: 0.9451, Precision: 0.8316, Recall: 0.8256, F1: 0.8284
Validation Loss: 0.7872, Accuracy: 0.7955, Precision: 0.6304, Recall: 0.5749, F1: 0.5937
Testing Loss: 0.8665, Accuracy: 0.8191, Precision: 0.6272, Recall: 0.5895, F1: 0.6039
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 3, 2, 5, 3, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9008, Accuracy: 0.4286, Precision: 0.5611, Recall: 0.3321, F1: 0.3946
Epoch 43/70
Train Loss: 0.1813, Accuracy: 0.9503, Precision: 0.8431, Recall: 0.8442, F1: 0.8431
Validation Loss: 0.7034, Accuracy: 0.8097, Precision: 0.6804, Recall: 0.6358, F1: 0.6375
Testing Loss: 0.9054, Accuracy: 0.8032, Precision: 0.6093, Recall: 0.5776, F1: 0.5819
LM Predictions:  [0, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7850, Accuracy: 0.5000, Precision: 0.5556, Recall: 0.3976, F1: 0.4570
Epoch 44/70
Train Loss: 0.1649, Accuracy: 0.9542, Precision: 0.8562, Recall: 0.8634, F1: 0.8593
Validation Loss: 0.8313, Accuracy: 0.7955, Precision: 0.6120, Recall: 0.6436, F1: 0.6121
Testing Loss: 0.8012, Accuracy: 0.8191, Precision: 0.5757, Recall: 0.5895, F1: 0.5819
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8857, Accuracy: 0.4643, Precision: 0.5778, Recall: 0.3655, F1: 0.4466
Epoch 45/70
Train Loss: 0.1718, Accuracy: 0.9514, Precision: 0.8436, Recall: 0.8415, F1: 0.8421
Validation Loss: 0.9488, Accuracy: 0.8040, Precision: 0.6422, Recall: 0.5829, F1: 0.5986
Testing Loss: 0.8850, Accuracy: 0.8271, Precision: 0.5842, Recall: 0.5738, F1: 0.5722
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 3, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0287, Accuracy: 0.4643, Precision: 0.6056, Recall: 0.3738, F1: 0.4517
Epoch 46/70
Train Loss: 0.1828, Accuracy: 0.9524, Precision: 0.8485, Recall: 0.8569, F1: 0.8523
Validation Loss: 0.8130, Accuracy: 0.8125, Precision: 0.6527, Recall: 0.6377, F1: 0.6225
Testing Loss: 0.8720, Accuracy: 0.8112, Precision: 0.5922, Recall: 0.5981, F1: 0.5929
LM Predictions:  [0, 5, 5, 4, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 5, 2, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1005, Accuracy: 0.4286, Precision: 0.5833, Recall: 0.3321, F1: 0.4162
Epoch 47/70
Train Loss: 0.1566, Accuracy: 0.9577, Precision: 0.8632, Recall: 0.8732, F1: 0.8674
Validation Loss: 0.8303, Accuracy: 0.8267, Precision: 0.6938, Recall: 0.5989, F1: 0.6308
Testing Loss: 0.8577, Accuracy: 0.8245, Precision: 0.6267, Recall: 0.5907, F1: 0.6039
LM Predictions:  [0, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8692, Accuracy: 0.5000, Precision: 0.5722, Recall: 0.4071, F1: 0.4749
Epoch 48/70
Train Loss: 0.1621, Accuracy: 0.9559, Precision: 0.8603, Recall: 0.8700, F1: 0.8638
Validation Loss: 0.8278, Accuracy: 0.8097, Precision: 0.6654, Recall: 0.6425, F1: 0.6253
Testing Loss: 0.8501, Accuracy: 0.8191, Precision: 0.5946, Recall: 0.6059, F1: 0.5976
LM Predictions:  [5, 5, 5, 2, 5, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7067, Accuracy: 0.4643, Precision: 0.6056, Recall: 0.3655, F1: 0.4545
Epoch 49/70
Train Loss: 0.1493, Accuracy: 0.9570, Precision: 0.8545, Recall: 0.8589, F1: 0.8561
Validation Loss: 0.7785, Accuracy: 0.8182, Precision: 0.6828, Recall: 0.7015, F1: 0.6601
Testing Loss: 0.8936, Accuracy: 0.8032, Precision: 0.5988, Recall: 0.5919, F1: 0.5919
LM Predictions:  [4, 5, 5, 4, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 1, 1, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7774, Accuracy: 0.4643, Precision: 0.6000, Recall: 0.3738, F1: 0.4484
Epoch 50/70
Train Loss: 0.1309, Accuracy: 0.9633, Precision: 0.8780, Recall: 0.8764, F1: 0.8769
Validation Loss: 0.8687, Accuracy: 0.8153, Precision: 0.6988, Recall: 0.6409, F1: 0.6382
Testing Loss: 0.9766, Accuracy: 0.7872, Precision: 0.5765, Recall: 0.5427, F1: 0.5536
LM Predictions:  [0, 5, 5, 4, 0, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9968, Accuracy: 0.4643, Precision: 0.5278, Recall: 0.3476, F1: 0.4175
Epoch 51/70
Train Loss: 0.1422, Accuracy: 0.9612, Precision: 0.8653, Recall: 0.8969, F1: 0.8778
Validation Loss: 0.8273, Accuracy: 0.8125, Precision: 0.6529, Recall: 0.6984, F1: 0.6415
Testing Loss: 0.9141, Accuracy: 0.7899, Precision: 0.5748, Recall: 0.5706, F1: 0.5708
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9342, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4310, F1: 0.5220
Epoch 52/70
Train Loss: 0.1345, Accuracy: 0.9633, Precision: 0.8763, Recall: 0.8907, F1: 0.8828
Validation Loss: 1.0034, Accuracy: 0.8239, Precision: 0.6650, Recall: 0.6000, F1: 0.6270
Testing Loss: 1.0649, Accuracy: 0.8165, Precision: 0.5917, Recall: 0.6037, F1: 0.5907
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6856, Accuracy: 0.5000, Precision: 0.6944, Recall: 0.4071, F1: 0.5043
Epoch 53/70
Train Loss: 0.1560, Accuracy: 0.9559, Precision: 0.8389, Recall: 0.8653, F1: 0.8509
Validation Loss: 0.9792, Accuracy: 0.8125, Precision: 0.7593, Recall: 0.6343, F1: 0.6530
Testing Loss: 1.0800, Accuracy: 0.8005, Precision: 0.6199, Recall: 0.5659, F1: 0.5725
LM Predictions:  [2, 5, 5, 2, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6270, Accuracy: 0.5357, Precision: 0.6746, Recall: 0.4310, F1: 0.5141
Epoch 54/70
Train Loss: 0.2407, Accuracy: 0.9356, Precision: 0.8502, Recall: 0.8448, F1: 0.8450
Validation Loss: 0.9070, Accuracy: 0.7812, Precision: 0.5907, Recall: 0.5643, F1: 0.5747
Testing Loss: 0.8611, Accuracy: 0.8218, Precision: 0.6006, Recall: 0.6067, F1: 0.6021
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7037, Accuracy: 0.4643, Precision: 0.6944, Recall: 0.3738, F1: 0.4805
Epoch 55/70
Train Loss: 0.1697, Accuracy: 0.9549, Precision: 0.8651, Recall: 0.8776, F1: 0.8697
Validation Loss: 0.9319, Accuracy: 0.8153, Precision: 0.6937, Recall: 0.6454, F1: 0.6447
Testing Loss: 0.8858, Accuracy: 0.8218, Precision: 0.6206, Recall: 0.6145, F1: 0.6131
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 5, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8804, Accuracy: 0.5000, Precision: 0.6944, Recall: 0.4071, F1: 0.5043
Epoch 56/70
Train Loss: 0.1575, Accuracy: 0.9622, Precision: 0.8860, Recall: 0.9197, F1: 0.9003
Validation Loss: 0.8626, Accuracy: 0.8125, Precision: 0.6522, Recall: 0.5803, F1: 0.6088
Testing Loss: 0.8594, Accuracy: 0.8218, Precision: 0.6166, Recall: 0.6169, F1: 0.6140
LM Predictions:  [5, 5, 5, 2, 1, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8675, Accuracy: 0.4643, Precision: 0.6389, Recall: 0.3738, F1: 0.4619
Epoch 57/70
Train Loss: 0.1392, Accuracy: 0.9612, Precision: 0.8703, Recall: 0.8895, F1: 0.8786
Validation Loss: 0.8006, Accuracy: 0.8153, Precision: 0.6828, Recall: 0.6422, F1: 0.6507
Testing Loss: 0.7849, Accuracy: 0.8218, Precision: 0.5966, Recall: 0.6124, F1: 0.5978
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8849, Accuracy: 0.5000, Precision: 0.6889, Recall: 0.3976, F1: 0.4841
Epoch 58/70
Train Loss: 0.1305, Accuracy: 0.9657, Precision: 0.8928, Recall: 0.9093, F1: 0.9000
Validation Loss: 0.9174, Accuracy: 0.8125, Precision: 0.6550, Recall: 0.5916, F1: 0.6145
Testing Loss: 0.8975, Accuracy: 0.8165, Precision: 0.6154, Recall: 0.6178, F1: 0.6080
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8466, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4310, F1: 0.5220
Epoch 59/70
Train Loss: 0.1053, Accuracy: 0.9710, Precision: 0.9046, Recall: 0.9127, F1: 0.9078
Validation Loss: 0.9831, Accuracy: 0.8182, Precision: 0.6707, Recall: 0.5910, F1: 0.6209
Testing Loss: 0.9132, Accuracy: 0.8271, Precision: 0.6261, Recall: 0.6199, F1: 0.6199
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7695, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4310, F1: 0.5220
Epoch 60/70
Train Loss: 0.1017, Accuracy: 0.9731, Precision: 0.9173, Recall: 0.9210, F1: 0.9186
Validation Loss: 1.1216, Accuracy: 0.7926, Precision: 0.6858, Recall: 0.6180, F1: 0.6228
Testing Loss: 1.0146, Accuracy: 0.8191, Precision: 0.6139, Recall: 0.6166, F1: 0.6051
LM Predictions:  [0, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 1, 4, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7820, Accuracy: 0.5357, Precision: 0.6222, Recall: 0.4310, F1: 0.5008
Epoch 61/70
Train Loss: 0.1295, Accuracy: 0.9689, Precision: 0.9167, Recall: 0.9126, F1: 0.9129
Validation Loss: 0.9628, Accuracy: 0.8125, Precision: 0.7246, Recall: 0.6365, F1: 0.6469
Testing Loss: 0.9789, Accuracy: 0.8165, Precision: 0.6390, Recall: 0.5998, F1: 0.6027
LM Predictions:  [0, 5, 5, 4, 0, 3, 3, 4, 0, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6541, Accuracy: 0.5357, Precision: 0.6667, Recall: 0.4226, F1: 0.5018
Epoch 62/70
Train Loss: 0.0981, Accuracy: 0.9769, Precision: 0.9295, Recall: 0.9394, F1: 0.9340
Validation Loss: 0.9802, Accuracy: 0.7983, Precision: 0.5986, Recall: 0.6017, F1: 0.5965
Testing Loss: 0.9225, Accuracy: 0.8138, Precision: 0.5941, Recall: 0.6211, F1: 0.6013
LM Predictions:  [0, 5, 5, 4, 1, 3, 3, 4, 5, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 5, 5, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0026, Accuracy: 0.5000, Precision: 0.6944, Recall: 0.3893, F1: 0.4942
Epoch 63/70
Train Loss: 0.1123, Accuracy: 0.9731, Precision: 0.9111, Recall: 0.9135, F1: 0.9115
Validation Loss: 1.0623, Accuracy: 0.8182, Precision: 0.7209, Recall: 0.6253, F1: 0.6387
Testing Loss: 1.1368, Accuracy: 0.8085, Precision: 0.6007, Recall: 0.5558, F1: 0.5589
LM Predictions:  [0, 5, 5, 4, 1, 3, 3, 4, 3, 0, 5, 0, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7013, Accuracy: 0.5714, Precision: 0.6278, Recall: 0.4643, F1: 0.5280
Epoch 64/70
Train Loss: 0.0992, Accuracy: 0.9752, Precision: 0.9137, Recall: 0.9200, F1: 0.9163
Validation Loss: 0.9037, Accuracy: 0.7955, Precision: 0.6271, Recall: 0.7113, F1: 0.6291
Testing Loss: 0.9442, Accuracy: 0.7793, Precision: 0.5771, Recall: 0.6043, F1: 0.5704
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 5, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7146, Accuracy: 0.5357, Precision: 0.7500, Recall: 0.4310, F1: 0.5339
Epoch 65/70
Train Loss: 0.0997, Accuracy: 0.9755, Precision: 0.9072, Recall: 0.9367, F1: 0.9206
Validation Loss: 0.8356, Accuracy: 0.8352, Precision: 0.7292, Recall: 0.6661, F1: 0.6707
Testing Loss: 0.9457, Accuracy: 0.8324, Precision: 0.6124, Recall: 0.6272, F1: 0.6159
LM Predictions:  [0, 5, 5, 4, 0, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7588, Accuracy: 0.5357, Precision: 0.6833, Recall: 0.4226, F1: 0.5008
Epoch 66/70
Train Loss: 0.1039, Accuracy: 0.9724, Precision: 0.9092, Recall: 0.9331, F1: 0.9204
Validation Loss: 0.8533, Accuracy: 0.8267, Precision: 0.6737, Recall: 0.6085, F1: 0.6328
Testing Loss: 0.8760, Accuracy: 0.8324, Precision: 0.6306, Recall: 0.6367, F1: 0.6271
LM Predictions:  [1, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6637, Accuracy: 0.5714, Precision: 0.6944, Recall: 0.4643, F1: 0.5518
Epoch 67/70
Train Loss: 0.1066, Accuracy: 0.9720, Precision: 0.9152, Recall: 0.9204, F1: 0.9171
Validation Loss: 0.9061, Accuracy: 0.8295, Precision: 0.6986, Recall: 0.6056, F1: 0.6407
Testing Loss: 0.9902, Accuracy: 0.8059, Precision: 0.5998, Recall: 0.5883, F1: 0.5870
LM Predictions:  [1, 5, 5, 4, 1, 3, 3, 4, 3, 5, 5, 0, 2, 4, 4, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8141, Accuracy: 0.5357, Precision: 0.6333, Recall: 0.4310, F1: 0.5012
Epoch 68/70
Train Loss: 0.1013, Accuracy: 0.9734, Precision: 0.9158, Recall: 0.9156, F1: 0.9149
Validation Loss: 0.9701, Accuracy: 0.8267, Precision: 0.6765, Recall: 0.6098, F1: 0.6366
Testing Loss: 1.0445, Accuracy: 0.8218, Precision: 0.6186, Recall: 0.6211, F1: 0.6164
LM Predictions:  [5, 5, 5, 4, 1, 3, 3, 4, 1, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8645, Accuracy: 0.5357, Precision: 0.6889, Recall: 0.4310, F1: 0.5238
Epoch 69/70
Train Loss: 0.1173, Accuracy: 0.9675, Precision: 0.9071, Recall: 0.9126, F1: 0.9088
Validation Loss: 1.0332, Accuracy: 0.8097, Precision: 0.6624, Recall: 0.5806, F1: 0.6061
Testing Loss: 1.1665, Accuracy: 0.7872, Precision: 0.5612, Recall: 0.5611, F1: 0.5593
LM Predictions:  [4, 5, 5, 4, 1, 3, 3, 4, 4, 5, 5, 4, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 2, 4, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1599, Accuracy: 0.4643, Precision: 0.5037, Recall: 0.3643, F1: 0.4058
Epoch 70/70
Train Loss: 0.1379, Accuracy: 0.9626, Precision: 0.8939, Recall: 0.9099, F1: 0.9015
Validation Loss: 0.9449, Accuracy: 0.8125, Precision: 0.6270, Recall: 0.5604, F1: 0.5856
Testing Loss: 1.0514, Accuracy: 0.8085, Precision: 0.5891, Recall: 0.5730, F1: 0.5785
LM Predictions:  [4, 5, 5, 4, 1, 3, 3, 4, 5, 5, 5, 0, 2, 4, 5, 4, 5, 5, 5, 5, 4, 2, 1, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 4, 1, 3, 3, 4, 2, 1, 4, 0, 2, 4, 3, 4, 3, 3, 1, 4, 4, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9533, Accuracy: 0.5357, Precision: 0.7167, Recall: 0.4310, F1: 0.5250
Label Memorization Analysis: 
LM Loss: 1.9533, Accuracy: 0.5357, Precision: 0.7167, Recall: 0.4310, F1: 0.5250
---------------------------------------------------------------------------



