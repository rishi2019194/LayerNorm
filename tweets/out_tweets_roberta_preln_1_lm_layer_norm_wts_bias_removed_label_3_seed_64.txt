---------------------------------------------------------------------------
Results for seed:  89
Model: andreasmadsen/efficient_mlm_m0.40, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 974
  Label 2: 1105
  Label 5: 490
  Label 1: 117
  Label 0: 56
  Label 3: 116
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4224, Accuracy: 0.3677, Precision: 0.1568, Recall: 0.1661, F1: 0.1391
Validation Loss: 1.4263, Accuracy: 0.3494, Precision: 0.1256, Recall: 0.1678, F1: 0.1339
Testing Loss: 1.4466, Accuracy: 0.3936, Precision: 0.1377, Recall: 0.1842, F1: 0.1480
LM Predictions:  [4, 4, 2, 4, 2, 4, 2, 4, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 4, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1771, Accuracy: 0.3214, Precision: 0.1200, Recall: 0.2417, F1: 0.1571
Epoch 2/70
Train Loss: 1.3854, Accuracy: 0.3831, Precision: 0.2094, Recall: 0.1730, F1: 0.1449
Validation Loss: 1.3889, Accuracy: 0.4489, Precision: 0.1909, Recall: 0.2026, F1: 0.1559
Testing Loss: 1.4117, Accuracy: 0.4229, Precision: 0.1649, Recall: 0.1957, F1: 0.1467
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1964, Accuracy: 0.2500, Precision: 0.2444, Recall: 0.2250, F1: 0.1172
Epoch 3/70
Train Loss: 1.3683, Accuracy: 0.3978, Precision: 0.2001, Recall: 0.1800, F1: 0.1520
Validation Loss: 1.3709, Accuracy: 0.4716, Precision: 0.1667, Recall: 0.2159, F1: 0.1762
Testing Loss: 1.3956, Accuracy: 0.4441, Precision: 0.1637, Recall: 0.2060, F1: 0.1670
LM Predictions:  [2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1186, Accuracy: 0.2500, Precision: 0.1147, Recall: 0.2250, F1: 0.1138
Epoch 4/70
Train Loss: 1.3606, Accuracy: 0.4171, Precision: 0.1975, Recall: 0.1901, F1: 0.1634
Validation Loss: 1.3830, Accuracy: 0.4034, Precision: 0.2014, Recall: 0.1795, F1: 0.1170
Testing Loss: 1.4036, Accuracy: 0.4069, Precision: 0.2132, Recall: 0.1880, F1: 0.1307
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1513, Accuracy: 0.2500, Precision: 0.2444, Recall: 0.2250, F1: 0.1172
Epoch 5/70
Train Loss: 1.3346, Accuracy: 0.4360, Precision: 0.2019, Recall: 0.1990, F1: 0.1716
Validation Loss: 1.3154, Accuracy: 0.4517, Precision: 0.1502, Recall: 0.2106, F1: 0.1751
Testing Loss: 1.3130, Accuracy: 0.4734, Precision: 0.2412, Recall: 0.2220, F1: 0.1886
LM Predictions:  [2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 4, 4, 2, 2, 2, 2, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1187, Accuracy: 0.3214, Precision: 0.1303, Recall: 0.2583, F1: 0.1714
Epoch 6/70
Train Loss: 1.3053, Accuracy: 0.4857, Precision: 0.2432, Recall: 0.2278, F1: 0.2063
Validation Loss: 1.2291, Accuracy: 0.5312, Precision: 0.2782, Recall: 0.2553, F1: 0.2347
Testing Loss: 1.2536, Accuracy: 0.5532, Precision: 0.2714, Recall: 0.2701, F1: 0.2474
LM Predictions:  [4, 2, 2, 5, 2, 4, 2, 2, 5, 2, 5, 2, 2, 2, 2, 2, 2, 4, 4, 2, 2, 2, 5, 2, 2, 2, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2825, Accuracy: 0.2857, Precision: 0.2221, Recall: 0.2833, F1: 0.2196
Epoch 7/70
Train Loss: 1.2335, Accuracy: 0.5409, Precision: 0.2504, Recall: 0.2567, F1: 0.2370
Validation Loss: 1.1623, Accuracy: 0.5739, Precision: 0.2713, Recall: 0.2890, F1: 0.2709
Testing Loss: 1.1030, Accuracy: 0.6223, Precision: 0.2899, Recall: 0.3176, F1: 0.2976
LM Predictions:  [4, 4, 4, 5, 2, 4, 2, 5, 4, 2, 5, 2, 2, 4, 2, 2, 5, 4, 4, 5, 4, 2, 4, 2, 2, 2, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2359, Accuracy: 0.2500, Precision: 0.1553, Recall: 0.2333, F1: 0.1851
Epoch 8/70
Train Loss: 1.1904, Accuracy: 0.5693, Precision: 0.2637, Recall: 0.2715, F1: 0.2516
Validation Loss: 1.0961, Accuracy: 0.5824, Precision: 0.2799, Recall: 0.3175, F1: 0.2959
Testing Loss: 1.0782, Accuracy: 0.6383, Precision: 0.3065, Recall: 0.3546, F1: 0.3252
LM Predictions:  [5, 5, 2, 5, 2, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 2, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3974, Accuracy: 0.2857, Precision: 0.2375, Recall: 0.3000, F1: 0.2127
Epoch 9/70
Train Loss: 1.1215, Accuracy: 0.5959, Precision: 0.2765, Recall: 0.2919, F1: 0.2761
Validation Loss: 1.0470, Accuracy: 0.6080, Precision: 0.2824, Recall: 0.3153, F1: 0.2967
Testing Loss: 1.0451, Accuracy: 0.6489, Precision: 0.3123, Recall: 0.3447, F1: 0.3251
LM Predictions:  [5, 2, 2, 2, 2, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 2, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4890, Accuracy: 0.2143, Precision: 0.1497, Recall: 0.2167, F1: 0.1586
Epoch 10/70
Train Loss: 1.0945, Accuracy: 0.6151, Precision: 0.2888, Recall: 0.3038, F1: 0.2887
Validation Loss: 1.1273, Accuracy: 0.5767, Precision: 0.2398, Recall: 0.2778, F1: 0.2499
Testing Loss: 1.0911, Accuracy: 0.6356, Precision: 0.3061, Recall: 0.3237, F1: 0.3050
LM Predictions:  [4, 2, 2, 2, 2, 2, 2, 5, 2, 2, 5, 2, 2, 2, 2, 2, 5, 4, 4, 2, 4, 2, 2, 2, 5, 2, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3641, Accuracy: 0.1786, Precision: 0.1611, Recall: 0.1667, F1: 0.1431
Epoch 11/70
Train Loss: 1.0572, Accuracy: 0.6123, Precision: 0.2861, Recall: 0.3056, F1: 0.2909
Validation Loss: 0.9890, Accuracy: 0.6364, Precision: 0.2996, Recall: 0.3248, F1: 0.3057
Testing Loss: 0.9820, Accuracy: 0.6543, Precision: 0.3058, Recall: 0.3441, F1: 0.3227
LM Predictions:  [5, 2, 4, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 2, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3179, Accuracy: 0.2500, Precision: 0.1700, Recall: 0.2500, F1: 0.1853
Epoch 12/70
Train Loss: 1.0747, Accuracy: 0.6099, Precision: 0.2819, Recall: 0.2980, F1: 0.2816
Validation Loss: 1.0171, Accuracy: 0.6364, Precision: 0.3373, Recall: 0.3159, F1: 0.3016
Testing Loss: 1.0257, Accuracy: 0.6543, Precision: 0.3343, Recall: 0.3242, F1: 0.3049
LM Predictions:  [2, 2, 4, 5, 2, 2, 2, 5, 2, 2, 2, 5, 2, 2, 2, 4, 5, 2, 4, 5, 4, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4237, Accuracy: 0.2143, Precision: 0.1616, Recall: 0.2250, F1: 0.1702
Epoch 13/70
Train Loss: 1.0298, Accuracy: 0.6340, Precision: 0.3032, Recall: 0.3142, F1: 0.3000
Validation Loss: 1.0024, Accuracy: 0.6591, Precision: 0.3188, Recall: 0.3372, F1: 0.3188
Testing Loss: 1.0332, Accuracy: 0.6702, Precision: 0.3236, Recall: 0.3449, F1: 0.3258
LM Predictions:  [5, 2, 4, 5, 2, 2, 2, 5, 2, 2, 2, 2, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 2, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3879, Accuracy: 0.2500, Precision: 0.1638, Recall: 0.2500, F1: 0.1870
Epoch 14/70
Train Loss: 1.0019, Accuracy: 0.6372, Precision: 0.2998, Recall: 0.3183, F1: 0.3037
Validation Loss: 0.9910, Accuracy: 0.6648, Precision: 0.3296, Recall: 0.3449, F1: 0.3303
Testing Loss: 1.0238, Accuracy: 0.6596, Precision: 0.3202, Recall: 0.3448, F1: 0.3265
LM Predictions:  [5, 5, 4, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 2, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5544, Accuracy: 0.2500, Precision: 0.1679, Recall: 0.2667, F1: 0.1860
Epoch 15/70
Train Loss: 1.0050, Accuracy: 0.6428, Precision: 0.3016, Recall: 0.3230, F1: 0.3083
Validation Loss: 1.0263, Accuracy: 0.6705, Precision: 0.3315, Recall: 0.3490, F1: 0.3341
Testing Loss: 1.0394, Accuracy: 0.6755, Precision: 0.3225, Recall: 0.3539, F1: 0.3338
LM Predictions:  [5, 5, 2, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 5, 2, 4, 5, 4, 4, 5, 2, 2, 2, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4805, Accuracy: 0.2857, Precision: 0.1782, Recall: 0.3083, F1: 0.1956
Epoch 16/70
Train Loss: 0.9856, Accuracy: 0.6491, Precision: 0.3048, Recall: 0.3293, F1: 0.3144
Validation Loss: 0.9770, Accuracy: 0.6591, Precision: 0.3220, Recall: 0.3367, F1: 0.3202
Testing Loss: 0.9969, Accuracy: 0.6596, Precision: 0.3159, Recall: 0.3399, F1: 0.3209
LM Predictions:  [5, 2, 4, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1976, Accuracy: 0.2143, Precision: 0.1567, Recall: 0.2167, F1: 0.1663
Epoch 17/70
Train Loss: 0.9889, Accuracy: 0.6508, Precision: 0.3092, Recall: 0.3314, F1: 0.3171
Validation Loss: 0.9321, Accuracy: 0.6705, Precision: 0.3181, Recall: 0.3576, F1: 0.3362
Testing Loss: 0.9976, Accuracy: 0.6649, Precision: 0.3115, Recall: 0.3523, F1: 0.3299
LM Predictions:  [4, 5, 4, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2343, Accuracy: 0.2500, Precision: 0.1603, Recall: 0.2583, F1: 0.1850
Epoch 18/70
Train Loss: 0.9604, Accuracy: 0.6568, Precision: 0.3098, Recall: 0.3374, F1: 0.3218
Validation Loss: 0.9884, Accuracy: 0.6534, Precision: 0.3150, Recall: 0.3237, F1: 0.3020
Testing Loss: 1.0112, Accuracy: 0.6463, Precision: 0.3095, Recall: 0.3255, F1: 0.3057
LM Predictions:  [5, 2, 2, 2, 2, 2, 2, 5, 2, 2, 5, 2, 2, 4, 2, 4, 5, 2, 4, 5, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2683, Accuracy: 0.2143, Precision: 0.1733, Recall: 0.1833, F1: 0.1343
Epoch 19/70
Train Loss: 0.9779, Accuracy: 0.6484, Precision: 0.3079, Recall: 0.3336, F1: 0.3187
Validation Loss: 0.9713, Accuracy: 0.6534, Precision: 0.3098, Recall: 0.3533, F1: 0.3298
Testing Loss: 0.9885, Accuracy: 0.6676, Precision: 0.3171, Recall: 0.3652, F1: 0.3391
LM Predictions:  [4, 4, 4, 5, 2, 5, 2, 5, 2, 4, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1774, Accuracy: 0.3214, Precision: 0.1990, Recall: 0.3167, F1: 0.2333
Epoch 20/70
Train Loss: 0.9320, Accuracy: 0.6728, Precision: 0.3206, Recall: 0.3528, F1: 0.3354
Validation Loss: 0.9247, Accuracy: 0.6875, Precision: 0.3307, Recall: 0.3602, F1: 0.3422
Testing Loss: 0.9848, Accuracy: 0.6862, Precision: 0.3222, Recall: 0.3671, F1: 0.3426
LM Predictions:  [4, 2, 2, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2839, Accuracy: 0.2857, Precision: 0.1857, Recall: 0.2917, F1: 0.2168
Epoch 21/70
Train Loss: 0.8978, Accuracy: 0.6746, Precision: 0.3201, Recall: 0.3519, F1: 0.3347
Validation Loss: 0.9935, Accuracy: 0.6562, Precision: 0.3267, Recall: 0.3418, F1: 0.3279
Testing Loss: 1.0407, Accuracy: 0.6835, Precision: 0.3329, Recall: 0.3756, F1: 0.3498
LM Predictions:  [5, 5, 2, 5, 2, 5, 2, 5, 2, 2, 5, 5, 2, 5, 2, 5, 5, 2, 4, 5, 2, 2, 2, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5290, Accuracy: 0.2857, Precision: 0.3044, Recall: 0.3083, F1: 0.1953
Epoch 22/70
Train Loss: 0.8937, Accuracy: 0.6851, Precision: 0.3262, Recall: 0.3590, F1: 0.3412
Validation Loss: 0.9634, Accuracy: 0.6591, Precision: 0.3210, Recall: 0.3575, F1: 0.3359
Testing Loss: 0.9218, Accuracy: 0.6915, Precision: 0.3298, Recall: 0.3827, F1: 0.3526
LM Predictions:  [4, 5, 2, 5, 2, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 2, 2, 4, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2894, Accuracy: 0.3214, Precision: 0.2350, Recall: 0.3250, F1: 0.2412
Epoch 23/70
Train Loss: 0.9051, Accuracy: 0.6784, Precision: 0.3233, Recall: 0.3580, F1: 0.3394
Validation Loss: 0.9447, Accuracy: 0.6761, Precision: 0.3172, Recall: 0.3533, F1: 0.3323
Testing Loss: 0.9692, Accuracy: 0.6915, Precision: 0.3282, Recall: 0.3746, F1: 0.3497
LM Predictions:  [4, 4, 4, 5, 2, 5, 2, 5, 2, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2296, Accuracy: 0.2857, Precision: 0.1711, Recall: 0.2833, F1: 0.2065
Epoch 24/70
Train Loss: 0.8853, Accuracy: 0.6928, Precision: 0.3302, Recall: 0.3641, F1: 0.3461
Validation Loss: 0.9540, Accuracy: 0.6648, Precision: 0.3134, Recall: 0.3432, F1: 0.3251
Testing Loss: 0.9464, Accuracy: 0.6835, Precision: 0.3262, Recall: 0.3675, F1: 0.3448
LM Predictions:  [4, 5, 4, 5, 2, 5, 2, 5, 2, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2350, Accuracy: 0.2143, Precision: 0.1336, Recall: 0.2083, F1: 0.1550
Epoch 25/70
Train Loss: 0.8717, Accuracy: 0.6938, Precision: 0.3293, Recall: 0.3646, F1: 0.3459
Validation Loss: 0.9439, Accuracy: 0.6392, Precision: 0.2980, Recall: 0.3415, F1: 0.3178
Testing Loss: 0.9261, Accuracy: 0.6888, Precision: 0.3276, Recall: 0.3816, F1: 0.3520
LM Predictions:  [4, 4, 4, 5, 4, 5, 5, 5, 5, 4, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 4, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2214, Accuracy: 0.3214, Precision: 0.1929, Recall: 0.3083, F1: 0.2178
Epoch 26/70
Train Loss: 0.8895, Accuracy: 0.6851, Precision: 0.3250, Recall: 0.3599, F1: 0.3412
Validation Loss: 1.0325, Accuracy: 0.6335, Precision: 0.3032, Recall: 0.3279, F1: 0.3083
Testing Loss: 0.9870, Accuracy: 0.6835, Precision: 0.3241, Recall: 0.3628, F1: 0.3407
LM Predictions:  [4, 4, 5, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 4, 4, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2421, Accuracy: 0.3214, Precision: 0.1978, Recall: 0.3083, F1: 0.2350
Epoch 27/70
Train Loss: 0.8830, Accuracy: 0.6903, Precision: 0.3295, Recall: 0.3654, F1: 0.3463
Validation Loss: 1.1262, Accuracy: 0.5938, Precision: 0.2891, Recall: 0.3031, F1: 0.2849
Testing Loss: 1.0968, Accuracy: 0.6277, Precision: 0.2833, Recall: 0.3188, F1: 0.2968
LM Predictions:  [4, 4, 4, 5, 2, 2, 2, 5, 2, 2, 5, 5, 2, 4, 2, 4, 4, 5, 4, 5, 4, 2, 4, 2, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0338, Accuracy: 0.2857, Precision: 0.1700, Recall: 0.2583, F1: 0.2028
Epoch 28/70
Train Loss: 0.9597, Accuracy: 0.6529, Precision: 0.3102, Recall: 0.3416, F1: 0.3248
Validation Loss: 1.1889, Accuracy: 0.5625, Precision: 0.2980, Recall: 0.2780, F1: 0.2514
Testing Loss: 1.2023, Accuracy: 0.5931, Precision: 0.2591, Recall: 0.2865, F1: 0.2563
LM Predictions:  [4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 4, 4, 5, 2, 4, 4, 4, 4, 2, 4, 4, 2, 4, 2, 5, 4, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0460, Accuracy: 0.2500, Precision: 0.1056, Recall: 0.1917, F1: 0.1341
Epoch 29/70
Train Loss: 0.8674, Accuracy: 0.6973, Precision: 0.4993, Recall: 0.3698, F1: 0.3522
Validation Loss: 0.8955, Accuracy: 0.6989, Precision: 0.3309, Recall: 0.3554, F1: 0.3372
Testing Loss: 0.9259, Accuracy: 0.6995, Precision: 0.3404, Recall: 0.3732, F1: 0.3530
LM Predictions:  [5, 5, 2, 5, 2, 2, 2, 2, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 2, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1858, Accuracy: 0.3214, Precision: 0.2012, Recall: 0.3333, F1: 0.2304
Epoch 30/70
Train Loss: 0.8555, Accuracy: 0.6928, Precision: 0.3324, Recall: 0.3689, F1: 0.3491
Validation Loss: 0.9418, Accuracy: 0.6733, Precision: 0.3300, Recall: 0.3566, F1: 0.3393
Testing Loss: 0.9557, Accuracy: 0.6968, Precision: 0.3391, Recall: 0.3834, F1: 0.3564
LM Predictions:  [4, 5, 5, 5, 2, 2, 2, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 2, 5, 5, 2, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4358, Accuracy: 0.2500, Precision: 0.2764, Recall: 0.2667, F1: 0.1902
Epoch 31/70
Train Loss: 0.8768, Accuracy: 0.6907, Precision: 0.3307, Recall: 0.3684, F1: 0.3479
Validation Loss: 0.9983, Accuracy: 0.6193, Precision: 0.3004, Recall: 0.3157, F1: 0.2950
Testing Loss: 0.9849, Accuracy: 0.6968, Precision: 0.3434, Recall: 0.3624, F1: 0.3435
LM Predictions:  [4, 2, 2, 5, 4, 2, 2, 4, 2, 2, 4, 5, 2, 4, 4, 4, 5, 4, 4, 2, 4, 2, 4, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1380, Accuracy: 0.2857, Precision: 0.1861, Recall: 0.2667, F1: 0.2175
Epoch 32/70
Train Loss: 0.8644, Accuracy: 0.6991, Precision: 0.3351, Recall: 0.3713, F1: 0.3518
Validation Loss: 0.8974, Accuracy: 0.6932, Precision: 0.3298, Recall: 0.3646, F1: 0.3449
Testing Loss: 0.9103, Accuracy: 0.7074, Precision: 0.3356, Recall: 0.3819, F1: 0.3565
LM Predictions:  [4, 5, 5, 5, 2, 5, 2, 5, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3232, Accuracy: 0.2500, Precision: 0.1686, Recall: 0.2583, F1: 0.1796
Epoch 33/70
Train Loss: 0.8077, Accuracy: 0.7131, Precision: 0.5076, Recall: 0.3810, F1: 0.3616
Validation Loss: 1.0008, Accuracy: 0.6733, Precision: 0.3264, Recall: 0.3378, F1: 0.3216
Testing Loss: 0.9739, Accuracy: 0.6995, Precision: 0.3385, Recall: 0.3666, F1: 0.3467
LM Predictions:  [4, 2, 2, 5, 2, 2, 2, 2, 5, 2, 5, 5, 2, 4, 2, 4, 5, 2, 4, 5, 2, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3178, Accuracy: 0.3571, Precision: 0.2671, Recall: 0.3583, F1: 0.2657
Epoch 34/70
Train Loss: 0.8004, Accuracy: 0.7159, Precision: 0.3447, Recall: 0.3839, F1: 0.3627
Validation Loss: 0.8764, Accuracy: 0.6591, Precision: 0.3058, Recall: 0.3457, F1: 0.3238
Testing Loss: 0.9054, Accuracy: 0.6915, Precision: 0.4916, Recall: 0.3808, F1: 0.3591
LM Predictions:  [4, 2, 4, 5, 4, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2703, Accuracy: 0.2857, Precision: 0.1684, Recall: 0.2833, F1: 0.1980
Epoch 35/70
Train Loss: 0.7990, Accuracy: 0.7113, Precision: 0.3761, Recall: 0.3827, F1: 0.3653
Validation Loss: 0.8242, Accuracy: 0.6960, Precision: 0.3419, Recall: 0.3811, F1: 0.3573
Testing Loss: 0.8709, Accuracy: 0.6995, Precision: 0.3387, Recall: 0.3962, F1: 0.3604
LM Predictions:  [5, 5, 5, 5, 2, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 5, 4, 5, 4, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3350, Accuracy: 0.2143, Precision: 0.1982, Recall: 0.2333, F1: 0.1582
Epoch 36/70
Train Loss: 0.7958, Accuracy: 0.7138, Precision: 0.3897, Recall: 0.3872, F1: 0.3726
Validation Loss: 0.8927, Accuracy: 0.6989, Precision: 0.3446, Recall: 0.3635, F1: 0.3482
Testing Loss: 0.8959, Accuracy: 0.7048, Precision: 0.3406, Recall: 0.3855, F1: 0.3594
LM Predictions:  [3, 5, 5, 5, 4, 2, 5, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 2, 4, 5, 4, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3776, Accuracy: 0.2500, Precision: 0.1801, Recall: 0.2153, F1: 0.1590
Epoch 37/70
Train Loss: 0.8012, Accuracy: 0.7117, Precision: 0.5806, Recall: 0.3870, F1: 0.3733
Validation Loss: 0.8204, Accuracy: 0.6989, Precision: 0.3357, Recall: 0.3796, F1: 0.3557
Testing Loss: 0.8719, Accuracy: 0.7021, Precision: 0.3362, Recall: 0.3893, F1: 0.3587
LM Predictions:  [4, 5, 5, 5, 2, 5, 5, 5, 5, 2, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 4, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2989, Accuracy: 0.2500, Precision: 0.1708, Recall: 0.2583, F1: 0.1790
Epoch 38/70
Train Loss: 0.7769, Accuracy: 0.7222, Precision: 0.4304, Recall: 0.3941, F1: 0.3819
Validation Loss: 0.8726, Accuracy: 0.6818, Precision: 0.4095, Recall: 0.3815, F1: 0.3632
Testing Loss: 0.8937, Accuracy: 0.6968, Precision: 0.4315, Recall: 0.3989, F1: 0.3828
LM Predictions:  [4, 5, 4, 3, 4, 3, 5, 5, 5, 3, 5, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 4, 5, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2273, Accuracy: 0.2143, Precision: 0.1296, Recall: 0.1528, F1: 0.1326
Epoch 39/70
Train Loss: 0.7717, Accuracy: 0.7271, Precision: 0.4999, Recall: 0.4087, F1: 0.4033
Validation Loss: 0.9250, Accuracy: 0.6847, Precision: 0.3410, Recall: 0.3518, F1: 0.3369
Testing Loss: 0.9285, Accuracy: 0.7101, Precision: 0.4227, Recall: 0.3947, F1: 0.3916
LM Predictions:  [4, 5, 4, 3, 2, 5, 2, 2, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 2, 5, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2255, Accuracy: 0.2500, Precision: 0.1423, Recall: 0.1875, F1: 0.1569
Epoch 40/70
Train Loss: 0.7373, Accuracy: 0.7365, Precision: 0.4773, Recall: 0.4190, F1: 0.4151
Validation Loss: 0.8731, Accuracy: 0.7045, Precision: 0.3421, Recall: 0.3833, F1: 0.3610
Testing Loss: 0.8805, Accuracy: 0.7021, Precision: 0.3906, Recall: 0.3923, F1: 0.3685
LM Predictions:  [4, 2, 2, 3, 2, 3, 5, 5, 5, 2, 5, 5, 2, 4, 2, 4, 5, 3, 4, 5, 4, 2, 5, 5, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2719, Accuracy: 0.2500, Precision: 0.1694, Recall: 0.2014, F1: 0.1662
Epoch 41/70
Train Loss: 0.7352, Accuracy: 0.7334, Precision: 0.4772, Recall: 0.4189, F1: 0.4155
Validation Loss: 0.8923, Accuracy: 0.7074, Precision: 0.3867, Recall: 0.3896, F1: 0.3763
Testing Loss: 0.9406, Accuracy: 0.7234, Precision: 0.3986, Recall: 0.4034, F1: 0.3887
LM Predictions:  [4, 2, 2, 3, 2, 3, 2, 5, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 5, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3062, Accuracy: 0.2500, Precision: 0.1455, Recall: 0.2014, F1: 0.1624
Epoch 42/70
Train Loss: 0.7355, Accuracy: 0.7442, Precision: 0.5111, Recall: 0.4391, F1: 0.4407
Validation Loss: 0.9540, Accuracy: 0.6790, Precision: 0.4363, Recall: 0.3771, F1: 0.3782
Testing Loss: 0.9383, Accuracy: 0.7314, Precision: 0.4467, Recall: 0.4127, F1: 0.4057
LM Predictions:  [4, 5, 2, 1, 2, 3, 2, 5, 5, 3, 5, 5, 2, 4, 2, 4, 5, 5, 4, 5, 4, 2, 3, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3548, Accuracy: 0.2500, Precision: 0.1704, Recall: 0.2014, F1: 0.1690
Epoch 43/70
Train Loss: 0.7416, Accuracy: 0.7400, Precision: 0.4851, Recall: 0.4433, F1: 0.4475
Validation Loss: 0.8415, Accuracy: 0.7102, Precision: 0.3767, Recall: 0.3903, F1: 0.3787
Testing Loss: 0.8638, Accuracy: 0.7154, Precision: 0.4466, Recall: 0.4220, F1: 0.4194
LM Predictions:  [4, 5, 2, 3, 2, 3, 5, 5, 5, 3, 5, 5, 2, 4, 2, 4, 5, 3, 5, 5, 4, 2, 3, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4471, Accuracy: 0.2143, Precision: 0.1613, Recall: 0.1806, F1: 0.1513
Epoch 44/70
Train Loss: 0.7143, Accuracy: 0.7362, Precision: 0.5157, Recall: 0.4455, F1: 0.4511
Validation Loss: 0.8387, Accuracy: 0.7074, Precision: 0.4800, Recall: 0.4259, F1: 0.4200
Testing Loss: 0.8417, Accuracy: 0.7234, Precision: 0.4276, Recall: 0.4468, F1: 0.4359
LM Predictions:  [4, 5, 3, 3, 2, 3, 5, 5, 5, 2, 5, 3, 2, 2, 2, 4, 3, 3, 3, 5, 4, 2, 3, 2, 2, 5, 3, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.6226, Accuracy: 0.0714, Precision: 0.0741, Recall: 0.0486, F1: 0.0525
Epoch 45/70
Train Loss: 0.7340, Accuracy: 0.7421, Precision: 0.5126, Recall: 0.4529, F1: 0.4580
Validation Loss: 0.9064, Accuracy: 0.6733, Precision: 0.5331, Recall: 0.3754, F1: 0.3735
Testing Loss: 0.8861, Accuracy: 0.7074, Precision: 0.4987, Recall: 0.4185, F1: 0.4262
LM Predictions:  [4, 5, 2, 3, 2, 3, 5, 2, 5, 2, 3, 3, 2, 2, 2, 4, 3, 3, 3, 5, 4, 2, 3, 3, 2, 3, 3, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3855, Accuracy: 0.1429, Precision: 0.1056, Recall: 0.1042, F1: 0.0928
Epoch 46/70
Train Loss: 0.7144, Accuracy: 0.7526, Precision: 0.5046, Recall: 0.4577, F1: 0.4596
Validation Loss: 0.8623, Accuracy: 0.7216, Precision: 0.5477, Recall: 0.4138, F1: 0.4153
Testing Loss: 0.8939, Accuracy: 0.7154, Precision: 0.4221, Recall: 0.4149, F1: 0.4068
LM Predictions:  [4, 5, 2, 3, 2, 3, 5, 5, 5, 3, 5, 4, 2, 2, 2, 4, 3, 3, 4, 5, 4, 2, 4, 5, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2846, Accuracy: 0.1429, Precision: 0.1032, Recall: 0.0972, F1: 0.0989
Epoch 47/70
Train Loss: 0.6902, Accuracy: 0.7558, Precision: 0.5609, Recall: 0.4714, F1: 0.4765
Validation Loss: 0.8830, Accuracy: 0.7301, Precision: 0.5354, Recall: 0.4313, F1: 0.4407
Testing Loss: 0.8958, Accuracy: 0.7207, Precision: 0.5008, Recall: 0.4374, F1: 0.4484
LM Predictions:  [4, 5, 3, 3, 2, 3, 2, 2, 5, 2, 5, 4, 2, 4, 2, 4, 3, 3, 5, 5, 4, 2, 4, 5, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3621, Accuracy: 0.1429, Precision: 0.0972, Recall: 0.0972, F1: 0.0952
Epoch 48/70
Train Loss: 0.7144, Accuracy: 0.7351, Precision: 0.4842, Recall: 0.4495, F1: 0.4523
Validation Loss: 0.8920, Accuracy: 0.7159, Precision: 0.5589, Recall: 0.4138, F1: 0.4188
Testing Loss: 0.8963, Accuracy: 0.7207, Precision: 0.6158, Recall: 0.4338, F1: 0.4443
LM Predictions:  [4, 5, 3, 3, 4, 3, 5, 5, 5, 3, 5, 4, 3, 4, 2, 4, 3, 3, 3, 5, 4, 2, 4, 5, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3095, Accuracy: 0.1429, Precision: 0.1270, Recall: 0.0903, F1: 0.1037
Epoch 49/70
Train Loss: 0.7001, Accuracy: 0.7519, Precision: 0.7066, Recall: 0.4734, F1: 0.4828
Validation Loss: 0.9196, Accuracy: 0.7131, Precision: 0.4615, Recall: 0.3945, F1: 0.3891
Testing Loss: 0.9093, Accuracy: 0.7234, Precision: 0.4501, Recall: 0.4262, F1: 0.4196
LM Predictions:  [4, 5, 2, 1, 2, 3, 5, 5, 5, 2, 5, 5, 2, 4, 2, 4, 5, 5, 5, 5, 2, 2, 5, 5, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2167, Accuracy: 0.2500, Precision: 0.1905, Recall: 0.2083, F1: 0.1643
Epoch 50/70
Train Loss: 0.6934, Accuracy: 0.7512, Precision: 0.5159, Recall: 0.4604, F1: 0.4603
Validation Loss: 0.8315, Accuracy: 0.7188, Precision: 0.4819, Recall: 0.4279, F1: 0.4309
Testing Loss: 0.8408, Accuracy: 0.7473, Precision: 0.5217, Recall: 0.4727, F1: 0.4736
LM Predictions:  [4, 5, 3, 3, 2, 3, 5, 5, 5, 3, 5, 5, 3, 4, 2, 5, 5, 3, 5, 5, 4, 2, 5, 5, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3509, Accuracy: 0.1429, Precision: 0.1656, Recall: 0.1111, F1: 0.1135
Epoch 51/70
Train Loss: 0.6608, Accuracy: 0.7589, Precision: 0.7140, Recall: 0.4979, F1: 0.5087
Validation Loss: 0.8331, Accuracy: 0.7074, Precision: 0.5634, Recall: 0.4380, F1: 0.4266
Testing Loss: 0.8170, Accuracy: 0.7553, Precision: 0.4647, Recall: 0.4838, F1: 0.4721
LM Predictions:  [3, 5, 3, 3, 3, 3, 5, 5, 5, 3, 5, 5, 3, 4, 2, 5, 5, 3, 4, 5, 4, 2, 4, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4292, Accuracy: 0.1429, Precision: 0.1556, Recall: 0.1111, F1: 0.1164
Epoch 52/70
Train Loss: 0.6805, Accuracy: 0.7505, Precision: 0.5227, Recall: 0.4796, F1: 0.4830
Validation Loss: 0.8302, Accuracy: 0.7102, Precision: 0.5001, Recall: 0.4467, F1: 0.4482
Testing Loss: 0.8285, Accuracy: 0.7500, Precision: 0.5164, Recall: 0.4814, F1: 0.4809
LM Predictions:  [4, 5, 3, 3, 3, 3, 5, 3, 5, 3, 5, 5, 3, 4, 2, 4, 5, 3, 4, 5, 4, 2, 3, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3619, Accuracy: 0.1786, Precision: 0.1764, Recall: 0.1319, F1: 0.1417
Epoch 53/70
Train Loss: 0.6853, Accuracy: 0.7589, Precision: 0.5337, Recall: 0.4779, F1: 0.4822
Validation Loss: 0.8543, Accuracy: 0.7102, Precision: 0.4756, Recall: 0.4184, F1: 0.4185
Testing Loss: 0.8987, Accuracy: 0.6888, Precision: 0.4326, Recall: 0.4080, F1: 0.4057
LM Predictions:  [3, 5, 2, 3, 3, 3, 2, 5, 5, 3, 5, 5, 2, 4, 2, 4, 5, 1, 5, 3, 4, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3846, Accuracy: 0.1429, Precision: 0.1240, Recall: 0.1181, F1: 0.1094
Epoch 54/70
Train Loss: 0.7159, Accuracy: 0.7365, Precision: 0.5157, Recall: 0.4573, F1: 0.4574
Validation Loss: 0.8631, Accuracy: 0.7045, Precision: 0.4946, Recall: 0.4300, F1: 0.4389
Testing Loss: 0.9006, Accuracy: 0.6995, Precision: 0.4676, Recall: 0.4252, F1: 0.4279
LM Predictions:  [4, 5, 2, 3, 3, 2, 5, 5, 5, 3, 5, 5, 2, 4, 2, 5, 5, 2, 5, 5, 2, 2, 3, 5, 2, 5, 3, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2615, Accuracy: 0.2143, Precision: 0.2420, Recall: 0.1667, F1: 0.1577
Epoch 55/70
Train Loss: 0.7453, Accuracy: 0.7278, Precision: 0.4885, Recall: 0.4433, F1: 0.4481
Validation Loss: 0.8717, Accuracy: 0.6705, Precision: 0.3321, Recall: 0.3598, F1: 0.3418
Testing Loss: 0.9416, Accuracy: 0.6915, Precision: 0.5017, Recall: 0.4054, F1: 0.4177
LM Predictions:  [4, 5, 4, 4, 3, 4, 2, 5, 5, 3, 2, 5, 4, 4, 2, 4, 5, 2, 5, 4, 4, 2, 4, 2, 2, 5, 4, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1408, Accuracy: 0.1786, Precision: 0.0958, Recall: 0.1389, F1: 0.1130
Epoch 56/70
Train Loss: 0.7530, Accuracy: 0.7155, Precision: 0.4603, Recall: 0.4241, F1: 0.4220
Validation Loss: 0.9288, Accuracy: 0.7017, Precision: 0.3497, Recall: 0.3741, F1: 0.3559
Testing Loss: 1.0374, Accuracy: 0.6809, Precision: 0.4850, Recall: 0.3657, F1: 0.3545
LM Predictions:  [4, 4, 4, 4, 2, 4, 2, 5, 5, 2, 2, 5, 4, 4, 2, 4, 5, 4, 4, 3, 4, 2, 4, 2, 2, 2, 4, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0814, Accuracy: 0.2500, Precision: 0.1263, Recall: 0.1806, F1: 0.1468
Epoch 57/70
Train Loss: 0.7717, Accuracy: 0.7250, Precision: 0.4788, Recall: 0.4204, F1: 0.4256
Validation Loss: 1.0998, Accuracy: 0.5341, Precision: 0.4462, Recall: 0.2843, F1: 0.2706
Testing Loss: 1.1699, Accuracy: 0.5745, Precision: 0.3616, Recall: 0.3002, F1: 0.2893
LM Predictions:  [4, 4, 4, 3, 2, 4, 4, 4, 5, 2, 4, 4, 4, 5, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 4, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3626, Accuracy: 0.2143, Precision: 0.0701, Recall: 0.1319, F1: 0.0897
Epoch 58/70
Train Loss: 0.8283, Accuracy: 0.6977, Precision: 0.4738, Recall: 0.4060, F1: 0.4074
Validation Loss: 1.0400, Accuracy: 0.6392, Precision: 0.3323, Recall: 0.3404, F1: 0.3290
Testing Loss: 1.1817, Accuracy: 0.5904, Precision: 0.3474, Recall: 0.3131, F1: 0.3065
LM Predictions:  [4, 2, 2, 3, 2, 2, 2, 2, 5, 5, 2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 5, 2, 2, 2, 4, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1623, Accuracy: 0.3571, Precision: 0.2327, Recall: 0.2639, F1: 0.2168
Epoch 59/70
Train Loss: 0.8068, Accuracy: 0.7113, Precision: 0.4793, Recall: 0.4205, F1: 0.4235
Validation Loss: 0.8481, Accuracy: 0.7102, Precision: 0.5444, Recall: 0.4015, F1: 0.4115
Testing Loss: 0.9797, Accuracy: 0.6862, Precision: 0.5098, Recall: 0.4024, F1: 0.4168
LM Predictions:  [2, 2, 2, 4, 2, 2, 2, 4, 5, 2, 2, 5, 4, 4, 2, 3, 5, 2, 4, 3, 2, 4, 3, 3, 2, 2, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 1.8931, Accuracy: 0.2857, Precision: 0.1644, Recall: 0.2153, F1: 0.1785
Epoch 60/70
Train Loss: 0.7288, Accuracy: 0.7348, Precision: 0.4930, Recall: 0.4466, F1: 0.4518
Validation Loss: 0.8023, Accuracy: 0.7244, Precision: 0.4958, Recall: 0.4557, F1: 0.4497
Testing Loss: 0.8863, Accuracy: 0.7021, Precision: 0.5067, Recall: 0.4428, F1: 0.4371
LM Predictions:  [4, 5, 3, 3, 3, 3, 5, 5, 5, 1, 3, 5, 2, 4, 2, 4, 5, 1, 4, 5, 4, 1, 3, 3, 5, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2340, Accuracy: 0.1429, Precision: 0.1208, Recall: 0.1042, F1: 0.1047
Epoch 61/70
Train Loss: 0.7055, Accuracy: 0.7460, Precision: 0.5055, Recall: 0.4623, F1: 0.4648
Validation Loss: 0.7475, Accuracy: 0.7443, Precision: 0.5649, Recall: 0.4769, F1: 0.4856
Testing Loss: 0.8649, Accuracy: 0.7261, Precision: 0.4924, Recall: 0.4608, F1: 0.4632
LM Predictions:  [3, 5, 3, 3, 3, 3, 2, 5, 5, 1, 3, 5, 2, 4, 2, 5, 5, 1, 4, 3, 4, 2, 3, 3, 5, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2912, Accuracy: 0.1071, Precision: 0.1349, Recall: 0.0833, F1: 0.0909
Epoch 62/70
Train Loss: 0.6789, Accuracy: 0.7512, Precision: 0.6588, Recall: 0.4691, F1: 0.4747
Validation Loss: 0.7961, Accuracy: 0.7358, Precision: 0.5896, Recall: 0.4483, F1: 0.4465
Testing Loss: 0.8804, Accuracy: 0.7340, Precision: 0.4957, Recall: 0.4542, F1: 0.4561
LM Predictions:  [4, 5, 4, 3, 4, 3, 2, 5, 5, 5, 5, 5, 2, 4, 2, 5, 5, 1, 5, 3, 4, 1, 3, 3, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1773, Accuracy: 0.2143, Precision: 0.1553, Recall: 0.1736, F1: 0.1492
Epoch 63/70
Train Loss: 0.6823, Accuracy: 0.7523, Precision: 0.5021, Recall: 0.4738, F1: 0.4771
Validation Loss: 0.8163, Accuracy: 0.7216, Precision: 0.5830, Recall: 0.4861, F1: 0.5039
Testing Loss: 0.8917, Accuracy: 0.7261, Precision: 0.6165, Recall: 0.4751, F1: 0.4778
LM Predictions:  [4, 5, 3, 3, 3, 3, 5, 2, 5, 2, 3, 5, 2, 4, 2, 4, 3, 3, 4, 0, 2, 5, 3, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3869, Accuracy: 0.2857, Precision: 0.4028, Recall: 0.2083, F1: 0.2370
Epoch 64/70
Train Loss: 0.6521, Accuracy: 0.7663, Precision: 0.6378, Recall: 0.5150, F1: 0.5279
Validation Loss: 0.8079, Accuracy: 0.7216, Precision: 0.5593, Recall: 0.4846, F1: 0.4962
Testing Loss: 0.8582, Accuracy: 0.7261, Precision: 0.4949, Recall: 0.4780, F1: 0.4712
LM Predictions:  [4, 5, 3, 3, 3, 3, 5, 2, 5, 2, 3, 5, 2, 4, 2, 1, 3, 3, 3, 3, 2, 1, 3, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4889, Accuracy: 0.2143, Precision: 0.2833, Recall: 0.1667, F1: 0.1870
Epoch 65/70
Train Loss: 0.6468, Accuracy: 0.7673, Precision: 0.6377, Recall: 0.5119, F1: 0.5193
Validation Loss: 0.7840, Accuracy: 0.7301, Precision: 0.5549, Recall: 0.4912, F1: 0.5040
Testing Loss: 0.8021, Accuracy: 0.7340, Precision: 0.5746, Recall: 0.5006, F1: 0.5122
LM Predictions:  [3, 5, 3, 3, 3, 3, 5, 2, 5, 5, 3, 5, 2, 4, 2, 1, 3, 3, 4, 3, 4, 1, 3, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4900, Accuracy: 0.2143, Precision: 0.2500, Recall: 0.1806, F1: 0.1939
Epoch 66/70
Train Loss: 0.6468, Accuracy: 0.7747, Precision: 0.5503, Recall: 0.5171, F1: 0.5243
Validation Loss: 0.8212, Accuracy: 0.7216, Precision: 0.5365, Recall: 0.4492, F1: 0.4560
Testing Loss: 0.8486, Accuracy: 0.7207, Precision: 0.5200, Recall: 0.4714, F1: 0.4819
LM Predictions:  [4, 5, 3, 3, 3, 3, 5, 5, 5, 2, 3, 5, 2, 4, 2, 1, 3, 3, 4, 3, 4, 2, 3, 5, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2673, Accuracy: 0.1786, Precision: 0.1821, Recall: 0.1319, F1: 0.1439
Epoch 67/70
Train Loss: 0.6404, Accuracy: 0.7715, Precision: 0.6232, Recall: 0.5290, F1: 0.5390
Validation Loss: 0.7826, Accuracy: 0.7415, Precision: 0.5702, Recall: 0.4914, F1: 0.5014
Testing Loss: 0.8692, Accuracy: 0.7234, Precision: 0.6053, Recall: 0.4695, F1: 0.4699
LM Predictions:  [3, 5, 4, 3, 3, 3, 5, 5, 5, 1, 3, 5, 3, 4, 2, 4, 3, 3, 3, 3, 4, 2, 3, 3, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4076, Accuracy: 0.1071, Precision: 0.1167, Recall: 0.0903, F1: 0.0981
Epoch 68/70
Train Loss: 0.6212, Accuracy: 0.7792, Precision: 0.5526, Recall: 0.5308, F1: 0.5337
Validation Loss: 0.7763, Accuracy: 0.7415, Precision: 0.5715, Recall: 0.4886, F1: 0.5103
Testing Loss: 0.8492, Accuracy: 0.7314, Precision: 0.5627, Recall: 0.5010, F1: 0.5173
LM Predictions:  [3, 5, 4, 3, 3, 3, 2, 2, 5, 2, 3, 5, 2, 4, 2, 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3882, Accuracy: 0.1786, Precision: 0.1944, Recall: 0.1458, F1: 0.1476
Epoch 69/70
Train Loss: 0.6188, Accuracy: 0.7764, Precision: 0.6506, Recall: 0.5461, F1: 0.5628
Validation Loss: 0.7721, Accuracy: 0.7443, Precision: 0.5479, Recall: 0.5033, F1: 0.5126
Testing Loss: 0.8177, Accuracy: 0.7154, Precision: 0.5140, Recall: 0.5047, F1: 0.5085
LM Predictions:  [3, 5, 4, 3, 5, 3, 5, 2, 5, 5, 3, 5, 2, 4, 2, 1, 3, 3, 3, 3, 4, 1, 3, 3, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2942, Accuracy: 0.1786, Precision: 0.1778, Recall: 0.1597, F1: 0.1576
Epoch 70/70
Train Loss: 0.6081, Accuracy: 0.7750, Precision: 0.5880, Recall: 0.5257, F1: 0.5346
Validation Loss: 0.7916, Accuracy: 0.7301, Precision: 0.5902, Recall: 0.4805, F1: 0.4942
Testing Loss: 0.8637, Accuracy: 0.7314, Precision: 0.5012, Recall: 0.4744, F1: 0.4730
LM Predictions:  [3, 5, 3, 3, 3, 3, 5, 2, 5, 3, 3, 5, 2, 4, 2, 1, 3, 3, 4, 3, 4, 2, 3, 3, 5, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4584, Accuracy: 0.1429, Precision: 0.1778, Recall: 0.1111, F1: 0.1279
Label Memorization Analysis: 
LM Loss: 2.4584, Accuracy: 0.1429, Precision: 0.1778, Recall: 0.1111, F1: 0.1279
---------------------------------------------------------------------------



