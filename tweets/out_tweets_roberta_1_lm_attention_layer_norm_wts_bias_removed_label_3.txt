---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.0287, Accuracy: 0.6309, Precision: 0.3282, Recall: 0.3282, F1: 0.3149
Validation Loss: 0.7733, Accuracy: 0.6989, Precision: 0.3635, Recall: 0.4003, F1: 0.3663
Testing Loss: 0.7724, Accuracy: 0.7128, Precision: 0.3670, Recall: 0.4123, F1: 0.3716
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9248, Accuracy: 0.2143, Precision: 0.0444, Recall: 0.1714, F1: 0.0706
Epoch 2/70
Train Loss: 0.6706, Accuracy: 0.7694, Precision: 0.4712, Recall: 0.4662, F1: 0.4566
Validation Loss: 0.6838, Accuracy: 0.7557, Precision: 0.4620, Recall: 0.4929, F1: 0.4526
Testing Loss: 0.6751, Accuracy: 0.7793, Precision: 0.4857, Recall: 0.4913, F1: 0.4545
LM Predictions:  [2, 3, 3, 3, 3, 2, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8870, Accuracy: 0.1429, Precision: 0.2667, Recall: 0.0952, F1: 0.1250
Epoch 3/70
Train Loss: 0.5844, Accuracy: 0.8034, Precision: 0.5126, Recall: 0.5344, F1: 0.5113
Validation Loss: 0.6007, Accuracy: 0.8011, Precision: 0.4936, Recall: 0.5084, F1: 0.4892
Testing Loss: 0.6377, Accuracy: 0.7686, Precision: 0.4290, Recall: 0.4884, F1: 0.4522
LM Predictions:  [2, 1, 1, 5, 5, 5, 1, 5, 1, 3, 1, 5, 2, 2, 2, 1, 1, 1, 1, 2, 2, 2, 5, 2, 5, 2, 2, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0917, Accuracy: 0.2500, Precision: 0.1269, Recall: 0.1845, F1: 0.1485
Epoch 4/70
Train Loss: 0.5096, Accuracy: 0.8383, Precision: 0.5842, Recall: 0.5652, F1: 0.5714
Validation Loss: 0.5851, Accuracy: 0.8011, Precision: 0.5201, Recall: 0.5156, F1: 0.5021
Testing Loss: 0.5644, Accuracy: 0.8245, Precision: 0.5438, Recall: 0.5352, F1: 0.5303
LM Predictions:  [5, 3, 3, 5, 5, 2, 3, 5, 3, 3, 3, 5, 5, 5, 1, 3, 3, 3, 3, 5, 5, 1, 1, 2, 5, 1, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4119, Accuracy: 0.2500, Precision: 0.1944, Recall: 0.1667, F1: 0.1647
Epoch 5/70
Train Loss: 0.4478, Accuracy: 0.8548, Precision: 0.5872, Recall: 0.6138, F1: 0.5980
Validation Loss: 0.5523, Accuracy: 0.8125, Precision: 0.4878, Recall: 0.5505, F1: 0.5078
Testing Loss: 0.5852, Accuracy: 0.7899, Precision: 0.5722, Recall: 0.5501, F1: 0.4921
LM Predictions:  [1, 1, 3, 1, 5, 2, 1, 5, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 1, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4091, Accuracy: 0.2500, Precision: 0.2096, Recall: 0.2202, F1: 0.1638
Epoch 6/70
Train Loss: 0.3984, Accuracy: 0.8723, Precision: 0.6119, Recall: 0.6322, F1: 0.6216
Validation Loss: 0.6278, Accuracy: 0.7983, Precision: 0.5532, Recall: 0.5872, F1: 0.5594
Testing Loss: 0.5971, Accuracy: 0.8032, Precision: 0.5547, Recall: 0.5748, F1: 0.5565
LM Predictions:  [2, 3, 3, 3, 1, 2, 3, 5, 3, 3, 3, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.6438, Accuracy: 0.2143, Precision: 0.3556, Recall: 0.1607, F1: 0.2004
Epoch 7/70
Train Loss: 0.3704, Accuracy: 0.8824, Precision: 0.7235, Recall: 0.6796, F1: 0.6627
Validation Loss: 0.6253, Accuracy: 0.8352, Precision: 0.6347, Recall: 0.6120, F1: 0.6206
Testing Loss: 0.5579, Accuracy: 0.8191, Precision: 0.5911, Recall: 0.5898, F1: 0.5871
LM Predictions:  [3, 3, 3, 3, 5, 2, 3, 5, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 1, 3, 3, 1, 3, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5683, Accuracy: 0.1429, Precision: 0.2500, Recall: 0.1131, F1: 0.1513
Epoch 8/70
Train Loss: 0.3253, Accuracy: 0.8999, Precision: 0.7280, Recall: 0.7100, F1: 0.7108
Validation Loss: 0.6342, Accuracy: 0.8182, Precision: 0.6120, Recall: 0.6093, F1: 0.5955
Testing Loss: 0.5488, Accuracy: 0.8298, Precision: 0.6061, Recall: 0.6206, F1: 0.6111
LM Predictions:  [3, 3, 3, 3, 5, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 1, 3, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.9151, Accuracy: 0.0714, Precision: 0.2500, Recall: 0.0476, F1: 0.0787
Epoch 9/70
Train Loss: 0.2784, Accuracy: 0.9167, Precision: 0.7822, Recall: 0.7729, F1: 0.7683
Validation Loss: 0.6445, Accuracy: 0.8125, Precision: 0.6528, Recall: 0.7289, F1: 0.6601
Testing Loss: 0.6221, Accuracy: 0.8059, Precision: 0.7384, Recall: 0.6311, F1: 0.6089
LM Predictions:  [3, 3, 3, 3, 1, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 2, 3, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4771, Accuracy: 0.1429, Precision: 0.3611, Recall: 0.1131, F1: 0.1639
Epoch 10/70
Train Loss: 0.2655, Accuracy: 0.9230, Precision: 0.7924, Recall: 0.7806, F1: 0.7815
Validation Loss: 0.5280, Accuracy: 0.8608, Precision: 0.6639, Recall: 0.6576, F1: 0.6606
Testing Loss: 0.6092, Accuracy: 0.8457, Precision: 0.6280, Recall: 0.6498, F1: 0.6355
LM Predictions:  [1, 3, 3, 1, 1, 2, 3, 5, 3, 3, 3, 0, 2, 2, 3, 2, 3, 3, 1, 2, 2, 3, 1, 2, 3, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4533, Accuracy: 0.2500, Precision: 0.4556, Recall: 0.2119, F1: 0.2338
Epoch 11/70
Train Loss: 0.2697, Accuracy: 0.9227, Precision: 0.8168, Recall: 0.7838, F1: 0.7910
Validation Loss: 0.7607, Accuracy: 0.8153, Precision: 0.6031, Recall: 0.6500, F1: 0.6207
Testing Loss: 0.6367, Accuracy: 0.8564, Precision: 0.6244, Recall: 0.6719, F1: 0.6443
LM Predictions:  [3, 3, 3, 3, 1, 2, 3, 5, 3, 3, 3, 0, 2, 3, 3, 3, 3, 3, 3, 3, 5, 3, 1, 2, 3, 3, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5368, Accuracy: 0.2500, Precision: 0.5139, Recall: 0.1940, F1: 0.2682
Epoch 12/70
Train Loss: 0.2262, Accuracy: 0.9353, Precision: 0.8197, Recall: 0.8102, F1: 0.8058
Validation Loss: 0.6639, Accuracy: 0.8153, Precision: 0.6704, Recall: 0.7444, F1: 0.6666
Testing Loss: 0.6527, Accuracy: 0.8324, Precision: 0.6393, Recall: 0.6480, F1: 0.6378
LM Predictions:  [5, 3, 3, 3, 1, 4, 3, 0, 3, 3, 3, 0, 2, 2, 3, 5, 3, 3, 3, 5, 2, 3, 1, 2, 3, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1087, Accuracy: 0.2857, Precision: 0.5000, Recall: 0.2274, F1: 0.2963
Epoch 13/70
Train Loss: 0.1934, Accuracy: 0.9447, Precision: 0.8466, Recall: 0.8442, F1: 0.8435
Validation Loss: 0.6227, Accuracy: 0.8295, Precision: 0.6624, Recall: 0.6749, F1: 0.6517
Testing Loss: 0.6660, Accuracy: 0.8457, Precision: 0.6489, Recall: 0.6480, F1: 0.6442
LM Predictions:  [1, 3, 3, 1, 1, 2, 3, 5, 3, 3, 3, 0, 2, 5, 3, 2, 3, 3, 1, 3, 2, 3, 1, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0334, Accuracy: 0.3214, Precision: 0.4325, Recall: 0.2774, F1: 0.2937
Epoch 14/70
Train Loss: 0.1763, Accuracy: 0.9531, Precision: 0.8650, Recall: 0.8764, F1: 0.8703
Validation Loss: 0.6886, Accuracy: 0.8068, Precision: 0.6568, Recall: 0.6346, F1: 0.6287
Testing Loss: 0.7149, Accuracy: 0.8191, Precision: 0.6428, Recall: 0.6267, F1: 0.6200
LM Predictions:  [1, 3, 3, 1, 1, 2, 3, 5, 3, 3, 3, 0, 2, 2, 3, 2, 3, 3, 1, 1, 2, 3, 1, 2, 1, 2, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0768, Accuracy: 0.2857, Precision: 0.4583, Recall: 0.2536, F1: 0.2472
Epoch 15/70
Train Loss: 0.1648, Accuracy: 0.9524, Precision: 0.8698, Recall: 0.8682, F1: 0.8680
Validation Loss: 0.6955, Accuracy: 0.8324, Precision: 0.6446, Recall: 0.5852, F1: 0.6026
Testing Loss: 0.6825, Accuracy: 0.8404, Precision: 0.6388, Recall: 0.5965, F1: 0.6104
LM Predictions:  [5, 3, 3, 5, 5, 4, 3, 5, 3, 3, 3, 5, 2, 5, 3, 2, 3, 3, 5, 5, 5, 3, 5, 2, 5, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9775, Accuracy: 0.3214, Precision: 0.3611, Recall: 0.2238, F1: 0.2342
Epoch 16/70
Train Loss: 0.1527, Accuracy: 0.9598, Precision: 0.8995, Recall: 0.8926, F1: 0.8951
Validation Loss: 0.7872, Accuracy: 0.8409, Precision: 0.6817, Recall: 0.6211, F1: 0.6402
Testing Loss: 0.7987, Accuracy: 0.8404, Precision: 0.6332, Recall: 0.6178, F1: 0.6150
LM Predictions:  [2, 3, 3, 4, 1, 4, 3, 5, 3, 3, 3, 0, 2, 2, 3, 5, 3, 3, 5, 5, 2, 3, 1, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1275, Accuracy: 0.3929, Precision: 0.5694, Recall: 0.3167, F1: 0.3782
Epoch 17/70
Train Loss: 0.1655, Accuracy: 0.9538, Precision: 0.8887, Recall: 0.8852, F1: 0.8866
Validation Loss: 0.7053, Accuracy: 0.8324, Precision: 0.6662, Recall: 0.6641, F1: 0.6484
Testing Loss: 0.7180, Accuracy: 0.8404, Precision: 0.6730, Recall: 0.6966, F1: 0.6687
LM Predictions:  [2, 3, 3, 2, 1, 4, 3, 5, 3, 3, 3, 0, 2, 2, 3, 2, 3, 3, 2, 2, 2, 3, 1, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8390, Accuracy: 0.3214, Precision: 0.6667, Recall: 0.2690, F1: 0.3182
Epoch 18/70
Train Loss: 0.1429, Accuracy: 0.9622, Precision: 0.9070, Recall: 0.9181, F1: 0.9117
Validation Loss: 0.7213, Accuracy: 0.8239, Precision: 0.6419, Recall: 0.6588, F1: 0.6466
Testing Loss: 0.6836, Accuracy: 0.8404, Precision: 0.6213, Recall: 0.6519, F1: 0.6341
LM Predictions:  [2, 3, 3, 3, 1, 3, 3, 5, 3, 3, 3, 0, 2, 2, 3, 2, 3, 3, 5, 2, 2, 3, 1, 2, 3, 2, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4831, Accuracy: 0.2500, Precision: 0.3796, Recall: 0.1940, F1: 0.2235
Epoch 19/70
Train Loss: 0.1252, Accuracy: 0.9689, Precision: 0.9313, Recall: 0.9215, F1: 0.9248
Validation Loss: 0.7394, Accuracy: 0.8381, Precision: 0.7133, Recall: 0.6315, F1: 0.6599
Testing Loss: 0.6807, Accuracy: 0.8457, Precision: 0.7004, Recall: 0.6787, F1: 0.6823
LM Predictions:  [2, 3, 3, 5, 1, 4, 3, 5, 3, 3, 3, 0, 2, 2, 3, 2, 3, 3, 5, 2, 5, 3, 1, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1938, Accuracy: 0.3929, Precision: 0.6278, Recall: 0.3167, F1: 0.3786
Epoch 20/70
Train Loss: 0.1107, Accuracy: 0.9692, Precision: 0.9229, Recall: 0.9226, F1: 0.9209
Validation Loss: 0.8426, Accuracy: 0.8267, Precision: 0.6484, Recall: 0.6541, F1: 0.6509
Testing Loss: 0.6812, Accuracy: 0.8590, Precision: 0.6806, Recall: 0.6772, F1: 0.6763
LM Predictions:  [5, 3, 3, 0, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 5, 5, 5, 3, 1, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5736, Accuracy: 0.4286, Precision: 0.5903, Recall: 0.3405, F1: 0.4004
Epoch 21/70
Train Loss: 0.0949, Accuracy: 0.9762, Precision: 0.9394, Recall: 0.9318, F1: 0.9346
Validation Loss: 0.9736, Accuracy: 0.8182, Precision: 0.6589, Recall: 0.6473, F1: 0.6494
Testing Loss: 0.8672, Accuracy: 0.8484, Precision: 0.6427, Recall: 0.6581, F1: 0.6440
LM Predictions:  [2, 3, 3, 5, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 5, 2, 1, 2, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5350, Accuracy: 0.5357, Precision: 0.7345, Recall: 0.4298, F1: 0.4901
Epoch 22/70
Train Loss: 0.1483, Accuracy: 0.9636, Precision: 0.9061, Recall: 0.8947, F1: 0.8990
Validation Loss: 0.8064, Accuracy: 0.8381, Precision: 0.6662, Recall: 0.6341, F1: 0.6370
Testing Loss: 0.6810, Accuracy: 0.8484, Precision: 0.6382, Recall: 0.6490, F1: 0.6398
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9808, Accuracy: 0.4643, Precision: 0.7222, Recall: 0.3821, F1: 0.4552
Epoch 23/70
Train Loss: 0.0947, Accuracy: 0.9773, Precision: 0.9398, Recall: 0.9342, F1: 0.9350
Validation Loss: 0.7998, Accuracy: 0.8295, Precision: 0.6573, Recall: 0.6122, F1: 0.6279
Testing Loss: 0.7590, Accuracy: 0.8404, Precision: 0.6658, Recall: 0.6415, F1: 0.6489
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 5, 5, 5, 3, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7676, Accuracy: 0.4286, Precision: 0.7153, Recall: 0.3405, F1: 0.4175
Epoch 24/70
Train Loss: 0.1272, Accuracy: 0.9689, Precision: 0.9111, Recall: 0.9080, F1: 0.9084
Validation Loss: 0.7482, Accuracy: 0.8409, Precision: 0.6732, Recall: 0.6418, F1: 0.6563
Testing Loss: 0.6620, Accuracy: 0.8511, Precision: 0.6403, Recall: 0.6576, F1: 0.6471
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 5, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0250, Accuracy: 0.4643, Precision: 0.7389, Recall: 0.3821, F1: 0.4504
Epoch 25/70
Train Loss: 0.0787, Accuracy: 0.9804, Precision: 0.9513, Recall: 0.9394, F1: 0.9430
Validation Loss: 0.8556, Accuracy: 0.8466, Precision: 0.6852, Recall: 0.6448, F1: 0.6596
Testing Loss: 0.7864, Accuracy: 0.8670, Precision: 0.6736, Recall: 0.6712, F1: 0.6718
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 5, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7374, Accuracy: 0.5000, Precision: 0.7500, Recall: 0.4060, F1: 0.4847
Epoch 26/70
Train Loss: 0.0484, Accuracy: 0.9881, Precision: 0.9636, Recall: 0.9651, F1: 0.9633
Validation Loss: 1.0178, Accuracy: 0.8438, Precision: 0.6667, Recall: 0.6646, F1: 0.6644
Testing Loss: 0.8356, Accuracy: 0.8670, Precision: 0.6787, Recall: 0.7023, F1: 0.6872
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 5, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 3, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1224, Accuracy: 0.4643, Precision: 0.7222, Recall: 0.3821, F1: 0.4552
Epoch 27/70
Train Loss: 0.1376, Accuracy: 0.9657, Precision: 0.9255, Recall: 0.9356, F1: 0.9283
Validation Loss: 0.7622, Accuracy: 0.7983, Precision: 0.6712, Recall: 0.6461, F1: 0.6323
Testing Loss: 0.8554, Accuracy: 0.7926, Precision: 0.6168, Recall: 0.6058, F1: 0.5967
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6557, Accuracy: 0.4643, Precision: 0.7222, Recall: 0.3821, F1: 0.4552
Epoch 28/70
Train Loss: 0.1339, Accuracy: 0.9657, Precision: 0.8955, Recall: 0.9046, F1: 0.8982
Validation Loss: 0.8509, Accuracy: 0.8182, Precision: 0.6163, Recall: 0.6431, F1: 0.6256
Testing Loss: 0.8495, Accuracy: 0.8271, Precision: 0.5864, Recall: 0.6186, F1: 0.5949
LM Predictions:  [3, 3, 3, 3, 1, 3, 3, 5, 3, 3, 3, 0, 2, 5, 3, 3, 3, 3, 3, 1, 5, 3, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9797, Accuracy: 0.3214, Precision: 0.5167, Recall: 0.2595, F1: 0.3222
Epoch 29/70
Train Loss: 0.1071, Accuracy: 0.9741, Precision: 0.9257, Recall: 0.9255, F1: 0.9242
Validation Loss: 0.8757, Accuracy: 0.8295, Precision: 0.6718, Recall: 0.6212, F1: 0.6305
Testing Loss: 0.9972, Accuracy: 0.8271, Precision: 0.6520, Recall: 0.6468, F1: 0.6355
LM Predictions:  [3, 3, 3, 3, 1, 3, 3, 5, 5, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9395, Accuracy: 0.4286, Precision: 0.5357, Recall: 0.3488, F1: 0.3905
Epoch 30/70
Train Loss: 0.1408, Accuracy: 0.9626, Precision: 0.9150, Recall: 0.9153, F1: 0.9136
Validation Loss: 0.8229, Accuracy: 0.8352, Precision: 0.6724, Recall: 0.6799, F1: 0.6718
Testing Loss: 0.7886, Accuracy: 0.8457, Precision: 0.6556, Recall: 0.6891, F1: 0.6675
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 3, 3, 3, 2, 5, 5, 3, 5, 2, 1, 5, 2, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0536, Accuracy: 0.3571, Precision: 0.6944, Recall: 0.2929, F1: 0.3854
Epoch 31/70
Train Loss: 0.0840, Accuracy: 0.9815, Precision: 0.9542, Recall: 0.9572, F1: 0.9542
Validation Loss: 0.8326, Accuracy: 0.8409, Precision: 0.6603, Recall: 0.6355, F1: 0.6410
Testing Loss: 0.9286, Accuracy: 0.8245, Precision: 0.6596, Recall: 0.6538, F1: 0.6388
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 3, 3, 3, 1, 5, 5, 3, 1, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7796, Accuracy: 0.5000, Precision: 0.7583, Recall: 0.4155, F1: 0.5081
Epoch 32/70
Train Loss: 0.0688, Accuracy: 0.9839, Precision: 0.9546, Recall: 0.9608, F1: 0.9565
Validation Loss: 0.8841, Accuracy: 0.8551, Precision: 0.6825, Recall: 0.6672, F1: 0.6730
Testing Loss: 0.9173, Accuracy: 0.8431, Precision: 0.6652, Recall: 0.6768, F1: 0.6703
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 3, 3, 3, 1, 5, 5, 3, 1, 2, 1, 0, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5606, Accuracy: 0.4643, Precision: 0.7583, Recall: 0.3917, F1: 0.4869
Epoch 33/70
Train Loss: 0.0660, Accuracy: 0.9853, Precision: 0.9531, Recall: 0.9688, F1: 0.9601
Validation Loss: 0.7652, Accuracy: 0.8352, Precision: 0.6824, Recall: 0.6954, F1: 0.6816
Testing Loss: 0.9810, Accuracy: 0.8032, Precision: 0.6179, Recall: 0.6280, F1: 0.6050
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 0, 3, 3, 1, 2, 5, 3, 1, 2, 1, 1, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6361, Accuracy: 0.3929, Precision: 0.6250, Recall: 0.3345, F1: 0.3941
Epoch 34/70
Train Loss: 0.1146, Accuracy: 0.9717, Precision: 0.9250, Recall: 0.9308, F1: 0.9259
Validation Loss: 0.9715, Accuracy: 0.8097, Precision: 0.6621, Recall: 0.6646, F1: 0.6549
Testing Loss: 1.0021, Accuracy: 0.8005, Precision: 0.6066, Recall: 0.6572, F1: 0.6245
LM Predictions:  [3, 3, 3, 3, 5, 3, 3, 5, 3, 3, 3, 0, 2, 5, 3, 3, 3, 3, 1, 5, 5, 3, 5, 2, 1, 3, 5, 3]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8070, Accuracy: 0.3214, Precision: 0.5952, Recall: 0.2595, F1: 0.3360
Epoch 35/70
Train Loss: 0.1164, Accuracy: 0.9727, Precision: 0.9345, Recall: 0.9381, F1: 0.9353
Validation Loss: 0.8697, Accuracy: 0.8352, Precision: 0.6605, Recall: 0.6315, F1: 0.6362
Testing Loss: 0.9209, Accuracy: 0.8324, Precision: 0.7844, Recall: 0.6419, F1: 0.6492
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 0, 3, 3, 1, 5, 5, 2, 5, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7634, Accuracy: 0.4643, Precision: 0.6667, Recall: 0.3821, F1: 0.4639
Epoch 36/70
Train Loss: 0.0784, Accuracy: 0.9815, Precision: 0.9507, Recall: 0.9504, F1: 0.9494
Validation Loss: 0.9715, Accuracy: 0.8182, Precision: 0.6942, Recall: 0.5840, F1: 0.6033
Testing Loss: 1.0070, Accuracy: 0.8165, Precision: 0.5996, Recall: 0.5866, F1: 0.5810
LM Predictions:  [5, 3, 3, 3, 1, 4, 3, 5, 4, 3, 3, 0, 2, 5, 3, 5, 3, 3, 5, 5, 2, 2, 5, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9742, Accuracy: 0.4286, Precision: 0.6111, Recall: 0.3405, F1: 0.4057
Epoch 37/70
Train Loss: 0.0968, Accuracy: 0.9759, Precision: 0.9463, Recall: 0.9371, F1: 0.9405
Validation Loss: 0.8340, Accuracy: 0.8324, Precision: 0.6875, Recall: 0.6093, F1: 0.6345
Testing Loss: 0.9510, Accuracy: 0.8245, Precision: 0.6612, Recall: 0.6243, F1: 0.6389
LM Predictions:  [5, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 5, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8119, Accuracy: 0.4643, Precision: 0.7292, Recall: 0.3821, F1: 0.4472
Epoch 38/70
Train Loss: 0.1240, Accuracy: 0.9664, Precision: 0.9257, Recall: 0.9306, F1: 0.9269
Validation Loss: 0.8433, Accuracy: 0.7756, Precision: 0.6927, Recall: 0.4916, F1: 0.5024
Testing Loss: 0.8982, Accuracy: 0.7580, Precision: 0.6150, Recall: 0.4802, F1: 0.4783
LM Predictions:  [4, 2, 3, 2, 1, 2, 2, 5, 2, 2, 2, 5, 2, 4, 2, 4, 3, 2, 4, 5, 5, 4, 1, 2, 1, 2, 4, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7875, Accuracy: 0.3214, Precision: 0.2874, Recall: 0.2500, F1: 0.2528
Epoch 39/70
Train Loss: 0.1309, Accuracy: 0.9661, Precision: 0.9213, Recall: 0.9092, F1: 0.9150
Validation Loss: 1.1465, Accuracy: 0.8210, Precision: 0.7389, Recall: 0.5387, F1: 0.5770
Testing Loss: 1.2006, Accuracy: 0.8059, Precision: 0.6204, Recall: 0.5343, F1: 0.5588
LM Predictions:  [2, 3, 3, 3, 1, 2, 3, 5, 2, 3, 3, 0, 2, 5, 3, 2, 3, 3, 5, 5, 5, 2, 4, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8017, Accuracy: 0.4643, Precision: 0.5667, Recall: 0.3548, F1: 0.3954
Epoch 40/70
Train Loss: 0.3568, Accuracy: 0.8838, Precision: 0.8232, Recall: 0.7922, F1: 0.7950
Validation Loss: 0.9435, Accuracy: 0.7955, Precision: 0.6510, Recall: 0.5915, F1: 0.6106
Testing Loss: 0.8377, Accuracy: 0.8271, Precision: 0.6203, Recall: 0.6178, F1: 0.6129
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 0, 2, 2, 3, 0, 3, 3, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6191, Accuracy: 0.5000, Precision: 0.6917, Recall: 0.4250, F1: 0.5004
Epoch 41/70
Train Loss: 0.0850, Accuracy: 0.9804, Precision: 0.9506, Recall: 0.9503, F1: 0.9496
Validation Loss: 0.9063, Accuracy: 0.8125, Precision: 0.6288, Recall: 0.6022, F1: 0.6123
Testing Loss: 0.7702, Accuracy: 0.8457, Precision: 0.6510, Recall: 0.6451, F1: 0.6459
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 3, 3, 3, 5, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9157, Accuracy: 0.5357, Precision: 0.8000, Recall: 0.4405, F1: 0.5471
Epoch 42/70
Train Loss: 0.0844, Accuracy: 0.9804, Precision: 0.9526, Recall: 0.9581, F1: 0.9548
Validation Loss: 0.9927, Accuracy: 0.8267, Precision: 0.6592, Recall: 0.6026, F1: 0.6263
Testing Loss: 0.9159, Accuracy: 0.8457, Precision: 0.6520, Recall: 0.6419, F1: 0.6438
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 5, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6104, Accuracy: 0.6071, Precision: 0.8056, Recall: 0.4881, F1: 0.5819
Epoch 43/70
Train Loss: 0.0820, Accuracy: 0.9822, Precision: 0.9494, Recall: 0.9514, F1: 0.9496
Validation Loss: 0.8662, Accuracy: 0.8352, Precision: 0.6900, Recall: 0.6150, F1: 0.6338
Testing Loss: 0.9589, Accuracy: 0.8378, Precision: 0.6187, Recall: 0.6031, F1: 0.5994
LM Predictions:  [2, 3, 3, 3, 1, 4, 3, 5, 2, 3, 2, 0, 2, 5, 3, 5, 3, 3, 5, 5, 5, 2, 1, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7647, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4119, F1: 0.4596
Epoch 44/70
Train Loss: 0.0661, Accuracy: 0.9864, Precision: 0.9647, Recall: 0.9686, F1: 0.9656
Validation Loss: 1.0093, Accuracy: 0.8153, Precision: 0.6281, Recall: 0.5997, F1: 0.6104
Testing Loss: 0.9700, Accuracy: 0.8378, Precision: 0.6606, Recall: 0.6704, F1: 0.6577
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 0, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4827, Accuracy: 0.6071, Precision: 0.8000, Recall: 0.4881, F1: 0.5778
Epoch 45/70
Train Loss: 0.0648, Accuracy: 0.9864, Precision: 0.9551, Recall: 0.9635, F1: 0.9588
Validation Loss: 0.9795, Accuracy: 0.8210, Precision: 0.6478, Recall: 0.6634, F1: 0.6426
Testing Loss: 0.9183, Accuracy: 0.8378, Precision: 0.6787, Recall: 0.6632, F1: 0.6593
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 5, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5450, Accuracy: 0.5357, Precision: 0.7500, Recall: 0.4393, F1: 0.5244
Epoch 46/70
Train Loss: 0.0599, Accuracy: 0.9878, Precision: 0.9656, Recall: 0.9650, F1: 0.9643
Validation Loss: 1.0467, Accuracy: 0.8267, Precision: 0.6458, Recall: 0.6374, F1: 0.6410
Testing Loss: 0.9478, Accuracy: 0.8537, Precision: 0.6383, Recall: 0.6662, F1: 0.6470
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 5, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4336, Accuracy: 0.6071, Precision: 0.8056, Recall: 0.5060, F1: 0.5960
Epoch 47/70
Train Loss: 0.0550, Accuracy: 0.9881, Precision: 0.9635, Recall: 0.9734, F1: 0.9679
Validation Loss: 1.0659, Accuracy: 0.8352, Precision: 0.6407, Recall: 0.6280, F1: 0.6336
Testing Loss: 0.9682, Accuracy: 0.8378, Precision: 0.6184, Recall: 0.6223, F1: 0.6162
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3151, Accuracy: 0.6429, Precision: 0.8333, Recall: 0.5298, F1: 0.6243
Epoch 48/70
Train Loss: 0.0456, Accuracy: 0.9913, Precision: 0.9749, Recall: 0.9837, F1: 0.9788
Validation Loss: 1.0360, Accuracy: 0.8267, Precision: 0.6439, Recall: 0.6245, F1: 0.6336
Testing Loss: 1.0056, Accuracy: 0.8404, Precision: 0.6261, Recall: 0.6395, F1: 0.6282
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3352, Accuracy: 0.6429, Precision: 0.8333, Recall: 0.5298, F1: 0.6243
Epoch 49/70
Train Loss: 0.0552, Accuracy: 0.9881, Precision: 0.9659, Recall: 0.9723, F1: 0.9686
Validation Loss: 1.0361, Accuracy: 0.8182, Precision: 0.6369, Recall: 0.6348, F1: 0.6345
Testing Loss: 0.9023, Accuracy: 0.8457, Precision: 0.6620, Recall: 0.6674, F1: 0.6622
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 3, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1654, Accuracy: 0.6071, Precision: 0.8333, Recall: 0.5060, F1: 0.6067
Epoch 50/70
Train Loss: 0.0728, Accuracy: 0.9822, Precision: 0.9243, Recall: 0.9463, F1: 0.9336
Validation Loss: 1.0329, Accuracy: 0.8040, Precision: 0.5343, Recall: 0.5210, F1: 0.5169
Testing Loss: 1.0295, Accuracy: 0.7899, Precision: 0.4624, Recall: 0.5180, F1: 0.4871
LM Predictions:  [2, 2, 2, 2, 1, 4, 2, 5, 5, 2, 2, 2, 2, 5, 2, 5, 2, 2, 1, 5, 5, 5, 2, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2327, Accuracy: 0.5000, Precision: 0.6017, Recall: 0.4757, F1: 0.4643
Epoch 51/70
Train Loss: 0.1159, Accuracy: 0.9671, Precision: 0.8134, Recall: 0.8120, F1: 0.7925
Validation Loss: 0.9682, Accuracy: 0.8182, Precision: 0.6325, Recall: 0.6219, F1: 0.6220
Testing Loss: 0.8969, Accuracy: 0.8484, Precision: 0.6262, Recall: 0.6326, F1: 0.6267
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 3, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.3519, Accuracy: 0.5357, Precision: 0.8333, Recall: 0.4393, F1: 0.5538
Epoch 52/70
Train Loss: 0.0734, Accuracy: 0.9801, Precision: 0.9295, Recall: 0.9124, F1: 0.9146
Validation Loss: 0.9044, Accuracy: 0.8352, Precision: 0.6843, Recall: 0.6366, F1: 0.6512
Testing Loss: 0.9067, Accuracy: 0.8590, Precision: 0.6446, Recall: 0.6309, F1: 0.6363
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4937, Accuracy: 0.6071, Precision: 0.8333, Recall: 0.5060, F1: 0.6067
Epoch 53/70
Train Loss: 0.0662, Accuracy: 0.9881, Precision: 0.9745, Recall: 0.9793, F1: 0.9761
Validation Loss: 1.1712, Accuracy: 0.7983, Precision: 0.6184, Recall: 0.6400, F1: 0.6245
Testing Loss: 1.0227, Accuracy: 0.8298, Precision: 0.6110, Recall: 0.6355, F1: 0.6152
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7435, Accuracy: 0.5000, Precision: 0.7667, Recall: 0.4155, F1: 0.5008
Epoch 54/70
Train Loss: 0.0765, Accuracy: 0.9832, Precision: 0.9411, Recall: 0.9331, F1: 0.9351
Validation Loss: 0.9170, Accuracy: 0.8267, Precision: 0.6484, Recall: 0.6494, F1: 0.6471
Testing Loss: 0.8848, Accuracy: 0.8378, Precision: 0.6143, Recall: 0.6330, F1: 0.6226
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 3, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 1, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7496, Accuracy: 0.4643, Precision: 0.7917, Recall: 0.3821, F1: 0.4750
Epoch 55/70
Train Loss: 0.1021, Accuracy: 0.9769, Precision: 0.9362, Recall: 0.9135, F1: 0.9209
Validation Loss: 0.9493, Accuracy: 0.8239, Precision: 0.7060, Recall: 0.6110, F1: 0.6360
Testing Loss: 0.9921, Accuracy: 0.8351, Precision: 0.6477, Recall: 0.6080, F1: 0.6201
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6552, Accuracy: 0.5714, Precision: 0.7389, Recall: 0.4726, F1: 0.5478
Epoch 56/70
Train Loss: 0.0746, Accuracy: 0.9829, Precision: 0.9483, Recall: 0.9313, F1: 0.9372
Validation Loss: 0.9518, Accuracy: 0.8494, Precision: 0.7289, Recall: 0.6873, F1: 0.6877
Testing Loss: 1.0592, Accuracy: 0.8351, Precision: 0.6693, Recall: 0.6475, F1: 0.6483
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 0, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4598, Accuracy: 0.5714, Precision: 0.7250, Recall: 0.4726, F1: 0.5407
Epoch 57/70
Train Loss: 0.1462, Accuracy: 0.9706, Precision: 0.9243, Recall: 0.9348, F1: 0.9275
Validation Loss: 0.9888, Accuracy: 0.8125, Precision: 0.6297, Recall: 0.6193, F1: 0.6185
Testing Loss: 0.9762, Accuracy: 0.8218, Precision: 0.6011, Recall: 0.6001, F1: 0.5963
LM Predictions:  [3, 3, 3, 3, 1, 4, 3, 5, 3, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 3, 4, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8031, Accuracy: 0.5000, Precision: 0.7500, Recall: 0.4155, F1: 0.5246
Epoch 58/70
Train Loss: 0.1231, Accuracy: 0.9752, Precision: 0.9336, Recall: 0.9473, F1: 0.9394
Validation Loss: 1.1836, Accuracy: 0.6903, Precision: 0.5598, Recall: 0.5881, F1: 0.5416
Testing Loss: 1.2570, Accuracy: 0.6915, Precision: 0.5611, Recall: 0.5999, F1: 0.5265
LM Predictions:  [3, 3, 3, 3, 1, 3, 3, 5, 0, 3, 3, 0, 2, 5, 3, 0, 3, 3, 1, 1, 5, 0, 1, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0771, Accuracy: 0.3929, Precision: 0.4833, Recall: 0.3345, F1: 0.3667
Epoch 59/70
Train Loss: 0.2482, Accuracy: 0.9251, Precision: 0.8623, Recall: 0.8932, F1: 0.8766
Validation Loss: 0.7689, Accuracy: 0.8239, Precision: 0.6567, Recall: 0.6207, F1: 0.6259
Testing Loss: 0.8453, Accuracy: 0.8191, Precision: 0.5887, Recall: 0.5870, F1: 0.5846
LM Predictions:  [5, 3, 3, 3, 5, 4, 3, 5, 2, 3, 3, 5, 2, 5, 3, 2, 3, 3, 1, 5, 5, 2, 5, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8569, Accuracy: 0.4286, Precision: 0.5265, Recall: 0.3310, F1: 0.3690
Epoch 60/70
Train Loss: 0.1335, Accuracy: 0.9640, Precision: 0.8839, Recall: 0.8108, F1: 0.8089
Validation Loss: 0.9012, Accuracy: 0.8438, Precision: 0.6825, Recall: 0.6842, F1: 0.6703
Testing Loss: 1.0113, Accuracy: 0.8431, Precision: 0.6716, Recall: 0.6271, F1: 0.6420
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 5, 2, 2, 3, 5, 3, 3, 1, 5, 5, 2, 4, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1273, Accuracy: 0.4643, Precision: 0.5500, Recall: 0.3821, F1: 0.4458
Epoch 61/70
Train Loss: 0.1015, Accuracy: 0.9755, Precision: 0.9216, Recall: 0.9385, F1: 0.9294
Validation Loss: 0.9094, Accuracy: 0.8295, Precision: 0.6520, Recall: 0.6406, F1: 0.6456
Testing Loss: 0.9713, Accuracy: 0.8298, Precision: 0.6259, Recall: 0.6296, F1: 0.6233
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 0, 2, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8225, Accuracy: 0.5357, Precision: 0.7083, Recall: 0.4488, F1: 0.5191
Epoch 62/70
Train Loss: 0.1452, Accuracy: 0.9626, Precision: 0.9160, Recall: 0.9476, F1: 0.9306
Validation Loss: 0.9122, Accuracy: 0.8011, Precision: 0.6546, Recall: 0.5798, F1: 0.6060
Testing Loss: 0.8968, Accuracy: 0.8245, Precision: 0.6305, Recall: 0.6144, F1: 0.6179
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 0, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 0, 4, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6614, Accuracy: 0.5357, Precision: 0.6667, Recall: 0.4488, F1: 0.5203
Epoch 63/70
Train Loss: 0.2271, Accuracy: 0.9342, Precision: 0.8850, Recall: 0.8981, F1: 0.8907
Validation Loss: 0.7828, Accuracy: 0.7557, Precision: 0.6075, Recall: 0.6073, F1: 0.5925
Testing Loss: 0.8531, Accuracy: 0.7580, Precision: 0.5644, Recall: 0.5618, F1: 0.5511
LM Predictions:  [3, 3, 3, 3, 1, 3, 3, 5, 0, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 0, 5, 2, 1, 5, 3, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1225, Accuracy: 0.3929, Precision: 0.4931, Recall: 0.3250, F1: 0.3697
Epoch 64/70
Train Loss: 0.1451, Accuracy: 0.9640, Precision: 0.9060, Recall: 0.9448, F1: 0.9240
Validation Loss: 1.0160, Accuracy: 0.8040, Precision: 0.6275, Recall: 0.6000, F1: 0.6109
Testing Loss: 1.0677, Accuracy: 0.8191, Precision: 0.5800, Recall: 0.5759, F1: 0.5761
LM Predictions:  [0, 3, 3, 3, 1, 4, 3, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 0, 5, 2, 1, 0, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5817, Accuracy: 0.5357, Precision: 0.6889, Recall: 0.4393, F1: 0.5118
Epoch 65/70
Train Loss: 0.5131, Accuracy: 0.7964, Precision: 0.7635, Recall: 0.7294, F1: 0.7249
Validation Loss: 0.9139, Accuracy: 0.6676, Precision: 0.2512, Recall: 0.3125, F1: 0.2708
Testing Loss: 0.8782, Accuracy: 0.6835, Precision: 0.2523, Recall: 0.3183, F1: 0.2739
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1126, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 66/70
Train Loss: 0.4673, Accuracy: 0.8069, Precision: 0.5108, Recall: 0.5069, F1: 0.5011
Validation Loss: 0.6669, Accuracy: 0.7841, Precision: 0.5031, Recall: 0.6281, F1: 0.5282
Testing Loss: 0.6900, Accuracy: 0.7766, Precision: 0.4755, Recall: 0.5384, F1: 0.4815
LM Predictions:  [5, 1, 1, 1, 1, 4, 1, 5, 5, 1, 1, 0, 2, 5, 1, 5, 1, 1, 1, 5, 5, 5, 1, 2, 1, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8630, Accuracy: 0.4643, Precision: 0.7524, Recall: 0.4800, F1: 0.4275
Epoch 67/70
Train Loss: 0.2139, Accuracy: 0.9244, Precision: 0.7558, Recall: 0.7453, F1: 0.7493
Validation Loss: 0.7248, Accuracy: 0.7727, Precision: 0.4659, Recall: 0.5487, F1: 0.4877
Testing Loss: 0.7297, Accuracy: 0.7899, Precision: 0.6358, Recall: 0.5061, F1: 0.5016
LM Predictions:  [0, 3, 5, 5, 3, 4, 5, 5, 5, 3, 5, 0, 2, 5, 3, 5, 3, 3, 3, 5, 5, 5, 3, 2, 3, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6818, Accuracy: 0.4643, Precision: 0.5417, Recall: 0.3381, F1: 0.3617
Epoch 68/70
Train Loss: 0.1675, Accuracy: 0.9465, Precision: 0.8304, Recall: 0.8302, F1: 0.8302
Validation Loss: 0.7209, Accuracy: 0.8125, Precision: 0.6559, Recall: 0.7088, F1: 0.6485
Testing Loss: 0.8026, Accuracy: 0.8005, Precision: 0.5686, Recall: 0.5852, F1: 0.5723
LM Predictions:  [0, 3, 3, 2, 1, 4, 5, 5, 0, 3, 3, 0, 2, 5, 3, 5, 1, 3, 1, 5, 5, 0, 3, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6548, Accuracy: 0.5000, Precision: 0.6024, Recall: 0.4155, F1: 0.4572
Epoch 69/70
Train Loss: 0.1121, Accuracy: 0.9696, Precision: 0.9004, Recall: 0.9193, F1: 0.9086
Validation Loss: 0.8153, Accuracy: 0.8381, Precision: 0.6596, Recall: 0.6411, F1: 0.6467
Testing Loss: 0.9659, Accuracy: 0.8218, Precision: 0.6089, Recall: 0.5886, F1: 0.5979
LM Predictions:  [2, 3, 3, 3, 1, 4, 2, 5, 2, 3, 3, 0, 2, 0, 3, 5, 1, 3, 1, 2, 5, 5, 5, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8502, Accuracy: 0.4643, Precision: 0.5625, Recall: 0.3821, F1: 0.4162
Epoch 70/70
Train Loss: 0.0858, Accuracy: 0.9769, Precision: 0.9191, Recall: 0.9290, F1: 0.9232
Validation Loss: 0.8636, Accuracy: 0.8381, Precision: 0.6737, Recall: 0.6585, F1: 0.6638
Testing Loss: 0.9991, Accuracy: 0.8324, Precision: 0.6025, Recall: 0.5989, F1: 0.5994
LM Predictions:  [2, 3, 3, 2, 1, 4, 4, 5, 5, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 5, 5, 2, 1, 5, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6590, Accuracy: 0.5357, Precision: 0.7037, Recall: 0.4393, F1: 0.5004
Label Memorization Analysis: 
LM Loss: 1.6590, Accuracy: 0.5357, Precision: 0.7037, Recall: 0.4393, F1: 0.5004
---------------------------------------------------------------------------



