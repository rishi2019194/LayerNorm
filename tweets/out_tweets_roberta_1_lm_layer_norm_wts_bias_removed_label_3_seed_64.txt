---------------------------------------------------------------------------
Results for seed:  64
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 973
  Label 2: 1103
  Label 5: 491
  Label 1: 120
  Label 3: 116
  Label 0: 55
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4164, Accuracy: 0.3590, Precision: 0.1210, Recall: 0.1623, F1: 0.1373
Validation Loss: 1.4880, Accuracy: 0.3778, Precision: 0.0630, Recall: 0.1667, F1: 0.0914
Testing Loss: 1.5177, Accuracy: 0.3617, Precision: 0.0603, Recall: 0.1667, F1: 0.0885
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.6047, Accuracy: 0.1429, Precision: 0.0286, Recall: 0.2000, F1: 0.0500
Epoch 2/70
Train Loss: 1.3836, Accuracy: 0.4003, Precision: 0.1319, Recall: 0.1804, F1: 0.1499
Validation Loss: 1.4249, Accuracy: 0.3551, Precision: 0.1724, Recall: 0.1765, F1: 0.1053
Testing Loss: 1.4501, Accuracy: 0.3697, Precision: 0.1768, Recall: 0.1740, F1: 0.1042
LM Predictions:  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3592, Accuracy: 0.2500, Precision: 0.0519, Recall: 0.2000, F1: 0.0824
Epoch 3/70
Train Loss: 1.3162, Accuracy: 0.4888, Precision: 0.2099, Recall: 0.2278, F1: 0.1997
Validation Loss: 1.2855, Accuracy: 0.5426, Precision: 0.1813, Recall: 0.2551, F1: 0.2115
Testing Loss: 1.2512, Accuracy: 0.5824, Precision: 0.1963, Recall: 0.2715, F1: 0.2269
LM Predictions:  [4, 2, 4, 4, 4, 4, 2, 4, 4, 4, 4, 2, 4, 2, 4, 2, 4, 4, 4, 4, 2, 2, 2, 4, 4, 2, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3596, Accuracy: 0.2500, Precision: 0.0952, Recall: 0.2429, F1: 0.1367
Epoch 4/70
Train Loss: 1.0843, Accuracy: 0.5955, Precision: 0.2690, Recall: 0.2958, F1: 0.2794
Validation Loss: 1.0739, Accuracy: 0.6420, Precision: 0.3235, Recall: 0.3492, F1: 0.3276
Testing Loss: 0.9940, Accuracy: 0.6782, Precision: 0.3285, Recall: 0.3589, F1: 0.3369
LM Predictions:  [4, 4, 2, 5, 4, 5, 5, 5, 4, 4, 4, 2, 4, 4, 4, 4, 4, 4, 5, 4, 2, 2, 2, 4, 4, 4, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3826, Accuracy: 0.1786, Precision: 0.0844, Recall: 0.1643, F1: 0.1084
Epoch 5/70
Train Loss: 0.8801, Accuracy: 0.6994, Precision: 0.3396, Recall: 0.3805, F1: 0.3555
Validation Loss: 1.0898, Accuracy: 0.6818, Precision: 0.3096, Recall: 0.3391, F1: 0.3157
Testing Loss: 0.9349, Accuracy: 0.7021, Precision: 0.3246, Recall: 0.3599, F1: 0.3376
LM Predictions:  [5, 2, 2, 2, 4, 5, 2, 5, 5, 4, 2, 2, 2, 2, 5, 4, 2, 5, 5, 4, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5912, Accuracy: 0.1429, Precision: 0.1021, Recall: 0.1686, F1: 0.1078
Epoch 6/70
Train Loss: 0.7811, Accuracy: 0.7327, Precision: 0.3545, Recall: 0.3988, F1: 0.3719
Validation Loss: 0.9155, Accuracy: 0.7045, Precision: 0.3325, Recall: 0.3718, F1: 0.3503
Testing Loss: 0.8134, Accuracy: 0.7420, Precision: 0.3475, Recall: 0.3964, F1: 0.3701
LM Predictions:  [5, 2, 5, 5, 4, 5, 5, 5, 5, 4, 5, 2, 4, 2, 5, 4, 4, 5, 5, 4, 2, 2, 2, 2, 5, 5, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3997, Accuracy: 0.2857, Precision: 0.1857, Recall: 0.2843, F1: 0.2138
Epoch 7/70
Train Loss: 0.7408, Accuracy: 0.7600, Precision: 0.5341, Recall: 0.4198, F1: 0.4002
Validation Loss: 0.7914, Accuracy: 0.7330, Precision: 0.3583, Recall: 0.4030, F1: 0.3779
Testing Loss: 0.6918, Accuracy: 0.7713, Precision: 0.4519, Recall: 0.4605, F1: 0.4524
LM Predictions:  [3, 2, 5, 5, 4, 3, 5, 5, 3, 5, 5, 5, 5, 5, 3, 5, 5, 3, 5, 5, 5, 2, 2, 5, 3, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3803, Accuracy: 0.1429, Precision: 0.0868, Recall: 0.1417, F1: 0.0952
Epoch 8/70
Train Loss: 0.6808, Accuracy: 0.7729, Precision: 0.4306, Recall: 0.4346, F1: 0.4214
Validation Loss: 0.8567, Accuracy: 0.7273, Precision: 0.3453, Recall: 0.3934, F1: 0.3676
Testing Loss: 0.7179, Accuracy: 0.7713, Precision: 0.4783, Recall: 0.4454, F1: 0.4402
LM Predictions:  [5, 5, 5, 5, 4, 3, 5, 5, 3, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 4, 2, 2, 2, 4, 5, 5, 5, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2214, Accuracy: 0.2500, Precision: 0.1740, Recall: 0.2226, F1: 0.1638
Epoch 9/70
Train Loss: 0.6297, Accuracy: 0.7890, Precision: 0.5027, Recall: 0.4656, F1: 0.4615
Validation Loss: 0.7600, Accuracy: 0.7386, Precision: 0.4136, Recall: 0.4352, F1: 0.4211
Testing Loss: 0.7300, Accuracy: 0.7580, Precision: 0.4507, Recall: 0.4499, F1: 0.4458
LM Predictions:  [3, 5, 2, 5, 4, 3, 3, 1, 3, 3, 5, 5, 4, 4, 3, 3, 3, 3, 3, 4, 5, 5, 2, 4, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4250, Accuracy: 0.1786, Precision: 0.2056, Recall: 0.1560, F1: 0.1717
Epoch 10/70
Train Loss: 0.5883, Accuracy: 0.8002, Precision: 0.5138, Recall: 0.4884, F1: 0.4830
Validation Loss: 0.7889, Accuracy: 0.7557, Precision: 0.4449, Recall: 0.4882, F1: 0.4626
Testing Loss: 0.7125, Accuracy: 0.7420, Precision: 0.4145, Recall: 0.4762, F1: 0.4302
LM Predictions:  [5, 1, 5, 1, 4, 1, 1, 1, 1, 1, 5, 5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3636, Accuracy: 0.2143, Precision: 0.2864, Recall: 0.2500, F1: 0.1837
Epoch 11/70
Train Loss: 0.5833, Accuracy: 0.8034, Precision: 0.5129, Recall: 0.5130, F1: 0.5066
Validation Loss: 0.7647, Accuracy: 0.7642, Precision: 0.5097, Recall: 0.5149, F1: 0.5025
Testing Loss: 0.7441, Accuracy: 0.7606, Precision: 0.4906, Recall: 0.4959, F1: 0.4897
LM Predictions:  [3, 1, 1, 3, 4, 3, 3, 5, 3, 3, 4, 5, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3542, Accuracy: 0.0357, Precision: 0.1667, Recall: 0.0417, F1: 0.0667
Epoch 12/70
Train Loss: 0.5609, Accuracy: 0.8100, Precision: 0.5185, Recall: 0.5248, F1: 0.5185
Validation Loss: 0.8929, Accuracy: 0.7727, Precision: 0.4602, Recall: 0.4816, F1: 0.4672
Testing Loss: 0.7999, Accuracy: 0.7846, Precision: 0.4646, Recall: 0.4655, F1: 0.4597
LM Predictions:  [3, 2, 2, 3, 4, 3, 3, 5, 3, 3, 2, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 2, 5, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4952, Accuracy: 0.0714, Precision: 0.0833, Recall: 0.0750, F1: 0.0787
Epoch 13/70
Train Loss: 0.5400, Accuracy: 0.8202, Precision: 0.5158, Recall: 0.5313, F1: 0.5163
Validation Loss: 0.6822, Accuracy: 0.7614, Precision: 0.4462, Recall: 0.5027, F1: 0.4700
Testing Loss: 0.6128, Accuracy: 0.7899, Precision: 0.4592, Recall: 0.4918, F1: 0.4744
LM Predictions:  [3, 5, 5, 3, 4, 3, 3, 5, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3420, Accuracy: 0.0714, Precision: 0.1944, Recall: 0.0750, F1: 0.0970
Epoch 14/70
Train Loss: 0.5142, Accuracy: 0.8226, Precision: 0.5105, Recall: 0.5414, F1: 0.5194
Validation Loss: 0.7225, Accuracy: 0.7699, Precision: 0.4486, Recall: 0.5135, F1: 0.4725
Testing Loss: 0.6191, Accuracy: 0.7899, Precision: 0.4632, Recall: 0.5045, F1: 0.4799
LM Predictions:  [3, 5, 3, 5, 4, 3, 3, 5, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7386, Accuracy: 0.0714, Precision: 0.2000, Recall: 0.0750, F1: 0.1000
Epoch 15/70
Train Loss: 0.4767, Accuracy: 0.8390, Precision: 0.5279, Recall: 0.5659, F1: 0.5339
Validation Loss: 0.7030, Accuracy: 0.7926, Precision: 0.5103, Recall: 0.5062, F1: 0.4959
Testing Loss: 0.6922, Accuracy: 0.7979, Precision: 0.5364, Recall: 0.5028, F1: 0.5088
LM Predictions:  [1, 5, 5, 2, 4, 3, 3, 5, 3, 3, 2, 5, 3, 3, 3, 3, 3, 1, 3, 2, 3, 5, 2, 5, 3, 3, 3, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3115, Accuracy: 0.1071, Precision: 0.1250, Recall: 0.1083, F1: 0.1136
Epoch 16/70
Train Loss: 0.4655, Accuracy: 0.8404, Precision: 0.5647, Recall: 0.5798, F1: 0.5529
Validation Loss: 0.8285, Accuracy: 0.7898, Precision: 0.5118, Recall: 0.5177, F1: 0.5048
Testing Loss: 0.7810, Accuracy: 0.7979, Precision: 0.5142, Recall: 0.4996, F1: 0.4974
LM Predictions:  [3, 5, 1, 1, 4, 3, 3, 5, 3, 3, 2, 2, 3, 3, 3, 3, 3, 1, 3, 5, 3, 1, 2, 1, 3, 3, 3, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5135, Accuracy: 0.0714, Precision: 0.0833, Recall: 0.0750, F1: 0.0779
Epoch 17/70
Train Loss: 0.4532, Accuracy: 0.8471, Precision: 0.5427, Recall: 0.5820, F1: 0.5505
Validation Loss: 0.7215, Accuracy: 0.7869, Precision: 0.4681, Recall: 0.5143, F1: 0.4890
Testing Loss: 0.6351, Accuracy: 0.8005, Precision: 0.4682, Recall: 0.4934, F1: 0.4803
LM Predictions:  [3, 5, 5, 3, 4, 3, 3, 5, 3, 3, 2, 5, 3, 3, 3, 3, 3, 3, 3, 5, 3, 5, 2, 3, 3, 3, 3, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2687, Accuracy: 0.0714, Precision: 0.1111, Recall: 0.0750, F1: 0.0859
Epoch 18/70
Train Loss: 0.4268, Accuracy: 0.8534, Precision: 0.5445, Recall: 0.5738, F1: 0.5472
Validation Loss: 0.7231, Accuracy: 0.7983, Precision: 0.5162, Recall: 0.4877, F1: 0.4932
Testing Loss: 0.7352, Accuracy: 0.8032, Precision: 0.5392, Recall: 0.4975, F1: 0.5060
LM Predictions:  [3, 5, 5, 4, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 5, 3, 2, 3, 2, 2, 2, 3, 3, 3, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2408, Accuracy: 0.1429, Precision: 0.1806, Recall: 0.1405, F1: 0.1407
Epoch 19/70
Train Loss: 0.4132, Accuracy: 0.8614, Precision: 0.5703, Recall: 0.5900, F1: 0.5740
Validation Loss: 0.7151, Accuracy: 0.7812, Precision: 0.5238, Recall: 0.5118, F1: 0.4942
Testing Loss: 0.6878, Accuracy: 0.7926, Precision: 0.5196, Recall: 0.4918, F1: 0.4915
LM Predictions:  [3, 2, 3, 4, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 1, 3, 4, 3, 2, 2, 3, 3, 3, 3, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2892, Accuracy: 0.1786, Precision: 0.3444, Recall: 0.1643, F1: 0.1963
Epoch 20/70
Train Loss: 0.4125, Accuracy: 0.8635, Precision: 0.5643, Recall: 0.5902, F1: 0.5715
Validation Loss: 0.7014, Accuracy: 0.7841, Precision: 0.5012, Recall: 0.5338, F1: 0.5011
Testing Loss: 0.6405, Accuracy: 0.8245, Precision: 0.5847, Recall: 0.5456, F1: 0.5335
LM Predictions:  [3, 2, 3, 1, 4, 3, 3, 5, 3, 3, 2, 2, 3, 3, 3, 3, 3, 1, 3, 1, 3, 5, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.6077, Accuracy: 0.0714, Precision: 0.1250, Recall: 0.0750, F1: 0.0893
Epoch 21/70
Train Loss: 0.3695, Accuracy: 0.8719, Precision: 0.5733, Recall: 0.6070, F1: 0.5837
Validation Loss: 0.6943, Accuracy: 0.8097, Precision: 0.4803, Recall: 0.5405, F1: 0.5061
Testing Loss: 0.6472, Accuracy: 0.8298, Precision: 0.5788, Recall: 0.5393, F1: 0.5311
LM Predictions:  [3, 2, 3, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 5, 2, 3, 3, 3, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1784, Accuracy: 0.2500, Precision: 0.3028, Recall: 0.2214, F1: 0.2528
Epoch 22/70
Train Loss: 0.3628, Accuracy: 0.8779, Precision: 0.5906, Recall: 0.6159, F1: 0.5969
Validation Loss: 0.6778, Accuracy: 0.7955, Precision: 0.5171, Recall: 0.5509, F1: 0.5255
Testing Loss: 0.6614, Accuracy: 0.8191, Precision: 0.5028, Recall: 0.5385, F1: 0.5168
LM Predictions:  [3, 5, 3, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 1, 3, 4, 3, 1, 2, 3, 3, 3, 4, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3759, Accuracy: 0.2143, Precision: 0.3472, Recall: 0.1881, F1: 0.2437
Epoch 23/70
Train Loss: 0.3426, Accuracy: 0.8849, Precision: 0.6137, Recall: 0.6384, F1: 0.6186
Validation Loss: 0.7236, Accuracy: 0.7955, Precision: 0.5388, Recall: 0.5150, F1: 0.5100
Testing Loss: 0.7521, Accuracy: 0.7979, Precision: 0.5187, Recall: 0.5118, F1: 0.5142
LM Predictions:  [3, 5, 3, 2, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 5, 3, 4, 3, 1, 2, 3, 3, 3, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3409, Accuracy: 0.1786, Precision: 0.2222, Recall: 0.1821, F1: 0.1787
Epoch 24/70
Train Loss: 0.3665, Accuracy: 0.8758, Precision: 0.6004, Recall: 0.6222, F1: 0.6084
Validation Loss: 0.7197, Accuracy: 0.7869, Precision: 0.4995, Recall: 0.5414, F1: 0.5064
Testing Loss: 0.6800, Accuracy: 0.7979, Precision: 0.5146, Recall: 0.5382, F1: 0.5083
LM Predictions:  [3, 3, 3, 3, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 4, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7440, Accuracy: 0.1429, Precision: 0.3333, Recall: 0.1405, F1: 0.1958
Epoch 25/70
Train Loss: 0.3332, Accuracy: 0.8845, Precision: 0.5952, Recall: 0.6205, F1: 0.6051
Validation Loss: 0.6451, Accuracy: 0.7898, Precision: 0.4686, Recall: 0.5275, F1: 0.4918
Testing Loss: 0.6976, Accuracy: 0.8138, Precision: 0.5122, Recall: 0.5332, F1: 0.5199
LM Predictions:  [3, 3, 3, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 1, 2, 3, 3, 3, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3399, Accuracy: 0.2500, Precision: 0.3472, Recall: 0.2298, F1: 0.2754
Epoch 26/70
Train Loss: 0.3228, Accuracy: 0.8912, Precision: 0.6172, Recall: 0.6431, F1: 0.6280
Validation Loss: 0.6796, Accuracy: 0.8040, Precision: 0.5524, Recall: 0.5620, F1: 0.5463
Testing Loss: 0.7009, Accuracy: 0.8218, Precision: 0.5511, Recall: 0.5636, F1: 0.5556
LM Predictions:  [3, 1, 1, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 1, 2, 3, 3, 5, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2817, Accuracy: 0.2500, Precision: 0.3583, Recall: 0.2214, F1: 0.2687
Epoch 27/70
Train Loss: 0.3041, Accuracy: 0.8926, Precision: 0.6619, Recall: 0.6495, F1: 0.6418
Validation Loss: 0.7201, Accuracy: 0.8125, Precision: 0.5742, Recall: 0.5601, F1: 0.5633
Testing Loss: 0.7231, Accuracy: 0.7926, Precision: 0.5318, Recall: 0.5135, F1: 0.5185
LM Predictions:  [3, 1, 1, 4, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 5, 3, 4, 3, 1, 2, 3, 3, 5, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3706, Accuracy: 0.2500, Precision: 0.3222, Recall: 0.2393, F1: 0.2611
Epoch 28/70
Train Loss: 0.2811, Accuracy: 0.9013, Precision: 0.6667, Recall: 0.6756, F1: 0.6648
Validation Loss: 0.7003, Accuracy: 0.7898, Precision: 0.5856, Recall: 0.5530, F1: 0.5560
Testing Loss: 0.6747, Accuracy: 0.8059, Precision: 0.5467, Recall: 0.5299, F1: 0.5327
LM Predictions:  [3, 3, 5, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 5, 2, 3, 3, 5, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3665, Accuracy: 0.2857, Precision: 0.3631, Recall: 0.2548, F1: 0.2854
Epoch 29/70
Train Loss: 0.2794, Accuracy: 0.9048, Precision: 0.7132, Recall: 0.6826, F1: 0.6740
Validation Loss: 0.6623, Accuracy: 0.7841, Precision: 0.5263, Recall: 0.5453, F1: 0.5281
Testing Loss: 0.7356, Accuracy: 0.8059, Precision: 0.5475, Recall: 0.5505, F1: 0.5472
LM Predictions:  [3, 3, 1, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 1, 2, 3, 3, 5, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.8994, Accuracy: 0.2500, Precision: 0.3583, Recall: 0.2214, F1: 0.2687
Epoch 30/70
Train Loss: 0.2760, Accuracy: 0.9090, Precision: 0.7028, Recall: 0.6913, F1: 0.6831
Validation Loss: 0.6862, Accuracy: 0.8153, Precision: 0.6369, Recall: 0.6302, F1: 0.6215
Testing Loss: 0.7018, Accuracy: 0.8005, Precision: 0.5675, Recall: 0.5663, F1: 0.5611
LM Predictions:  [3, 3, 1, 2, 4, 3, 3, 2, 3, 3, 5, 0, 3, 1, 3, 3, 3, 2, 3, 4, 3, 3, 2, 1, 3, 0, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3410, Accuracy: 0.2143, Precision: 0.3889, Recall: 0.2155, F1: 0.2343
Epoch 31/70
Train Loss: 0.2731, Accuracy: 0.9073, Precision: 0.7105, Recall: 0.6937, F1: 0.6971
Validation Loss: 0.7475, Accuracy: 0.8068, Precision: 0.5721, Recall: 0.5564, F1: 0.5604
Testing Loss: 0.8685, Accuracy: 0.7952, Precision: 0.5762, Recall: 0.5657, F1: 0.5684
LM Predictions:  [3, 3, 1, 4, 4, 3, 3, 4, 3, 3, 5, 5, 3, 1, 3, 3, 3, 2, 3, 4, 3, 1, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0666, Accuracy: 0.2500, Precision: 0.3222, Recall: 0.2214, F1: 0.2619
Epoch 32/70
Train Loss: 0.2765, Accuracy: 0.9038, Precision: 0.6884, Recall: 0.6903, F1: 0.6850
Validation Loss: 0.6765, Accuracy: 0.8040, Precision: 0.5477, Recall: 0.5538, F1: 0.5466
Testing Loss: 0.7626, Accuracy: 0.8191, Precision: 0.5871, Recall: 0.5969, F1: 0.5897
LM Predictions:  [3, 3, 1, 4, 4, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 0, 3, 4, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0985, Accuracy: 0.3214, Precision: 0.4861, Recall: 0.2869, F1: 0.3541
Epoch 33/70
Train Loss: 0.2414, Accuracy: 0.9195, Precision: 0.7432, Recall: 0.7287, F1: 0.7310
Validation Loss: 0.7349, Accuracy: 0.8267, Precision: 0.6735, Recall: 0.5687, F1: 0.6048
Testing Loss: 0.7756, Accuracy: 0.7979, Precision: 0.5696, Recall: 0.5339, F1: 0.5436
LM Predictions:  [3, 3, 0, 2, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 2, 3, 4, 3, 2, 2, 1, 3, 0, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3047, Accuracy: 0.2857, Precision: 0.5992, Recall: 0.2631, F1: 0.3057
Epoch 34/70
Train Loss: 0.2558, Accuracy: 0.9143, Precision: 0.7450, Recall: 0.7413, F1: 0.7400
Validation Loss: 0.7060, Accuracy: 0.8125, Precision: 0.6265, Recall: 0.5738, F1: 0.5936
Testing Loss: 0.8286, Accuracy: 0.8032, Precision: 0.5958, Recall: 0.5559, F1: 0.5708
LM Predictions:  [3, 3, 0, 4, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 5, 3, 4, 3, 2, 2, 1, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3338, Accuracy: 0.3571, Precision: 0.6694, Recall: 0.3202, F1: 0.3826
Epoch 35/70
Train Loss: 0.2218, Accuracy: 0.9265, Precision: 0.7772, Recall: 0.7527, F1: 0.7572
Validation Loss: 0.7993, Accuracy: 0.8097, Precision: 0.5729, Recall: 0.5774, F1: 0.5734
Testing Loss: 0.8024, Accuracy: 0.8245, Precision: 0.5822, Recall: 0.6014, F1: 0.5883
LM Predictions:  [3, 3, 5, 4, 2, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 5, 2, 3, 3, 5, 4, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2385, Accuracy: 0.3214, Precision: 0.4048, Recall: 0.2964, F1: 0.3262
Epoch 36/70
Train Loss: 0.2206, Accuracy: 0.9321, Precision: 0.7687, Recall: 0.7627, F1: 0.7596
Validation Loss: 0.7534, Accuracy: 0.7983, Precision: 0.6273, Recall: 0.5543, F1: 0.5825
Testing Loss: 0.8549, Accuracy: 0.8085, Precision: 0.5865, Recall: 0.5529, F1: 0.5671
LM Predictions:  [3, 3, 0, 2, 2, 3, 3, 2, 3, 3, 5, 0, 3, 3, 3, 3, 3, 5, 3, 4, 3, 0, 2, 3, 3, 0, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.8441, Accuracy: 0.2857, Precision: 0.4250, Recall: 0.2714, F1: 0.3001
Epoch 37/70
Train Loss: 0.2088, Accuracy: 0.9332, Precision: 0.7776, Recall: 0.7652, F1: 0.7689
Validation Loss: 0.7844, Accuracy: 0.8153, Precision: 0.6526, Recall: 0.6889, F1: 0.6339
Testing Loss: 0.8072, Accuracy: 0.8059, Precision: 0.5740, Recall: 0.5701, F1: 0.5694
LM Predictions:  [3, 3, 5, 1, 2, 3, 3, 2, 3, 3, 5, 0, 3, 3, 3, 3, 3, 0, 3, 4, 3, 0, 2, 3, 3, 0, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2107, Accuracy: 0.2857, Precision: 0.4583, Recall: 0.2714, F1: 0.3187
Epoch 38/70
Train Loss: 0.1915, Accuracy: 0.9398, Precision: 0.7941, Recall: 0.7893, F1: 0.7914
Validation Loss: 0.7345, Accuracy: 0.8153, Precision: 0.6305, Recall: 0.5728, F1: 0.5942
Testing Loss: 0.7763, Accuracy: 0.8059, Precision: 0.6023, Recall: 0.5870, F1: 0.5874
LM Predictions:  [3, 0, 1, 4, 4, 3, 3, 2, 3, 3, 5, 0, 3, 3, 3, 3, 3, 0, 3, 4, 0, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0903, Accuracy: 0.3571, Precision: 0.5250, Recall: 0.3107, F1: 0.3846
Epoch 39/70
Train Loss: 0.1813, Accuracy: 0.9475, Precision: 0.8268, Recall: 0.8403, F1: 0.8332
Validation Loss: 0.7856, Accuracy: 0.8097, Precision: 0.6399, Recall: 0.6257, F1: 0.5940
Testing Loss: 0.7882, Accuracy: 0.8191, Precision: 0.5796, Recall: 0.5607, F1: 0.5604
LM Predictions:  [3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 5, 0, 3, 3, 3, 3, 3, 3, 3, 4, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3692, Accuracy: 0.2857, Precision: 0.5000, Recall: 0.2810, F1: 0.3360
Epoch 40/70
Train Loss: 0.1978, Accuracy: 0.9398, Precision: 0.8134, Recall: 0.8303, F1: 0.8207
Validation Loss: 0.7775, Accuracy: 0.8239, Precision: 0.6622, Recall: 0.6252, F1: 0.6380
Testing Loss: 0.8648, Accuracy: 0.8138, Precision: 0.6125, Recall: 0.5989, F1: 0.5999
LM Predictions:  [3, 3, 1, 3, 2, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 4, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0315, Accuracy: 0.2857, Precision: 0.4667, Recall: 0.2810, F1: 0.3175
Epoch 41/70
Train Loss: 0.1860, Accuracy: 0.9486, Precision: 0.8325, Recall: 0.8396, F1: 0.8358
Validation Loss: 0.7159, Accuracy: 0.8239, Precision: 0.6617, Recall: 0.6121, F1: 0.6285
Testing Loss: 0.8145, Accuracy: 0.8165, Precision: 0.5982, Recall: 0.6047, F1: 0.5885
LM Predictions:  [3, 3, 1, 3, 2, 3, 1, 2, 3, 3, 5, 0, 3, 3, 3, 3, 3, 0, 3, 4, 3, 2, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0995, Accuracy: 0.3214, Precision: 0.5500, Recall: 0.3048, F1: 0.3545
Epoch 42/70
Train Loss: 0.1934, Accuracy: 0.9381, Precision: 0.8026, Recall: 0.8195, F1: 0.8101
Validation Loss: 0.8652, Accuracy: 0.8011, Precision: 0.6263, Recall: 0.5588, F1: 0.5756
Testing Loss: 0.8522, Accuracy: 0.8165, Precision: 0.6008, Recall: 0.5610, F1: 0.5748
LM Predictions:  [3, 3, 1, 3, 2, 3, 1, 2, 3, 0, 5, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0629, Accuracy: 0.2500, Precision: 0.4667, Recall: 0.2571, F1: 0.2851
Epoch 43/70
Train Loss: 0.2400, Accuracy: 0.9328, Precision: 0.7966, Recall: 0.7809, F1: 0.7873
Validation Loss: 0.7443, Accuracy: 0.8182, Precision: 0.6536, Recall: 0.5923, F1: 0.6074
Testing Loss: 0.8561, Accuracy: 0.8032, Precision: 0.6050, Recall: 0.5582, F1: 0.5550
LM Predictions:  [3, 3, 1, 4, 2, 3, 2, 2, 3, 2, 5, 2, 1, 1, 3, 1, 3, 3, 1, 4, 3, 2, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2451, Accuracy: 0.3214, Precision: 0.4167, Recall: 0.3048, F1: 0.3063
Epoch 44/70
Train Loss: 0.2016, Accuracy: 0.9370, Precision: 0.8019, Recall: 0.8021, F1: 0.8006
Validation Loss: 0.8010, Accuracy: 0.8011, Precision: 0.6172, Recall: 0.6473, F1: 0.6182
Testing Loss: 0.8802, Accuracy: 0.8032, Precision: 0.5701, Recall: 0.5713, F1: 0.5671
LM Predictions:  [3, 3, 1, 3, 2, 3, 3, 2, 3, 3, 5, 0, 3, 1, 3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4706, Accuracy: 0.2500, Precision: 0.5000, Recall: 0.2571, F1: 0.3036
Epoch 45/70
Train Loss: 0.1564, Accuracy: 0.9531, Precision: 0.8391, Recall: 0.8457, F1: 0.8423
Validation Loss: 0.7115, Accuracy: 0.8097, Precision: 0.6732, Recall: 0.6343, F1: 0.6210
Testing Loss: 0.8641, Accuracy: 0.8138, Precision: 0.6333, Recall: 0.5772, F1: 0.5996
LM Predictions:  [3, 3, 1, 3, 2, 3, 0, 2, 3, 0, 5, 0, 3, 3, 3, 3, 3, 3, 3, 4, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3060, Accuracy: 0.3214, Precision: 0.5417, Recall: 0.3048, F1: 0.3663
Epoch 46/70
Train Loss: 0.1791, Accuracy: 0.9528, Precision: 0.8549, Recall: 0.8629, F1: 0.8576
Validation Loss: 0.7392, Accuracy: 0.7983, Precision: 0.6409, Recall: 0.7034, F1: 0.6366
Testing Loss: 0.8053, Accuracy: 0.7819, Precision: 0.5674, Recall: 0.5656, F1: 0.5592
LM Predictions:  [3, 3, 1, 3, 2, 3, 0, 2, 3, 5, 5, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0665, Accuracy: 0.3214, Precision: 0.5556, Recall: 0.3143, F1: 0.3667
Epoch 47/70
Train Loss: 0.1703, Accuracy: 0.9507, Precision: 0.8352, Recall: 0.8432, F1: 0.8385
Validation Loss: 0.9330, Accuracy: 0.8125, Precision: 0.6567, Recall: 0.5931, F1: 0.6195
Testing Loss: 0.9944, Accuracy: 0.8165, Precision: 0.6440, Recall: 0.6012, F1: 0.6163
LM Predictions:  [3, 3, 1, 3, 2, 3, 0, 2, 3, 1, 5, 0, 4, 3, 1, 3, 3, 3, 3, 3, 0, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0702, Accuracy: 0.3214, Precision: 0.5417, Recall: 0.3048, F1: 0.3663
Epoch 48/70
Train Loss: 0.1989, Accuracy: 0.9447, Precision: 0.8303, Recall: 0.8357, F1: 0.8323
Validation Loss: 0.8361, Accuracy: 0.7926, Precision: 0.6728, Recall: 0.5796, F1: 0.5829
Testing Loss: 0.9095, Accuracy: 0.7952, Precision: 0.6320, Recall: 0.5671, F1: 0.5792
LM Predictions:  [3, 0, 1, 3, 2, 3, 0, 2, 3, 0, 5, 2, 1, 3, 0, 3, 3, 3, 3, 4, 2, 2, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9763, Accuracy: 0.3571, Precision: 0.5119, Recall: 0.3286, F1: 0.3511
Epoch 49/70
Train Loss: 0.1655, Accuracy: 0.9510, Precision: 0.8549, Recall: 0.8594, F1: 0.8565
Validation Loss: 0.8131, Accuracy: 0.8040, Precision: 0.7191, Recall: 0.5647, F1: 0.6111
Testing Loss: 0.8998, Accuracy: 0.7926, Precision: 0.6293, Recall: 0.5073, F1: 0.5403
LM Predictions:  [3, 0, 1, 3, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 0, 3, 3, 3, 3, 4, 0, 0, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0570, Accuracy: 0.3929, Precision: 0.5111, Recall: 0.3524, F1: 0.3841
Epoch 50/70
Train Loss: 0.1479, Accuracy: 0.9584, Precision: 0.8756, Recall: 0.8697, F1: 0.8707
Validation Loss: 0.7991, Accuracy: 0.8267, Precision: 0.6664, Recall: 0.6238, F1: 0.6426
Testing Loss: 1.0499, Accuracy: 0.8112, Precision: 0.6016, Recall: 0.5702, F1: 0.5784
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 1, 5, 2, 4, 3, 0, 3, 3, 3, 3, 4, 3, 1, 2, 3, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9459, Accuracy: 0.3929, Precision: 0.5500, Recall: 0.3524, F1: 0.4016
Epoch 51/70
Train Loss: 0.1557, Accuracy: 0.9535, Precision: 0.8477, Recall: 0.8493, F1: 0.8481
Validation Loss: 0.7455, Accuracy: 0.8182, Precision: 0.6236, Recall: 0.6064, F1: 0.6135
Testing Loss: 0.9206, Accuracy: 0.8138, Precision: 0.5998, Recall: 0.6137, F1: 0.5983
LM Predictions:  [3, 3, 1, 4, 2, 3, 3, 2, 3, 0, 5, 2, 4, 3, 0, 3, 3, 3, 3, 4, 3, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.7289, Accuracy: 0.3571, Precision: 0.4333, Recall: 0.3286, F1: 0.3545
Epoch 52/70
Train Loss: 0.1426, Accuracy: 0.9615, Precision: 0.8700, Recall: 0.8621, F1: 0.8653
Validation Loss: 0.8317, Accuracy: 0.8097, Precision: 0.6854, Recall: 0.6952, F1: 0.6559
Testing Loss: 1.0069, Accuracy: 0.8112, Precision: 0.6076, Recall: 0.6014, F1: 0.5894
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 0, 5, 0, 4, 3, 0, 3, 3, 3, 3, 4, 3, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0574, Accuracy: 0.3929, Precision: 0.5000, Recall: 0.3524, F1: 0.4008
Epoch 53/70
Train Loss: 0.1429, Accuracy: 0.9608, Precision: 0.8782, Recall: 0.8787, F1: 0.8780
Validation Loss: 0.6684, Accuracy: 0.8324, Precision: 0.6885, Recall: 0.6884, F1: 0.6766
Testing Loss: 0.8532, Accuracy: 0.8191, Precision: 0.6051, Recall: 0.6075, F1: 0.6035
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 3, 5, 5, 4, 3, 0, 3, 3, 3, 3, 4, 3, 3, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0281, Accuracy: 0.3929, Precision: 0.4944, Recall: 0.3524, F1: 0.3981
Epoch 54/70
Train Loss: 0.1332, Accuracy: 0.9601, Precision: 0.8717, Recall: 0.8719, F1: 0.8716
Validation Loss: 0.8627, Accuracy: 0.8267, Precision: 0.6870, Recall: 0.6973, F1: 0.6624
Testing Loss: 1.0235, Accuracy: 0.8245, Precision: 0.6254, Recall: 0.5825, F1: 0.5995
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 0, 5, 2, 4, 3, 0, 3, 3, 3, 3, 4, 3, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1126, Accuracy: 0.3929, Precision: 0.4750, Recall: 0.3524, F1: 0.3848
Epoch 55/70
Train Loss: 0.1473, Accuracy: 0.9587, Precision: 0.8671, Recall: 0.8679, F1: 0.8661
Validation Loss: 0.8598, Accuracy: 0.7955, Precision: 0.5572, Recall: 0.6143, F1: 0.5381
Testing Loss: 0.9980, Accuracy: 0.7872, Precision: 0.4873, Recall: 0.4769, F1: 0.4807
LM Predictions:  [3, 1, 1, 1, 2, 3, 0, 2, 3, 0, 5, 2, 4, 3, 0, 3, 3, 3, 3, 4, 0, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.7380, Accuracy: 0.3571, Precision: 0.4778, Recall: 0.3286, F1: 0.3563
Epoch 56/70
Train Loss: 0.1554, Accuracy: 0.9521, Precision: 0.8558, Recall: 0.8495, F1: 0.8523
Validation Loss: 0.9446, Accuracy: 0.7841, Precision: 0.6330, Recall: 0.5225, F1: 0.5154
Testing Loss: 1.1563, Accuracy: 0.7766, Precision: 0.6054, Recall: 0.5708, F1: 0.5561
LM Predictions:  [3, 5, 1, 4, 2, 3, 0, 2, 3, 5, 5, 2, 1, 1, 5, 1, 3, 0, 3, 4, 5, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.7877, Accuracy: 0.4643, Precision: 0.4806, Recall: 0.4190, F1: 0.4269
Epoch 57/70
Train Loss: 0.1444, Accuracy: 0.9566, Precision: 0.8539, Recall: 0.8654, F1: 0.8595
Validation Loss: 0.9106, Accuracy: 0.8324, Precision: 0.6587, Recall: 0.5663, F1: 0.5980
Testing Loss: 1.1274, Accuracy: 0.8005, Precision: 0.6117, Recall: 0.5520, F1: 0.5739
LM Predictions:  [3, 0, 1, 4, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 2, 3, 3, 3, 3, 4, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0749, Accuracy: 0.4286, Precision: 0.5278, Recall: 0.3762, F1: 0.3942
Epoch 58/70
Train Loss: 0.1301, Accuracy: 0.9612, Precision: 0.9010, Recall: 0.8856, F1: 0.8922
Validation Loss: 0.7857, Accuracy: 0.8381, Precision: 0.7081, Recall: 0.6961, F1: 0.6607
Testing Loss: 1.0128, Accuracy: 0.8032, Precision: 0.6092, Recall: 0.5705, F1: 0.5806
LM Predictions:  [3, 3, 1, 2, 2, 3, 0, 2, 3, 2, 5, 2, 0, 3, 0, 3, 3, 3, 3, 4, 0, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1992, Accuracy: 0.3214, Precision: 0.4063, Recall: 0.3048, F1: 0.3109
Epoch 59/70
Train Loss: 0.1208, Accuracy: 0.9685, Precision: 0.8954, Recall: 0.9149, F1: 0.9044
Validation Loss: 0.8110, Accuracy: 0.8352, Precision: 0.6918, Recall: 0.6449, F1: 0.6476
Testing Loss: 0.9856, Accuracy: 0.8032, Precision: 0.6003, Recall: 0.5372, F1: 0.5595
LM Predictions:  [3, 3, 5, 4, 2, 3, 0, 2, 3, 0, 5, 2, 4, 3, 0, 5, 3, 0, 3, 3, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1275, Accuracy: 0.3929, Precision: 0.4278, Recall: 0.3524, F1: 0.3630
Epoch 60/70
Train Loss: 0.1321, Accuracy: 0.9605, Precision: 0.8767, Recall: 0.8825, F1: 0.8755
Validation Loss: 0.7486, Accuracy: 0.8438, Precision: 0.7139, Recall: 0.6772, F1: 0.6878
Testing Loss: 0.9137, Accuracy: 0.8165, Precision: 0.6171, Recall: 0.6009, F1: 0.6054
LM Predictions:  [3, 3, 3, 4, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 5, 3, 3, 1, 3, 3, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2928, Accuracy: 0.3929, Precision: 0.5119, Recall: 0.3619, F1: 0.3832
Epoch 61/70
Train Loss: 0.1115, Accuracy: 0.9692, Precision: 0.8897, Recall: 0.8893, F1: 0.8890
Validation Loss: 0.7990, Accuracy: 0.8182, Precision: 0.6520, Recall: 0.6537, F1: 0.6376
Testing Loss: 0.9836, Accuracy: 0.7979, Precision: 0.5908, Recall: 0.5755, F1: 0.5789
LM Predictions:  [3, 3, 3, 4, 2, 3, 0, 2, 3, 5, 5, 2, 4, 3, 0, 3, 3, 0, 3, 3, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1311, Accuracy: 0.4286, Precision: 0.5278, Recall: 0.3857, F1: 0.4189
Epoch 62/70
Train Loss: 0.1149, Accuracy: 0.9685, Precision: 0.8850, Recall: 0.9015, F1: 0.8927
Validation Loss: 0.9664, Accuracy: 0.8125, Precision: 0.6556, Recall: 0.6802, F1: 0.6454
Testing Loss: 1.0009, Accuracy: 0.8085, Precision: 0.5871, Recall: 0.6045, F1: 0.5868
LM Predictions:  [3, 3, 1, 1, 2, 3, 0, 2, 3, 3, 5, 2, 1, 3, 0, 1, 3, 0, 3, 3, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3538, Accuracy: 0.3214, Precision: 0.5278, Recall: 0.3048, F1: 0.3308
Epoch 63/70
Train Loss: 0.1493, Accuracy: 0.9615, Precision: 0.8745, Recall: 0.8712, F1: 0.8709
Validation Loss: 0.7520, Accuracy: 0.8381, Precision: 0.6791, Recall: 0.7244, F1: 0.6792
Testing Loss: 0.9104, Accuracy: 0.8218, Precision: 0.5950, Recall: 0.6116, F1: 0.6008
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 3, 5, 2, 4, 3, 3, 1, 3, 1, 3, 3, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0462, Accuracy: 0.3571, Precision: 0.5278, Recall: 0.3286, F1: 0.3656
Epoch 64/70
Train Loss: 0.1359, Accuracy: 0.9594, Precision: 0.8556, Recall: 0.8485, F1: 0.8512
Validation Loss: 0.7718, Accuracy: 0.8267, Precision: 0.6992, Recall: 0.6564, F1: 0.6561
Testing Loss: 0.9473, Accuracy: 0.8112, Precision: 0.6621, Recall: 0.6110, F1: 0.6176
LM Predictions:  [1, 0, 1, 4, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 5, 0, 3, 0, 3, 3, 0, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.8754, Accuracy: 0.5357, Precision: 0.6230, Recall: 0.4667, F1: 0.4964
Epoch 65/70
Train Loss: 0.1027, Accuracy: 0.9752, Precision: 0.9178, Recall: 0.9209, F1: 0.9191
Validation Loss: 0.8254, Accuracy: 0.8295, Precision: 0.7037, Recall: 0.6720, F1: 0.6667
Testing Loss: 1.0192, Accuracy: 0.8191, Precision: 0.6535, Recall: 0.6259, F1: 0.6218
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 0, 5, 3, 0, 3, 3, 0, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9475, Accuracy: 0.3929, Precision: 0.4397, Recall: 0.3524, F1: 0.3601
Epoch 66/70
Train Loss: 0.0903, Accuracy: 0.9790, Precision: 0.9314, Recall: 0.9348, F1: 0.9329
Validation Loss: 0.8892, Accuracy: 0.8267, Precision: 0.7238, Recall: 0.7055, F1: 0.6559
Testing Loss: 1.0250, Accuracy: 0.8112, Precision: 0.6103, Recall: 0.5615, F1: 0.5760
LM Predictions:  [3, 3, 5, 4, 2, 3, 0, 2, 3, 5, 5, 0, 4, 3, 5, 5, 3, 0, 3, 3, 0, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0510, Accuracy: 0.4643, Precision: 0.4694, Recall: 0.4190, F1: 0.4343
Epoch 67/70
Train Loss: 0.1064, Accuracy: 0.9713, Precision: 0.9049, Recall: 0.9019, F1: 0.9031
Validation Loss: 0.9304, Accuracy: 0.8239, Precision: 0.6735, Recall: 0.6563, F1: 0.6490
Testing Loss: 0.9914, Accuracy: 0.8218, Precision: 0.6156, Recall: 0.6125, F1: 0.6025
LM Predictions:  [4, 3, 3, 4, 2, 3, 0, 2, 3, 2, 5, 2, 4, 3, 5, 1, 3, 0, 3, 4, 3, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.5971, Accuracy: 0.4643, Precision: 0.5063, Recall: 0.4095, F1: 0.4240
Epoch 68/70
Train Loss: 0.0977, Accuracy: 0.9738, Precision: 0.9220, Recall: 0.9173, F1: 0.9186
Validation Loss: 0.8400, Accuracy: 0.8239, Precision: 0.6686, Recall: 0.6169, F1: 0.6339
Testing Loss: 0.8772, Accuracy: 0.8324, Precision: 0.6260, Recall: 0.6273, F1: 0.6192
LM Predictions:  [1, 3, 1, 4, 2, 3, 0, 2, 3, 1, 5, 2, 4, 3, 5, 1, 3, 0, 3, 4, 3, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.5942, Accuracy: 0.5000, Precision: 0.5861, Recall: 0.4429, F1: 0.4880
Epoch 69/70
Train Loss: 0.1096, Accuracy: 0.9710, Precision: 0.9199, Recall: 0.9167, F1: 0.9178
Validation Loss: 0.9732, Accuracy: 0.8097, Precision: 0.6668, Recall: 0.6078, F1: 0.6246
Testing Loss: 1.0167, Accuracy: 0.7979, Precision: 0.5816, Recall: 0.5965, F1: 0.5721
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 3, 5, 4, 4, 3, 1, 3, 3, 0, 3, 3, 3, 0, 2, 4, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9828, Accuracy: 0.4286, Precision: 0.5778, Recall: 0.3762, F1: 0.4397
Epoch 70/70
Train Loss: 0.0798, Accuracy: 0.9783, Precision: 0.9356, Recall: 0.9378, F1: 0.9363
Validation Loss: 0.9886, Accuracy: 0.7983, Precision: 0.6694, Recall: 0.6703, F1: 0.6264
Testing Loss: 1.0798, Accuracy: 0.7979, Precision: 0.6390, Recall: 0.6249, F1: 0.6117
LM Predictions:  [3, 3, 1, 4, 2, 3, 0, 2, 3, 5, 0, 2, 4, 3, 0, 3, 3, 0, 3, 4, 0, 0, 2, 2, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.6541, Accuracy: 0.4286, Precision: 0.5000, Recall: 0.3762, F1: 0.4011
Label Memorization Analysis: 
LM Loss: 1.6541, Accuracy: 0.4286, Precision: 0.5000, Recall: 0.3762, F1: 0.4011
---------------------------------------------------------------------------



