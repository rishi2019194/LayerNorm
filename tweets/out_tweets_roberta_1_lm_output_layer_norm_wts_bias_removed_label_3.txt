---------------------------------------------------------------------------
Results for seed:  28
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 971
  Label 2: 1106
  Label 5: 493
  Label 1: 119
  Label 3: 116
  Label 0: 53
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.3095, Accuracy: 0.4727, Precision: 0.2340, Recall: 0.2161, F1: 0.1886
Validation Loss: 1.0709, Accuracy: 0.6023, Precision: 0.2408, Recall: 0.2794, F1: 0.2424
Testing Loss: 1.0711, Accuracy: 0.6197, Precision: 0.4129, Recall: 0.2899, F1: 0.2546
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8980, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 2/70
Train Loss: 0.8480, Accuracy: 0.7089, Precision: 0.3434, Recall: 0.3757, F1: 0.3568
Validation Loss: 0.8212, Accuracy: 0.6989, Precision: 0.3713, Recall: 0.4123, F1: 0.3667
Testing Loss: 0.7606, Accuracy: 0.7101, Precision: 0.3735, Recall: 0.4197, F1: 0.3703
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.3505, Accuracy: 0.2500, Precision: 0.0500, Recall: 0.2000, F1: 0.0800
Epoch 3/70
Train Loss: 0.6927, Accuracy: 0.7754, Precision: 0.4997, Recall: 0.4327, F1: 0.4132
Validation Loss: 0.6743, Accuracy: 0.7670, Precision: 0.4979, Recall: 0.4576, F1: 0.4324
Testing Loss: 0.6184, Accuracy: 0.7899, Precision: 0.5116, Recall: 0.4737, F1: 0.4570
LM Predictions:  [5, 3, 5, 5, 5, 2, 5, 5, 5, 3, 3, 5, 3, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1375, Accuracy: 0.2143, Precision: 0.0476, Recall: 0.1429, F1: 0.0714
Epoch 4/70
Train Loss: 0.6184, Accuracy: 0.7908, Precision: 0.4648, Recall: 0.4634, F1: 0.4515
Validation Loss: 0.6853, Accuracy: 0.7812, Precision: 0.4364, Recall: 0.4631, F1: 0.4356
Testing Loss: 0.6464, Accuracy: 0.7793, Precision: 0.4878, Recall: 0.4582, F1: 0.4367
LM Predictions:  [5, 3, 2, 5, 5, 2, 2, 5, 5, 3, 3, 5, 5, 5, 3, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7909, Accuracy: 0.2143, Precision: 0.0772, Recall: 0.1429, F1: 0.0919
Epoch 5/70
Train Loss: 0.5579, Accuracy: 0.8118, Precision: 0.5008, Recall: 0.5047, F1: 0.4936
Validation Loss: 0.5804, Accuracy: 0.8040, Precision: 0.4798, Recall: 0.4947, F1: 0.4766
Testing Loss: 0.5682, Accuracy: 0.8085, Precision: 0.5370, Recall: 0.4926, F1: 0.4850
LM Predictions:  [5, 3, 5, 5, 5, 2, 5, 5, 5, 3, 3, 3, 3, 5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 3, 3, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.0104, Accuracy: 0.2143, Precision: 0.0556, Recall: 0.1429, F1: 0.0800
Epoch 6/70
Train Loss: 0.5210, Accuracy: 0.8205, Precision: 0.5227, Recall: 0.5301, F1: 0.5155
Validation Loss: 0.5185, Accuracy: 0.8097, Precision: 0.4723, Recall: 0.5483, F1: 0.4976
Testing Loss: 0.4909, Accuracy: 0.8298, Precision: 0.5335, Recall: 0.5612, F1: 0.5404
LM Predictions:  [3, 3, 3, 5, 3, 2, 2, 5, 3, 3, 3, 3, 3, 5, 3, 3, 4, 3, 3, 5, 3, 5, 1, 2, 3, 2, 5, 5]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9859, Accuracy: 0.2143, Precision: 0.3036, Recall: 0.1524, F1: 0.1811
Epoch 7/70
Train Loss: 0.4658, Accuracy: 0.8495, Precision: 0.5663, Recall: 0.5705, F1: 0.5614
Validation Loss: 0.5510, Accuracy: 0.8210, Precision: 0.6355, Recall: 0.5711, F1: 0.5937
Testing Loss: 0.5435, Accuracy: 0.8351, Precision: 0.6057, Recall: 0.5919, F1: 0.5872
LM Predictions:  [3, 3, 3, 2, 1, 2, 2, 5, 3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 2, 2, 1, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9996, Accuracy: 0.1786, Precision: 0.2677, Recall: 0.1369, F1: 0.1448
Epoch 8/70
Train Loss: 0.4187, Accuracy: 0.8625, Precision: 0.6044, Recall: 0.6048, F1: 0.6037
Validation Loss: 0.5454, Accuracy: 0.8409, Precision: 0.6361, Recall: 0.5930, F1: 0.6021
Testing Loss: 0.5035, Accuracy: 0.8644, Precision: 0.6458, Recall: 0.6337, F1: 0.6373
LM Predictions:  [3, 3, 3, 3, 5, 2, 2, 5, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3, 2, 5, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.7327, Accuracy: 0.1429, Precision: 0.1270, Recall: 0.0952, F1: 0.1048
Epoch 9/70
Train Loss: 0.3735, Accuracy: 0.8814, Precision: 0.6498, Recall: 0.6594, F1: 0.6518
Validation Loss: 0.6039, Accuracy: 0.8267, Precision: 0.6339, Recall: 0.5928, F1: 0.6004
Testing Loss: 0.5348, Accuracy: 0.8245, Precision: 0.6018, Recall: 0.5862, F1: 0.5727
LM Predictions:  [3, 3, 2, 2, 1, 2, 2, 5, 3, 3, 1, 3, 3, 2, 3, 2, 1, 2, 3, 1, 2, 2, 1, 2, 3, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9747, Accuracy: 0.2143, Precision: 0.2513, Recall: 0.1607, F1: 0.1454
Epoch 10/70
Train Loss: 0.3399, Accuracy: 0.8940, Precision: 0.6610, Recall: 0.6754, F1: 0.6667
Validation Loss: 0.5575, Accuracy: 0.8466, Precision: 0.6480, Recall: 0.6601, F1: 0.6474
Testing Loss: 0.5156, Accuracy: 0.8590, Precision: 0.6408, Recall: 0.6695, F1: 0.6512
LM Predictions:  [3, 3, 3, 3, 1, 2, 2, 5, 3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 1, 2, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4647, Accuracy: 0.1429, Precision: 0.2698, Recall: 0.1131, F1: 0.1369
Epoch 11/70
Train Loss: 0.3156, Accuracy: 0.8971, Precision: 0.6703, Recall: 0.6880, F1: 0.6769
Validation Loss: 0.4858, Accuracy: 0.8551, Precision: 0.6452, Recall: 0.6618, F1: 0.6436
Testing Loss: 0.4730, Accuracy: 0.8697, Precision: 0.6322, Recall: 0.6708, F1: 0.6490
LM Predictions:  [3, 3, 2, 3, 1, 2, 2, 5, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 5, 3, 2, 1, 3, 3, 2, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.4376, Accuracy: 0.2143, Precision: 0.3056, Recall: 0.1607, F1: 0.1986
Epoch 12/70
Train Loss: 0.2822, Accuracy: 0.9167, Precision: 0.8625, Recall: 0.7049, F1: 0.7047
Validation Loss: 0.5467, Accuracy: 0.8580, Precision: 0.6481, Recall: 0.6375, F1: 0.6415
Testing Loss: 0.5804, Accuracy: 0.8564, Precision: 0.6348, Recall: 0.6683, F1: 0.6412
LM Predictions:  [3, 3, 3, 5, 1, 2, 2, 5, 3, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 2, 3, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2725, Accuracy: 0.2500, Precision: 0.2269, Recall: 0.1845, F1: 0.1917
Epoch 13/70
Train Loss: 0.2695, Accuracy: 0.9230, Precision: 0.7788, Recall: 0.7304, F1: 0.7303
Validation Loss: 0.5980, Accuracy: 0.8523, Precision: 0.6720, Recall: 0.6117, F1: 0.6299
Testing Loss: 0.5610, Accuracy: 0.8670, Precision: 0.6626, Recall: 0.6395, F1: 0.6444
LM Predictions:  [2, 3, 3, 5, 5, 2, 4, 5, 5, 3, 3, 3, 2, 2, 3, 2, 1, 3, 3, 1, 4, 2, 1, 2, 1, 2, 2, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.9521, Accuracy: 0.3214, Precision: 0.2917, Recall: 0.2417, F1: 0.2479
Epoch 14/70
Train Loss: 0.2392, Accuracy: 0.9318, Precision: 0.8067, Recall: 0.7576, F1: 0.7667
Validation Loss: 0.6155, Accuracy: 0.8494, Precision: 0.6520, Recall: 0.6300, F1: 0.6388
Testing Loss: 0.6035, Accuracy: 0.8644, Precision: 0.6522, Recall: 0.6609, F1: 0.6435
LM Predictions:  [2, 3, 2, 0, 1, 2, 2, 5, 3, 3, 3, 3, 2, 2, 3, 3, 1, 3, 3, 1, 2, 2, 1, 2, 3, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.1620, Accuracy: 0.3214, Precision: 0.3826, Recall: 0.2417, F1: 0.2421
Epoch 15/70
Train Loss: 0.2019, Accuracy: 0.9402, Precision: 0.8474, Recall: 0.7982, F1: 0.8095
Validation Loss: 0.5641, Accuracy: 0.8324, Precision: 0.6745, Recall: 0.6732, F1: 0.6301
Testing Loss: 0.6226, Accuracy: 0.8511, Precision: 0.7006, Recall: 0.6905, F1: 0.6839
LM Predictions:  [2, 3, 2, 0, 1, 2, 2, 5, 0, 3, 3, 3, 2, 2, 3, 0, 3, 2, 3, 0, 2, 2, 1, 2, 3, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.2053, Accuracy: 0.3214, Precision: 0.3667, Recall: 0.2417, F1: 0.2358
Epoch 16/70
Train Loss: 0.1958, Accuracy: 0.9398, Precision: 0.8105, Recall: 0.8059, F1: 0.8065
Validation Loss: 0.6531, Accuracy: 0.8381, Precision: 0.6208, Recall: 0.6499, F1: 0.6292
Testing Loss: 0.6163, Accuracy: 0.8590, Precision: 0.6265, Recall: 0.6733, F1: 0.6367
LM Predictions:  [3, 3, 2, 1, 1, 3, 3, 5, 1, 3, 3, 3, 3, 3, 3, 5, 1, 3, 3, 1, 3, 2, 1, 2, 3, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 2.5682, Accuracy: 0.2500, Precision: 0.3056, Recall: 0.1845, F1: 0.2212
Epoch 17/70
Train Loss: 0.2036, Accuracy: 0.9433, Precision: 0.8202, Recall: 0.8020, F1: 0.8045
Validation Loss: 0.6168, Accuracy: 0.8409, Precision: 0.6834, Recall: 0.6079, F1: 0.6330
Testing Loss: 0.6394, Accuracy: 0.8457, Precision: 0.6799, Recall: 0.6456, F1: 0.6544
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 0, 3, 3, 4, 2, 0, 3, 5, 1, 4, 1, 0, 2, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6175, Accuracy: 0.5357, Precision: 0.5679, Recall: 0.4488, F1: 0.4725
Epoch 18/70
Train Loss: 0.1675, Accuracy: 0.9531, Precision: 0.8663, Recall: 0.8540, F1: 0.8577
Validation Loss: 0.5969, Accuracy: 0.8608, Precision: 0.7433, Recall: 0.7370, F1: 0.6975
Testing Loss: 0.6806, Accuracy: 0.8457, Precision: 0.6667, Recall: 0.6633, F1: 0.6532
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 2, 3, 3, 4, 2, 0, 3, 5, 3, 3, 1, 3, 2, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7056, Accuracy: 0.5357, Precision: 0.6042, Recall: 0.4393, F1: 0.4796
Epoch 19/70
Train Loss: 0.1537, Accuracy: 0.9563, Precision: 0.8673, Recall: 0.8610, F1: 0.8636
Validation Loss: 0.5784, Accuracy: 0.8551, Precision: 0.6824, Recall: 0.6472, F1: 0.6579
Testing Loss: 0.5980, Accuracy: 0.8617, Precision: 0.7147, Recall: 0.6773, F1: 0.6907
LM Predictions:  [0, 3, 2, 0, 1, 4, 4, 5, 3, 3, 3, 3, 2, 5, 3, 5, 3, 3, 1, 3, 2, 2, 5, 2, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7392, Accuracy: 0.4643, Precision: 0.5972, Recall: 0.3821, F1: 0.4572
Epoch 20/70
Train Loss: 0.1516, Accuracy: 0.9608, Precision: 0.8792, Recall: 0.8812, F1: 0.8800
Validation Loss: 0.5994, Accuracy: 0.8608, Precision: 0.6996, Recall: 0.7078, F1: 0.6909
Testing Loss: 0.6033, Accuracy: 0.8644, Precision: 0.6872, Recall: 0.6907, F1: 0.6878
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 0, 3, 3, 3, 2, 0, 3, 5, 1, 3, 1, 3, 3, 2, 5, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5217, Accuracy: 0.5000, Precision: 0.6111, Recall: 0.4155, F1: 0.4778
Epoch 21/70
Train Loss: 0.1460, Accuracy: 0.9622, Precision: 0.8899, Recall: 0.8770, F1: 0.8824
Validation Loss: 0.6754, Accuracy: 0.8466, Precision: 0.6791, Recall: 0.6955, F1: 0.6723
Testing Loss: 0.7068, Accuracy: 0.8511, Precision: 0.6766, Recall: 0.6491, F1: 0.6586
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 3, 3, 3, 0, 0, 5, 3, 5, 3, 3, 3, 3, 3, 2, 1, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.8475, Accuracy: 0.4643, Precision: 0.6250, Recall: 0.3738, F1: 0.4619
Epoch 22/70
Train Loss: 0.1290, Accuracy: 0.9643, Precision: 0.8922, Recall: 0.9092, F1: 0.9002
Validation Loss: 0.6306, Accuracy: 0.8466, Precision: 0.6890, Recall: 0.6906, F1: 0.6754
Testing Loss: 0.6884, Accuracy: 0.8431, Precision: 0.6671, Recall: 0.6067, F1: 0.6295
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 3, 3, 3, 0, 2, 0, 3, 5, 3, 3, 1, 3, 3, 3, 5, 2, 1, 3, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.5745, Accuracy: 0.5000, Precision: 0.6833, Recall: 0.4155, F1: 0.5079
Epoch 23/70
Train Loss: 0.1181, Accuracy: 0.9692, Precision: 0.9040, Recall: 0.9148, F1: 0.9091
Validation Loss: 0.6461, Accuracy: 0.8722, Precision: 0.6941, Recall: 0.6710, F1: 0.6810
Testing Loss: 0.7679, Accuracy: 0.8537, Precision: 0.6466, Recall: 0.6539, F1: 0.6455
LM Predictions:  [2, 3, 2, 1, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 3, 2, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.7178, Accuracy: 0.6071, Precision: 0.7458, Recall: 0.4964, F1: 0.5571
Epoch 24/70
Train Loss: 0.1147, Accuracy: 0.9696, Precision: 0.9083, Recall: 0.9200, F1: 0.9137
Validation Loss: 0.7276, Accuracy: 0.8438, Precision: 0.7006, Recall: 0.6073, F1: 0.6389
Testing Loss: 0.8522, Accuracy: 0.8112, Precision: 0.6123, Recall: 0.5948, F1: 0.5899
LM Predictions:  [2, 3, 2, 0, 1, 4, 4, 5, 2, 3, 3, 0, 2, 0, 3, 5, 3, 3, 1, 0, 2, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.6746, Accuracy: 0.5714, Precision: 0.6708, Recall: 0.4726, F1: 0.5165
Epoch 25/70
Train Loss: 0.1134, Accuracy: 0.9706, Precision: 0.9115, Recall: 0.9160, F1: 0.9134
Validation Loss: 0.6021, Accuracy: 0.8722, Precision: 0.7035, Recall: 0.6369, F1: 0.6562
Testing Loss: 0.7286, Accuracy: 0.8564, Precision: 0.6563, Recall: 0.6663, F1: 0.6547
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 0, 3, 5, 3, 3, 1, 2, 2, 2, 5, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2911, Accuracy: 0.5714, Precision: 0.6722, Recall: 0.4726, F1: 0.5254
Epoch 26/70
Train Loss: 0.1328, Accuracy: 0.9668, Precision: 0.9163, Recall: 0.9130, F1: 0.9142
Validation Loss: 0.5620, Accuracy: 0.8551, Precision: 0.6773, Recall: 0.6345, F1: 0.6486
Testing Loss: 0.6964, Accuracy: 0.8404, Precision: 0.6448, Recall: 0.6478, F1: 0.6398
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 1, 2, 1, 2, 5, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.4060, Accuracy: 0.6786, Precision: 0.7470, Recall: 0.5345, F1: 0.5742
Epoch 27/70
Train Loss: 0.0879, Accuracy: 0.9804, Precision: 0.9484, Recall: 0.9450, F1: 0.9462
Validation Loss: 0.6512, Accuracy: 0.8693, Precision: 0.6936, Recall: 0.6711, F1: 0.6792
Testing Loss: 0.7805, Accuracy: 0.8644, Precision: 0.6803, Recall: 0.6531, F1: 0.6644
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 0, 3, 5, 3, 3, 1, 5, 2, 2, 2, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2467, Accuracy: 0.6429, Precision: 0.7407, Recall: 0.5202, F1: 0.5885
Epoch 28/70
Train Loss: 0.1170, Accuracy: 0.9713, Precision: 0.9280, Recall: 0.9229, F1: 0.9249
Validation Loss: 0.6992, Accuracy: 0.8295, Precision: 0.6270, Recall: 0.6065, F1: 0.6111
Testing Loss: 0.6602, Accuracy: 0.8484, Precision: 0.6407, Recall: 0.6199, F1: 0.6277
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 0, 2, 3, 2, 3, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2797, Accuracy: 0.6429, Precision: 0.7778, Recall: 0.5024, F1: 0.5952
Epoch 29/70
Train Loss: 0.1346, Accuracy: 0.9678, Precision: 0.9133, Recall: 0.9046, F1: 0.9071
Validation Loss: 0.6827, Accuracy: 0.8494, Precision: 0.6483, Recall: 0.6524, F1: 0.6491
Testing Loss: 0.8023, Accuracy: 0.8564, Precision: 0.6445, Recall: 0.6609, F1: 0.6488
LM Predictions:  [2, 3, 2, 2, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2390, Accuracy: 0.6786, Precision: 0.7708, Recall: 0.5440, F1: 0.6099
Epoch 30/70
Train Loss: 0.0850, Accuracy: 0.9776, Precision: 0.9433, Recall: 0.9346, F1: 0.9374
Validation Loss: 0.8827, Accuracy: 0.8324, Precision: 0.6601, Recall: 0.7101, F1: 0.6720
Testing Loss: 0.8958, Accuracy: 0.8298, Precision: 0.6165, Recall: 0.6552, F1: 0.6195
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 1, 3, 5, 3, 3, 1, 5, 5, 2, 1, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1779, Accuracy: 0.6786, Precision: 0.7667, Recall: 0.5440, F1: 0.6071
Epoch 31/70
Train Loss: 0.0930, Accuracy: 0.9766, Precision: 0.9395, Recall: 0.9335, F1: 0.9357
Validation Loss: 0.8267, Accuracy: 0.8580, Precision: 0.6928, Recall: 0.6317, F1: 0.6567
Testing Loss: 0.8920, Accuracy: 0.8457, Precision: 0.6656, Recall: 0.6395, F1: 0.6494
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 3, 1, 5, 5, 2, 3, 2, 1, 3, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.2091, Accuracy: 0.7143, Precision: 0.8333, Recall: 0.5679, F1: 0.6538
Epoch 32/70
Train Loss: 0.0603, Accuracy: 0.9850, Precision: 0.9549, Recall: 0.9537, F1: 0.9535
Validation Loss: 0.7798, Accuracy: 0.8665, Precision: 0.6822, Recall: 0.6457, F1: 0.6601
Testing Loss: 0.9320, Accuracy: 0.8537, Precision: 0.6722, Recall: 0.6617, F1: 0.6652
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 2, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9581, Accuracy: 0.7500, Precision: 0.8125, Recall: 0.6012, F1: 0.6725
Epoch 33/70
Train Loss: 0.0771, Accuracy: 0.9797, Precision: 0.9466, Recall: 0.9552, F1: 0.9504
Validation Loss: 0.8158, Accuracy: 0.8523, Precision: 0.6433, Recall: 0.6449, F1: 0.6419
Testing Loss: 0.8841, Accuracy: 0.8351, Precision: 0.6087, Recall: 0.6289, F1: 0.6102
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 1, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0658, Accuracy: 0.7500, Precision: 0.7667, Recall: 0.6012, F1: 0.6519
Epoch 34/70
Train Loss: 0.0965, Accuracy: 0.9738, Precision: 0.9335, Recall: 0.9151, F1: 0.9230
Validation Loss: 0.7661, Accuracy: 0.8608, Precision: 0.6934, Recall: 0.6427, F1: 0.6649
Testing Loss: 0.9504, Accuracy: 0.8484, Precision: 0.6792, Recall: 0.6703, F1: 0.6683
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8526, Accuracy: 0.7500, Precision: 0.7917, Recall: 0.6012, F1: 0.6631
Epoch 35/70
Train Loss: 0.0767, Accuracy: 0.9836, Precision: 0.9502, Recall: 0.9576, F1: 0.9536
Validation Loss: 0.6722, Accuracy: 0.8636, Precision: 0.7055, Recall: 0.7179, F1: 0.6969
Testing Loss: 1.0105, Accuracy: 0.8298, Precision: 0.6629, Recall: 0.6545, F1: 0.6538
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 3, 5, 2, 5, 2, 1, 0, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0161, Accuracy: 0.7143, Precision: 0.8056, Recall: 0.5774, F1: 0.6580
Epoch 36/70
Train Loss: 0.0795, Accuracy: 0.9832, Precision: 0.9658, Recall: 0.9658, F1: 0.9654
Validation Loss: 0.7615, Accuracy: 0.8665, Precision: 0.6823, Recall: 0.6537, F1: 0.6652
Testing Loss: 0.8294, Accuracy: 0.8617, Precision: 0.6696, Recall: 0.6687, F1: 0.6668
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9042, Accuracy: 0.7500, Precision: 0.7917, Recall: 0.6012, F1: 0.6631
Epoch 37/70
Train Loss: 0.0607, Accuracy: 0.9860, Precision: 0.9675, Recall: 0.9711, F1: 0.9692
Validation Loss: 0.7968, Accuracy: 0.8551, Precision: 0.7191, Recall: 0.6942, F1: 0.7010
Testing Loss: 0.8623, Accuracy: 0.8511, Precision: 0.6462, Recall: 0.6539, F1: 0.6454
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 3, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8067, Accuracy: 0.7857, Precision: 0.8333, Recall: 0.6345, F1: 0.7134
Epoch 38/70
Train Loss: 0.0460, Accuracy: 0.9906, Precision: 0.9695, Recall: 0.9679, F1: 0.9684
Validation Loss: 0.9509, Accuracy: 0.8381, Precision: 0.6719, Recall: 0.6327, F1: 0.6482
Testing Loss: 0.9313, Accuracy: 0.8564, Precision: 0.6567, Recall: 0.6589, F1: 0.6510
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8252, Accuracy: 0.8214, Precision: 0.8333, Recall: 0.6679, F1: 0.7365
Epoch 39/70
Train Loss: 0.0462, Accuracy: 0.9895, Precision: 0.9658, Recall: 0.9698, F1: 0.9677
Validation Loss: 0.7216, Accuracy: 0.8551, Precision: 0.6609, Recall: 0.6387, F1: 0.6467
Testing Loss: 0.7774, Accuracy: 0.8590, Precision: 0.6563, Recall: 0.6506, F1: 0.6526
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8940, Accuracy: 0.7857, Precision: 0.8095, Recall: 0.6345, F1: 0.7024
Epoch 40/70
Train Loss: 0.0418, Accuracy: 0.9920, Precision: 0.9784, Recall: 0.9743, F1: 0.9760
Validation Loss: 0.8654, Accuracy: 0.8494, Precision: 0.6752, Recall: 0.6789, F1: 0.6681
Testing Loss: 0.9032, Accuracy: 0.8431, Precision: 0.6499, Recall: 0.6441, F1: 0.6440
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6937, Accuracy: 0.8214, Precision: 0.8333, Recall: 0.6679, F1: 0.7365
Epoch 41/70
Train Loss: 0.0683, Accuracy: 0.9839, Precision: 0.9632, Recall: 0.9617, F1: 0.9623
Validation Loss: 0.8503, Accuracy: 0.8409, Precision: 0.6890, Recall: 0.5699, F1: 0.6023
Testing Loss: 1.1030, Accuracy: 0.8218, Precision: 0.6560, Recall: 0.5820, F1: 0.5852
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5308, Accuracy: 0.8571, Precision: 0.8333, Recall: 0.7012, F1: 0.7597
Epoch 42/70
Train Loss: 0.0599, Accuracy: 0.9892, Precision: 0.9687, Recall: 0.9685, F1: 0.9686
Validation Loss: 0.7667, Accuracy: 0.8523, Precision: 0.6788, Recall: 0.6550, F1: 0.6637
Testing Loss: 0.9136, Accuracy: 0.8457, Precision: 0.6608, Recall: 0.6403, F1: 0.6472
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 2, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6870, Accuracy: 0.8214, Precision: 0.8125, Recall: 0.6679, F1: 0.7254
Epoch 43/70
Train Loss: 0.0466, Accuracy: 0.9885, Precision: 0.9771, Recall: 0.9760, F1: 0.9763
Validation Loss: 0.9307, Accuracy: 0.8409, Precision: 0.7044, Recall: 0.6748, F1: 0.6673
Testing Loss: 1.0710, Accuracy: 0.8351, Precision: 0.6613, Recall: 0.6525, F1: 0.6451
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 2, 3, 0, 2, 0, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8013, Accuracy: 0.7857, Precision: 0.7792, Recall: 0.6440, F1: 0.6956
Epoch 44/70
Train Loss: 0.1187, Accuracy: 0.9647, Precision: 0.8371, Recall: 0.8309, F1: 0.8276
Validation Loss: 0.7732, Accuracy: 0.8210, Precision: 0.6400, Recall: 0.6252, F1: 0.6227
Testing Loss: 0.7727, Accuracy: 0.8324, Precision: 0.6317, Recall: 0.6252, F1: 0.6211
LM Predictions:  [5, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 3, 2, 5, 2, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 3, 3, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7206, Accuracy: 0.6429, Precision: 0.6012, Recall: 0.5107, F1: 0.5440
Epoch 45/70
Train Loss: 0.0995, Accuracy: 0.9766, Precision: 0.9457, Recall: 0.9372, F1: 0.9411
Validation Loss: 0.6366, Accuracy: 0.8494, Precision: 0.7064, Recall: 0.6298, F1: 0.6494
Testing Loss: 0.7898, Accuracy: 0.8431, Precision: 0.6627, Recall: 0.6059, F1: 0.6217
LM Predictions:  [2, 3, 0, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.1975, Accuracy: 0.7500, Precision: 0.7679, Recall: 0.6107, F1: 0.6757
Epoch 46/70
Train Loss: 0.0758, Accuracy: 0.9853, Precision: 0.9491, Recall: 0.9519, F1: 0.9505
Validation Loss: 0.7655, Accuracy: 0.8693, Precision: 0.7143, Recall: 0.6866, F1: 0.6973
Testing Loss: 0.9033, Accuracy: 0.8431, Precision: 0.6331, Recall: 0.6491, F1: 0.6332
LM Predictions:  [2, 3, 0, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7863, Accuracy: 0.7857, Precision: 0.8000, Recall: 0.6440, F1: 0.7089
Epoch 47/70
Train Loss: 0.0507, Accuracy: 0.9906, Precision: 0.9731, Recall: 0.9754, F1: 0.9742
Validation Loss: 0.8690, Accuracy: 0.8551, Precision: 0.6959, Recall: 0.6589, F1: 0.6747
Testing Loss: 0.8977, Accuracy: 0.8564, Precision: 0.6528, Recall: 0.6490, F1: 0.6495
LM Predictions:  [2, 3, 3, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 3, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6860, Accuracy: 0.7857, Precision: 0.8095, Recall: 0.6440, F1: 0.7127
Epoch 48/70
Train Loss: 0.0445, Accuracy: 0.9916, Precision: 0.9730, Recall: 0.9783, F1: 0.9755
Validation Loss: 0.7551, Accuracy: 0.8551, Precision: 0.6718, Recall: 0.6453, F1: 0.6522
Testing Loss: 0.9315, Accuracy: 0.8271, Precision: 0.5910, Recall: 0.6215, F1: 0.5843
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 0, 1, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.9455, Accuracy: 0.7857, Precision: 0.7262, Recall: 0.6345, F1: 0.6529
Epoch 49/70
Train Loss: 0.0501, Accuracy: 0.9871, Precision: 0.9690, Recall: 0.9562, F1: 0.9622
Validation Loss: 1.0164, Accuracy: 0.8494, Precision: 0.6963, Recall: 0.6482, F1: 0.6652
Testing Loss: 1.0785, Accuracy: 0.8245, Precision: 0.6304, Recall: 0.6132, F1: 0.6099
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 1.0154, Accuracy: 0.8214, Precision: 0.7917, Recall: 0.6679, F1: 0.7160
Epoch 50/70
Train Loss: 0.0560, Accuracy: 0.9881, Precision: 0.9629, Recall: 0.9677, F1: 0.9652
Validation Loss: 0.9352, Accuracy: 0.8466, Precision: 0.6866, Recall: 0.6144, F1: 0.6406
Testing Loss: 0.9006, Accuracy: 0.8511, Precision: 0.6479, Recall: 0.6236, F1: 0.6332
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7717, Accuracy: 0.8214, Precision: 0.7917, Recall: 0.6679, F1: 0.7160
Epoch 51/70
Train Loss: 0.0640, Accuracy: 0.9839, Precision: 0.9641, Recall: 0.9561, F1: 0.9600
Validation Loss: 0.7420, Accuracy: 0.8381, Precision: 0.7245, Recall: 0.7287, F1: 0.6824
Testing Loss: 0.8034, Accuracy: 0.8271, Precision: 0.6586, Recall: 0.6341, F1: 0.6349
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 0, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7114, Accuracy: 0.8571, Precision: 0.8000, Recall: 0.7012, F1: 0.7449
Epoch 52/70
Train Loss: 0.1354, Accuracy: 0.9678, Precision: 0.9308, Recall: 0.9493, F1: 0.9397
Validation Loss: 0.6769, Accuracy: 0.8438, Precision: 0.7017, Recall: 0.6880, F1: 0.6812
Testing Loss: 0.7387, Accuracy: 0.8431, Precision: 0.6559, Recall: 0.6211, F1: 0.6368
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 4, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6320, Accuracy: 0.8571, Precision: 0.8000, Recall: 0.7012, F1: 0.7449
Epoch 53/70
Train Loss: 0.0650, Accuracy: 0.9857, Precision: 0.9675, Recall: 0.9502, F1: 0.9580
Validation Loss: 0.7755, Accuracy: 0.8608, Precision: 0.7201, Recall: 0.6418, F1: 0.6698
Testing Loss: 0.8681, Accuracy: 0.8457, Precision: 0.6511, Recall: 0.6478, F1: 0.6456
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5350, Accuracy: 0.8571, Precision: 0.8095, Recall: 0.7012, F1: 0.7487
Epoch 54/70
Train Loss: 0.0485, Accuracy: 0.9888, Precision: 0.9666, Recall: 0.9690, F1: 0.9676
Validation Loss: 0.8439, Accuracy: 0.8580, Precision: 0.6851, Recall: 0.6533, F1: 0.6670
Testing Loss: 0.8601, Accuracy: 0.8590, Precision: 0.6674, Recall: 0.6716, F1: 0.6687
LM Predictions:  [3, 3, 2, 5, 1, 4, 4, 5, 2, 4, 3, 0, 3, 5, 4, 5, 3, 4, 1, 5, 3, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.7919, Accuracy: 0.7500, Precision: 0.8000, Recall: 0.6298, F1: 0.7021
Epoch 55/70
Train Loss: 0.0489, Accuracy: 0.9888, Precision: 0.9687, Recall: 0.9654, F1: 0.9667
Validation Loss: 0.9512, Accuracy: 0.8466, Precision: 0.6741, Recall: 0.6515, F1: 0.6578
Testing Loss: 1.0664, Accuracy: 0.8404, Precision: 0.6560, Recall: 0.6528, F1: 0.6517
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 3, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6589, Accuracy: 0.8571, Precision: 0.8333, Recall: 0.7012, F1: 0.7597
Epoch 56/70
Train Loss: 0.0374, Accuracy: 0.9934, Precision: 0.9844, Recall: 0.9800, F1: 0.9821
Validation Loss: 0.8951, Accuracy: 0.8551, Precision: 0.6650, Recall: 0.6617, F1: 0.6608
Testing Loss: 0.9172, Accuracy: 0.8511, Precision: 0.6459, Recall: 0.6659, F1: 0.6478
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 1, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8209, Accuracy: 0.7857, Precision: 0.7500, Recall: 0.6345, F1: 0.6684
Epoch 57/70
Train Loss: 0.0330, Accuracy: 0.9927, Precision: 0.9826, Recall: 0.9753, F1: 0.9787
Validation Loss: 0.8518, Accuracy: 0.8494, Precision: 0.6772, Recall: 0.6375, F1: 0.6522
Testing Loss: 0.8396, Accuracy: 0.8670, Precision: 0.6911, Recall: 0.6683, F1: 0.6777
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 4, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3342, Accuracy: 0.8929, Precision: 0.7722, Recall: 0.7345, F1: 0.7482
Epoch 58/70
Train Loss: 0.0293, Accuracy: 0.9937, Precision: 0.9827, Recall: 0.9779, F1: 0.9803
Validation Loss: 0.8995, Accuracy: 0.8608, Precision: 0.7015, Recall: 0.6509, F1: 0.6703
Testing Loss: 1.0472, Accuracy: 0.8484, Precision: 0.6654, Recall: 0.6343, F1: 0.6422
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3784, Accuracy: 0.9286, Precision: 0.8056, Recall: 0.7762, F1: 0.7868
Epoch 59/70
Train Loss: 0.0451, Accuracy: 0.9916, Precision: 0.9789, Recall: 0.9729, F1: 0.9759
Validation Loss: 0.7943, Accuracy: 0.8665, Precision: 0.6826, Recall: 0.6629, F1: 0.6690
Testing Loss: 0.7887, Accuracy: 0.8697, Precision: 0.6611, Recall: 0.6786, F1: 0.6654
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 1, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5986, Accuracy: 0.8571, Precision: 0.7762, Recall: 0.7095, F1: 0.7308
Epoch 60/70
Train Loss: 0.0337, Accuracy: 0.9927, Precision: 0.9702, Recall: 0.9697, F1: 0.9697
Validation Loss: 0.8306, Accuracy: 0.8665, Precision: 0.7099, Recall: 0.6390, F1: 0.6639
Testing Loss: 0.9051, Accuracy: 0.8457, Precision: 0.6618, Recall: 0.6388, F1: 0.6432
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 4, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4405, Accuracy: 0.8571, Precision: 0.7429, Recall: 0.7012, F1: 0.7190
Epoch 61/70
Train Loss: 0.0379, Accuracy: 0.9909, Precision: 0.9774, Recall: 0.9783, F1: 0.9778
Validation Loss: 0.8155, Accuracy: 0.8580, Precision: 0.7138, Recall: 0.6153, F1: 0.6482
Testing Loss: 1.0243, Accuracy: 0.8404, Precision: 0.6492, Recall: 0.6071, F1: 0.6100
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 3, 0, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4261, Accuracy: 0.8929, Precision: 0.8056, Recall: 0.7345, F1: 0.7630
Epoch 62/70
Train Loss: 0.0691, Accuracy: 0.9846, Precision: 0.9568, Recall: 0.9689, F1: 0.9627
Validation Loss: 0.7455, Accuracy: 0.8523, Precision: 0.6676, Recall: 0.7112, F1: 0.6830
Testing Loss: 0.7504, Accuracy: 0.8537, Precision: 0.6470, Recall: 0.6770, F1: 0.6574
LM Predictions:  [2, 5, 2, 5, 1, 4, 4, 5, 2, 5, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5963, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.6679, F1: 0.7077
Epoch 63/70
Train Loss: 0.1302, Accuracy: 0.9671, Precision: 0.9323, Recall: 0.9043, F1: 0.9161
Validation Loss: 0.8431, Accuracy: 0.7869, Precision: 0.6636, Recall: 0.6514, F1: 0.6207
Testing Loss: 0.9636, Accuracy: 0.7819, Precision: 0.6017, Recall: 0.5766, F1: 0.5412
LM Predictions:  [2, 0, 2, 1, 1, 4, 4, 5, 2, 4, 1, 0, 2, 1, 4, 1, 1, 4, 1, 5, 5, 2, 0, 2, 2, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6807, Accuracy: 0.7500, Precision: 0.7921, Recall: 0.7457, F1: 0.7394
Epoch 64/70
Train Loss: 0.0881, Accuracy: 0.9776, Precision: 0.9470, Recall: 0.9283, F1: 0.9369
Validation Loss: 0.8063, Accuracy: 0.8466, Precision: 0.6467, Recall: 0.5915, F1: 0.6077
Testing Loss: 0.9261, Accuracy: 0.8059, Precision: 0.5729, Recall: 0.5579, F1: 0.5397
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 4, 1, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.8337, Accuracy: 0.8214, Precision: 0.7095, Recall: 0.6679, F1: 0.6790
Epoch 65/70
Train Loss: 0.0831, Accuracy: 0.9822, Precision: 0.9572, Recall: 0.9432, F1: 0.9493
Validation Loss: 0.8478, Accuracy: 0.8494, Precision: 0.6598, Recall: 0.6201, F1: 0.6372
Testing Loss: 0.8638, Accuracy: 0.8564, Precision: 0.7307, Recall: 0.6678, F1: 0.6843
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 1, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3946, Accuracy: 0.8929, Precision: 0.8000, Recall: 0.7429, F1: 0.7650
Epoch 66/70
Train Loss: 0.0361, Accuracy: 0.9930, Precision: 0.9851, Recall: 0.9771, F1: 0.9810
Validation Loss: 0.8882, Accuracy: 0.8580, Precision: 0.6734, Recall: 0.6584, F1: 0.6652
Testing Loss: 0.8437, Accuracy: 0.8564, Precision: 0.6875, Recall: 0.6768, F1: 0.6786
LM Predictions:  [2, 3, 2, 5, 1, 4, 4, 5, 2, 1, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.5733, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7429, F1: 0.7835
Epoch 67/70
Train Loss: 0.0468, Accuracy: 0.9916, Precision: 0.9824, Recall: 0.9717, F1: 0.9768
Validation Loss: 0.8943, Accuracy: 0.8580, Precision: 0.7023, Recall: 0.6194, F1: 0.6476
Testing Loss: 1.0049, Accuracy: 0.8351, Precision: 0.6285, Recall: 0.5952, F1: 0.6041
LM Predictions:  [2, 1, 2, 5, 1, 4, 4, 5, 2, 1, 1, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 5, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6137, Accuracy: 0.8571, Precision: 0.7540, Recall: 0.7095, F1: 0.7160
Epoch 68/70
Train Loss: 0.0365, Accuracy: 0.9930, Precision: 0.9808, Recall: 0.9791, F1: 0.9799
Validation Loss: 0.9534, Accuracy: 0.8523, Precision: 0.6602, Recall: 0.6534, F1: 0.6547
Testing Loss: 0.9533, Accuracy: 0.8404, Precision: 0.6844, Recall: 0.6727, F1: 0.6748
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 3, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.3508, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7762, F1: 0.8020
Epoch 69/70
Train Loss: 0.0806, Accuracy: 0.9825, Precision: 0.9470, Recall: 0.9401, F1: 0.9433
Validation Loss: 0.8494, Accuracy: 0.8580, Precision: 0.6922, Recall: 0.6981, F1: 0.6825
Testing Loss: 0.8857, Accuracy: 0.8431, Precision: 0.6748, Recall: 0.6656, F1: 0.6650
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 1, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.4669, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7762, F1: 0.7835
Epoch 70/70
Train Loss: 0.0446, Accuracy: 0.9902, Precision: 0.9735, Recall: 0.9852, F1: 0.9792
Validation Loss: 0.7840, Accuracy: 0.8636, Precision: 0.7096, Recall: 0.6181, F1: 0.6458
Testing Loss: 0.9345, Accuracy: 0.8457, Precision: 0.6485, Recall: 0.6276, F1: 0.6313
LM Predictions:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 5, 1, 0, 2, 5, 4, 5, 3, 4, 1, 5, 5, 2, 0, 2, 1, 5, 0, 2]
LM Labels:  [2, 0, 2, 5, 1, 4, 4, 5, 2, 1, 5, 0, 2, 5, 4, 5, 4, 4, 1, 5, 5, 2, 0, 2, 1, 0, 0, 2]
LM Loss: 0.6877, Accuracy: 0.8571, Precision: 0.7500, Recall: 0.7012, F1: 0.7213
Label Memorization Analysis: 
LM Loss: 0.6877, Accuracy: 0.8571, Precision: 0.7500, Recall: 0.7012, F1: 0.7213
---------------------------------------------------------------------------



