---------------------------------------------------------------------------
Results for seed:  64
Model: roberta-base, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 973
  Label 2: 1103
  Label 5: 491
  Label 1: 120
  Label 3: 116
  Label 0: 55
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.1452, Accuracy: 0.5696, Precision: 0.3506, Recall: 0.2888, F1: 0.2769
Validation Loss: 0.8184, Accuracy: 0.7131, Precision: 0.3548, Recall: 0.4042, F1: 0.3697
Testing Loss: 0.8122, Accuracy: 0.7128, Precision: 0.3553, Recall: 0.4091, F1: 0.3691
LM Predictions:  [5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 2, 2, 5, 5, 5, 5, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1160, Accuracy: 0.1786, Precision: 0.1000, Recall: 0.2100, F1: 0.1123
Epoch 2/70
Train Loss: 0.7590, Accuracy: 0.7418, Precision: 0.4164, Recall: 0.4281, F1: 0.4153
Validation Loss: 0.7580, Accuracy: 0.7415, Precision: 0.3791, Recall: 0.4034, F1: 0.3873
Testing Loss: 0.7590, Accuracy: 0.7500, Precision: 0.4509, Recall: 0.4346, F1: 0.4339
LM Predictions:  [5, 2, 2, 5, 4, 3, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 2, 5, 5, 2, 3, 2, 2, 3, 3, 3, 3, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1780, Accuracy: 0.1429, Precision: 0.0593, Recall: 0.1417, F1: 0.0833
Epoch 3/70
Train Loss: 0.6253, Accuracy: 0.7806, Precision: 0.4940, Recall: 0.4881, F1: 0.4891
Validation Loss: 0.6850, Accuracy: 0.7983, Precision: 0.5236, Recall: 0.5354, F1: 0.5155
Testing Loss: 0.6592, Accuracy: 0.7553, Precision: 0.4924, Recall: 0.5320, F1: 0.4684
LM Predictions:  [5, 2, 1, 1, 4, 1, 3, 5, 1, 1, 5, 5, 1, 1, 1, 1, 1, 1, 1, 5, 1, 3, 2, 1, 1, 1, 1, 1]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1695, Accuracy: 0.2143, Precision: 0.1537, Recall: 0.2083, F1: 0.1469
Epoch 4/70
Train Loss: 0.5524, Accuracy: 0.8128, Precision: 0.5346, Recall: 0.5380, F1: 0.5282
Validation Loss: 0.6133, Accuracy: 0.8011, Precision: 0.5823, Recall: 0.5355, F1: 0.5493
Testing Loss: 0.6496, Accuracy: 0.7846, Precision: 0.5595, Recall: 0.5192, F1: 0.5115
LM Predictions:  [1, 2, 1, 2, 4, 1, 3, 5, 3, 1, 5, 2, 1, 1, 1, 1, 3, 3, 1, 2, 1, 2, 2, 3, 1, 1, 1, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4107, Accuracy: 0.2143, Precision: 0.1624, Recall: 0.2083, F1: 0.1550
Epoch 5/70
Train Loss: 0.4903, Accuracy: 0.8324, Precision: 0.5539, Recall: 0.5534, F1: 0.5503
Validation Loss: 0.5834, Accuracy: 0.8068, Precision: 0.5917, Recall: 0.5826, F1: 0.5858
Testing Loss: 0.5592, Accuracy: 0.8032, Precision: 0.5506, Recall: 0.5468, F1: 0.5344
LM Predictions:  [5, 0, 1, 3, 4, 1, 3, 5, 3, 3, 5, 2, 1, 1, 1, 1, 3, 3, 3, 5, 3, 3, 2, 3, 3, 3, 1, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4349, Accuracy: 0.1429, Precision: 0.3155, Recall: 0.1321, F1: 0.1620
Epoch 6/70
Train Loss: 0.4577, Accuracy: 0.8397, Precision: 0.6079, Recall: 0.5851, F1: 0.5871
Validation Loss: 0.5700, Accuracy: 0.8097, Precision: 0.5793, Recall: 0.5911, F1: 0.5839
Testing Loss: 0.5551, Accuracy: 0.7979, Precision: 0.5396, Recall: 0.5644, F1: 0.5426
LM Predictions:  [3, 3, 3, 1, 4, 3, 3, 5, 3, 3, 5, 2, 1, 1, 1, 3, 3, 3, 3, 2, 1, 3, 2, 3, 3, 3, 1, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4818, Accuracy: 0.1071, Precision: 0.1667, Recall: 0.1083, F1: 0.1255
Epoch 7/70
Train Loss: 0.3924, Accuracy: 0.8695, Precision: 0.6941, Recall: 0.6441, F1: 0.6473
Validation Loss: 0.5648, Accuracy: 0.8381, Precision: 0.6096, Recall: 0.6290, F1: 0.6162
Testing Loss: 0.5456, Accuracy: 0.8298, Precision: 0.5951, Recall: 0.6206, F1: 0.6021
LM Predictions:  [5, 3, 5, 1, 4, 3, 3, 5, 3, 3, 5, 2, 1, 1, 1, 3, 3, 3, 1, 5, 1, 3, 2, 3, 3, 3, 1, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5022, Accuracy: 0.1071, Precision: 0.1405, Recall: 0.1083, F1: 0.1167
Epoch 8/70
Train Loss: 0.3543, Accuracy: 0.8786, Precision: 0.7070, Recall: 0.6737, F1: 0.6813
Validation Loss: 0.6290, Accuracy: 0.7898, Precision: 0.6093, Recall: 0.6374, F1: 0.5907
Testing Loss: 0.5627, Accuracy: 0.8165, Precision: 0.5807, Recall: 0.5840, F1: 0.5723
LM Predictions:  [3, 3, 3, 1, 4, 3, 3, 1, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.8894, Accuracy: 0.0714, Precision: 0.3333, Recall: 0.0750, F1: 0.1222
Epoch 9/70
Train Loss: 0.3253, Accuracy: 0.8982, Precision: 0.7382, Recall: 0.7120, F1: 0.7175
Validation Loss: 0.5979, Accuracy: 0.8153, Precision: 0.5934, Recall: 0.6100, F1: 0.5991
Testing Loss: 0.5690, Accuracy: 0.8271, Precision: 0.5991, Recall: 0.6260, F1: 0.5973
LM Predictions:  [3, 3, 5, 1, 4, 3, 3, 1, 3, 3, 5, 2, 3, 1, 3, 3, 3, 3, 1, 2, 1, 3, 2, 3, 3, 1, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4164, Accuracy: 0.1071, Precision: 0.1667, Recall: 0.1083, F1: 0.1255
Epoch 10/70
Train Loss: 0.2898, Accuracy: 0.9129, Precision: 0.7821, Recall: 0.7541, F1: 0.7610
Validation Loss: 0.6665, Accuracy: 0.8125, Precision: 0.6035, Recall: 0.5488, F1: 0.5632
Testing Loss: 0.6278, Accuracy: 0.8431, Precision: 0.6983, Recall: 0.6308, F1: 0.6335
LM Predictions:  [5, 3, 5, 4, 4, 1, 0, 5, 3, 3, 5, 2, 1, 3, 5, 5, 5, 0, 1, 2, 1, 3, 2, 3, 3, 1, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0099, Accuracy: 0.2857, Precision: 0.4198, Recall: 0.2464, F1: 0.2810
Epoch 11/70
Train Loss: 0.2583, Accuracy: 0.9192, Precision: 0.8027, Recall: 0.7845, F1: 0.7910
Validation Loss: 0.6325, Accuracy: 0.8210, Precision: 0.6065, Recall: 0.6138, F1: 0.6084
Testing Loss: 0.6512, Accuracy: 0.8404, Precision: 0.6212, Recall: 0.6556, F1: 0.6271
LM Predictions:  [1, 3, 5, 1, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 1, 3, 3, 3, 1, 2, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7102, Accuracy: 0.1429, Precision: 0.2083, Recall: 0.1500, F1: 0.1680
Epoch 12/70
Train Loss: 0.2409, Accuracy: 0.9251, Precision: 0.8039, Recall: 0.7866, F1: 0.7915
Validation Loss: 0.5708, Accuracy: 0.8267, Precision: 0.6418, Recall: 0.6186, F1: 0.6290
Testing Loss: 0.5821, Accuracy: 0.8404, Precision: 0.6345, Recall: 0.6309, F1: 0.6239
LM Predictions:  [5, 3, 5, 2, 4, 3, 0, 2, 3, 3, 5, 2, 3, 3, 5, 3, 3, 0, 2, 2, 1, 3, 2, 3, 3, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.0062, Accuracy: 0.2857, Precision: 0.4889, Recall: 0.2643, F1: 0.2963
Epoch 13/70
Train Loss: 0.2150, Accuracy: 0.9349, Precision: 0.8346, Recall: 0.8221, F1: 0.8275
Validation Loss: 0.6856, Accuracy: 0.8267, Precision: 0.6382, Recall: 0.6608, F1: 0.6415
Testing Loss: 0.6260, Accuracy: 0.8484, Precision: 0.6344, Recall: 0.6642, F1: 0.6460
LM Predictions:  [3, 3, 3, 4, 4, 3, 3, 2, 3, 3, 5, 3, 1, 3, 3, 3, 3, 3, 4, 2, 1, 3, 3, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.4415, Accuracy: 0.1786, Precision: 0.4444, Recall: 0.1560, F1: 0.2254
Epoch 14/70
Train Loss: 0.1931, Accuracy: 0.9430, Precision: 0.8524, Recall: 0.8354, F1: 0.8413
Validation Loss: 0.6600, Accuracy: 0.8381, Precision: 0.6630, Recall: 0.6217, F1: 0.6400
Testing Loss: 0.6309, Accuracy: 0.8590, Precision: 0.6612, Recall: 0.6687, F1: 0.6625
LM Predictions:  [3, 2, 3, 4, 4, 3, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 3, 0, 2, 2, 1, 3, 2, 3, 3, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9775, Accuracy: 0.3214, Precision: 0.5556, Recall: 0.2881, F1: 0.3504
Epoch 15/70
Train Loss: 0.1774, Accuracy: 0.9482, Precision: 0.8559, Recall: 0.8586, F1: 0.8561
Validation Loss: 0.7235, Accuracy: 0.8295, Precision: 0.6350, Recall: 0.6168, F1: 0.6224
Testing Loss: 0.6687, Accuracy: 0.8431, Precision: 0.6732, Recall: 0.6698, F1: 0.6667
LM Predictions:  [3, 2, 5, 4, 4, 3, 0, 2, 3, 3, 5, 2, 1, 3, 5, 5, 3, 0, 4, 2, 1, 3, 2, 1, 3, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.8453, Accuracy: 0.3929, Precision: 0.5556, Recall: 0.3452, F1: 0.3981
Epoch 16/70
Train Loss: 0.1677, Accuracy: 0.9524, Precision: 0.8774, Recall: 0.8683, F1: 0.8718
Validation Loss: 0.8043, Accuracy: 0.8381, Precision: 0.6496, Recall: 0.6491, F1: 0.6452
Testing Loss: 0.7715, Accuracy: 0.8431, Precision: 0.6507, Recall: 0.6673, F1: 0.6529
LM Predictions:  [3, 3, 0, 4, 4, 3, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 2, 0, 1, 2, 1, 0, 2, 3, 3, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3159, Accuracy: 0.3571, Precision: 0.4972, Recall: 0.3119, F1: 0.3687
Epoch 17/70
Train Loss: 0.1454, Accuracy: 0.9577, Precision: 0.9007, Recall: 0.8826, F1: 0.8904
Validation Loss: 0.7553, Accuracy: 0.8324, Precision: 0.6652, Recall: 0.7002, F1: 0.6741
Testing Loss: 0.7083, Accuracy: 0.8590, Precision: 0.6866, Recall: 0.7047, F1: 0.6897
LM Predictions:  [3, 3, 0, 4, 4, 1, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 2, 0, 4, 3, 1, 2, 2, 1, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.5920, Accuracy: 0.5000, Precision: 0.6444, Recall: 0.4357, F1: 0.4991
Epoch 18/70
Train Loss: 0.1407, Accuracy: 0.9559, Precision: 0.8764, Recall: 0.8717, F1: 0.8736
Validation Loss: 0.6961, Accuracy: 0.8352, Precision: 0.6718, Recall: 0.7226, F1: 0.6789
Testing Loss: 0.6549, Accuracy: 0.8324, Precision: 0.6421, Recall: 0.6624, F1: 0.6344
LM Predictions:  [1, 3, 3, 4, 4, 1, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 2, 0, 4, 3, 1, 0, 2, 1, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.7299, Accuracy: 0.5000, Precision: 0.6111, Recall: 0.4452, F1: 0.4932
Epoch 19/70
Train Loss: 0.1363, Accuracy: 0.9608, Precision: 0.8970, Recall: 0.8976, F1: 0.8968
Validation Loss: 0.7418, Accuracy: 0.8381, Precision: 0.6882, Recall: 0.6989, F1: 0.6834
Testing Loss: 0.7215, Accuracy: 0.8404, Precision: 0.6409, Recall: 0.6473, F1: 0.6306
LM Predictions:  [1, 3, 5, 4, 4, 1, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 2, 0, 4, 3, 1, 2, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.8794, Accuracy: 0.5357, Precision: 0.6250, Recall: 0.4869, F1: 0.5034
Epoch 20/70
Train Loss: 0.1242, Accuracy: 0.9654, Precision: 0.9124, Recall: 0.9052, F1: 0.9082
Validation Loss: 0.8234, Accuracy: 0.8352, Precision: 0.6658, Recall: 0.6888, F1: 0.6700
Testing Loss: 0.7908, Accuracy: 0.8351, Precision: 0.6195, Recall: 0.6425, F1: 0.6198
LM Predictions:  [3, 3, 5, 4, 4, 1, 0, 2, 3, 3, 5, 2, 1, 3, 5, 3, 2, 0, 4, 3, 1, 2, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.7612, Accuracy: 0.5000, Precision: 0.6194, Recall: 0.4536, F1: 0.4852
Epoch 21/70
Train Loss: 0.1102, Accuracy: 0.9703, Precision: 0.9282, Recall: 0.9204, F1: 0.9235
Validation Loss: 0.8684, Accuracy: 0.8352, Precision: 0.6618, Recall: 0.7029, F1: 0.6750
Testing Loss: 0.7201, Accuracy: 0.8564, Precision: 0.6656, Recall: 0.6940, F1: 0.6756
LM Predictions:  [3, 3, 5, 4, 4, 1, 0, 2, 0, 3, 5, 3, 1, 3, 5, 0, 3, 0, 4, 3, 1, 3, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.5747, Accuracy: 0.5714, Precision: 0.7028, Recall: 0.5012, F1: 0.5752
Epoch 22/70
Train Loss: 0.1052, Accuracy: 0.9706, Precision: 0.9123, Recall: 0.9249, F1: 0.9179
Validation Loss: 0.8968, Accuracy: 0.8381, Precision: 0.7068, Recall: 0.6581, F1: 0.6622
Testing Loss: 0.7778, Accuracy: 0.8431, Precision: 0.6951, Recall: 0.6770, F1: 0.6793
LM Predictions:  [3, 0, 3, 4, 4, 1, 0, 2, 0, 0, 5, 2, 1, 3, 5, 0, 3, 0, 4, 2, 1, 0, 2, 4, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.4674, Accuracy: 0.5714, Precision: 0.5940, Recall: 0.4917, F1: 0.5269
Epoch 23/70
Train Loss: 0.0853, Accuracy: 0.9755, Precision: 0.9322, Recall: 0.9229, F1: 0.9269
Validation Loss: 0.9119, Accuracy: 0.8153, Precision: 0.6358, Recall: 0.6215, F1: 0.6140
Testing Loss: 0.7035, Accuracy: 0.8537, Precision: 0.7071, Recall: 0.6910, F1: 0.6900
LM Predictions:  [3, 0, 3, 4, 4, 1, 0, 2, 0, 0, 5, 2, 2, 2, 5, 0, 2, 0, 4, 2, 1, 0, 2, 1, 1, 5, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.0326, Accuracy: 0.6071, Precision: 0.6190, Recall: 0.5250, F1: 0.5358
Epoch 24/70
Train Loss: 0.0746, Accuracy: 0.9790, Precision: 0.9548, Recall: 0.9523, F1: 0.9532
Validation Loss: 0.8922, Accuracy: 0.8438, Precision: 0.6712, Recall: 0.6446, F1: 0.6572
Testing Loss: 0.9180, Accuracy: 0.8431, Precision: 0.6696, Recall: 0.6718, F1: 0.6617
LM Predictions:  [3, 0, 0, 4, 4, 1, 0, 2, 0, 0, 5, 3, 1, 0, 5, 0, 3, 0, 4, 3, 1, 0, 2, 1, 1, 5, 1, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.9421, Accuracy: 0.6786, Precision: 0.6852, Recall: 0.5726, F1: 0.6016
Epoch 25/70
Train Loss: 0.0899, Accuracy: 0.9769, Precision: 0.9353, Recall: 0.9384, F1: 0.9368
Validation Loss: 0.9431, Accuracy: 0.8438, Precision: 0.6284, Recall: 0.6062, F1: 0.6077
Testing Loss: 0.8757, Accuracy: 0.8431, Precision: 0.6297, Recall: 0.6498, F1: 0.6220
LM Predictions:  [1, 0, 3, 4, 4, 1, 0, 2, 0, 3, 5, 3, 4, 0, 5, 0, 2, 0, 4, 3, 1, 0, 2, 1, 1, 5, 2, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.8740, Accuracy: 0.7143, Precision: 0.7012, Recall: 0.6060, F1: 0.6365
Epoch 26/70
Train Loss: 0.0937, Accuracy: 0.9755, Precision: 0.9382, Recall: 0.9302, F1: 0.9333
Validation Loss: 1.0115, Accuracy: 0.8210, Precision: 0.6417, Recall: 0.6711, F1: 0.6511
Testing Loss: 0.7502, Accuracy: 0.8484, Precision: 0.6357, Recall: 0.6749, F1: 0.6510
LM Predictions:  [3, 3, 3, 4, 4, 1, 0, 2, 0, 3, 5, 3, 4, 3, 5, 0, 3, 0, 4, 3, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.2093, Accuracy: 0.6429, Precision: 0.8000, Recall: 0.5488, F1: 0.6483
Epoch 27/70
Train Loss: 0.0792, Accuracy: 0.9794, Precision: 0.9492, Recall: 0.9443, F1: 0.9462
Validation Loss: 0.9627, Accuracy: 0.8239, Precision: 0.6379, Recall: 0.6007, F1: 0.6096
Testing Loss: 0.7385, Accuracy: 0.8723, Precision: 0.7325, Recall: 0.7169, F1: 0.7211
LM Predictions:  [3, 0, 0, 4, 4, 3, 0, 2, 0, 0, 5, 3, 4, 0, 5, 0, 3, 0, 4, 3, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.1862, Accuracy: 0.7143, Precision: 0.7630, Recall: 0.5869, F1: 0.6498
Epoch 28/70
Train Loss: 0.0593, Accuracy: 0.9839, Precision: 0.9530, Recall: 0.9619, F1: 0.9573
Validation Loss: 0.8158, Accuracy: 0.8381, Precision: 0.6582, Recall: 0.6568, F1: 0.6564
Testing Loss: 0.6985, Accuracy: 0.8564, Precision: 0.6813, Recall: 0.6928, F1: 0.6792
LM Predictions:  [1, 0, 0, 4, 1, 1, 0, 2, 0, 0, 5, 3, 4, 0, 5, 0, 3, 0, 4, 2, 1, 3, 2, 1, 1, 5, 1, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6043, Accuracy: 0.7500, Precision: 0.7232, Recall: 0.6298, F1: 0.6444
Epoch 29/70
Train Loss: 0.0792, Accuracy: 0.9804, Precision: 0.9396, Recall: 0.9400, F1: 0.9397
Validation Loss: 1.0247, Accuracy: 0.8182, Precision: 0.6383, Recall: 0.6112, F1: 0.6214
Testing Loss: 0.7424, Accuracy: 0.8617, Precision: 0.7434, Recall: 0.6965, F1: 0.7043
LM Predictions:  [1, 3, 5, 4, 2, 1, 0, 2, 0, 3, 5, 3, 4, 0, 5, 5, 3, 0, 4, 2, 1, 3, 2, 4, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.3209, Accuracy: 0.6786, Precision: 0.7000, Recall: 0.5905, F1: 0.6286
Epoch 30/70
Train Loss: 0.0592, Accuracy: 0.9808, Precision: 0.9424, Recall: 0.9360, F1: 0.9390
Validation Loss: 1.1284, Accuracy: 0.8210, Precision: 0.6319, Recall: 0.6388, F1: 0.6329
Testing Loss: 0.8527, Accuracy: 0.8590, Precision: 0.6501, Recall: 0.6711, F1: 0.6568
LM Predictions:  [3, 3, 0, 4, 2, 1, 0, 2, 0, 0, 5, 3, 4, 3, 5, 0, 3, 0, 4, 2, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.9082, Accuracy: 0.7143, Precision: 0.7722, Recall: 0.6143, F1: 0.6707
Epoch 31/70
Train Loss: 0.0654, Accuracy: 0.9839, Precision: 0.9680, Recall: 0.9604, F1: 0.9638
Validation Loss: 0.9939, Accuracy: 0.8352, Precision: 0.6666, Recall: 0.6755, F1: 0.6640
Testing Loss: 0.8130, Accuracy: 0.8537, Precision: 0.6850, Recall: 0.7025, F1: 0.6899
LM Predictions:  [3, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 5, 4, 0, 5, 0, 2, 0, 4, 5, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6708, Accuracy: 0.7857, Precision: 0.6963, Recall: 0.6619, F1: 0.6633
Epoch 32/70
Train Loss: 0.0650, Accuracy: 0.9839, Precision: 0.9636, Recall: 0.9687, F1: 0.9660
Validation Loss: 1.1191, Accuracy: 0.7926, Precision: 0.6406, Recall: 0.5801, F1: 0.5988
Testing Loss: 1.0568, Accuracy: 0.7899, Precision: 0.6746, Recall: 0.6033, F1: 0.6070
LM Predictions:  [3, 0, 5, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 5, 5, 5, 4, 0, 4, 2, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.8561, Accuracy: 0.7857, Precision: 0.7063, Recall: 0.6714, F1: 0.6677
Epoch 33/70
Train Loss: 0.1158, Accuracy: 0.9664, Precision: 0.9348, Recall: 0.9390, F1: 0.9369
Validation Loss: 0.9628, Accuracy: 0.8153, Precision: 0.6094, Recall: 0.6200, F1: 0.6049
Testing Loss: 0.9244, Accuracy: 0.8271, Precision: 0.6198, Recall: 0.6350, F1: 0.6004
LM Predictions:  [1, 5, 5, 4, 2, 1, 5, 2, 5, 2, 5, 3, 4, 0, 5, 5, 3, 5, 4, 3, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.4719, Accuracy: 0.6071, Precision: 0.6889, Recall: 0.5524, F1: 0.5491
Epoch 34/70
Train Loss: 0.1068, Accuracy: 0.9699, Precision: 0.9189, Recall: 0.9241, F1: 0.9213
Validation Loss: 0.9191, Accuracy: 0.8153, Precision: 0.6047, Recall: 0.6295, F1: 0.6122
Testing Loss: 0.8326, Accuracy: 0.8537, Precision: 0.7050, Recall: 0.6990, F1: 0.6958
LM Predictions:  [5, 0, 0, 4, 4, 1, 0, 2, 0, 0, 5, 3, 4, 0, 5, 0, 4, 0, 4, 3, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.5480, Accuracy: 0.7857, Precision: 0.7431, Recall: 0.6440, F1: 0.6859
Epoch 35/70
Train Loss: 0.0834, Accuracy: 0.9811, Precision: 0.9582, Recall: 0.9320, F1: 0.9431
Validation Loss: 0.9535, Accuracy: 0.8295, Precision: 0.6701, Recall: 0.6798, F1: 0.6564
Testing Loss: 0.8856, Accuracy: 0.8404, Precision: 0.6184, Recall: 0.6420, F1: 0.6219
LM Predictions:  [5, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 5, 4, 0, 5, 0, 5, 0, 4, 5, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.5415, Accuracy: 0.7857, Precision: 0.7173, Recall: 0.6619, F1: 0.6749
Epoch 36/70
Train Loss: 0.0610, Accuracy: 0.9867, Precision: 0.9537, Recall: 0.9603, F1: 0.9570
Validation Loss: 0.9322, Accuracy: 0.8352, Precision: 0.6707, Recall: 0.6920, F1: 0.6740
Testing Loss: 0.8732, Accuracy: 0.8564, Precision: 0.6323, Recall: 0.6634, F1: 0.6433
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 3, 5, 3, 4, 0, 5, 0, 4, 0, 4, 3, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3147, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7524, F1: 0.7870
Epoch 37/70
Train Loss: 0.0560, Accuracy: 0.9822, Precision: 0.9629, Recall: 0.9589, F1: 0.9609
Validation Loss: 0.9323, Accuracy: 0.8494, Precision: 0.6852, Recall: 0.7008, F1: 0.6846
Testing Loss: 0.8158, Accuracy: 0.8537, Precision: 0.6330, Recall: 0.6448, F1: 0.6297
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 3, 4, 0, 5, 0, 3, 0, 4, 3, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4293, Accuracy: 0.8214, Precision: 0.7792, Recall: 0.6952, F1: 0.7166
Epoch 38/70
Train Loss: 0.0525, Accuracy: 0.9857, Precision: 0.9730, Recall: 0.9691, F1: 0.9710
Validation Loss: 0.8611, Accuracy: 0.8523, Precision: 0.6922, Recall: 0.7058, F1: 0.6863
Testing Loss: 0.8064, Accuracy: 0.8617, Precision: 0.6886, Recall: 0.6883, F1: 0.6873
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 3, 4, 0, 5, 0, 4, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3696, Accuracy: 0.9286, Precision: 0.8056, Recall: 0.7857, F1: 0.7904
Epoch 39/70
Train Loss: 0.0524, Accuracy: 0.9850, Precision: 0.9761, Recall: 0.9789, F1: 0.9775
Validation Loss: 0.9371, Accuracy: 0.8466, Precision: 0.6904, Recall: 0.7057, F1: 0.6933
Testing Loss: 0.8852, Accuracy: 0.8537, Precision: 0.6429, Recall: 0.6556, F1: 0.6491
LM Predictions:  [1, 0, 0, 4, 2, 3, 0, 2, 0, 5, 5, 5, 4, 0, 5, 0, 5, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3129, Accuracy: 0.8571, Precision: 0.7708, Recall: 0.7286, F1: 0.7309
Epoch 40/70
Train Loss: 0.0261, Accuracy: 0.9920, Precision: 0.9860, Recall: 0.9822, F1: 0.9841
Validation Loss: 1.0645, Accuracy: 0.8438, Precision: 0.6795, Recall: 0.6147, F1: 0.6363
Testing Loss: 0.9053, Accuracy: 0.8484, Precision: 0.6750, Recall: 0.6350, F1: 0.6505
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 5, 4, 0, 5, 5, 5, 0, 4, 5, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6300, Accuracy: 0.8214, Precision: 0.8600, Recall: 0.8457, F1: 0.8309
Epoch 41/70
Train Loss: 0.0562, Accuracy: 0.9864, Precision: 0.9798, Recall: 0.9634, F1: 0.9713
Validation Loss: 1.0441, Accuracy: 0.8210, Precision: 0.6373, Recall: 0.6688, F1: 0.6339
Testing Loss: 0.8851, Accuracy: 0.8431, Precision: 0.6698, Recall: 0.6952, F1: 0.6656
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2825, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7762, F1: 0.7909
Epoch 42/70
Train Loss: 0.0430, Accuracy: 0.9906, Precision: 0.9854, Recall: 0.9792, F1: 0.9822
Validation Loss: 1.0523, Accuracy: 0.8267, Precision: 0.6334, Recall: 0.6268, F1: 0.6291
Testing Loss: 0.9084, Accuracy: 0.8564, Precision: 0.6456, Recall: 0.6679, F1: 0.6506
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 3, 4, 0, 5, 0, 3, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2879, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7857, F1: 0.8056
Epoch 43/70
Train Loss: 0.0591, Accuracy: 0.9846, Precision: 0.9640, Recall: 0.9519, F1: 0.9576
Validation Loss: 1.2843, Accuracy: 0.7898, Precision: 0.6058, Recall: 0.5657, F1: 0.5604
Testing Loss: 1.1010, Accuracy: 0.8032, Precision: 0.6606, Recall: 0.6570, F1: 0.6128
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3665, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7762, F1: 0.7909
Epoch 44/70
Train Loss: 0.0609, Accuracy: 0.9839, Precision: 0.9679, Recall: 0.9673, F1: 0.9675
Validation Loss: 0.9075, Accuracy: 0.8295, Precision: 0.6316, Recall: 0.6184, F1: 0.6233
Testing Loss: 0.8327, Accuracy: 0.8404, Precision: 0.6725, Recall: 0.6681, F1: 0.6627
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 3, 4, 0, 5, 0, 4, 0, 4, 4, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4523, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7762, F1: 0.7835
Epoch 45/70
Train Loss: 0.0377, Accuracy: 0.9888, Precision: 0.9816, Recall: 0.9858, F1: 0.9837
Validation Loss: 1.0203, Accuracy: 0.8153, Precision: 0.6240, Recall: 0.5912, F1: 0.6019
Testing Loss: 0.9083, Accuracy: 0.8404, Precision: 0.7226, Recall: 0.6796, F1: 0.6779
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 4, 5, 4, 4, 3, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3086, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7762, F1: 0.7909
Epoch 46/70
Train Loss: 0.0296, Accuracy: 0.9916, Precision: 0.9884, Recall: 0.9808, F1: 0.9845
Validation Loss: 1.0751, Accuracy: 0.8295, Precision: 0.6184, Recall: 0.6002, F1: 0.5983
Testing Loss: 0.9740, Accuracy: 0.8431, Precision: 0.6336, Recall: 0.6527, F1: 0.6285
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 5, 4, 0, 5, 0, 5, 0, 4, 5, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2291, Accuracy: 0.8571, Precision: 0.8893, Recall: 0.8743, F1: 0.8655
Epoch 47/70
Train Loss: 0.0353, Accuracy: 0.9899, Precision: 0.9849, Recall: 0.9717, F1: 0.9778
Validation Loss: 1.1223, Accuracy: 0.8125, Precision: 0.6074, Recall: 0.5677, F1: 0.5782
Testing Loss: 0.9635, Accuracy: 0.8511, Precision: 0.7150, Recall: 0.6768, F1: 0.6839
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 2, 4, 0, 5, 0, 5, 0, 4, 2, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3819, Accuracy: 0.8929, Precision: 0.9000, Recall: 0.9143, F1: 0.8873
Epoch 48/70
Train Loss: 0.0538, Accuracy: 0.9874, Precision: 0.9758, Recall: 0.9516, F1: 0.9625
Validation Loss: 1.0568, Accuracy: 0.8267, Precision: 0.6151, Recall: 0.5935, F1: 0.5996
Testing Loss: 0.9179, Accuracy: 0.8457, Precision: 0.6406, Recall: 0.6535, F1: 0.6335
LM Predictions:  [1, 5, 0, 4, 2, 1, 5, 2, 5, 2, 5, 5, 4, 0, 5, 5, 2, 5, 4, 5, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.3234, Accuracy: 0.6429, Precision: 0.7743, Recall: 0.6914, F1: 0.6598
Epoch 49/70
Train Loss: 0.0432, Accuracy: 0.9895, Precision: 0.9764, Recall: 0.9612, F1: 0.9684
Validation Loss: 1.0973, Accuracy: 0.8295, Precision: 0.6242, Recall: 0.6018, F1: 0.6111
Testing Loss: 0.9585, Accuracy: 0.8564, Precision: 0.6937, Recall: 0.6924, F1: 0.6851
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 2, 5, 3, 4, 0, 5, 0, 2, 0, 4, 3, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4933, Accuracy: 0.8214, Precision: 0.7778, Recall: 0.6952, F1: 0.7129
Epoch 50/70
Train Loss: 0.0337, Accuracy: 0.9920, Precision: 0.9854, Recall: 0.9866, F1: 0.9860
Validation Loss: 0.9442, Accuracy: 0.8381, Precision: 0.6718, Recall: 0.5935, F1: 0.6223
Testing Loss: 0.9200, Accuracy: 0.8431, Precision: 0.6634, Recall: 0.6258, F1: 0.6384
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 4, 4, 0, 5, 0, 4, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2616, Accuracy: 0.9286, Precision: 0.9350, Recall: 0.9314, F1: 0.9313
Epoch 51/70
Train Loss: 0.0414, Accuracy: 0.9899, Precision: 0.9773, Recall: 0.9741, F1: 0.9757
Validation Loss: 1.1230, Accuracy: 0.8352, Precision: 0.6409, Recall: 0.6440, F1: 0.6394
Testing Loss: 1.0362, Accuracy: 0.8511, Precision: 0.6278, Recall: 0.6777, F1: 0.6448
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 3, 5, 3, 4, 0, 4, 3, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.8663, Accuracy: 0.8571, Precision: 0.8333, Recall: 0.7286, F1: 0.7742
Epoch 52/70
Train Loss: 0.0400, Accuracy: 0.9885, Precision: 0.9786, Recall: 0.9785, F1: 0.9784
Validation Loss: 1.0923, Accuracy: 0.8210, Precision: 0.6255, Recall: 0.5835, F1: 0.5942
Testing Loss: 0.9864, Accuracy: 0.8484, Precision: 0.6361, Recall: 0.6576, F1: 0.6390
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 5, 4, 0, 5, 0, 5, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3609, Accuracy: 0.8571, Precision: 0.8893, Recall: 0.8743, F1: 0.8655
Epoch 53/70
Train Loss: 0.0409, Accuracy: 0.9906, Precision: 0.9693, Recall: 0.9734, F1: 0.9712
Validation Loss: 1.0773, Accuracy: 0.8352, Precision: 0.6232, Recall: 0.6539, F1: 0.6337
Testing Loss: 1.1024, Accuracy: 0.8404, Precision: 0.6177, Recall: 0.6795, F1: 0.6316
LM Predictions:  [1, 3, 3, 4, 2, 1, 0, 2, 3, 5, 5, 2, 4, 3, 5, 3, 5, 0, 4, 5, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.9728, Accuracy: 0.6786, Precision: 0.7444, Recall: 0.6095, F1: 0.6313
Epoch 54/70
Train Loss: 0.0698, Accuracy: 0.9832, Precision: 0.9716, Recall: 0.9399, F1: 0.9521
Validation Loss: 0.8446, Accuracy: 0.8153, Precision: 0.6428, Recall: 0.6585, F1: 0.6439
Testing Loss: 0.7643, Accuracy: 0.8404, Precision: 0.6069, Recall: 0.6528, F1: 0.6243
LM Predictions:  [3, 0, 0, 4, 2, 1, 0, 2, 0, 4, 5, 4, 4, 3, 5, 0, 4, 0, 4, 4, 1, 3, 2, 1, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.5920, Accuracy: 0.8214, Precision: 0.8125, Recall: 0.6762, F1: 0.7261
Epoch 55/70
Train Loss: 0.0558, Accuracy: 0.9874, Precision: 0.9711, Recall: 0.9723, F1: 0.9713
Validation Loss: 0.9594, Accuracy: 0.8324, Precision: 0.6385, Recall: 0.6141, F1: 0.6224
Testing Loss: 0.8497, Accuracy: 0.8537, Precision: 0.6465, Recall: 0.6199, F1: 0.6277
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 2, 4, 3, 5, 0, 2, 0, 4, 2, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.5578, Accuracy: 0.7857, Precision: 0.7381, Recall: 0.6714, F1: 0.6769
Epoch 56/70
Train Loss: 0.0271, Accuracy: 0.9934, Precision: 0.9877, Recall: 0.9887, F1: 0.9882
Validation Loss: 1.0466, Accuracy: 0.8409, Precision: 0.6410, Recall: 0.6532, F1: 0.6436
Testing Loss: 0.8173, Accuracy: 0.8511, Precision: 0.6786, Recall: 0.6973, F1: 0.6776
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 4, 5, 4, 4, 3, 5, 0, 4, 0, 4, 4, 1, 4, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3282, Accuracy: 0.8929, Precision: 0.7963, Recall: 0.7429, F1: 0.7580
Epoch 57/70
Train Loss: 0.0220, Accuracy: 0.9937, Precision: 0.9893, Recall: 0.9808, F1: 0.9849
Validation Loss: 1.1391, Accuracy: 0.8324, Precision: 0.6397, Recall: 0.6241, F1: 0.6304
Testing Loss: 0.9459, Accuracy: 0.8511, Precision: 0.6822, Recall: 0.6783, F1: 0.6737
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1553, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0292, Accuracy: 0.9916, Precision: 0.9832, Recall: 0.9827, F1: 0.9830
Validation Loss: 1.1131, Accuracy: 0.8324, Precision: 0.6516, Recall: 0.6374, F1: 0.6416
Testing Loss: 0.9872, Accuracy: 0.8697, Precision: 0.6666, Recall: 0.6699, F1: 0.6642
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 5, 4, 0, 5, 0, 5, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1626, Accuracy: 0.8929, Precision: 0.9250, Recall: 0.9143, F1: 0.8993
Epoch 59/70
Train Loss: 0.0169, Accuracy: 0.9955, Precision: 0.9927, Recall: 0.9919, F1: 0.9923
Validation Loss: 1.1281, Accuracy: 0.8381, Precision: 0.6608, Recall: 0.6276, F1: 0.6388
Testing Loss: 1.0784, Accuracy: 0.8590, Precision: 0.6565, Recall: 0.6605, F1: 0.6527
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1014, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0202, Accuracy: 0.9962, Precision: 0.9952, Recall: 0.9873, F1: 0.9912
Validation Loss: 1.1141, Accuracy: 0.8381, Precision: 0.6545, Recall: 0.6444, F1: 0.6470
Testing Loss: 1.0362, Accuracy: 0.8697, Precision: 0.6687, Recall: 0.6790, F1: 0.6706
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 3, 4, 0, 5, 0, 4, 0, 4, 4, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3390, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7762, F1: 0.7909
Epoch 61/70
Train Loss: 0.0346, Accuracy: 0.9902, Precision: 0.9900, Recall: 0.9888, F1: 0.9894
Validation Loss: 1.3770, Accuracy: 0.7898, Precision: 0.6121, Recall: 0.5916, F1: 0.5936
Testing Loss: 1.1531, Accuracy: 0.8378, Precision: 0.6381, Recall: 0.6655, F1: 0.6422
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 3, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3161, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 62/70
Train Loss: 0.0492, Accuracy: 0.9888, Precision: 0.9765, Recall: 0.9722, F1: 0.9742
Validation Loss: 1.1949, Accuracy: 0.8210, Precision: 0.6487, Recall: 0.5602, F1: 0.5901
Testing Loss: 1.3217, Accuracy: 0.8165, Precision: 0.6066, Recall: 0.5583, F1: 0.5643
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 2, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 5, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2459, Accuracy: 0.9286, Precision: 0.9200, Recall: 0.9200, F1: 0.9156
Epoch 63/70
Train Loss: 0.0491, Accuracy: 0.9871, Precision: 0.9783, Recall: 0.9748, F1: 0.9765
Validation Loss: 1.0229, Accuracy: 0.8182, Precision: 0.6447, Recall: 0.6302, F1: 0.6337
Testing Loss: 0.9780, Accuracy: 0.8324, Precision: 0.6180, Recall: 0.6437, F1: 0.6195
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1580, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0169, Accuracy: 0.9948, Precision: 0.9955, Recall: 0.9911, F1: 0.9933
Validation Loss: 1.0607, Accuracy: 0.8580, Precision: 0.6762, Recall: 0.6464, F1: 0.6593
Testing Loss: 1.0862, Accuracy: 0.8564, Precision: 0.6475, Recall: 0.6544, F1: 0.6455
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 2, 5, 4, 4, 0, 5, 0, 5, 0, 4, 4, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1755, Accuracy: 0.8929, Precision: 0.8850, Recall: 0.8914, F1: 0.8824
Epoch 65/70
Train Loss: 0.0367, Accuracy: 0.9899, Precision: 0.9800, Recall: 0.9776, F1: 0.9788
Validation Loss: 1.0530, Accuracy: 0.8409, Precision: 0.6599, Recall: 0.6705, F1: 0.6568
Testing Loss: 0.9643, Accuracy: 0.8590, Precision: 0.6448, Recall: 0.6642, F1: 0.6485
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 5, 0, 4, 5, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1510, Accuracy: 0.9286, Precision: 0.9429, Recall: 0.9429, F1: 0.9333
Epoch 66/70
Train Loss: 0.0510, Accuracy: 0.9864, Precision: 0.9755, Recall: 0.9458, F1: 0.9589
Validation Loss: 0.9233, Accuracy: 0.8352, Precision: 0.6493, Recall: 0.6097, F1: 0.6244
Testing Loss: 0.8456, Accuracy: 0.8590, Precision: 0.6489, Recall: 0.6400, F1: 0.6438
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 4, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1251, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 67/70
Train Loss: 0.0151, Accuracy: 0.9976, Precision: 0.9944, Recall: 0.9946, F1: 0.9945
Validation Loss: 1.1213, Accuracy: 0.8438, Precision: 0.6843, Recall: 0.6756, F1: 0.6719
Testing Loss: 1.0550, Accuracy: 0.8537, Precision: 0.6434, Recall: 0.6437, F1: 0.6412
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 5, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0660, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 68/70
Train Loss: 0.0200, Accuracy: 0.9955, Precision: 0.9941, Recall: 0.9896, F1: 0.9918
Validation Loss: 0.9703, Accuracy: 0.8466, Precision: 0.6730, Recall: 0.6503, F1: 0.6590
Testing Loss: 0.9892, Accuracy: 0.8564, Precision: 0.6577, Recall: 0.6617, F1: 0.6562
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0805, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 69/70
Train Loss: 0.0509, Accuracy: 0.9899, Precision: 0.9893, Recall: 0.9918, F1: 0.9905
Validation Loss: 0.9155, Accuracy: 0.8494, Precision: 0.6882, Recall: 0.6056, F1: 0.6247
Testing Loss: 1.0717, Accuracy: 0.8404, Precision: 0.6935, Recall: 0.6341, F1: 0.6424
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 2, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0515, Accuracy: 0.9643, Precision: 0.9600, Recall: 0.9600, F1: 0.9556
Epoch 70/70
Train Loss: 0.0227, Accuracy: 0.9955, Precision: 0.9941, Recall: 0.9946, F1: 0.9943
Validation Loss: 1.1141, Accuracy: 0.8324, Precision: 0.6537, Recall: 0.6724, F1: 0.6411
Testing Loss: 1.0205, Accuracy: 0.8537, Precision: 0.6496, Recall: 0.6650, F1: 0.6423
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 0, 5, 4, 4, 0, 5, 0, 4, 0, 1, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3915, Accuracy: 0.9286, Precision: 0.9417, Recall: 0.9314, F1: 0.9309
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.0073, Accuracy: 0.6193, Precision: 0.3029, Recall: 0.3182, F1: 0.3071
Validation Loss: 0.7115, Accuracy: 0.7670, Precision: 0.3629, Recall: 0.4095, F1: 0.3846
Testing Loss: 0.6929, Accuracy: 0.7739, Precision: 0.3629, Recall: 0.4178, F1: 0.3873
LM Predictions:  [5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 5, 5, 5, 2, 5, 2, 5, 5, 5, 5, 2, 5]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5197, Accuracy: 0.1429, Precision: 0.0364, Recall: 0.1600, F1: 0.0593
Epoch 2/70
Train Loss: 0.5619, Accuracy: 0.8146, Precision: 0.5094, Recall: 0.5129, F1: 0.5039
Validation Loss: 0.6080, Accuracy: 0.7670, Precision: 0.4791, Recall: 0.5227, F1: 0.4727
Testing Loss: 0.5957, Accuracy: 0.8032, Precision: 0.4995, Recall: 0.5498, F1: 0.5042
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7984, Accuracy: 0.1071, Precision: 0.3333, Recall: 0.1167, F1: 0.1667
Epoch 3/70
Train Loss: 0.4136, Accuracy: 0.8674, Precision: 0.6004, Recall: 0.6202, F1: 0.6079
Validation Loss: 0.4138, Accuracy: 0.8693, Precision: 0.6536, Recall: 0.6522, F1: 0.6494
Testing Loss: 0.4751, Accuracy: 0.8484, Precision: 0.5961, Recall: 0.6322, F1: 0.6097
LM Predictions:  [5, 3, 1, 3, 4, 3, 3, 2, 3, 3, 5, 2, 3, 3, 3, 3, 3, 5, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7736, Accuracy: 0.1071, Precision: 0.1222, Recall: 0.1167, F1: 0.1157
Epoch 4/70
Train Loss: 0.3406, Accuracy: 0.9031, Precision: 0.6705, Recall: 0.6899, F1: 0.6779
Validation Loss: 0.4429, Accuracy: 0.8778, Precision: 0.6738, Recall: 0.6790, F1: 0.6744
Testing Loss: 0.5365, Accuracy: 0.8590, Precision: 0.6341, Recall: 0.6637, F1: 0.6273
LM Predictions:  [1, 3, 1, 3, 4, 3, 1, 2, 3, 3, 5, 2, 3, 2, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.9981, Accuracy: 0.1429, Precision: 0.2889, Recall: 0.1500, F1: 0.1713
Epoch 5/70
Train Loss: 0.2980, Accuracy: 0.9108, Precision: 0.7051, Recall: 0.7026, F1: 0.6886
Validation Loss: 0.4304, Accuracy: 0.8722, Precision: 0.6784, Recall: 0.6439, F1: 0.6546
Testing Loss: 0.4784, Accuracy: 0.8697, Precision: 0.6546, Recall: 0.6879, F1: 0.6579
LM Predictions:  [1, 3, 1, 2, 4, 3, 3, 2, 3, 3, 5, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5885, Accuracy: 0.1786, Precision: 0.3611, Recall: 0.1833, F1: 0.2222
Epoch 6/70
Train Loss: 0.2467, Accuracy: 0.9279, Precision: 0.7941, Recall: 0.7657, F1: 0.7642
Validation Loss: 0.4897, Accuracy: 0.8608, Precision: 0.6856, Recall: 0.6647, F1: 0.6687
Testing Loss: 0.5378, Accuracy: 0.8537, Precision: 0.6475, Recall: 0.6756, F1: 0.6387
LM Predictions:  [1, 3, 1, 2, 4, 3, 3, 2, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.9272, Accuracy: 0.1429, Precision: 0.3611, Recall: 0.1500, F1: 0.1984
Epoch 7/70
Train Loss: 0.2068, Accuracy: 0.9398, Precision: 0.8184, Recall: 0.8046, F1: 0.8039
Validation Loss: 0.4880, Accuracy: 0.8608, Precision: 0.6526, Recall: 0.6654, F1: 0.6482
Testing Loss: 0.5058, Accuracy: 0.8723, Precision: 0.6655, Recall: 0.6990, F1: 0.6803
LM Predictions:  [3, 3, 1, 2, 3, 3, 3, 2, 0, 3, 5, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3963, Accuracy: 0.1429, Precision: 0.4000, Recall: 0.1405, F1: 0.1713
Epoch 8/70
Train Loss: 0.1837, Accuracy: 0.9486, Precision: 0.8607, Recall: 0.8619, F1: 0.8584
Validation Loss: 0.5280, Accuracy: 0.8636, Precision: 0.6845, Recall: 0.6180, F1: 0.6421
Testing Loss: 0.4865, Accuracy: 0.8830, Precision: 0.7020, Recall: 0.6703, F1: 0.6836
LM Predictions:  [3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 5, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 0, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.7902, Accuracy: 0.1071, Precision: 0.2222, Recall: 0.1167, F1: 0.1222
Epoch 9/70
Train Loss: 0.1488, Accuracy: 0.9619, Precision: 0.9018, Recall: 0.8906, F1: 0.8936
Validation Loss: 0.5934, Accuracy: 0.8580, Precision: 0.6682, Recall: 0.6724, F1: 0.6625
Testing Loss: 0.5593, Accuracy: 0.8617, Precision: 0.6446, Recall: 0.6748, F1: 0.6484
LM Predictions:  [3, 3, 1, 3, 2, 3, 3, 2, 0, 3, 5, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1764, Accuracy: 0.1786, Precision: 0.4583, Recall: 0.1821, F1: 0.2222
Epoch 10/70
Train Loss: 0.1574, Accuracy: 0.9563, Precision: 0.8835, Recall: 0.8767, F1: 0.8776
Validation Loss: 0.6311, Accuracy: 0.8466, Precision: 0.6633, Recall: 0.6437, F1: 0.6474
Testing Loss: 0.6266, Accuracy: 0.8537, Precision: 0.6485, Recall: 0.6429, F1: 0.6377
LM Predictions:  [1, 2, 1, 2, 5, 3, 5, 2, 5, 3, 5, 2, 2, 2, 3, 3, 5, 5, 5, 3, 3, 5, 2, 3, 3, 5, 3, 0]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.1878, Accuracy: 0.2143, Precision: 0.1865, Recall: 0.2167, F1: 0.1797
Epoch 11/70
Train Loss: 0.1164, Accuracy: 0.9678, Precision: 0.9116, Recall: 0.9038, F1: 0.9061
Validation Loss: 0.5305, Accuracy: 0.8494, Precision: 0.6655, Recall: 0.6794, F1: 0.6601
Testing Loss: 0.6124, Accuracy: 0.8484, Precision: 0.6499, Recall: 0.6699, F1: 0.6512
LM Predictions:  [1, 4, 3, 2, 2, 3, 3, 2, 4, 3, 5, 4, 4, 3, 3, 3, 4, 0, 4, 4, 4, 0, 2, 1, 3, 3, 4, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.8598, Accuracy: 0.4643, Precision: 0.6528, Recall: 0.3917, F1: 0.4378
Epoch 12/70
Train Loss: 0.1002, Accuracy: 0.9671, Precision: 0.8994, Recall: 0.8992, F1: 0.8973
Validation Loss: 0.6316, Accuracy: 0.8438, Precision: 0.7031, Recall: 0.6189, F1: 0.6339
Testing Loss: 0.6595, Accuracy: 0.8378, Precision: 0.6106, Recall: 0.6178, F1: 0.5784
LM Predictions:  [1, 0, 1, 2, 0, 3, 0, 2, 0, 0, 5, 5, 2, 0, 0, 1, 4, 0, 5, 4, 1, 5, 2, 1, 1, 5, 3, 0]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.4488, Accuracy: 0.5714, Precision: 0.5537, Recall: 0.4833, F1: 0.4828
Epoch 13/70
Train Loss: 0.0887, Accuracy: 0.9762, Precision: 0.9197, Recall: 0.9275, F1: 0.9232
Validation Loss: 0.6296, Accuracy: 0.8523, Precision: 0.7043, Recall: 0.5868, F1: 0.6197
Testing Loss: 0.5548, Accuracy: 0.8723, Precision: 0.6760, Recall: 0.6494, F1: 0.6525
LM Predictions:  [1, 4, 3, 4, 2, 3, 3, 2, 5, 3, 5, 5, 2, 4, 3, 3, 4, 4, 4, 4, 4, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.6924, Accuracy: 0.5000, Precision: 0.4750, Recall: 0.4440, F1: 0.4480
Epoch 14/70
Train Loss: 0.1000, Accuracy: 0.9741, Precision: 0.9378, Recall: 0.9413, F1: 0.9390
Validation Loss: 0.6865, Accuracy: 0.8580, Precision: 0.7070, Recall: 0.6485, F1: 0.6712
Testing Loss: 0.6438, Accuracy: 0.8670, Precision: 0.6419, Recall: 0.6351, F1: 0.6216
LM Predictions:  [1, 0, 3, 2, 2, 3, 5, 2, 5, 5, 5, 4, 2, 0, 5, 3, 4, 0, 5, 2, 2, 5, 2, 1, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.3941, Accuracy: 0.5714, Precision: 0.6756, Recall: 0.5107, F1: 0.5182
Epoch 15/70
Train Loss: 0.0808, Accuracy: 0.9790, Precision: 0.9451, Recall: 0.9473, F1: 0.9459
Validation Loss: 0.5826, Accuracy: 0.8608, Precision: 0.6951, Recall: 0.6594, F1: 0.6705
Testing Loss: 0.6242, Accuracy: 0.8484, Precision: 0.6506, Recall: 0.6425, F1: 0.6437
LM Predictions:  [1, 3, 3, 4, 2, 3, 0, 2, 0, 3, 5, 4, 4, 0, 5, 3, 4, 0, 4, 3, 3, 5, 2, 1, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.0644, Accuracy: 0.6786, Precision: 0.8333, Recall: 0.5726, F1: 0.6761
Epoch 16/70
Train Loss: 0.0672, Accuracy: 0.9822, Precision: 0.9582, Recall: 0.9497, F1: 0.9535
Validation Loss: 0.6036, Accuracy: 0.8778, Precision: 0.7025, Recall: 0.7252, F1: 0.7121
Testing Loss: 0.6417, Accuracy: 0.8670, Precision: 0.6471, Recall: 0.7000, F1: 0.6663
LM Predictions:  [1, 3, 3, 4, 4, 3, 3, 3, 3, 3, 5, 4, 4, 0, 5, 3, 4, 0, 4, 3, 3, 5, 2, 3, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.2304, Accuracy: 0.5000, Precision: 0.8056, Recall: 0.4083, F1: 0.5123
Epoch 17/70
Train Loss: 0.0744, Accuracy: 0.9818, Precision: 0.9537, Recall: 0.9518, F1: 0.9524
Validation Loss: 0.5973, Accuracy: 0.8750, Precision: 0.7710, Recall: 0.7119, F1: 0.7256
Testing Loss: 0.6929, Accuracy: 0.8511, Precision: 0.6490, Recall: 0.6178, F1: 0.6156
LM Predictions:  [1, 2, 3, 2, 2, 3, 0, 2, 0, 0, 5, 2, 4, 0, 5, 3, 4, 0, 2, 4, 2, 5, 2, 4, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.2453, Accuracy: 0.5714, Precision: 0.6542, Recall: 0.4917, F1: 0.5287
Epoch 18/70
Train Loss: 0.0579, Accuracy: 0.9846, Precision: 0.9560, Recall: 0.9507, F1: 0.9531
Validation Loss: 0.5979, Accuracy: 0.8693, Precision: 0.6847, Recall: 0.6990, F1: 0.6894
Testing Loss: 0.6232, Accuracy: 0.8750, Precision: 0.6532, Recall: 0.6954, F1: 0.6602
LM Predictions:  [1, 4, 3, 3, 2, 3, 0, 2, 3, 3, 5, 4, 4, 0, 5, 3, 3, 0, 2, 2, 2, 5, 2, 1, 1, 5, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.1728, Accuracy: 0.5357, Precision: 0.6944, Recall: 0.4774, F1: 0.5398
Epoch 19/70
Train Loss: 0.0739, Accuracy: 0.9818, Precision: 0.9642, Recall: 0.9609, F1: 0.9621
Validation Loss: 0.6225, Accuracy: 0.8608, Precision: 0.7247, Recall: 0.6333, F1: 0.6646
Testing Loss: 0.7063, Accuracy: 0.8590, Precision: 0.6609, Recall: 0.6404, F1: 0.6386
LM Predictions:  [1, 0, 3, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 2, 4, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6789, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6536, F1: 0.6806
Epoch 20/70
Train Loss: 0.0276, Accuracy: 0.9948, Precision: 0.9882, Recall: 0.9819, F1: 0.9848
Validation Loss: 0.7541, Accuracy: 0.8636, Precision: 0.7055, Recall: 0.6570, F1: 0.6783
Testing Loss: 0.6998, Accuracy: 0.8750, Precision: 0.6744, Recall: 0.6670, F1: 0.6606
LM Predictions:  [1, 0, 3, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 4, 5, 2, 1, 1, 5, 4, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4884, Accuracy: 0.8214, Precision: 0.7963, Recall: 0.6774, F1: 0.7192
Epoch 21/70
Train Loss: 0.0400, Accuracy: 0.9916, Precision: 0.9824, Recall: 0.9803, F1: 0.9810
Validation Loss: 0.6255, Accuracy: 0.8722, Precision: 0.7023, Recall: 0.6666, F1: 0.6824
Testing Loss: 0.6203, Accuracy: 0.8750, Precision: 0.6752, Recall: 0.6752, F1: 0.6615
LM Predictions:  [1, 0, 3, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 4, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4701, Accuracy: 0.8214, Precision: 0.7833, Recall: 0.6774, F1: 0.7107
Epoch 22/70
Train Loss: 0.0363, Accuracy: 0.9902, Precision: 0.9837, Recall: 0.9755, F1: 0.9794
Validation Loss: 0.6550, Accuracy: 0.8778, Precision: 0.7054, Recall: 0.6892, F1: 0.6969
Testing Loss: 0.7283, Accuracy: 0.8617, Precision: 0.6827, Recall: 0.6760, F1: 0.6721
LM Predictions:  [1, 0, 3, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 2, 2, 5, 2, 1, 1, 5, 4, 0]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.7749, Accuracy: 0.7857, Precision: 0.7151, Recall: 0.6536, F1: 0.6738
Epoch 23/70
Train Loss: 0.0515, Accuracy: 0.9871, Precision: 0.9708, Recall: 0.9613, F1: 0.9658
Validation Loss: 0.6378, Accuracy: 0.8494, Precision: 0.6778, Recall: 0.6664, F1: 0.6711
Testing Loss: 0.6950, Accuracy: 0.8670, Precision: 0.6599, Recall: 0.6847, F1: 0.6674
LM Predictions:  [1, 0, 3, 4, 0, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 3, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4767, Accuracy: 0.7857, Precision: 0.7685, Recall: 0.6357, F1: 0.6768
Epoch 24/70
Train Loss: 0.0613, Accuracy: 0.9832, Precision: 0.9594, Recall: 0.9579, F1: 0.9586
Validation Loss: 0.6523, Accuracy: 0.8551, Precision: 0.6866, Recall: 0.6426, F1: 0.6605
Testing Loss: 0.7434, Accuracy: 0.8431, Precision: 0.6266, Recall: 0.6190, F1: 0.6088
LM Predictions:  [1, 0, 3, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 1, 5, 4, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3733, Accuracy: 0.8571, Precision: 0.7963, Recall: 0.7024, F1: 0.7292
Epoch 25/70
Train Loss: 0.0491, Accuracy: 0.9878, Precision: 0.9801, Recall: 0.9761, F1: 0.9779
Validation Loss: 0.7129, Accuracy: 0.8864, Precision: 0.7269, Recall: 0.6616, F1: 0.6868
Testing Loss: 0.6352, Accuracy: 0.8883, Precision: 0.6777, Recall: 0.6740, F1: 0.6742
LM Predictions:  [1, 0, 3, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2975, Accuracy: 0.8929, Precision: 0.8125, Recall: 0.7440, F1: 0.7706
Epoch 26/70
Train Loss: 0.0342, Accuracy: 0.9920, Precision: 0.9751, Recall: 0.9780, F1: 0.9764
Validation Loss: 0.8498, Accuracy: 0.8523, Precision: 0.6713, Recall: 0.5999, F1: 0.6190
Testing Loss: 0.9229, Accuracy: 0.8245, Precision: 0.6103, Recall: 0.5944, F1: 0.5661
LM Predictions:  [1, 0, 5, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 1, 4, 5, 4, 4, 4, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6671, Accuracy: 0.7857, Precision: 0.8329, Recall: 0.7843, F1: 0.7816
Epoch 27/70
Train Loss: 0.0452, Accuracy: 0.9892, Precision: 0.9794, Recall: 0.9741, F1: 0.9767
Validation Loss: 0.6087, Accuracy: 0.8693, Precision: 0.7448, Recall: 0.7006, F1: 0.7077
Testing Loss: 0.6396, Accuracy: 0.8723, Precision: 0.6805, Recall: 0.6744, F1: 0.6761
LM Predictions:  [1, 0, 3, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2983, Accuracy: 0.9286, Precision: 0.8125, Recall: 0.7762, F1: 0.7909
Epoch 28/70
Train Loss: 0.0535, Accuracy: 0.9871, Precision: 0.9598, Recall: 0.9429, F1: 0.9495
Validation Loss: 0.7038, Accuracy: 0.8580, Precision: 0.6796, Recall: 0.6656, F1: 0.6667
Testing Loss: 0.6820, Accuracy: 0.8644, Precision: 0.6759, Recall: 0.6795, F1: 0.6753
LM Predictions:  [1, 0, 3, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 0]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3402, Accuracy: 0.9286, Precision: 0.8095, Recall: 0.7679, F1: 0.7857
Epoch 29/70
Train Loss: 0.0250, Accuracy: 0.9948, Precision: 0.9904, Recall: 0.9884, F1: 0.9894
Validation Loss: 0.7649, Accuracy: 0.8722, Precision: 0.6850, Recall: 0.6690, F1: 0.6757
Testing Loss: 0.6414, Accuracy: 0.8883, Precision: 0.6992, Recall: 0.6921, F1: 0.6874
LM Predictions:  [1, 0, 1, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1469, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 30/70
Train Loss: 0.0111, Accuracy: 0.9972, Precision: 0.9873, Recall: 0.9924, F1: 0.9898
Validation Loss: 0.7558, Accuracy: 0.8750, Precision: 0.7254, Recall: 0.7199, F1: 0.7115
Testing Loss: 0.7223, Accuracy: 0.8856, Precision: 0.6966, Recall: 0.6970, F1: 0.6914
LM Predictions:  [1, 0, 1, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 4]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0938, Accuracy: 0.9286, Precision: 0.9417, Recall: 0.9214, F1: 0.9245
Epoch 31/70
Train Loss: 0.0194, Accuracy: 0.9958, Precision: 0.9848, Recall: 0.9889, F1: 0.9868
Validation Loss: 0.8163, Accuracy: 0.8693, Precision: 0.6812, Recall: 0.6631, F1: 0.6661
Testing Loss: 0.9070, Accuracy: 0.8644, Precision: 0.6539, Recall: 0.6687, F1: 0.6375
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0254, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0293, Accuracy: 0.9920, Precision: 0.9867, Recall: 0.9842, F1: 0.9855
Validation Loss: 0.7270, Accuracy: 0.8722, Precision: 0.6947, Recall: 0.6814, F1: 0.6861
Testing Loss: 0.7931, Accuracy: 0.8750, Precision: 0.6923, Recall: 0.6667, F1: 0.6758
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0644, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0722, Accuracy: 0.9815, Precision: 0.9579, Recall: 0.9573, F1: 0.9575
Validation Loss: 0.7672, Accuracy: 0.8722, Precision: 0.7454, Recall: 0.7118, F1: 0.7263
Testing Loss: 0.7976, Accuracy: 0.8670, Precision: 0.6607, Recall: 0.6568, F1: 0.6531
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0407, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0221, Accuracy: 0.9948, Precision: 0.9916, Recall: 0.9872, F1: 0.9894
Validation Loss: 0.8396, Accuracy: 0.8636, Precision: 0.7385, Recall: 0.6657, F1: 0.6607
Testing Loss: 0.8114, Accuracy: 0.8670, Precision: 0.6570, Recall: 0.6293, F1: 0.6272
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 0, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0717, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 35/70
Train Loss: 0.0196, Accuracy: 0.9958, Precision: 0.9934, Recall: 0.9961, F1: 0.9947
Validation Loss: 0.8378, Accuracy: 0.8608, Precision: 0.6861, Recall: 0.6755, F1: 0.6803
Testing Loss: 0.8030, Accuracy: 0.8723, Precision: 0.6499, Recall: 0.6543, F1: 0.6486
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0191, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0216, Accuracy: 0.9965, Precision: 0.9955, Recall: 0.9957, F1: 0.9956
Validation Loss: 0.7696, Accuracy: 0.8665, Precision: 0.7000, Recall: 0.7235, F1: 0.6989
Testing Loss: 0.7210, Accuracy: 0.8723, Precision: 0.6825, Recall: 0.6642, F1: 0.6716
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0635, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0268, Accuracy: 0.9937, Precision: 0.9874, Recall: 0.9851, F1: 0.9862
Validation Loss: 0.7636, Accuracy: 0.8636, Precision: 0.6968, Recall: 0.6978, F1: 0.6916
Testing Loss: 0.7119, Accuracy: 0.8590, Precision: 0.6433, Recall: 0.6359, F1: 0.6369
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0253, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0139, Accuracy: 0.9976, Precision: 0.9920, Recall: 0.9877, F1: 0.9898
Validation Loss: 0.9206, Accuracy: 0.8580, Precision: 0.7237, Recall: 0.6684, F1: 0.6799
Testing Loss: 0.8251, Accuracy: 0.8670, Precision: 0.6883, Recall: 0.6210, F1: 0.6452
LM Predictions:  [1, 0, 5, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1313, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 39/70
Train Loss: 0.0240, Accuracy: 0.9941, Precision: 0.9830, Recall: 0.9796, F1: 0.9813
Validation Loss: 0.6828, Accuracy: 0.8864, Precision: 0.7214, Recall: 0.7319, F1: 0.7176
Testing Loss: 0.7686, Accuracy: 0.8670, Precision: 0.6591, Recall: 0.6703, F1: 0.6604
LM Predictions:  [1, 0, 0, 4, 2, 0, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2598, Accuracy: 0.9286, Precision: 0.8095, Recall: 0.7762, F1: 0.7910
Epoch 40/70
Train Loss: 0.0252, Accuracy: 0.9937, Precision: 0.9823, Recall: 0.9828, F1: 0.9825
Validation Loss: 0.7399, Accuracy: 0.8778, Precision: 0.6945, Recall: 0.6646, F1: 0.6782
Testing Loss: 0.7794, Accuracy: 0.8697, Precision: 0.6785, Recall: 0.6662, F1: 0.6711
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0242, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0198, Accuracy: 0.9958, Precision: 0.9867, Recall: 0.9916, F1: 0.9891
Validation Loss: 0.6993, Accuracy: 0.8693, Precision: 0.6780, Recall: 0.6769, F1: 0.6773
Testing Loss: 0.8520, Accuracy: 0.8644, Precision: 0.6433, Recall: 0.6551, F1: 0.6410
LM Predictions:  [1, 0, 0, 4, 2, 3, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0531, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 42/70
Train Loss: 0.0103, Accuracy: 0.9983, Precision: 0.9976, Recall: 0.9965, F1: 0.9971
Validation Loss: 0.7665, Accuracy: 0.8949, Precision: 0.7457, Recall: 0.7339, F1: 0.7352
Testing Loss: 0.8753, Accuracy: 0.8697, Precision: 0.6555, Recall: 0.6559, F1: 0.6520
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0069, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0224, Accuracy: 0.9927, Precision: 0.9869, Recall: 0.9868, F1: 0.9868
Validation Loss: 0.8781, Accuracy: 0.8722, Precision: 0.7029, Recall: 0.7187, F1: 0.7072
Testing Loss: 0.8755, Accuracy: 0.8723, Precision: 0.6698, Recall: 0.6757, F1: 0.6701
LM Predictions:  [1, 0, 0, 4, 4, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1826, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9500, F1: 0.9581
Epoch 44/70
Train Loss: 0.0330, Accuracy: 0.9937, Precision: 0.9828, Recall: 0.9856, F1: 0.9841
Validation Loss: 0.7729, Accuracy: 0.8722, Precision: 0.6975, Recall: 0.7256, F1: 0.7039
Testing Loss: 0.7818, Accuracy: 0.8910, Precision: 0.6952, Recall: 0.7125, F1: 0.7034
LM Predictions:  [3, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 3, 4, 0, 4, 4, 1, 5, 2, 3, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.6656, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7429, F1: 0.7788
Epoch 45/70
Train Loss: 0.0287, Accuracy: 0.9916, Precision: 0.9826, Recall: 0.9791, F1: 0.9807
Validation Loss: 0.8282, Accuracy: 0.8523, Precision: 0.6897, Recall: 0.6963, F1: 0.6799
Testing Loss: 0.7607, Accuracy: 0.8777, Precision: 0.6871, Recall: 0.6851, F1: 0.6779
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0139, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0355, Accuracy: 0.9930, Precision: 0.9838, Recall: 0.9837, F1: 0.9838
Validation Loss: 0.7217, Accuracy: 0.8636, Precision: 0.7068, Recall: 0.7087, F1: 0.6936
Testing Loss: 0.6828, Accuracy: 0.8803, Precision: 0.6816, Recall: 0.6785, F1: 0.6764
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0197, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0064, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9968, F1: 0.9982
Validation Loss: 0.8389, Accuracy: 0.8693, Precision: 0.6696, Recall: 0.6545, F1: 0.6528
Testing Loss: 0.8373, Accuracy: 0.8803, Precision: 0.6661, Recall: 0.6600, F1: 0.6443
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0034, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0230, Accuracy: 0.9948, Precision: 0.9955, Recall: 0.9915, F1: 0.9935
Validation Loss: 0.8312, Accuracy: 0.8523, Precision: 0.6615, Recall: 0.6792, F1: 0.6664
Testing Loss: 0.9204, Accuracy: 0.8644, Precision: 0.6543, Recall: 0.6962, F1: 0.6712
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0036, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0269, Accuracy: 0.9951, Precision: 0.9894, Recall: 0.9874, F1: 0.9884
Validation Loss: 0.7206, Accuracy: 0.8864, Precision: 0.7409, Recall: 0.7493, F1: 0.7309
Testing Loss: 0.8100, Accuracy: 0.8670, Precision: 0.6771, Recall: 0.6584, F1: 0.6668
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0037, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0388, Accuracy: 0.9899, Precision: 0.9650, Recall: 0.9672, F1: 0.9660
Validation Loss: 0.7816, Accuracy: 0.8750, Precision: 0.7172, Recall: 0.6892, F1: 0.6997
Testing Loss: 0.8203, Accuracy: 0.8697, Precision: 0.6741, Recall: 0.6802, F1: 0.6722
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 3, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0881, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 51/70
Train Loss: 0.0338, Accuracy: 0.9930, Precision: 0.9871, Recall: 0.9844, F1: 0.9857
Validation Loss: 0.7159, Accuracy: 0.8778, Precision: 0.7277, Recall: 0.6437, F1: 0.6649
Testing Loss: 0.8278, Accuracy: 0.8644, Precision: 0.6621, Recall: 0.6490, F1: 0.6455
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 4, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0848, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 52/70
Train Loss: 0.0247, Accuracy: 0.9955, Precision: 0.9883, Recall: 0.9893, F1: 0.9888
Validation Loss: 0.7269, Accuracy: 0.8778, Precision: 0.7260, Recall: 0.6620, F1: 0.6868
Testing Loss: 0.7428, Accuracy: 0.8750, Precision: 0.6672, Recall: 0.6585, F1: 0.6589
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0310, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0202, Accuracy: 0.9965, Precision: 0.9934, Recall: 0.9915, F1: 0.9924
Validation Loss: 0.9186, Accuracy: 0.8551, Precision: 0.6895, Recall: 0.6443, F1: 0.6626
Testing Loss: 0.8474, Accuracy: 0.8644, Precision: 0.6520, Recall: 0.6347, F1: 0.6361
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0131, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0250, Accuracy: 0.9941, Precision: 0.9840, Recall: 0.9811, F1: 0.9825
Validation Loss: 0.7586, Accuracy: 0.8778, Precision: 0.7477, Recall: 0.7213, F1: 0.7201
Testing Loss: 0.8735, Accuracy: 0.8537, Precision: 0.6747, Recall: 0.6297, F1: 0.6418
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 1, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0786, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 55/70
Train Loss: 0.0298, Accuracy: 0.9937, Precision: 0.9929, Recall: 0.9915, F1: 0.9922
Validation Loss: 0.6613, Accuracy: 0.8835, Precision: 0.7295, Recall: 0.7320, F1: 0.7188
Testing Loss: 0.8285, Accuracy: 0.8723, Precision: 0.6711, Recall: 0.6564, F1: 0.6631
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0065, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0267, Accuracy: 0.9941, Precision: 0.9903, Recall: 0.9917, F1: 0.9910
Validation Loss: 0.9701, Accuracy: 0.8608, Precision: 0.7451, Recall: 0.6667, F1: 0.6756
Testing Loss: 1.0455, Accuracy: 0.8378, Precision: 0.6679, Recall: 0.6096, F1: 0.6177
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0096, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0183, Accuracy: 0.9958, Precision: 0.9910, Recall: 0.9864, F1: 0.9886
Validation Loss: 0.6931, Accuracy: 0.8920, Precision: 0.7276, Recall: 0.6889, F1: 0.7058
Testing Loss: 0.8692, Accuracy: 0.8777, Precision: 0.6670, Recall: 0.6687, F1: 0.6656
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0683, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 58/70
Train Loss: 0.0077, Accuracy: 0.9986, Precision: 0.9969, Recall: 0.9965, F1: 0.9966
Validation Loss: 0.9112, Accuracy: 0.8665, Precision: 0.7063, Recall: 0.7265, F1: 0.7031
Testing Loss: 0.9737, Accuracy: 0.8564, Precision: 0.6558, Recall: 0.6445, F1: 0.6467
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0100, Accuracy: 0.9979, Precision: 0.9947, Recall: 0.9920, F1: 0.9933
Validation Loss: 0.8032, Accuracy: 0.8750, Precision: 0.7184, Recall: 0.7303, F1: 0.7116
Testing Loss: 0.8971, Accuracy: 0.8617, Precision: 0.6679, Recall: 0.6503, F1: 0.6562
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0061, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0054, Accuracy: 0.9986, Precision: 0.9968, Recall: 0.9981, F1: 0.9974
Validation Loss: 0.8681, Accuracy: 0.8807, Precision: 0.7562, Recall: 0.7133, F1: 0.7227
Testing Loss: 1.0085, Accuracy: 0.8723, Precision: 0.6663, Recall: 0.6572, F1: 0.6572
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0025, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0265, Accuracy: 0.9930, Precision: 0.9772, Recall: 0.9796, F1: 0.9784
Validation Loss: 0.8851, Accuracy: 0.8778, Precision: 0.7209, Recall: 0.6571, F1: 0.6796
Testing Loss: 0.9302, Accuracy: 0.8750, Precision: 0.6792, Recall: 0.6593, F1: 0.6662
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0077, Accuracy: 0.9986, Precision: 0.9966, Recall: 0.9981, F1: 0.9973
Validation Loss: 0.9020, Accuracy: 0.8807, Precision: 0.7222, Recall: 0.6869, F1: 0.7013
Testing Loss: 1.0874, Accuracy: 0.8564, Precision: 0.6654, Recall: 0.6502, F1: 0.6562
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0017, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0038, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.8805, Accuracy: 0.8807, Precision: 0.7207, Recall: 0.6929, F1: 0.7032
Testing Loss: 1.0398, Accuracy: 0.8750, Precision: 0.6706, Recall: 0.6724, F1: 0.6686
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 64/70
Train Loss: 0.0061, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9997, F1: 0.9997
Validation Loss: 0.8535, Accuracy: 0.8920, Precision: 0.7280, Recall: 0.7017, F1: 0.7135
Testing Loss: 1.0751, Accuracy: 0.8803, Precision: 0.6771, Recall: 0.6761, F1: 0.6751
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0033, Accuracy: 0.9997, Precision: 0.9998, Recall: 0.9998, F1: 0.9998
Validation Loss: 0.8649, Accuracy: 0.8892, Precision: 0.7264, Recall: 0.6919, F1: 0.7071
Testing Loss: 1.0868, Accuracy: 0.8803, Precision: 0.6771, Recall: 0.6761, F1: 0.6751
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0006, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0085, Accuracy: 0.9983, Precision: 0.9979, Recall: 0.9966, F1: 0.9973
Validation Loss: 0.9358, Accuracy: 0.8722, Precision: 0.7010, Recall: 0.6868, F1: 0.6918
Testing Loss: 1.0854, Accuracy: 0.8697, Precision: 0.6594, Recall: 0.6724, F1: 0.6643
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0275, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0692, Accuracy: 0.9843, Precision: 0.9849, Recall: 0.9865, F1: 0.9856
Validation Loss: 0.8256, Accuracy: 0.8494, Precision: 0.7147, Recall: 0.6809, F1: 0.6915
Testing Loss: 1.0120, Accuracy: 0.8271, Precision: 0.6356, Recall: 0.6462, F1: 0.6334
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0187, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0422, Accuracy: 0.9902, Precision: 0.9706, Recall: 0.9778, F1: 0.9741
Validation Loss: 0.7717, Accuracy: 0.8778, Precision: 0.7623, Recall: 0.7571, F1: 0.7510
Testing Loss: 1.0075, Accuracy: 0.8617, Precision: 0.6547, Recall: 0.6371, F1: 0.6433
LM Predictions:  [5, 0, 0, 4, 2, 4, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1373, Accuracy: 0.9286, Precision: 0.9417, Recall: 0.9200, F1: 0.9185
Epoch 69/70
Train Loss: 0.0254, Accuracy: 0.9958, Precision: 0.9932, Recall: 0.9895, F1: 0.9914
Validation Loss: 0.7699, Accuracy: 0.8750, Precision: 0.7240, Recall: 0.6540, F1: 0.6672
Testing Loss: 0.8390, Accuracy: 0.8697, Precision: 0.6555, Recall: 0.6576, F1: 0.6500
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0248, Accuracy: 0.9958, Precision: 0.9831, Recall: 0.9878, F1: 0.9854
Validation Loss: 0.8010, Accuracy: 0.8778, Precision: 0.7361, Recall: 0.6448, F1: 0.6716
Testing Loss: 0.9502, Accuracy: 0.8670, Precision: 0.6654, Recall: 0.6322, F1: 0.6425
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0401, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([514, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.8162, Accuracy: 0.7057, Precision: 0.5538, Recall: 0.4356, F1: 0.4512
Validation Loss: 0.4032, Accuracy: 0.8722, Precision: 0.6967, Recall: 0.6306, F1: 0.6324
Testing Loss: 0.4425, Accuracy: 0.8697, Precision: 0.6756, Recall: 0.6194, F1: 0.6261
LM Predictions:  [3, 3, 1, 3, 5, 3, 3, 5, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.5520, Accuracy: 0.0357, Precision: 0.0417, Recall: 0.0333, F1: 0.0370
Epoch 2/70
Train Loss: 0.3867, Accuracy: 0.8800, Precision: 0.7892, Recall: 0.6883, F1: 0.6894
Validation Loss: 0.4174, Accuracy: 0.8693, Precision: 0.6965, Recall: 0.6982, F1: 0.6819
Testing Loss: 0.4080, Accuracy: 0.8617, Precision: 0.6591, Recall: 0.6647, F1: 0.6563
LM Predictions:  [3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 5, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.3012, Accuracy: 0.0714, Precision: 0.2500, Recall: 0.0750, F1: 0.1143
Epoch 3/70
Train Loss: 0.2552, Accuracy: 0.9195, Precision: 0.8231, Recall: 0.7789, F1: 0.7844
Validation Loss: 0.4046, Accuracy: 0.8608, Precision: 0.7046, Recall: 0.6553, F1: 0.6701
Testing Loss: 0.4696, Accuracy: 0.8617, Precision: 0.6986, Recall: 0.6596, F1: 0.6693
LM Predictions:  [3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 5, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 2.6572, Accuracy: 0.1786, Precision: 0.2500, Recall: 0.2000, F1: 0.1667
Epoch 4/70
Train Loss: 0.1758, Accuracy: 0.9437, Precision: 0.8592, Recall: 0.8515, F1: 0.8522
Validation Loss: 0.4083, Accuracy: 0.8750, Precision: 0.7223, Recall: 0.6653, F1: 0.6735
Testing Loss: 0.3992, Accuracy: 0.8856, Precision: 0.7089, Recall: 0.6875, F1: 0.6961
LM Predictions:  [1, 2, 3, 3, 2, 3, 3, 2, 3, 3, 5, 2, 2, 3, 3, 3, 3, 5, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.9614, Accuracy: 0.2143, Precision: 0.3106, Recall: 0.2333, F1: 0.1921
Epoch 5/70
Train Loss: 0.1174, Accuracy: 0.9668, Precision: 0.9124, Recall: 0.9023, F1: 0.9048
Validation Loss: 0.4131, Accuracy: 0.8807, Precision: 0.7730, Recall: 0.7169, F1: 0.7369
Testing Loss: 0.4655, Accuracy: 0.8777, Precision: 0.7576, Recall: 0.6846, F1: 0.7103
LM Predictions:  [1, 2, 3, 4, 2, 3, 5, 2, 3, 3, 5, 4, 2, 3, 3, 0, 3, 0, 3, 3, 3, 2, 2, 1, 3, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.3829, Accuracy: 0.4286, Precision: 0.7063, Recall: 0.3952, F1: 0.4479
Epoch 6/70
Train Loss: 0.0887, Accuracy: 0.9762, Precision: 0.9519, Recall: 0.9379, F1: 0.9431
Validation Loss: 0.3802, Accuracy: 0.9034, Precision: 0.7589, Recall: 0.7859, F1: 0.7666
Testing Loss: 0.4401, Accuracy: 0.8989, Precision: 0.6982, Recall: 0.7315, F1: 0.7100
LM Predictions:  [1, 3, 3, 4, 2, 3, 0, 2, 3, 3, 5, 4, 3, 3, 3, 0, 3, 0, 3, 3, 3, 5, 2, 1, 3, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.2555, Accuracy: 0.5000, Precision: 0.8333, Recall: 0.4524, F1: 0.5610
Epoch 7/70
Train Loss: 0.0733, Accuracy: 0.9783, Precision: 0.9646, Recall: 0.9594, F1: 0.9610
Validation Loss: 0.4230, Accuracy: 0.9034, Precision: 0.7584, Recall: 0.7837, F1: 0.7611
Testing Loss: 0.5128, Accuracy: 0.8883, Precision: 0.6725, Recall: 0.7142, F1: 0.6871
LM Predictions:  [1, 0, 3, 4, 2, 3, 0, 1, 3, 3, 5, 4, 3, 0, 3, 0, 3, 0, 3, 3, 3, 3, 2, 1, 3, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 1.0588, Accuracy: 0.5000, Precision: 0.7778, Recall: 0.4250, F1: 0.5344
Epoch 8/70
Train Loss: 0.0817, Accuracy: 0.9773, Precision: 0.9470, Recall: 0.9468, F1: 0.9460
Validation Loss: 0.4858, Accuracy: 0.8750, Precision: 0.7391, Recall: 0.8030, F1: 0.7410
Testing Loss: 0.5418, Accuracy: 0.8670, Precision: 0.6804, Recall: 0.6829, F1: 0.6729
LM Predictions:  [1, 0, 3, 4, 2, 1, 0, 2, 0, 0, 5, 4, 2, 0, 5, 0, 4, 0, 4, 2, 3, 5, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.4938, Accuracy: 0.7857, Precision: 0.7540, Recall: 0.6714, F1: 0.6937
Epoch 9/70
Train Loss: 0.0364, Accuracy: 0.9902, Precision: 0.9661, Recall: 0.9702, F1: 0.9680
Validation Loss: 0.4724, Accuracy: 0.8977, Precision: 0.7654, Recall: 0.7584, F1: 0.7595
Testing Loss: 0.5725, Accuracy: 0.8856, Precision: 0.6639, Recall: 0.6572, F1: 0.6595
LM Predictions:  [1, 0, 3, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 3, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2995, Accuracy: 0.8929, Precision: 0.8333, Recall: 0.7619, F1: 0.7927
Epoch 10/70
Train Loss: 0.0312, Accuracy: 0.9906, Precision: 0.9750, Recall: 0.9744, F1: 0.9745
Validation Loss: 0.6045, Accuracy: 0.8835, Precision: 0.7356, Recall: 0.7497, F1: 0.7319
Testing Loss: 0.6766, Accuracy: 0.8537, Precision: 0.6433, Recall: 0.6416, F1: 0.6339
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 3, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1739, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 11/70
Train Loss: 0.0506, Accuracy: 0.9850, Precision: 0.9707, Recall: 0.9724, F1: 0.9715
Validation Loss: 0.5016, Accuracy: 0.8977, Precision: 0.7584, Recall: 0.7700, F1: 0.7630
Testing Loss: 0.5871, Accuracy: 0.8777, Precision: 0.6940, Recall: 0.7132, F1: 0.6984
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0966, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 12/70
Train Loss: 0.0220, Accuracy: 0.9948, Precision: 0.9884, Recall: 0.9830, F1: 0.9857
Validation Loss: 0.5915, Accuracy: 0.8778, Precision: 0.7008, Recall: 0.6653, F1: 0.6804
Testing Loss: 0.6038, Accuracy: 0.8750, Precision: 0.6919, Recall: 0.6919, F1: 0.6864
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0553, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 13/70
Train Loss: 0.0615, Accuracy: 0.9829, Precision: 0.9783, Recall: 0.9734, F1: 0.9757
Validation Loss: 0.5077, Accuracy: 0.8920, Precision: 0.7684, Recall: 0.6658, F1: 0.7024
Testing Loss: 0.6110, Accuracy: 0.8617, Precision: 0.6636, Recall: 0.6115, F1: 0.6178
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0595, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 14/70
Train Loss: 0.0409, Accuracy: 0.9878, Precision: 0.9701, Recall: 0.9655, F1: 0.9678
Validation Loss: 0.4864, Accuracy: 0.8835, Precision: 0.7243, Recall: 0.7721, F1: 0.7347
Testing Loss: 0.6108, Accuracy: 0.8617, Precision: 0.6564, Recall: 0.6793, F1: 0.6641
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0521, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 15/70
Train Loss: 0.0220, Accuracy: 0.9948, Precision: 0.9898, Recall: 0.9922, F1: 0.9910
Validation Loss: 0.5060, Accuracy: 0.9091, Precision: 0.7256, Recall: 0.7344, F1: 0.7284
Testing Loss: 0.6734, Accuracy: 0.8777, Precision: 0.6564, Recall: 0.6773, F1: 0.6632
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0148, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 16/70
Train Loss: 0.0043, Accuracy: 0.9990, Precision: 0.9965, Recall: 0.9992, F1: 0.9978
Validation Loss: 0.5650, Accuracy: 0.8835, Precision: 0.7259, Recall: 0.6752, F1: 0.6939
Testing Loss: 0.7746, Accuracy: 0.8484, Precision: 0.6317, Recall: 0.5935, F1: 0.6088
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0356, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 17/70
Train Loss: 0.0126, Accuracy: 0.9976, Precision: 0.9960, Recall: 0.9960, F1: 0.9960
Validation Loss: 0.6171, Accuracy: 0.8892, Precision: 0.7242, Recall: 0.7600, F1: 0.7316
Testing Loss: 0.7293, Accuracy: 0.8750, Precision: 0.6608, Recall: 0.6777, F1: 0.6657
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0103, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 18/70
Train Loss: 0.0094, Accuracy: 0.9979, Precision: 0.9978, Recall: 0.9962, F1: 0.9969
Validation Loss: 0.5906, Accuracy: 0.8835, Precision: 0.7220, Recall: 0.7429, F1: 0.7192
Testing Loss: 0.7834, Accuracy: 0.8511, Precision: 0.6567, Recall: 0.6498, F1: 0.6450
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0203, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0383, Accuracy: 0.9899, Precision: 0.9738, Recall: 0.9705, F1: 0.9722
Validation Loss: 0.6639, Accuracy: 0.8835, Precision: 0.7229, Recall: 0.7036, F1: 0.7000
Testing Loss: 0.7667, Accuracy: 0.8590, Precision: 0.6789, Recall: 0.6426, F1: 0.6564
LM Predictions:  [1, 0, 0, 4, 2, 3, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2115, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 20/70
Train Loss: 0.0376, Accuracy: 0.9881, Precision: 0.9638, Recall: 0.9724, F1: 0.9680
Validation Loss: 0.5829, Accuracy: 0.8864, Precision: 0.7076, Recall: 0.6868, F1: 0.6952
Testing Loss: 0.7299, Accuracy: 0.8617, Precision: 0.6454, Recall: 0.6506, F1: 0.6359
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0157, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 21/70
Train Loss: 0.0184, Accuracy: 0.9944, Precision: 0.9951, Recall: 0.9926, F1: 0.9938
Validation Loss: 0.6302, Accuracy: 0.8892, Precision: 0.7364, Recall: 0.7562, F1: 0.7331
Testing Loss: 0.7923, Accuracy: 0.8617, Precision: 0.6468, Recall: 0.6667, F1: 0.6520
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0070, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 22/70
Train Loss: 0.0064, Accuracy: 0.9983, Precision: 0.9965, Recall: 0.9926, F1: 0.9945
Validation Loss: 0.6403, Accuracy: 0.8920, Precision: 0.7520, Recall: 0.7453, F1: 0.7457
Testing Loss: 0.7604, Accuracy: 0.8777, Precision: 0.6556, Recall: 0.6683, F1: 0.6596
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0816, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 23/70
Train Loss: 0.0207, Accuracy: 0.9958, Precision: 0.9933, Recall: 0.9933, F1: 0.9933
Validation Loss: 0.6135, Accuracy: 0.8892, Precision: 0.7450, Recall: 0.7234, F1: 0.7245
Testing Loss: 0.7234, Accuracy: 0.8670, Precision: 0.6704, Recall: 0.6502, F1: 0.6556
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0064, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 24/70
Train Loss: 0.0130, Accuracy: 0.9965, Precision: 0.9967, Recall: 0.9973, F1: 0.9970
Validation Loss: 0.6234, Accuracy: 0.8750, Precision: 0.7323, Recall: 0.7501, F1: 0.7346
Testing Loss: 0.7069, Accuracy: 0.8723, Precision: 0.6904, Recall: 0.6809, F1: 0.6817
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0124, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 25/70
Train Loss: 0.0177, Accuracy: 0.9955, Precision: 0.9970, Recall: 0.9963, F1: 0.9967
Validation Loss: 0.5300, Accuracy: 0.9006, Precision: 0.7511, Recall: 0.7517, F1: 0.7407
Testing Loss: 0.6783, Accuracy: 0.8617, Precision: 0.7095, Recall: 0.6685, F1: 0.6769
LM Predictions:  [1, 0, 5, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0372, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 26/70
Train Loss: 0.0373, Accuracy: 0.9888, Precision: 0.9707, Recall: 0.9722, F1: 0.9715
Validation Loss: 0.6602, Accuracy: 0.8608, Precision: 0.7241, Recall: 0.6336, F1: 0.6636
Testing Loss: 0.8604, Accuracy: 0.8484, Precision: 0.6506, Recall: 0.6297, F1: 0.6298
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 3, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1818, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7762, F1: 0.8020
Epoch 27/70
Train Loss: 0.0393, Accuracy: 0.9888, Precision: 0.9711, Recall: 0.9712, F1: 0.9711
Validation Loss: 0.5338, Accuracy: 0.8977, Precision: 0.7131, Recall: 0.7221, F1: 0.7146
Testing Loss: 0.6929, Accuracy: 0.8644, Precision: 0.6501, Recall: 0.6585, F1: 0.6508
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0135, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0116, Accuracy: 0.9979, Precision: 0.9963, Recall: 0.9962, F1: 0.9963
Validation Loss: 0.5885, Accuracy: 0.8920, Precision: 0.7128, Recall: 0.6989, F1: 0.7052
Testing Loss: 0.6875, Accuracy: 0.8803, Precision: 0.6652, Recall: 0.6733, F1: 0.6671
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0032, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 29/70
Train Loss: 0.0314, Accuracy: 0.9909, Precision: 0.9813, Recall: 0.9832, F1: 0.9822
Validation Loss: 0.5187, Accuracy: 0.8835, Precision: 0.7722, Recall: 0.7774, F1: 0.7527
Testing Loss: 0.8165, Accuracy: 0.8484, Precision: 0.6939, Recall: 0.6299, F1: 0.6430
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0033, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0221, Accuracy: 0.9941, Precision: 0.9869, Recall: 0.9892, F1: 0.9880
Validation Loss: 0.6234, Accuracy: 0.8807, Precision: 0.7288, Recall: 0.6951, F1: 0.7088
Testing Loss: 0.7326, Accuracy: 0.8697, Precision: 0.6672, Recall: 0.6506, F1: 0.6577
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0417, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0182, Accuracy: 0.9941, Precision: 0.9915, Recall: 0.9925, F1: 0.9920
Validation Loss: 0.6496, Accuracy: 0.8949, Precision: 0.7379, Recall: 0.7054, F1: 0.7202
Testing Loss: 0.7516, Accuracy: 0.8617, Precision: 0.6406, Recall: 0.6334, F1: 0.6364
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0029, Accuracy: 0.9993, Precision: 0.9982, Recall: 0.9982, F1: 0.9982
Validation Loss: 0.6817, Accuracy: 0.8835, Precision: 0.7242, Recall: 0.6920, F1: 0.7065
Testing Loss: 0.7853, Accuracy: 0.8750, Precision: 0.6663, Recall: 0.6568, F1: 0.6604
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0016, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 33/70
Train Loss: 0.0085, Accuracy: 0.9965, Precision: 0.9923, Recall: 0.9913, F1: 0.9918
Validation Loss: 0.6726, Accuracy: 0.8835, Precision: 0.7597, Recall: 0.8007, F1: 0.7480
Testing Loss: 0.8145, Accuracy: 0.8590, Precision: 0.6752, Recall: 0.6788, F1: 0.6715
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 0, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0781, Accuracy: 0.9643, Precision: 0.9750, Recall: 0.9600, F1: 0.9644
Epoch 34/70
Train Loss: 0.0074, Accuracy: 0.9983, Precision: 0.9936, Recall: 0.9910, F1: 0.9922
Validation Loss: 0.7228, Accuracy: 0.8949, Precision: 0.7484, Recall: 0.6856, F1: 0.7105
Testing Loss: 0.8546, Accuracy: 0.8697, Precision: 0.6759, Recall: 0.6538, F1: 0.6587
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0147, Accuracy: 0.9958, Precision: 0.9821, Recall: 0.9891, F1: 0.9855
Validation Loss: 0.7517, Accuracy: 0.8807, Precision: 0.6925, Recall: 0.7028, F1: 0.6964
Testing Loss: 0.7174, Accuracy: 0.8777, Precision: 0.6815, Recall: 0.6921, F1: 0.6841
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 3, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.1264, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8000, F1: 0.8148
Epoch 36/70
Train Loss: 0.0065, Accuracy: 0.9986, Precision: 0.9977, Recall: 0.9953, F1: 0.9965
Validation Loss: 0.6955, Accuracy: 0.8835, Precision: 0.7475, Recall: 0.7659, F1: 0.7546
Testing Loss: 0.8678, Accuracy: 0.8537, Precision: 0.6283, Recall: 0.6420, F1: 0.6234
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0093, Accuracy: 0.9976, Precision: 0.9933, Recall: 0.9958, F1: 0.9945
Validation Loss: 0.7415, Accuracy: 0.8864, Precision: 0.7206, Recall: 0.7159, F1: 0.7178
Testing Loss: 0.8419, Accuracy: 0.8644, Precision: 0.6480, Recall: 0.6519, F1: 0.6495
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0020, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 38/70
Train Loss: 0.0027, Accuracy: 0.9993, Precision: 0.9955, Recall: 0.9955, F1: 0.9955
Validation Loss: 0.7451, Accuracy: 0.8892, Precision: 0.7213, Recall: 0.6937, F1: 0.7064
Testing Loss: 0.8313, Accuracy: 0.8723, Precision: 0.6806, Recall: 0.6560, F1: 0.6615
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0010, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 39/70
Train Loss: 0.0004, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.7182, Accuracy: 0.8920, Precision: 0.7190, Recall: 0.7047, F1: 0.7114
Testing Loss: 0.8313, Accuracy: 0.8830, Precision: 0.6889, Recall: 0.6670, F1: 0.6722
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0284, Accuracy: 0.9930, Precision: 0.9920, Recall: 0.9910, F1: 0.9915
Validation Loss: 0.5250, Accuracy: 0.8920, Precision: 0.7091, Recall: 0.7057, F1: 0.7054
Testing Loss: 0.7285, Accuracy: 0.8644, Precision: 0.6573, Recall: 0.6588, F1: 0.6566
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 3, 5, 0, 4, 3, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.3579, Accuracy: 0.9286, Precision: 0.8333, Recall: 0.7857, F1: 0.8056
Epoch 41/70
Train Loss: 0.0228, Accuracy: 0.9951, Precision: 0.9899, Recall: 0.9844, F1: 0.9869
Validation Loss: 0.5248, Accuracy: 0.9034, Precision: 0.7119, Recall: 0.7288, F1: 0.7195
Testing Loss: 0.7350, Accuracy: 0.8670, Precision: 0.6574, Recall: 0.6715, F1: 0.6585
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 3, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0543, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 42/70
Train Loss: 0.0220, Accuracy: 0.9937, Precision: 0.9760, Recall: 0.9788, F1: 0.9773
Validation Loss: 0.6243, Accuracy: 0.8807, Precision: 0.7621, Recall: 0.7564, F1: 0.7306
Testing Loss: 0.7424, Accuracy: 0.8670, Precision: 0.6973, Recall: 0.6472, F1: 0.6657
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0415, Accuracy: 0.9871, Precision: 0.9698, Recall: 0.9697, F1: 0.9697
Validation Loss: 0.5452, Accuracy: 0.8892, Precision: 0.7335, Recall: 0.7462, F1: 0.7346
Testing Loss: 0.7559, Accuracy: 0.8590, Precision: 0.6760, Recall: 0.6402, F1: 0.6356
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0120, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0138, Accuracy: 0.9958, Precision: 0.9938, Recall: 0.9965, F1: 0.9951
Validation Loss: 0.6268, Accuracy: 0.8920, Precision: 0.7445, Recall: 0.7586, F1: 0.7479
Testing Loss: 0.8015, Accuracy: 0.8590, Precision: 0.6757, Recall: 0.6669, F1: 0.6610
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0035, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0069, Accuracy: 0.9979, Precision: 0.9946, Recall: 0.9933, F1: 0.9939
Validation Loss: 0.6411, Accuracy: 0.8835, Precision: 0.7561, Recall: 0.7924, F1: 0.7638
Testing Loss: 0.7473, Accuracy: 0.8670, Precision: 0.6529, Recall: 0.6399, F1: 0.6389
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0024, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0146, Accuracy: 0.9944, Precision: 0.9939, Recall: 0.9921, F1: 0.9930
Validation Loss: 0.6044, Accuracy: 0.8778, Precision: 0.7494, Recall: 0.7732, F1: 0.7297
Testing Loss: 0.8912, Accuracy: 0.8378, Precision: 0.6315, Recall: 0.5788, F1: 0.5914
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0046, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 47/70
Train Loss: 0.0248, Accuracy: 0.9916, Precision: 0.9751, Recall: 0.9740, F1: 0.9745
Validation Loss: 0.5448, Accuracy: 0.8920, Precision: 0.7343, Recall: 0.7679, F1: 0.7441
Testing Loss: 0.7915, Accuracy: 0.8670, Precision: 0.6323, Recall: 0.6317, F1: 0.6309
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 3, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0939, Accuracy: 0.9643, Precision: 0.8333, Recall: 0.8095, F1: 0.8205
Epoch 48/70
Train Loss: 0.0099, Accuracy: 0.9965, Precision: 0.9910, Recall: 0.9925, F1: 0.9917
Validation Loss: 0.6346, Accuracy: 0.8920, Precision: 0.7551, Recall: 0.7578, F1: 0.7488
Testing Loss: 0.7514, Accuracy: 0.8803, Precision: 0.6702, Recall: 0.6885, F1: 0.6765
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0018, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0011, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Validation Loss: 0.6912, Accuracy: 0.8949, Precision: 0.7680, Recall: 0.7524, F1: 0.7555
Testing Loss: 0.7374, Accuracy: 0.8963, Precision: 0.6917, Recall: 0.6925, F1: 0.6917
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0008, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 50/70
Train Loss: 0.0102, Accuracy: 0.9965, Precision: 0.9970, Recall: 0.9966, F1: 0.9968
Validation Loss: 0.5930, Accuracy: 0.8835, Precision: 0.6977, Recall: 0.7163, F1: 0.7055
Testing Loss: 0.7646, Accuracy: 0.8803, Precision: 0.6631, Recall: 0.6884, F1: 0.6744
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0179, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0023, Accuracy: 0.9993, Precision: 0.9984, Recall: 0.9968, F1: 0.9976
Validation Loss: 0.6234, Accuracy: 0.8920, Precision: 0.7492, Recall: 0.7708, F1: 0.7529
Testing Loss: 0.8619, Accuracy: 0.8617, Precision: 0.6493, Recall: 0.6609, F1: 0.6532
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 52/70
Train Loss: 0.0031, Accuracy: 0.9983, Precision: 0.9950, Recall: 0.9978, F1: 0.9964
Validation Loss: 0.6867, Accuracy: 0.8892, Precision: 0.7630, Recall: 0.7649, F1: 0.7637
Testing Loss: 0.8350, Accuracy: 0.8670, Precision: 0.6422, Recall: 0.6682, F1: 0.6530
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0007, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0095, Accuracy: 0.9976, Precision: 0.9956, Recall: 0.9928, F1: 0.9942
Validation Loss: 0.7578, Accuracy: 0.8722, Precision: 0.7128, Recall: 0.6585, F1: 0.6747
Testing Loss: 0.9610, Accuracy: 0.8404, Precision: 0.6298, Recall: 0.6047, F1: 0.6043
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0041, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 54/70
Train Loss: 0.0215, Accuracy: 0.9948, Precision: 0.9906, Recall: 0.9872, F1: 0.9889
Validation Loss: 0.8241, Accuracy: 0.8466, Precision: 0.7531, Recall: 0.7515, F1: 0.7306
Testing Loss: 1.0444, Accuracy: 0.8324, Precision: 0.6543, Recall: 0.6027, F1: 0.6126
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 5, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0428, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9500, F1: 0.9532
Epoch 55/70
Train Loss: 0.0084, Accuracy: 0.9976, Precision: 0.9958, Recall: 0.9924, F1: 0.9941
Validation Loss: 0.8113, Accuracy: 0.8636, Precision: 0.7364, Recall: 0.7742, F1: 0.7338
Testing Loss: 0.8806, Accuracy: 0.8697, Precision: 0.6649, Recall: 0.6465, F1: 0.6540
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0190, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0020, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9998, F1: 0.9992
Validation Loss: 0.7926, Accuracy: 0.8778, Precision: 0.7660, Recall: 0.7848, F1: 0.7620
Testing Loss: 0.9886, Accuracy: 0.8484, Precision: 0.6453, Recall: 0.6161, F1: 0.6204
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0009, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0056, Accuracy: 0.9990, Precision: 0.9994, Recall: 0.9981, F1: 0.9987
Validation Loss: 0.8820, Accuracy: 0.8665, Precision: 0.7000, Recall: 0.7763, F1: 0.7276
Testing Loss: 0.9552, Accuracy: 0.8564, Precision: 0.6521, Recall: 0.6855, F1: 0.6639
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0117, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0107, Accuracy: 0.9976, Precision: 0.9888, Recall: 0.9915, F1: 0.9902
Validation Loss: 0.7393, Accuracy: 0.8807, Precision: 0.7242, Recall: 0.7716, F1: 0.7339
Testing Loss: 0.8994, Accuracy: 0.8457, Precision: 0.6142, Recall: 0.6395, F1: 0.6241
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0201, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0177, Accuracy: 0.9951, Precision: 0.9808, Recall: 0.9883, F1: 0.9845
Validation Loss: 0.7329, Accuracy: 0.8864, Precision: 0.7853, Recall: 0.7244, F1: 0.7456
Testing Loss: 0.9056, Accuracy: 0.8484, Precision: 0.6374, Recall: 0.5955, F1: 0.6066
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0158, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0082, Accuracy: 0.9986, Precision: 0.9964, Recall: 0.9990, F1: 0.9977
Validation Loss: 0.8624, Accuracy: 0.8580, Precision: 0.7467, Recall: 0.7464, F1: 0.7406
Testing Loss: 1.0238, Accuracy: 0.8271, Precision: 0.6026, Recall: 0.6084, F1: 0.6001
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0055, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0112, Accuracy: 0.9976, Precision: 0.9980, Recall: 0.9974, F1: 0.9977
Validation Loss: 0.7453, Accuracy: 0.8892, Precision: 0.7620, Recall: 0.7311, F1: 0.7374
Testing Loss: 0.9635, Accuracy: 0.8511, Precision: 0.6213, Recall: 0.6075, F1: 0.6044
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 62/70
Train Loss: 0.0269, Accuracy: 0.9927, Precision: 0.9871, Recall: 0.9866, F1: 0.9869
Validation Loss: 0.6675, Accuracy: 0.9062, Precision: 0.7828, Recall: 0.7765, F1: 0.7753
Testing Loss: 0.7275, Accuracy: 0.8777, Precision: 0.6741, Recall: 0.6490, F1: 0.6587
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0022, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 63/70
Train Loss: 0.0082, Accuracy: 0.9972, Precision: 0.9946, Recall: 0.9947, F1: 0.9946
Validation Loss: 0.6593, Accuracy: 0.8920, Precision: 0.7686, Recall: 0.7598, F1: 0.7620
Testing Loss: 0.8575, Accuracy: 0.8750, Precision: 0.6671, Recall: 0.6633, F1: 0.6629
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 5, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.2247, Accuracy: 0.9643, Precision: 0.9667, Recall: 0.9714, F1: 0.9664
Epoch 64/70
Train Loss: 0.0337, Accuracy: 0.9930, Precision: 0.9870, Recall: 0.9844, F1: 0.9857
Validation Loss: 0.5402, Accuracy: 0.8892, Precision: 0.7610, Recall: 0.7391, F1: 0.7399
Testing Loss: 0.6774, Accuracy: 0.8644, Precision: 0.6437, Recall: 0.6321, F1: 0.6268
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0063, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0021, Accuracy: 0.9993, Precision: 0.9997, Recall: 0.9993, F1: 0.9995
Validation Loss: 0.6825, Accuracy: 0.8949, Precision: 0.7429, Recall: 0.7623, F1: 0.7459
Testing Loss: 0.7683, Accuracy: 0.8830, Precision: 0.6810, Recall: 0.6736, F1: 0.6713
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0019, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 66/70
Train Loss: 0.0106, Accuracy: 0.9965, Precision: 0.9857, Recall: 0.9830, F1: 0.9843
Validation Loss: 0.6591, Accuracy: 0.8892, Precision: 0.7554, Recall: 0.7631, F1: 0.7527
Testing Loss: 0.7579, Accuracy: 0.8830, Precision: 0.7115, Recall: 0.6887, F1: 0.6946
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0064, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 67/70
Train Loss: 0.0010, Accuracy: 0.9993, Precision: 0.9983, Recall: 0.9983, F1: 0.9983
Validation Loss: 0.7856, Accuracy: 0.8864, Precision: 0.7175, Recall: 0.7122, F1: 0.7107
Testing Loss: 0.8158, Accuracy: 0.8723, Precision: 0.6387, Recall: 0.6646, F1: 0.6449
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0026, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0071, Accuracy: 0.9976, Precision: 0.9950, Recall: 0.9962, F1: 0.9956
Validation Loss: 0.7481, Accuracy: 0.8722, Precision: 0.7099, Recall: 0.6899, F1: 0.6951
Testing Loss: 0.9008, Accuracy: 0.8617, Precision: 0.6958, Recall: 0.6567, F1: 0.6575
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0013, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0022, Accuracy: 0.9997, Precision: 0.9986, Recall: 0.9986, F1: 0.9986
Validation Loss: 0.7907, Accuracy: 0.8807, Precision: 0.7270, Recall: 0.6769, F1: 0.6980
Testing Loss: 0.8512, Accuracy: 0.8750, Precision: 0.7069, Recall: 0.6583, F1: 0.6736
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0017, Accuracy: 0.9997, Precision: 0.9970, Recall: 0.9986, F1: 0.9978
Validation Loss: 0.8356, Accuracy: 0.8750, Precision: 0.7022, Recall: 0.6923, F1: 0.6944
Testing Loss: 0.9000, Accuracy: 0.8697, Precision: 0.6312, Recall: 0.6362, F1: 0.6314
LM Predictions:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Labels:  [1, 0, 0, 4, 2, 1, 0, 2, 0, 5, 5, 4, 4, 0, 5, 0, 4, 0, 4, 4, 1, 5, 2, 1, 1, 5, 4, 2]
LM Loss: 0.0044, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
---------------------------------------------------------------------------



