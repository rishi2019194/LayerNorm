---------------------------------------------------------------------------
Results for seed:  89
Model: andreasmadsen/efficient_mlm_m0.40, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 3
Label counts for Train:
  Label 4: 966
  Label 2: 1099
  Label 5: 486
  Label 1: 115
  Label 3: 144
  Label 0: 48
Label counts for Validation:
  Label 4: 117
  Label 5: 60
  Label 0: 3
  Label 3: 17
  Label 1: 22
  Label 2: 133
Label counts for Test:
  Label 4: 133
  Label 2: 136
  Label 0: 6
  Label 1: 14
  Label 3: 29
  Label 5: 58
28
Actual labels:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]
Label counts for Train:
  Label 4: 974
  Label 2: 1105
  Label 5: 490
  Label 1: 117
  Label 0: 56
  Label 3: 116
Layer: backbone.roberta_prelayernorm.embeddings.word_embeddings.weight, Size: torch.Size([50265, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.position_embeddings.weight, Size: torch.Size([514, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.0.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.1.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.2.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.3.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.4.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.5.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.6.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.7.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.8.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.9.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.10.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.11.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.12.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.13.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.14.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.15.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.16.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.17.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.18.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.19.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.20.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.21.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.22.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.query.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.key.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.self.value.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.attention.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.weight, Size: torch.Size([4096, 1024]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.intermediate.dense.bias, Size: torch.Size([4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.weight, Size: torch.Size([1024, 4096]), req grad: True
Layer: backbone.roberta_prelayernorm.encoder.layer.23.output.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([1024, 1024]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([1024]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 1024]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.4228, Accuracy: 0.3639, Precision: 0.1827, Recall: 0.1641, F1: 0.1371
Validation Loss: 1.4107, Accuracy: 0.3835, Precision: 0.1357, Recall: 0.1861, F1: 0.1412
Testing Loss: 1.4315, Accuracy: 0.4096, Precision: 0.1482, Recall: 0.1918, F1: 0.1510
LM Predictions:  [4, 2, 4, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 4, 2, 4, 4, 4, 2, 2, 4, 2, 4, 4, 2, 4, 2, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1933, Accuracy: 0.2143, Precision: 0.0865, Recall: 0.1667, F1: 0.1126
Epoch 2/70
Train Loss: 1.3794, Accuracy: 0.3835, Precision: 0.1254, Recall: 0.1717, F1: 0.1411
Validation Loss: 1.3826, Accuracy: 0.3807, Precision: 0.2298, Recall: 0.1681, F1: 0.0944
Testing Loss: 1.4075, Accuracy: 0.3697, Precision: 0.1940, Recall: 0.1705, F1: 0.0984
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2648, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 3/70
Train Loss: 1.3663, Accuracy: 0.4136, Precision: 0.1942, Recall: 0.1882, F1: 0.1618
Validation Loss: 1.3580, Accuracy: 0.4489, Precision: 0.1624, Recall: 0.2055, F1: 0.1684
Testing Loss: 1.3895, Accuracy: 0.4734, Precision: 0.1674, Recall: 0.2197, F1: 0.1793
LM Predictions:  [2, 2, 2, 4, 2, 2, 2, 2, 4, 2, 2, 4, 2, 2, 2, 4, 2, 4, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2575, Accuracy: 0.1786, Precision: 0.0455, Recall: 0.1667, F1: 0.0714
Epoch 4/70
Train Loss: 1.3557, Accuracy: 0.4279, Precision: 0.2372, Recall: 0.1938, F1: 0.1639
Validation Loss: 1.2979, Accuracy: 0.5114, Precision: 0.1938, Recall: 0.2352, F1: 0.1970
Testing Loss: 1.3330, Accuracy: 0.4947, Precision: 0.1793, Recall: 0.2297, F1: 0.1896
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 4, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1941, Accuracy: 0.1786, Precision: 0.0400, Recall: 0.1667, F1: 0.0645
Epoch 5/70
Train Loss: 1.3284, Accuracy: 0.4699, Precision: 0.2353, Recall: 0.2145, F1: 0.1854
Validation Loss: 1.2233, Accuracy: 0.5369, Precision: 0.2981, Recall: 0.2643, F1: 0.2419
Testing Loss: 1.2617, Accuracy: 0.5293, Precision: 0.2559, Recall: 0.2598, F1: 0.2383
LM Predictions:  [2, 2, 4, 5, 2, 2, 2, 2, 4, 2, 2, 5, 2, 4, 2, 4, 2, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2160, Accuracy: 0.3214, Precision: 0.3167, Recall: 0.3167, F1: 0.2674
Epoch 6/70
Train Loss: 1.2574, Accuracy: 0.5276, Precision: 0.2666, Recall: 0.2439, F1: 0.2151
Validation Loss: 1.2965, Accuracy: 0.5312, Precision: 0.2283, Recall: 0.2549, F1: 0.2167
Testing Loss: 1.2854, Accuracy: 0.5798, Precision: 0.2805, Recall: 0.2770, F1: 0.2442
LM Predictions:  [4, 4, 4, 2, 2, 2, 2, 2, 4, 2, 2, 5, 2, 4, 2, 4, 5, 4, 4, 2, 2, 2, 4, 2, 2, 4, 2, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2041, Accuracy: 0.3571, Precision: 0.2442, Recall: 0.3083, F1: 0.2481
Epoch 7/70
Train Loss: 1.2060, Accuracy: 0.5644, Precision: 0.2633, Recall: 0.2657, F1: 0.2430
Validation Loss: 1.1449, Accuracy: 0.5966, Precision: 0.2786, Recall: 0.2874, F1: 0.2611
Testing Loss: 1.0795, Accuracy: 0.6330, Precision: 0.3086, Recall: 0.3093, F1: 0.2846
LM Predictions:  [4, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 4, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2404, Accuracy: 0.2857, Precision: 0.2500, Recall: 0.2500, F1: 0.1600
Epoch 8/70
Train Loss: 1.1117, Accuracy: 0.6046, Precision: 0.2796, Recall: 0.2941, F1: 0.2775
Validation Loss: 1.0896, Accuracy: 0.5852, Precision: 0.2619, Recall: 0.2915, F1: 0.2685
Testing Loss: 1.0997, Accuracy: 0.6223, Precision: 0.2774, Recall: 0.3146, F1: 0.2920
LM Predictions:  [4, 2, 2, 4, 4, 4, 2, 4, 5, 4, 5, 2, 2, 4, 2, 4, 5, 4, 4, 2, 2, 2, 2, 2, 4, 2, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1470, Accuracy: 0.3214, Precision: 0.1833, Recall: 0.2750, F1: 0.2167
Epoch 9/70
Train Loss: 1.0698, Accuracy: 0.6137, Precision: 0.2865, Recall: 0.3069, F1: 0.2923
Validation Loss: 1.0378, Accuracy: 0.6165, Precision: 0.2866, Recall: 0.3185, F1: 0.3004
Testing Loss: 0.9835, Accuracy: 0.6569, Precision: 0.3065, Recall: 0.3469, F1: 0.3245
LM Predictions:  [4, 2, 2, 5, 2, 5, 2, 2, 5, 2, 5, 2, 2, 4, 2, 2, 5, 4, 5, 2, 2, 2, 5, 2, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4620, Accuracy: 0.3214, Precision: 0.2478, Recall: 0.3083, F1: 0.2377
Epoch 10/70
Train Loss: 1.0462, Accuracy: 0.6316, Precision: 0.3002, Recall: 0.3183, F1: 0.3048
Validation Loss: 1.0627, Accuracy: 0.6136, Precision: 0.2792, Recall: 0.3086, F1: 0.2875
Testing Loss: 0.9858, Accuracy: 0.6755, Precision: 0.3164, Recall: 0.3442, F1: 0.3224
LM Predictions:  [4, 2, 2, 2, 2, 2, 2, 5, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 4, 2, 2, 5, 5, 4]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2865, Accuracy: 0.2857, Precision: 0.1857, Recall: 0.2500, F1: 0.2030
Epoch 11/70
Train Loss: 1.0234, Accuracy: 0.6379, Precision: 0.3019, Recall: 0.3238, F1: 0.3097
Validation Loss: 1.0267, Accuracy: 0.6364, Precision: 0.3016, Recall: 0.3324, F1: 0.3115
Testing Loss: 1.0107, Accuracy: 0.6383, Precision: 0.3047, Recall: 0.3370, F1: 0.3160
LM Predictions:  [5, 2, 2, 5, 4, 2, 2, 4, 5, 4, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 4, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4024, Accuracy: 0.2500, Precision: 0.1530, Recall: 0.2417, F1: 0.1843
Epoch 12/70
Train Loss: 1.0036, Accuracy: 0.6470, Precision: 0.3067, Recall: 0.3286, F1: 0.3143
Validation Loss: 1.0391, Accuracy: 0.6222, Precision: 0.2890, Recall: 0.3166, F1: 0.2977
Testing Loss: 0.9647, Accuracy: 0.6782, Precision: 0.3214, Recall: 0.3569, F1: 0.3358
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4314, Accuracy: 0.2500, Precision: 0.1756, Recall: 0.2333, F1: 0.1726
Epoch 13/70
Train Loss: 0.9947, Accuracy: 0.6473, Precision: 0.3061, Recall: 0.3308, F1: 0.3161
Validation Loss: 1.0265, Accuracy: 0.6335, Precision: 0.2983, Recall: 0.3250, F1: 0.3061
Testing Loss: 0.9860, Accuracy: 0.6676, Precision: 0.3160, Recall: 0.3470, F1: 0.3266
LM Predictions:  [5, 2, 2, 2, 2, 2, 2, 4, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5850, Accuracy: 0.2143, Precision: 0.1450, Recall: 0.2000, F1: 0.1520
Epoch 14/70
Train Loss: 0.9556, Accuracy: 0.6648, Precision: 0.3157, Recall: 0.3416, F1: 0.3264
Validation Loss: 1.0203, Accuracy: 0.6193, Precision: 0.2929, Recall: 0.3171, F1: 0.2997
Testing Loss: 0.9301, Accuracy: 0.6995, Precision: 0.3436, Recall: 0.3699, F1: 0.3511
LM Predictions:  [5, 2, 2, 2, 2, 4, 2, 4, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 2, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3380, Accuracy: 0.2143, Precision: 0.1286, Recall: 0.2000, F1: 0.1497
Epoch 15/70
Train Loss: 0.9564, Accuracy: 0.6617, Precision: 0.3136, Recall: 0.3415, F1: 0.3259
Validation Loss: 1.0815, Accuracy: 0.6222, Precision: 0.3013, Recall: 0.3141, F1: 0.2981
Testing Loss: 0.9629, Accuracy: 0.6809, Precision: 0.3384, Recall: 0.3497, F1: 0.3323
LM Predictions:  [2, 2, 2, 2, 2, 4, 2, 2, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 2, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4961, Accuracy: 0.2857, Precision: 0.1722, Recall: 0.2667, F1: 0.1885
Epoch 16/70
Train Loss: 0.9554, Accuracy: 0.6540, Precision: 0.3098, Recall: 0.3340, F1: 0.3193
Validation Loss: 1.0132, Accuracy: 0.6136, Precision: 0.2911, Recall: 0.3137, F1: 0.2972
Testing Loss: 0.9582, Accuracy: 0.6862, Precision: 0.3351, Recall: 0.3587, F1: 0.3401
LM Predictions:  [5, 2, 2, 2, 2, 4, 2, 2, 5, 2, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3897, Accuracy: 0.2857, Precision: 0.1816, Recall: 0.2833, F1: 0.2031
Epoch 17/70
Train Loss: 0.9406, Accuracy: 0.6620, Precision: 0.3175, Recall: 0.3446, F1: 0.3294
Validation Loss: 0.9752, Accuracy: 0.6506, Precision: 0.3123, Recall: 0.3387, F1: 0.3215
Testing Loss: 0.9104, Accuracy: 0.6862, Precision: 0.4879, Recall: 0.3633, F1: 0.3472
LM Predictions:  [5, 2, 4, 4, 4, 5, 2, 2, 5, 2, 5, 2, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3143, Accuracy: 0.2143, Precision: 0.1336, Recall: 0.1917, F1: 0.1528
Epoch 18/70
Train Loss: 0.9226, Accuracy: 0.6690, Precision: 0.4295, Recall: 0.3497, F1: 0.3367
Validation Loss: 0.9765, Accuracy: 0.6392, Precision: 0.3036, Recall: 0.3336, F1: 0.3161
Testing Loss: 0.9245, Accuracy: 0.6835, Precision: 0.3170, Recall: 0.3577, F1: 0.3348
LM Predictions:  [5, 2, 2, 4, 2, 2, 2, 2, 5, 2, 3, 2, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2541, Accuracy: 0.2143, Precision: 0.1151, Recall: 0.1667, F1: 0.1279
Epoch 19/70
Train Loss: 0.9271, Accuracy: 0.6634, Precision: 0.3714, Recall: 0.3436, F1: 0.3299
Validation Loss: 0.9859, Accuracy: 0.6449, Precision: 0.3124, Recall: 0.3233, F1: 0.3055
Testing Loss: 0.9294, Accuracy: 0.6835, Precision: 0.5013, Recall: 0.3584, F1: 0.3509
LM Predictions:  [4, 2, 2, 4, 2, 2, 2, 2, 5, 2, 4, 2, 2, 4, 2, 4, 5, 4, 4, 5, 2, 2, 2, 2, 2, 2, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2608, Accuracy: 0.3214, Precision: 0.1945, Recall: 0.2917, F1: 0.2170
Epoch 20/70
Train Loss: 0.8786, Accuracy: 0.6819, Precision: 0.4103, Recall: 0.3615, F1: 0.3507
Validation Loss: 0.9494, Accuracy: 0.6392, Precision: 0.3005, Recall: 0.3416, F1: 0.3197
Testing Loss: 0.9017, Accuracy: 0.6702, Precision: 0.3145, Recall: 0.3647, F1: 0.3365
LM Predictions:  [4, 2, 2, 4, 2, 2, 2, 2, 5, 4, 4, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2203, Accuracy: 0.2857, Precision: 0.1712, Recall: 0.2750, F1: 0.2078
Epoch 21/70
Train Loss: 0.8936, Accuracy: 0.6809, Precision: 0.3497, Recall: 0.3572, F1: 0.3423
Validation Loss: 0.9898, Accuracy: 0.6278, Precision: 0.3068, Recall: 0.3210, F1: 0.3067
Testing Loss: 0.9347, Accuracy: 0.6702, Precision: 0.3279, Recall: 0.3496, F1: 0.3312
LM Predictions:  [4, 2, 2, 4, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 5, 2, 2, 5, 2, 2, 5, 5, 2]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2137, Accuracy: 0.2500, Precision: 0.1833, Recall: 0.2417, F1: 0.1569
Epoch 22/70
Train Loss: 0.9031, Accuracy: 0.6693, Precision: 0.5299, Recall: 0.3521, F1: 0.3427
Validation Loss: 0.9236, Accuracy: 0.6562, Precision: 0.4726, Recall: 0.3495, F1: 0.3369
Testing Loss: 0.8893, Accuracy: 0.6995, Precision: 0.3254, Recall: 0.3733, F1: 0.3477
LM Predictions:  [4, 2, 2, 4, 4, 2, 2, 4, 5, 4, 4, 5, 2, 4, 2, 4, 5, 2, 5, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2474, Accuracy: 0.2500, Precision: 0.1511, Recall: 0.2417, F1: 0.1821
Epoch 23/70
Train Loss: 0.8736, Accuracy: 0.6826, Precision: 0.4142, Recall: 0.3696, F1: 0.3643
Validation Loss: 0.9142, Accuracy: 0.6676, Precision: 0.4816, Recall: 0.3606, F1: 0.3471
Testing Loss: 0.9106, Accuracy: 0.6915, Precision: 0.4908, Recall: 0.3742, F1: 0.3557
LM Predictions:  [4, 2, 2, 4, 4, 2, 2, 4, 5, 4, 5, 5, 2, 4, 2, 4, 5, 2, 5, 5, 2, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2665, Accuracy: 0.2857, Precision: 0.1803, Recall: 0.2750, F1: 0.2077
Epoch 24/70
Train Loss: 0.8658, Accuracy: 0.6809, Precision: 0.4826, Recall: 0.3673, F1: 0.3613
Validation Loss: 0.9182, Accuracy: 0.6705, Precision: 0.6533, Recall: 0.3564, F1: 0.3554
Testing Loss: 0.9063, Accuracy: 0.6941, Precision: 0.5069, Recall: 0.3744, F1: 0.3720
LM Predictions:  [4, 2, 2, 4, 4, 2, 2, 2, 5, 4, 5, 2, 2, 4, 2, 4, 5, 4, 5, 5, 2, 2, 2, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4014, Accuracy: 0.3214, Precision: 0.1876, Recall: 0.2917, F1: 0.2186
Epoch 25/70
Train Loss: 0.8615, Accuracy: 0.6889, Precision: 0.5271, Recall: 0.3762, F1: 0.3755
Validation Loss: 0.9034, Accuracy: 0.6534, Precision: 0.4752, Recall: 0.3484, F1: 0.3375
Testing Loss: 0.8553, Accuracy: 0.6888, Precision: 0.3583, Recall: 0.3714, F1: 0.3556
LM Predictions:  [4, 2, 4, 3, 4, 2, 2, 4, 5, 2, 5, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4278, Accuracy: 0.2143, Precision: 0.1116, Recall: 0.1528, F1: 0.1270
Epoch 26/70
Train Loss: 0.8644, Accuracy: 0.6914, Precision: 0.5087, Recall: 0.3731, F1: 0.3704
Validation Loss: 0.9074, Accuracy: 0.6705, Precision: 0.4824, Recall: 0.3614, F1: 0.3564
Testing Loss: 0.8758, Accuracy: 0.7101, Precision: 0.4374, Recall: 0.3931, F1: 0.3898
LM Predictions:  [4, 2, 4, 3, 4, 2, 2, 2, 5, 3, 5, 4, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2804, Accuracy: 0.2500, Precision: 0.1375, Recall: 0.1806, F1: 0.1528
Epoch 27/70
Train Loss: 0.8356, Accuracy: 0.7026, Precision: 0.4755, Recall: 0.3799, F1: 0.3766
Validation Loss: 0.9023, Accuracy: 0.6761, Precision: 0.5279, Recall: 0.3750, F1: 0.3679
Testing Loss: 0.8677, Accuracy: 0.6941, Precision: 0.4002, Recall: 0.3861, F1: 0.3754
LM Predictions:  [4, 2, 4, 3, 4, 2, 5, 2, 5, 3, 5, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3465, Accuracy: 0.2857, Precision: 0.1656, Recall: 0.2222, F1: 0.1810
Epoch 28/70
Train Loss: 0.8318, Accuracy: 0.6963, Precision: 0.5426, Recall: 0.3929, F1: 0.4005
Validation Loss: 0.8944, Accuracy: 0.6790, Precision: 0.5289, Recall: 0.3848, F1: 0.3815
Testing Loss: 0.8473, Accuracy: 0.7048, Precision: 0.4091, Recall: 0.4091, F1: 0.4045
LM Predictions:  [4, 2, 4, 3, 2, 2, 2, 2, 5, 3, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 3, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4344, Accuracy: 0.2500, Precision: 0.1464, Recall: 0.2014, F1: 0.1639
Epoch 29/70
Train Loss: 0.8366, Accuracy: 0.7057, Precision: 0.5187, Recall: 0.3942, F1: 0.3965
Validation Loss: 0.8617, Accuracy: 0.6818, Precision: 0.6040, Recall: 0.3902, F1: 0.4000
Testing Loss: 0.8471, Accuracy: 0.7074, Precision: 0.5781, Recall: 0.4000, F1: 0.4001
LM Predictions:  [4, 2, 4, 3, 2, 2, 2, 2, 5, 4, 5, 5, 2, 4, 2, 5, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3671, Accuracy: 0.2500, Precision: 0.1470, Recall: 0.2014, F1: 0.1583
Epoch 30/70
Train Loss: 0.8215, Accuracy: 0.7075, Precision: 0.4787, Recall: 0.3941, F1: 0.3955
Validation Loss: 0.8518, Accuracy: 0.6733, Precision: 0.5261, Recall: 0.3693, F1: 0.3638
Testing Loss: 0.8423, Accuracy: 0.7234, Precision: 0.5962, Recall: 0.4148, F1: 0.4195
LM Predictions:  [4, 2, 4, 3, 2, 2, 5, 2, 5, 4, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4394, Accuracy: 0.2500, Precision: 0.1329, Recall: 0.2014, F1: 0.1546
Epoch 31/70
Train Loss: 0.8036, Accuracy: 0.7103, Precision: 0.4853, Recall: 0.4071, F1: 0.4102
Validation Loss: 0.8742, Accuracy: 0.6761, Precision: 0.5391, Recall: 0.3722, F1: 0.3734
Testing Loss: 0.8782, Accuracy: 0.7207, Precision: 0.4769, Recall: 0.4216, F1: 0.4312
LM Predictions:  [4, 2, 4, 3, 2, 2, 2, 2, 5, 4, 3, 5, 2, 4, 2, 4, 3, 2, 2, 5, 2, 2, 5, 2, 2, 5, 2, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3201, Accuracy: 0.2143, Precision: 0.1302, Recall: 0.1667, F1: 0.1346
Epoch 32/70
Train Loss: 0.8286, Accuracy: 0.6949, Precision: 0.4911, Recall: 0.3919, F1: 0.3970
Validation Loss: 0.8361, Accuracy: 0.6818, Precision: 0.5453, Recall: 0.3930, F1: 0.3961
Testing Loss: 0.8128, Accuracy: 0.7128, Precision: 0.4563, Recall: 0.4178, F1: 0.4170
LM Predictions:  [4, 2, 4, 3, 4, 3, 5, 2, 5, 4, 3, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 3, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1635, Accuracy: 0.2857, Precision: 0.1634, Recall: 0.2222, F1: 0.1853
Epoch 33/70
Train Loss: 0.8088, Accuracy: 0.7166, Precision: 0.4819, Recall: 0.4029, F1: 0.4018
Validation Loss: 0.8425, Accuracy: 0.6790, Precision: 0.5284, Recall: 0.3819, F1: 0.3804
Testing Loss: 0.8531, Accuracy: 0.7181, Precision: 0.4409, Recall: 0.4185, F1: 0.4168
LM Predictions:  [4, 2, 4, 4, 4, 2, 2, 4, 5, 4, 4, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 4, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2982, Accuracy: 0.2143, Precision: 0.1151, Recall: 0.1833, F1: 0.1411
Epoch 34/70
Train Loss: 0.7978, Accuracy: 0.7145, Precision: 0.4790, Recall: 0.4076, F1: 0.4126
Validation Loss: 0.8050, Accuracy: 0.6903, Precision: 0.5563, Recall: 0.4060, F1: 0.3979
Testing Loss: 0.8277, Accuracy: 0.6968, Precision: 0.4298, Recall: 0.4083, F1: 0.3918
LM Predictions:  [4, 5, 4, 5, 2, 2, 5, 2, 5, 4, 5, 4, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3328, Accuracy: 0.2500, Precision: 0.1530, Recall: 0.2417, F1: 0.1811
Epoch 35/70
Train Loss: 0.7832, Accuracy: 0.7180, Precision: 0.5012, Recall: 0.4269, F1: 0.4341
Validation Loss: 0.8312, Accuracy: 0.7045, Precision: 0.5709, Recall: 0.4286, F1: 0.4271
Testing Loss: 0.8397, Accuracy: 0.7101, Precision: 0.4444, Recall: 0.4248, F1: 0.4165
LM Predictions:  [3, 5, 4, 5, 2, 3, 2, 2, 5, 3, 5, 5, 2, 4, 2, 4, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3029, Accuracy: 0.2500, Precision: 0.1427, Recall: 0.2222, F1: 0.1619
Epoch 36/70
Train Loss: 0.7644, Accuracy: 0.7264, Precision: 0.5341, Recall: 0.4353, F1: 0.4396
Validation Loss: 0.8302, Accuracy: 0.7045, Precision: 0.5116, Recall: 0.4243, F1: 0.4261
Testing Loss: 0.8089, Accuracy: 0.7234, Precision: 0.5001, Recall: 0.4410, F1: 0.4476
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 4, 1, 5, 2, 4, 2, 4, 5, 4, 5, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3254, Accuracy: 0.1786, Precision: 0.1019, Recall: 0.1597, F1: 0.1195
Epoch 37/70
Train Loss: 0.7675, Accuracy: 0.7271, Precision: 0.5212, Recall: 0.4393, F1: 0.4483
Validation Loss: 0.8734, Accuracy: 0.6790, Precision: 0.5653, Recall: 0.4089, F1: 0.4110
Testing Loss: 0.8600, Accuracy: 0.7154, Precision: 0.4568, Recall: 0.4396, F1: 0.4411
LM Predictions:  [3, 2, 3, 4, 2, 3, 2, 2, 5, 3, 3, 5, 2, 4, 2, 4, 5, 2, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3925, Accuracy: 0.2143, Precision: 0.1417, Recall: 0.1806, F1: 0.1485
Epoch 38/70
Train Loss: 0.7942, Accuracy: 0.7141, Precision: 0.5005, Recall: 0.4120, F1: 0.4161
Validation Loss: 0.8587, Accuracy: 0.6818, Precision: 0.5041, Recall: 0.4088, F1: 0.4164
Testing Loss: 0.8323, Accuracy: 0.7074, Precision: 0.4475, Recall: 0.4146, F1: 0.4181
LM Predictions:  [3, 2, 2, 4, 2, 3, 2, 2, 5, 4, 3, 5, 2, 4, 2, 4, 5, 2, 4, 5, 2, 2, 3, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1983, Accuracy: 0.2857, Precision: 0.1698, Recall: 0.2361, F1: 0.1860
Epoch 39/70
Train Loss: 0.7745, Accuracy: 0.7180, Precision: 0.5065, Recall: 0.4288, F1: 0.4405
Validation Loss: 0.8345, Accuracy: 0.6989, Precision: 0.5707, Recall: 0.4596, F1: 0.4481
Testing Loss: 0.8126, Accuracy: 0.7074, Precision: 0.4237, Recall: 0.4438, F1: 0.4321
LM Predictions:  [3, 2, 3, 4, 2, 3, 5, 2, 5, 3, 3, 5, 2, 3, 2, 5, 3, 3, 4, 5, 2, 2, 3, 2, 2, 3, 3, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4226, Accuracy: 0.1786, Precision: 0.1667, Recall: 0.1458, F1: 0.1333
Epoch 40/70
Train Loss: 0.7564, Accuracy: 0.7309, Precision: 0.5063, Recall: 0.4479, F1: 0.4583
Validation Loss: 0.8355, Accuracy: 0.7301, Precision: 0.6056, Recall: 0.4576, F1: 0.4731
Testing Loss: 0.7708, Accuracy: 0.7553, Precision: 0.4768, Recall: 0.4625, F1: 0.4646
LM Predictions:  [3, 2, 2, 4, 2, 3, 2, 2, 5, 3, 4, 5, 2, 4, 2, 4, 3, 4, 5, 5, 2, 2, 5, 2, 2, 3, 4, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2239, Accuracy: 0.2143, Precision: 0.1217, Recall: 0.1736, F1: 0.1393
Epoch 41/70
Train Loss: 0.7536, Accuracy: 0.7295, Precision: 0.5271, Recall: 0.4464, F1: 0.4576
Validation Loss: 0.8469, Accuracy: 0.7102, Precision: 0.5794, Recall: 0.4205, F1: 0.4306
Testing Loss: 0.8242, Accuracy: 0.7101, Precision: 0.4109, Recall: 0.4022, F1: 0.3993
LM Predictions:  [4, 2, 3, 4, 2, 3, 2, 2, 5, 3, 4, 4, 2, 3, 2, 4, 3, 4, 2, 5, 4, 2, 5, 2, 2, 3, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4013, Accuracy: 0.1429, Precision: 0.0905, Recall: 0.1181, F1: 0.1009
Epoch 42/70
Train Loss: 0.7241, Accuracy: 0.7376, Precision: 0.5015, Recall: 0.4408, F1: 0.4488
Validation Loss: 0.8093, Accuracy: 0.7216, Precision: 0.5450, Recall: 0.4824, F1: 0.4762
Testing Loss: 0.7646, Accuracy: 0.7207, Precision: 0.4296, Recall: 0.4436, F1: 0.4360
LM Predictions:  [3, 2, 3, 3, 2, 3, 2, 2, 5, 3, 3, 5, 2, 3, 2, 4, 5, 4, 4, 5, 2, 2, 5, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3609, Accuracy: 0.1786, Precision: 0.1389, Recall: 0.1458, F1: 0.1298
Epoch 43/70
Train Loss: 0.7495, Accuracy: 0.7281, Precision: 0.5256, Recall: 0.4476, F1: 0.4616
Validation Loss: 0.8376, Accuracy: 0.6960, Precision: 0.5992, Recall: 0.4359, F1: 0.4559
Testing Loss: 0.7827, Accuracy: 0.7314, Precision: 0.4610, Recall: 0.4231, F1: 0.4252
LM Predictions:  [3, 2, 4, 4, 2, 3, 2, 2, 5, 4, 2, 5, 2, 2, 2, 4, 5, 4, 4, 3, 4, 2, 5, 2, 2, 3, 3, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4277, Accuracy: 0.1786, Precision: 0.1026, Recall: 0.1458, F1: 0.1181
Epoch 44/70
Train Loss: 0.7162, Accuracy: 0.7470, Precision: 0.6894, Recall: 0.4694, F1: 0.4846
Validation Loss: 0.8403, Accuracy: 0.7045, Precision: 0.5599, Recall: 0.4469, F1: 0.4615
Testing Loss: 0.7600, Accuracy: 0.7261, Precision: 0.5583, Recall: 0.4620, F1: 0.4803
LM Predictions:  [3, 2, 4, 4, 2, 3, 5, 2, 5, 3, 4, 5, 2, 4, 2, 5, 5, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4106, Accuracy: 0.2143, Precision: 0.1226, Recall: 0.1806, F1: 0.1397
Epoch 45/70
Train Loss: 0.7434, Accuracy: 0.7330, Precision: 0.6542, Recall: 0.4597, F1: 0.4819
Validation Loss: 0.8211, Accuracy: 0.6847, Precision: 0.5696, Recall: 0.4266, F1: 0.4471
Testing Loss: 0.7879, Accuracy: 0.7261, Precision: 0.5143, Recall: 0.4342, F1: 0.4453
LM Predictions:  [3, 2, 4, 4, 2, 3, 2, 2, 5, 4, 2, 5, 2, 4, 2, 4, 5, 4, 5, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2852, Accuracy: 0.2143, Precision: 0.1108, Recall: 0.1875, F1: 0.1360
Epoch 46/70
Train Loss: 0.7155, Accuracy: 0.7470, Precision: 0.6204, Recall: 0.4782, F1: 0.4983
Validation Loss: 0.8398, Accuracy: 0.6932, Precision: 0.5669, Recall: 0.4111, F1: 0.4244
Testing Loss: 0.8191, Accuracy: 0.7181, Precision: 0.6192, Recall: 0.4391, F1: 0.4537
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 4, 4, 5, 2, 4, 2, 4, 3, 4, 4, 5, 4, 2, 5, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2746, Accuracy: 0.2143, Precision: 0.1263, Recall: 0.1806, F1: 0.1467
Epoch 47/70
Train Loss: 0.7241, Accuracy: 0.7449, Precision: 0.5223, Recall: 0.4605, F1: 0.4710
Validation Loss: 0.7882, Accuracy: 0.7244, Precision: 0.5321, Recall: 0.4680, F1: 0.4787
Testing Loss: 0.7530, Accuracy: 0.7500, Precision: 0.5415, Recall: 0.4957, F1: 0.5097
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 3, 1, 5, 2, 4, 2, 5, 3, 4, 5, 5, 4, 2, 1, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2143, Accuracy: 0.1429, Precision: 0.1065, Recall: 0.1181, F1: 0.1056
Epoch 48/70
Train Loss: 0.7251, Accuracy: 0.7386, Precision: 0.6938, Recall: 0.4675, F1: 0.4860
Validation Loss: 0.8277, Accuracy: 0.6932, Precision: 0.5688, Recall: 0.4511, F1: 0.4535
Testing Loss: 0.7632, Accuracy: 0.7527, Precision: 0.5629, Recall: 0.4990, F1: 0.5099
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 3, 5, 3, 1, 5, 2, 4, 2, 4, 3, 4, 4, 5, 5, 2, 5, 3, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2285, Accuracy: 0.1786, Precision: 0.1381, Recall: 0.1528, F1: 0.1375
Epoch 49/70
Train Loss: 0.7003, Accuracy: 0.7463, Precision: 0.6418, Recall: 0.4815, F1: 0.4993
Validation Loss: 0.8809, Accuracy: 0.6960, Precision: 0.5651, Recall: 0.4091, F1: 0.4285
Testing Loss: 0.8592, Accuracy: 0.7207, Precision: 0.5714, Recall: 0.4425, F1: 0.4608
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 2, 2, 5, 2, 2, 2, 5, 3, 5, 4, 5, 2, 2, 2, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1547, Accuracy: 0.2500, Precision: 0.1706, Recall: 0.2014, F1: 0.1500
Epoch 50/70
Train Loss: 0.7130, Accuracy: 0.7386, Precision: 0.6768, Recall: 0.4658, F1: 0.4853
Validation Loss: 0.8474, Accuracy: 0.6989, Precision: 0.5758, Recall: 0.4441, F1: 0.4366
Testing Loss: 0.8454, Accuracy: 0.7154, Precision: 0.5277, Recall: 0.4425, F1: 0.4544
LM Predictions:  [4, 2, 4, 3, 4, 3, 2, 2, 5, 4, 2, 5, 2, 4, 2, 4, 3, 4, 4, 3, 2, 2, 5, 3, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1423, Accuracy: 0.3214, Precision: 0.1991, Recall: 0.2361, F1: 0.2139
Epoch 51/70
Train Loss: 0.6965, Accuracy: 0.7516, Precision: 0.6155, Recall: 0.4910, F1: 0.5089
Validation Loss: 0.8066, Accuracy: 0.7330, Precision: 0.5751, Recall: 0.4712, F1: 0.4874
Testing Loss: 0.7879, Accuracy: 0.7261, Precision: 0.5077, Recall: 0.4533, F1: 0.4623
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 3, 1, 5, 2, 4, 2, 5, 3, 5, 4, 0, 2, 2, 1, 2, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3973, Accuracy: 0.2500, Precision: 0.3649, Recall: 0.1875, F1: 0.1981
Epoch 52/70
Train Loss: 0.6906, Accuracy: 0.7491, Precision: 0.6117, Recall: 0.4834, F1: 0.5082
Validation Loss: 0.8621, Accuracy: 0.7131, Precision: 0.5794, Recall: 0.4394, F1: 0.4512
Testing Loss: 0.8506, Accuracy: 0.7154, Precision: 0.4372, Recall: 0.4320, F1: 0.4314
LM Predictions:  [3, 2, 3, 3, 2, 3, 2, 2, 5, 3, 2, 5, 2, 4, 2, 5, 3, 5, 4, 5, 2, 2, 5, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1512, Accuracy: 0.2857, Precision: 0.2698, Recall: 0.2361, F1: 0.2013
Epoch 53/70
Train Loss: 0.6976, Accuracy: 0.7470, Precision: 0.6087, Recall: 0.4728, F1: 0.4878
Validation Loss: 0.8536, Accuracy: 0.6960, Precision: 0.5556, Recall: 0.4332, F1: 0.4287
Testing Loss: 0.8374, Accuracy: 0.7181, Precision: 0.4315, Recall: 0.4363, F1: 0.4312
LM Predictions:  [3, 2, 3, 4, 2, 3, 5, 2, 5, 4, 3, 5, 2, 4, 2, 5, 3, 3, 4, 3, 2, 2, 3, 2, 2, 3, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1846, Accuracy: 0.2500, Precision: 0.2056, Recall: 0.2083, F1: 0.1963
Epoch 54/70
Train Loss: 0.7018, Accuracy: 0.7484, Precision: 0.6554, Recall: 0.4930, F1: 0.5123
Validation Loss: 0.8747, Accuracy: 0.6903, Precision: 0.5810, Recall: 0.4127, F1: 0.4153
Testing Loss: 0.8397, Accuracy: 0.7154, Precision: 0.5121, Recall: 0.4354, F1: 0.4415
LM Predictions:  [3, 2, 3, 4, 4, 3, 5, 2, 5, 4, 3, 5, 2, 4, 2, 5, 3, 2, 4, 3, 2, 2, 5, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1501, Accuracy: 0.2857, Precision: 0.2032, Recall: 0.2292, F1: 0.2042
Epoch 55/70
Train Loss: 0.6982, Accuracy: 0.7540, Precision: 0.6296, Recall: 0.5074, F1: 0.5341
Validation Loss: 0.8811, Accuracy: 0.6818, Precision: 0.3842, Recall: 0.3870, F1: 0.3810
Testing Loss: 0.8759, Accuracy: 0.6968, Precision: 0.4310, Recall: 0.4158, F1: 0.4161
LM Predictions:  [3, 2, 3, 4, 4, 3, 2, 2, 5, 3, 3, 5, 2, 2, 2, 5, 3, 4, 4, 3, 2, 2, 2, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1271, Accuracy: 0.2857, Precision: 0.2222, Recall: 0.2361, F1: 0.2130
Epoch 56/70
Train Loss: 0.7063, Accuracy: 0.7337, Precision: 0.6330, Recall: 0.4906, F1: 0.5127
Validation Loss: 0.8894, Accuracy: 0.6818, Precision: 0.4080, Recall: 0.3819, F1: 0.3814
Testing Loss: 0.8818, Accuracy: 0.7048, Precision: 0.4321, Recall: 0.3975, F1: 0.4005
LM Predictions:  [3, 2, 3, 4, 4, 3, 2, 2, 5, 4, 2, 5, 2, 2, 2, 2, 3, 4, 4, 3, 2, 2, 2, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4271, Accuracy: 0.3214, Precision: 0.2373, Recall: 0.2639, F1: 0.2299
Epoch 57/70
Train Loss: 0.7004, Accuracy: 0.7421, Precision: 0.5999, Recall: 0.4882, F1: 0.5142
Validation Loss: 0.8908, Accuracy: 0.6818, Precision: 0.5190, Recall: 0.3689, F1: 0.3695
Testing Loss: 0.8859, Accuracy: 0.7154, Precision: 0.4776, Recall: 0.4161, F1: 0.4265
LM Predictions:  [3, 2, 2, 4, 4, 3, 2, 2, 5, 4, 2, 5, 2, 4, 2, 4, 3, 4, 4, 5, 2, 2, 2, 2, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1610, Accuracy: 0.3929, Precision: 0.2317, Recall: 0.3125, F1: 0.2553
Epoch 58/70
Train Loss: 0.6835, Accuracy: 0.7561, Precision: 0.6352, Recall: 0.5229, F1: 0.5478
Validation Loss: 0.8762, Accuracy: 0.6733, Precision: 0.5348, Recall: 0.3725, F1: 0.3764
Testing Loss: 0.8687, Accuracy: 0.6995, Precision: 0.4497, Recall: 0.4022, F1: 0.4073
LM Predictions:  [4, 2, 4, 4, 4, 3, 2, 2, 5, 4, 2, 5, 2, 4, 2, 4, 3, 4, 4, 3, 4, 4, 4, 2, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1134, Accuracy: 0.3214, Precision: 0.2028, Recall: 0.2292, F1: 0.2014
Epoch 59/70
Train Loss: 0.6713, Accuracy: 0.7512, Precision: 0.6304, Recall: 0.4976, F1: 0.5217
Validation Loss: 0.8364, Accuracy: 0.6847, Precision: 0.5149, Recall: 0.3780, F1: 0.3882
Testing Loss: 0.8110, Accuracy: 0.7287, Precision: 0.5333, Recall: 0.4344, F1: 0.4523
LM Predictions:  [3, 2, 2, 4, 2, 3, 2, 2, 5, 4, 2, 5, 2, 2, 2, 2, 3, 4, 4, 5, 2, 2, 2, 2, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.0995, Accuracy: 0.2857, Precision: 0.1560, Recall: 0.2292, F1: 0.1624
Epoch 60/70
Train Loss: 0.6781, Accuracy: 0.7614, Precision: 0.6539, Recall: 0.5218, F1: 0.5493
Validation Loss: 0.8151, Accuracy: 0.6903, Precision: 0.5608, Recall: 0.4390, F1: 0.4351
Testing Loss: 0.7924, Accuracy: 0.7287, Precision: 0.4394, Recall: 0.4448, F1: 0.4392
LM Predictions:  [3, 5, 3, 4, 4, 3, 2, 2, 5, 3, 3, 5, 5, 3, 2, 5, 3, 4, 4, 3, 4, 2, 5, 2, 2, 5, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3255, Accuracy: 0.2143, Precision: 0.1639, Recall: 0.1806, F1: 0.1624
Epoch 61/70
Train Loss: 0.6668, Accuracy: 0.7561, Precision: 0.5867, Recall: 0.4975, F1: 0.5137
Validation Loss: 0.8338, Accuracy: 0.7017, Precision: 0.5684, Recall: 0.4391, F1: 0.4314
Testing Loss: 0.8493, Accuracy: 0.7021, Precision: 0.4141, Recall: 0.4051, F1: 0.4028
LM Predictions:  [3, 2, 3, 4, 4, 3, 2, 2, 5, 3, 2, 5, 2, 3, 2, 4, 3, 4, 4, 3, 2, 2, 2, 3, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.4622, Accuracy: 0.2857, Precision: 0.2194, Recall: 0.2222, F1: 0.1994
Epoch 62/70
Train Loss: 0.6616, Accuracy: 0.7631, Precision: 0.6351, Recall: 0.5162, F1: 0.5399
Validation Loss: 0.8059, Accuracy: 0.6989, Precision: 0.5010, Recall: 0.4322, F1: 0.4407
Testing Loss: 0.8012, Accuracy: 0.7394, Precision: 0.5905, Recall: 0.4785, F1: 0.4987
LM Predictions:  [3, 2, 4, 4, 2, 3, 2, 2, 5, 3, 4, 5, 2, 3, 2, 5, 3, 4, 4, 5, 2, 2, 5, 3, 2, 2, 5, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3686, Accuracy: 0.2143, Precision: 0.1389, Recall: 0.1875, F1: 0.1548
Epoch 63/70
Train Loss: 0.6423, Accuracy: 0.7656, Precision: 0.6589, Recall: 0.5383, F1: 0.5646
Validation Loss: 0.8377, Accuracy: 0.6903, Precision: 0.5212, Recall: 0.4072, F1: 0.4260
Testing Loss: 0.8399, Accuracy: 0.7207, Precision: 0.5401, Recall: 0.4505, F1: 0.4730
LM Predictions:  [3, 2, 4, 4, 2, 3, 2, 2, 5, 4, 2, 5, 2, 3, 2, 4, 3, 2, 4, 5, 2, 2, 2, 3, 2, 2, 2, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3231, Accuracy: 0.2500, Precision: 0.1484, Recall: 0.2014, F1: 0.1566
Epoch 64/70
Train Loss: 0.6456, Accuracy: 0.7670, Precision: 0.6567, Recall: 0.5272, F1: 0.5565
Validation Loss: 0.7392, Accuracy: 0.7443, Precision: 0.5362, Recall: 0.4793, F1: 0.4898
Testing Loss: 0.7446, Accuracy: 0.7367, Precision: 0.5311, Recall: 0.4658, F1: 0.4777
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 2, 5, 4, 2, 5, 2, 3, 2, 5, 3, 1, 4, 0, 4, 2, 3, 2, 2, 5, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.2158, Accuracy: 0.2500, Precision: 0.3139, Recall: 0.2083, F1: 0.1940
Epoch 65/70
Train Loss: 0.6486, Accuracy: 0.7642, Precision: 0.6776, Recall: 0.5224, F1: 0.5521
Validation Loss: 0.7666, Accuracy: 0.7074, Precision: 0.5127, Recall: 0.4459, F1: 0.4520
Testing Loss: 0.7551, Accuracy: 0.7553, Precision: 0.5636, Recall: 0.5023, F1: 0.5210
LM Predictions:  [3, 2, 4, 3, 2, 3, 2, 3, 5, 3, 2, 5, 2, 3, 2, 5, 3, 1, 4, 0, 2, 2, 3, 2, 2, 2, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3456, Accuracy: 0.2143, Precision: 0.3510, Recall: 0.1667, F1: 0.1768
Epoch 66/70
Train Loss: 0.6402, Accuracy: 0.7719, Precision: 0.6245, Recall: 0.5264, F1: 0.5518
Validation Loss: 0.7685, Accuracy: 0.7330, Precision: 0.5468, Recall: 0.5205, F1: 0.5246
Testing Loss: 0.7294, Accuracy: 0.7553, Precision: 0.5525, Recall: 0.5497, F1: 0.5487
LM Predictions:  [3, 2, 2, 3, 2, 3, 2, 3, 5, 3, 3, 5, 2, 3, 2, 5, 3, 1, 4, 0, 2, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.6031, Accuracy: 0.2143, Precision: 0.4250, Recall: 0.1667, F1: 0.1782
Epoch 67/70
Train Loss: 0.6343, Accuracy: 0.7659, Precision: 0.6450, Recall: 0.5226, F1: 0.5455
Validation Loss: 0.7865, Accuracy: 0.7102, Precision: 0.5447, Recall: 0.4522, F1: 0.4611
Testing Loss: 0.7824, Accuracy: 0.7473, Precision: 0.6339, Recall: 0.5176, F1: 0.5464
LM Predictions:  [3, 2, 4, 3, 4, 3, 2, 2, 5, 4, 4, 5, 2, 4, 2, 5, 3, 1, 4, 0, 2, 2, 3, 2, 2, 5, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1379, Accuracy: 0.2857, Precision: 0.3472, Recall: 0.2083, F1: 0.2168
Epoch 68/70
Train Loss: 0.6366, Accuracy: 0.7719, Precision: 0.6616, Recall: 0.5333, F1: 0.5633
Validation Loss: 0.8040, Accuracy: 0.7017, Precision: 0.5186, Recall: 0.4577, F1: 0.4581
Testing Loss: 0.7663, Accuracy: 0.7340, Precision: 0.6398, Recall: 0.4996, F1: 0.5336
LM Predictions:  [3, 2, 4, 4, 4, 3, 2, 2, 5, 4, 4, 5, 2, 3, 2, 5, 3, 4, 4, 4, 2, 2, 4, 2, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.1743, Accuracy: 0.2143, Precision: 0.1481, Recall: 0.1667, F1: 0.1535
Epoch 69/70
Train Loss: 0.6357, Accuracy: 0.7691, Precision: 0.6811, Recall: 0.5386, F1: 0.5699
Validation Loss: 0.7689, Accuracy: 0.7159, Precision: 0.5813, Recall: 0.4353, F1: 0.4405
Testing Loss: 0.7673, Accuracy: 0.7314, Precision: 0.6130, Recall: 0.4576, F1: 0.4599
LM Predictions:  [3, 2, 4, 4, 5, 3, 5, 3, 5, 4, 3, 5, 5, 4, 2, 5, 3, 5, 4, 5, 2, 2, 5, 2, 2, 2, 5, 5]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.3167, Accuracy: 0.2143, Precision: 0.1446, Recall: 0.1806, F1: 0.1470
Epoch 70/70
Train Loss: 0.6375, Accuracy: 0.7663, Precision: 0.6289, Recall: 0.5315, F1: 0.5535
Validation Loss: 0.7786, Accuracy: 0.7017, Precision: 0.5276, Recall: 0.4832, F1: 0.4772
Testing Loss: 0.7057, Accuracy: 0.7527, Precision: 0.5413, Recall: 0.5084, F1: 0.5147
LM Predictions:  [3, 2, 3, 3, 2, 3, 5, 2, 5, 3, 3, 5, 2, 3, 2, 5, 3, 0, 4, 5, 2, 2, 5, 5, 2, 3, 3, 3]
LM Labels:  [4, 4, 2, 5, 4, 1, 0, 2, 0, 5, 2, 5, 1, 4, 0, 0, 4, 0, 4, 0, 2, 4, 2, 0, 2, 0, 5, 4]
LM Loss: 2.5317, Accuracy: 0.2143, Precision: 0.4196, Recall: 0.1667, F1: 0.1758
Label Memorization Analysis: 
LM Loss: 2.5317, Accuracy: 0.2143, Precision: 0.4196, Recall: 0.1667, F1: 0.1758
---------------------------------------------------------------------------



