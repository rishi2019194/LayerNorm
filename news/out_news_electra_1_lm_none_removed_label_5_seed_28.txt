Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 0.9261, Accuracy: 0.7010, Precision: 0.6013, Recall: 0.5654, F1: 0.5702
Validation Loss: 0.4956, Accuracy: 0.8529, Precision: 0.8944, Recall: 0.7558, F1: 0.7617
Testing Loss: 0.4356, Accuracy: 0.8659, Precision: 0.8327, Recall: 0.7548, F1: 0.7527
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0946, Accuracy: 0.0952, Precision: 0.0171, Recall: 0.1333, F1: 0.0303
Epoch 2/70
Train Loss: 0.4304, Accuracy: 0.8727, Precision: 0.8255, Recall: 0.8082, F1: 0.8148
Validation Loss: 0.4507, Accuracy: 0.8806, Precision: 0.8430, Recall: 0.8469, F1: 0.8443
Testing Loss: 0.3739, Accuracy: 0.9010, Precision: 0.8796, Recall: 0.8624, F1: 0.8694
LM Predictions:  [0, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9070, Accuracy: 0.0952, Precision: 0.1146, Recall: 0.1167, F1: 0.0754
Epoch 3/70
Train Loss: 0.3289, Accuracy: 0.9047, Precision: 0.8638, Recall: 0.8643, F1: 0.8635
Validation Loss: 0.4796, Accuracy: 0.8785, Precision: 0.8583, Recall: 0.8505, F1: 0.8520
Testing Loss: 0.3516, Accuracy: 0.8961, Precision: 0.8673, Recall: 0.8662, F1: 0.8659
LM Predictions:  [0, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 3, 0, 5, 3, 0, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9824, Accuracy: 0.0952, Precision: 0.2479, Recall: 0.1000, F1: 0.0930
Epoch 4/70
Train Loss: 0.2887, Accuracy: 0.9144, Precision: 0.8752, Recall: 0.8877, F1: 0.8799
Validation Loss: 0.4936, Accuracy: 0.8678, Precision: 0.8400, Recall: 0.8037, F1: 0.8180
Testing Loss: 0.4203, Accuracy: 0.8804, Precision: 0.8608, Recall: 0.8118, F1: 0.8312
LM Predictions:  [0, 0, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 5, 2, 0, 5, 0, 0, 3, 0, 0, 0, 0, 5, 5, 0, 4, 3, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7818, Accuracy: 0.2143, Precision: 0.4708, Recall: 0.2083, F1: 0.2095
Epoch 5/70
Train Loss: 0.2367, Accuracy: 0.9298, Precision: 0.8927, Recall: 0.8993, F1: 0.8951
Validation Loss: 0.4407, Accuracy: 0.8827, Precision: 0.8461, Recall: 0.8635, F1: 0.8504
Testing Loss: 0.3664, Accuracy: 0.8901, Precision: 0.8511, Recall: 0.8622, F1: 0.8546
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7810, Accuracy: 0.1429, Precision: 0.4833, Recall: 0.1417, F1: 0.1914
Epoch 6/70
Train Loss: 0.2087, Accuracy: 0.9367, Precision: 0.9014, Recall: 0.9101, F1: 0.9049
Validation Loss: 0.4569, Accuracy: 0.8785, Precision: 0.8386, Recall: 0.8560, F1: 0.8436
Testing Loss: 0.3856, Accuracy: 0.8744, Precision: 0.8386, Recall: 0.8452, F1: 0.8397
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 4, 5, 3, 0, 5, 3, 0, 0, 5, 5, 5, 5, 5, 4, 3, 5, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1781, Accuracy: 0.2381, Precision: 0.5417, Recall: 0.2083, F1: 0.2687
Epoch 7/70
Train Loss: 0.1857, Accuracy: 0.9398, Precision: 0.9041, Recall: 0.9129, F1: 0.9077
Validation Loss: 0.5220, Accuracy: 0.8763, Precision: 0.8423, Recall: 0.8567, F1: 0.8455
Testing Loss: 0.3932, Accuracy: 0.8865, Precision: 0.8546, Recall: 0.8593, F1: 0.8543
LM Predictions:  [0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 3, 2, 4, 5, 3, 5, 5, 3, 0, 0, 5, 5, 5, 5, 5, 4, 3, 3, 5, 5, 5, 0, 5, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2840, Accuracy: 0.3571, Precision: 0.7333, Recall: 0.3019, F1: 0.3917
Epoch 8/70
Train Loss: 0.1645, Accuracy: 0.9440, Precision: 0.9109, Recall: 0.9208, F1: 0.9151
Validation Loss: 0.5202, Accuracy: 0.8678, Precision: 0.8336, Recall: 0.8334, F1: 0.8318
Testing Loss: 0.4580, Accuracy: 0.8708, Precision: 0.8402, Recall: 0.8219, F1: 0.8295
LM Predictions:  [0, 1, 5, 3, 4, 0, 5, 5, 5, 5, 0, 0, 2, 5, 3, 3, 5, 4, 0, 3, 3, 0, 3, 1, 0, 0, 0, 5, 5, 0, 4, 3, 3, 5, 0, 0, 0, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7361, Accuracy: 0.4286, Precision: 0.6311, Recall: 0.3662, F1: 0.3987
Epoch 9/70
Train Loss: 0.1504, Accuracy: 0.9519, Precision: 0.9222, Recall: 0.9342, F1: 0.9274
Validation Loss: 0.6758, Accuracy: 0.8571, Precision: 0.8478, Recall: 0.8213, F1: 0.8297
Testing Loss: 0.5955, Accuracy: 0.8587, Precision: 0.8535, Recall: 0.8170, F1: 0.8297
LM Predictions:  [0, 1, 5, 0, 1, 0, 5, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 0, 0, 5, 5, 0, 4, 3, 0, 5, 0, 5, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0323, Accuracy: 0.3810, Precision: 0.6961, Recall: 0.3389, F1: 0.3907
Epoch 10/70
Train Loss: 0.1383, Accuracy: 0.9533, Precision: 0.9245, Recall: 0.9343, F1: 0.9287
Validation Loss: 0.6253, Accuracy: 0.8699, Precision: 0.8425, Recall: 0.8213, F1: 0.8303
Testing Loss: 0.4865, Accuracy: 0.8768, Precision: 0.8436, Recall: 0.8167, F1: 0.8283
LM Predictions:  [0, 1, 5, 0, 1, 0, 0, 0, 0, 5, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 4, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 5, 0, 0, 0, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4356, Accuracy: 0.4524, Precision: 0.7018, Recall: 0.4037, F1: 0.4292
Epoch 11/70
Train Loss: 0.1197, Accuracy: 0.9552, Precision: 0.9286, Recall: 0.9344, F1: 0.9312
Validation Loss: 0.7086, Accuracy: 0.8486, Precision: 0.8159, Recall: 0.7761, F1: 0.7922
Testing Loss: 0.5481, Accuracy: 0.8708, Precision: 0.8430, Recall: 0.7958, F1: 0.8125
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 0, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 0, 3, 1, 0, 0, 0, 4, 5, 0, 4, 3, 3, 0, 1, 4, 1, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9049, Accuracy: 0.6667, Precision: 0.7188, Recall: 0.5801, F1: 0.5859
Epoch 12/70
Train Loss: 0.1119, Accuracy: 0.9559, Precision: 0.9286, Recall: 0.9357, F1: 0.9320
Validation Loss: 0.6752, Accuracy: 0.8699, Precision: 0.8310, Recall: 0.7998, F1: 0.8117
Testing Loss: 0.5399, Accuracy: 0.8684, Precision: 0.8559, Recall: 0.7819, F1: 0.8013
LM Predictions:  [0, 1, 5, 1, 1, 0, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 0, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 0, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7519, Accuracy: 0.6905, Precision: 0.7157, Recall: 0.5968, F1: 0.5975
Epoch 13/70
Train Loss: 0.1020, Accuracy: 0.9590, Precision: 0.9322, Recall: 0.9363, F1: 0.9341
Validation Loss: 0.6803, Accuracy: 0.8657, Precision: 0.8367, Recall: 0.8148, F1: 0.8243
Testing Loss: 0.5047, Accuracy: 0.8732, Precision: 0.8415, Recall: 0.8211, F1: 0.8303
LM Predictions:  [0, 1, 2, 1, 1, 1, 2, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 0, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7210, Accuracy: 0.6905, Precision: 0.6873, Recall: 0.5861, F1: 0.5956
Epoch 14/70
Train Loss: 0.0965, Accuracy: 0.9621, Precision: 0.9378, Recall: 0.9422, F1: 0.9399
Validation Loss: 0.7466, Accuracy: 0.8635, Precision: 0.8334, Recall: 0.7904, F1: 0.8081
Testing Loss: 0.6582, Accuracy: 0.8696, Precision: 0.8469, Recall: 0.7914, F1: 0.8104
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4811, Accuracy: 0.7619, Precision: 0.7262, Recall: 0.6532, F1: 0.6375
Epoch 15/70
Train Loss: 0.0920, Accuracy: 0.9611, Precision: 0.9341, Recall: 0.9439, F1: 0.9386
Validation Loss: 0.7002, Accuracy: 0.8657, Precision: 0.8289, Recall: 0.8056, F1: 0.8147
Testing Loss: 0.5501, Accuracy: 0.8671, Precision: 0.8421, Recall: 0.8068, F1: 0.8215
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3827, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8706, F1: 0.8539
Epoch 16/70
Train Loss: 0.0877, Accuracy: 0.9611, Precision: 0.9367, Recall: 0.9434, F1: 0.9399
Validation Loss: 0.6946, Accuracy: 0.8657, Precision: 0.8283, Recall: 0.8179, F1: 0.8226
Testing Loss: 0.5575, Accuracy: 0.8720, Precision: 0.8419, Recall: 0.8177, F1: 0.8286
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3521, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 17/70
Train Loss: 0.0887, Accuracy: 0.9618, Precision: 0.9386, Recall: 0.9431, F1: 0.9408
Validation Loss: 0.7858, Accuracy: 0.8507, Precision: 0.8149, Recall: 0.7864, F1: 0.7988
Testing Loss: 0.6193, Accuracy: 0.8671, Precision: 0.8437, Recall: 0.7973, F1: 0.8164
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3963, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8128, F1: 0.7913
Epoch 18/70
Train Loss: 0.0814, Accuracy: 0.9611, Precision: 0.9371, Recall: 0.9404, F1: 0.9387
Validation Loss: 0.7351, Accuracy: 0.8593, Precision: 0.8036, Recall: 0.7992, F1: 0.8008
Testing Loss: 0.5938, Accuracy: 0.8647, Precision: 0.8214, Recall: 0.7899, F1: 0.8015
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3131, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 19/70
Train Loss: 0.0740, Accuracy: 0.9644, Precision: 0.9426, Recall: 0.9437, F1: 0.9432
Validation Loss: 0.7361, Accuracy: 0.8678, Precision: 0.8341, Recall: 0.7999, F1: 0.8145
Testing Loss: 0.6478, Accuracy: 0.8792, Precision: 0.8503, Recall: 0.8033, F1: 0.8221
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3183, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8550, F1: 0.8321
Epoch 20/70
Train Loss: 0.0648, Accuracy: 0.9663, Precision: 0.9478, Recall: 0.9467, F1: 0.9472
Validation Loss: 0.7891, Accuracy: 0.8678, Precision: 0.8234, Recall: 0.8115, F1: 0.8166
Testing Loss: 0.6707, Accuracy: 0.8684, Precision: 0.8391, Recall: 0.8076, F1: 0.8205
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4157, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8461, F1: 0.8249
Epoch 21/70
Train Loss: 0.0659, Accuracy: 0.9647, Precision: 0.9394, Recall: 0.9496, F1: 0.9442
Validation Loss: 0.8606, Accuracy: 0.8635, Precision: 0.8328, Recall: 0.7882, F1: 0.8060
Testing Loss: 0.7135, Accuracy: 0.8684, Precision: 0.8464, Recall: 0.7835, F1: 0.8038
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2801, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 22/70
Train Loss: 0.0627, Accuracy: 0.9666, Precision: 0.9482, Recall: 0.9389, F1: 0.9433
Validation Loss: 0.8785, Accuracy: 0.8678, Precision: 0.8248, Recall: 0.8058, F1: 0.8142
Testing Loss: 0.7377, Accuracy: 0.8756, Precision: 0.8411, Recall: 0.8142, F1: 0.8255
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1532, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 23/70
Train Loss: 0.0815, Accuracy: 0.9611, Precision: 0.9417, Recall: 0.9347, F1: 0.9381
Validation Loss: 0.7633, Accuracy: 0.8806, Precision: 0.8364, Recall: 0.8219, F1: 0.8281
Testing Loss: 0.7184, Accuracy: 0.8671, Precision: 0.8318, Recall: 0.7982, F1: 0.8124
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2433, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9178, F1: 0.8958
Epoch 24/70
Train Loss: 0.0680, Accuracy: 0.9625, Precision: 0.9389, Recall: 0.9448, F1: 0.9417
Validation Loss: 0.8340, Accuracy: 0.8635, Precision: 0.8459, Recall: 0.8059, F1: 0.8224
Testing Loss: 0.7617, Accuracy: 0.8539, Precision: 0.8239, Recall: 0.7710, F1: 0.7892
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3312, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7928, F1: 0.7749
Epoch 25/70
Train Loss: 0.0618, Accuracy: 0.9654, Precision: 0.9462, Recall: 0.9441, F1: 0.9452
Validation Loss: 0.8646, Accuracy: 0.8721, Precision: 0.8376, Recall: 0.8086, F1: 0.8213
Testing Loss: 0.7467, Accuracy: 0.8671, Precision: 0.8328, Recall: 0.7919, F1: 0.8089
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2697, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8750, F1: 0.8511
Epoch 26/70
Train Loss: 0.0638, Accuracy: 0.9666, Precision: 0.9456, Recall: 0.9532, F1: 0.9492
Validation Loss: 0.8874, Accuracy: 0.8550, Precision: 0.8045, Recall: 0.7648, F1: 0.7785
Testing Loss: 0.7855, Accuracy: 0.8647, Precision: 0.8237, Recall: 0.7726, F1: 0.7876
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2231, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9350, F1: 0.9183
Epoch 27/70
Train Loss: 0.0766, Accuracy: 0.9630, Precision: 0.9417, Recall: 0.9395, F1: 0.9406
Validation Loss: 0.8930, Accuracy: 0.8529, Precision: 0.8122, Recall: 0.7922, F1: 0.7995
Testing Loss: 0.8492, Accuracy: 0.8551, Precision: 0.8221, Recall: 0.7833, F1: 0.7922
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4744, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 28/70
Train Loss: 0.0558, Accuracy: 0.9675, Precision: 0.9479, Recall: 0.9459, F1: 0.9469
Validation Loss: 0.9307, Accuracy: 0.8593, Precision: 0.8308, Recall: 0.7961, F1: 0.8110
Testing Loss: 0.7798, Accuracy: 0.8587, Precision: 0.8311, Recall: 0.7980, F1: 0.8122
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1642, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9578, F1: 0.9444
Epoch 29/70
Train Loss: 0.0520, Accuracy: 0.9637, Precision: 0.9424, Recall: 0.9419, F1: 0.9421
Validation Loss: 0.9618, Accuracy: 0.8529, Precision: 0.8324, Recall: 0.7930, F1: 0.8096
Testing Loss: 0.8503, Accuracy: 0.8539, Precision: 0.8332, Recall: 0.7999, F1: 0.8136
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2956, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 30/70
Train Loss: 0.0589, Accuracy: 0.9689, Precision: 0.9522, Recall: 0.9506, F1: 0.9514
Validation Loss: 1.0922, Accuracy: 0.8422, Precision: 0.8164, Recall: 0.7814, F1: 0.7943
Testing Loss: 0.9398, Accuracy: 0.8514, Precision: 0.8299, Recall: 0.7770, F1: 0.7926
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2397, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 31/70
Train Loss: 0.0728, Accuracy: 0.9613, Precision: 0.9397, Recall: 0.9409, F1: 0.9403
Validation Loss: 0.9908, Accuracy: 0.8486, Precision: 0.8115, Recall: 0.7949, F1: 0.8016
Testing Loss: 0.7643, Accuracy: 0.8539, Precision: 0.8256, Recall: 0.7905, F1: 0.8049
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2114, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 32/70
Train Loss: 0.0693, Accuracy: 0.9630, Precision: 0.9440, Recall: 0.9420, F1: 0.9430
Validation Loss: 0.8541, Accuracy: 0.8550, Precision: 0.8246, Recall: 0.8012, F1: 0.8115
Testing Loss: 0.7293, Accuracy: 0.8527, Precision: 0.8301, Recall: 0.7789, F1: 0.7996
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2609, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 33/70
Train Loss: 0.0587, Accuracy: 0.9670, Precision: 0.9459, Recall: 0.9518, F1: 0.9488
Validation Loss: 0.9339, Accuracy: 0.8635, Precision: 0.8253, Recall: 0.8017, F1: 0.8119
Testing Loss: 0.7574, Accuracy: 0.8659, Precision: 0.8401, Recall: 0.7882, F1: 0.8059
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3401, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7928, F1: 0.7749
Epoch 34/70
Train Loss: 0.0512, Accuracy: 0.9677, Precision: 0.9456, Recall: 0.9525, F1: 0.9489
Validation Loss: 0.9121, Accuracy: 0.8593, Precision: 0.8274, Recall: 0.8060, F1: 0.8141
Testing Loss: 0.7831, Accuracy: 0.8659, Precision: 0.8266, Recall: 0.7973, F1: 0.8096
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2523, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 35/70
Train Loss: 0.0509, Accuracy: 0.9675, Precision: 0.9472, Recall: 0.9499, F1: 0.9485
Validation Loss: 0.9041, Accuracy: 0.8657, Precision: 0.8219, Recall: 0.7990, F1: 0.8081
Testing Loss: 0.8222, Accuracy: 0.8671, Precision: 0.8285, Recall: 0.7843, F1: 0.8001
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1883, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0508, Accuracy: 0.9656, Precision: 0.9442, Recall: 0.9440, F1: 0.9441
Validation Loss: 0.9727, Accuracy: 0.8678, Precision: 0.8228, Recall: 0.8023, F1: 0.8108
Testing Loss: 0.7458, Accuracy: 0.8768, Precision: 0.8447, Recall: 0.7942, F1: 0.8109
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2059, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9350, F1: 0.9183
Epoch 37/70
Train Loss: 0.0465, Accuracy: 0.9713, Precision: 0.9554, Recall: 0.9524, F1: 0.9538
Validation Loss: 0.9869, Accuracy: 0.8571, Precision: 0.8157, Recall: 0.7987, F1: 0.8064
Testing Loss: 0.8066, Accuracy: 0.8563, Precision: 0.8266, Recall: 0.7967, F1: 0.8095
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2544, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8911, F1: 0.8657
Epoch 38/70
Train Loss: 0.0473, Accuracy: 0.9692, Precision: 0.9507, Recall: 0.9502, F1: 0.9504
Validation Loss: 1.0472, Accuracy: 0.8550, Precision: 0.8198, Recall: 0.7848, F1: 0.7993
Testing Loss: 0.8822, Accuracy: 0.8539, Precision: 0.8328, Recall: 0.7771, F1: 0.7982
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3229, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 39/70
Train Loss: 0.0475, Accuracy: 0.9670, Precision: 0.9466, Recall: 0.9525, F1: 0.9493
Validation Loss: 1.0363, Accuracy: 0.8657, Precision: 0.8297, Recall: 0.8103, F1: 0.8189
Testing Loss: 0.8039, Accuracy: 0.8671, Precision: 0.8317, Recall: 0.7906, F1: 0.8052
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2510, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 40/70
Train Loss: 0.0462, Accuracy: 0.9687, Precision: 0.9517, Recall: 0.9472, F1: 0.9494
Validation Loss: 1.0549, Accuracy: 0.8635, Precision: 0.8309, Recall: 0.8059, F1: 0.8166
Testing Loss: 0.8494, Accuracy: 0.8563, Precision: 0.8348, Recall: 0.7690, F1: 0.7901
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3334, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8750, F1: 0.8511
Epoch 41/70
Train Loss: 0.0472, Accuracy: 0.9661, Precision: 0.9505, Recall: 0.9383, F1: 0.9441
Validation Loss: 0.9496, Accuracy: 0.8721, Precision: 0.8265, Recall: 0.8181, F1: 0.8217
Testing Loss: 0.7859, Accuracy: 0.8744, Precision: 0.8360, Recall: 0.8195, F1: 0.8266
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1771, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9306, F1: 0.9155
Epoch 42/70
Train Loss: 0.0603, Accuracy: 0.9668, Precision: 0.9465, Recall: 0.9498, F1: 0.9481
Validation Loss: 0.9681, Accuracy: 0.8614, Precision: 0.8357, Recall: 0.8019, F1: 0.8166
Testing Loss: 0.8110, Accuracy: 0.8635, Precision: 0.8326, Recall: 0.7829, F1: 0.8019
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3035, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8756, F1: 0.8500
Epoch 43/70
Train Loss: 0.0556, Accuracy: 0.9654, Precision: 0.9496, Recall: 0.9366, F1: 0.9427
Validation Loss: 1.0279, Accuracy: 0.8571, Precision: 0.8369, Recall: 0.7942, F1: 0.8114
Testing Loss: 0.8995, Accuracy: 0.8647, Precision: 0.8419, Recall: 0.7985, F1: 0.8162
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2820, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8711, F1: 0.8456
Epoch 44/70
Train Loss: 0.0574, Accuracy: 0.9651, Precision: 0.9450, Recall: 0.9465, F1: 0.9457
Validation Loss: 1.0813, Accuracy: 0.8614, Precision: 0.8418, Recall: 0.8008, F1: 0.8183
Testing Loss: 0.9661, Accuracy: 0.8514, Precision: 0.8254, Recall: 0.7647, F1: 0.7862
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2652, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9150, F1: 0.8968
Epoch 45/70
Train Loss: 0.0862, Accuracy: 0.9578, Precision: 0.9349, Recall: 0.9440, F1: 0.9393
Validation Loss: 1.0360, Accuracy: 0.8550, Precision: 0.8174, Recall: 0.8073, F1: 0.8119
Testing Loss: 0.8048, Accuracy: 0.8611, Precision: 0.8234, Recall: 0.7906, F1: 0.8030
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3321, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 46/70
Train Loss: 0.0584, Accuracy: 0.9656, Precision: 0.9474, Recall: 0.9405, F1: 0.9439
Validation Loss: 0.9989, Accuracy: 0.8614, Precision: 0.8332, Recall: 0.7950, F1: 0.8106
Testing Loss: 0.9271, Accuracy: 0.8514, Precision: 0.8175, Recall: 0.7532, F1: 0.7708
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2164, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8750, F1: 0.8511
Epoch 47/70
Train Loss: 0.0489, Accuracy: 0.9668, Precision: 0.9458, Recall: 0.9469, F1: 0.9464
Validation Loss: 1.0561, Accuracy: 0.8721, Precision: 0.8362, Recall: 0.8115, F1: 0.8219
Testing Loss: 0.9198, Accuracy: 0.8599, Precision: 0.8219, Recall: 0.7737, F1: 0.7891
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2520, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7720
Epoch 48/70
Train Loss: 0.0466, Accuracy: 0.9659, Precision: 0.9436, Recall: 0.9482, F1: 0.9458
Validation Loss: 1.0528, Accuracy: 0.8571, Precision: 0.8170, Recall: 0.8016, F1: 0.8086
Testing Loss: 0.9050, Accuracy: 0.8563, Precision: 0.8201, Recall: 0.7881, F1: 0.8020
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2298, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 49/70
Train Loss: 0.0494, Accuracy: 0.9668, Precision: 0.9464, Recall: 0.9448, F1: 0.9456
Validation Loss: 0.9937, Accuracy: 0.8571, Precision: 0.8217, Recall: 0.8047, F1: 0.8120
Testing Loss: 0.8971, Accuracy: 0.8659, Precision: 0.8301, Recall: 0.8011, F1: 0.8140
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1656, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9600, F1: 0.9444
Epoch 50/70
Train Loss: 0.0459, Accuracy: 0.9666, Precision: 0.9484, Recall: 0.9408, F1: 0.9444
Validation Loss: 1.0329, Accuracy: 0.8614, Precision: 0.8291, Recall: 0.8017, F1: 0.8138
Testing Loss: 0.9492, Accuracy: 0.8611, Precision: 0.8299, Recall: 0.7882, F1: 0.8055
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2304, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 51/70
Train Loss: 0.0459, Accuracy: 0.9668, Precision: 0.9469, Recall: 0.9476, F1: 0.9472
Validation Loss: 1.0184, Accuracy: 0.8614, Precision: 0.8336, Recall: 0.8147, F1: 0.8229
Testing Loss: 0.9190, Accuracy: 0.8623, Precision: 0.8308, Recall: 0.7995, F1: 0.8132
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2155, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 52/70
Train Loss: 0.0448, Accuracy: 0.9673, Precision: 0.9453, Recall: 0.9508, F1: 0.9480
Validation Loss: 1.0737, Accuracy: 0.8699, Precision: 0.8519, Recall: 0.8153, F1: 0.8311
Testing Loss: 0.9953, Accuracy: 0.8623, Precision: 0.8360, Recall: 0.7936, F1: 0.8114
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2277, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9350, F1: 0.9183
Epoch 53/70
Train Loss: 0.0645, Accuracy: 0.9628, Precision: 0.9403, Recall: 0.9428, F1: 0.9415
Validation Loss: 1.0603, Accuracy: 0.8593, Precision: 0.8294, Recall: 0.8022, F1: 0.8141
Testing Loss: 0.9355, Accuracy: 0.8671, Precision: 0.8359, Recall: 0.7915, F1: 0.8083
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1495, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 54/70
Train Loss: 0.0545, Accuracy: 0.9661, Precision: 0.9472, Recall: 0.9416, F1: 0.9442
Validation Loss: 1.0513, Accuracy: 0.8678, Precision: 0.8486, Recall: 0.8087, F1: 0.8258
Testing Loss: 0.8983, Accuracy: 0.8671, Precision: 0.8306, Recall: 0.7874, F1: 0.8036
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2260, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7890
Epoch 55/70
Train Loss: 0.0489, Accuracy: 0.9661, Precision: 0.9466, Recall: 0.9446, F1: 0.9456
Validation Loss: 1.1675, Accuracy: 0.8571, Precision: 0.8411, Recall: 0.7935, F1: 0.8130
Testing Loss: 1.0318, Accuracy: 0.8539, Precision: 0.8361, Recall: 0.7701, F1: 0.7929
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3150, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 56/70
Train Loss: 0.0446, Accuracy: 0.9682, Precision: 0.9507, Recall: 0.9454, F1: 0.9480
Validation Loss: 1.0693, Accuracy: 0.8614, Precision: 0.8202, Recall: 0.8131, F1: 0.8154
Testing Loss: 0.9030, Accuracy: 0.8575, Precision: 0.8164, Recall: 0.7844, F1: 0.7963
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2055, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 57/70
Train Loss: 0.0641, Accuracy: 0.9670, Precision: 0.9430, Recall: 0.9553, F1: 0.9488
Validation Loss: 1.1000, Accuracy: 0.8507, Precision: 0.8180, Recall: 0.7721, F1: 0.7902
Testing Loss: 0.9756, Accuracy: 0.8575, Precision: 0.8266, Recall: 0.7653, F1: 0.7812
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1962, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 58/70
Train Loss: 0.0511, Accuracy: 0.9675, Precision: 0.9532, Recall: 0.9430, F1: 0.9477
Validation Loss: 1.0064, Accuracy: 0.8657, Precision: 0.8189, Recall: 0.7894, F1: 0.8011
Testing Loss: 0.9372, Accuracy: 0.8623, Precision: 0.8032, Recall: 0.7707, F1: 0.7774
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2046, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8800, F1: 0.8528
Epoch 59/70
Train Loss: 0.0510, Accuracy: 0.9666, Precision: 0.9519, Recall: 0.9421, F1: 0.9468
Validation Loss: 0.8799, Accuracy: 0.8401, Precision: 0.8085, Recall: 0.7817, F1: 0.7915
Testing Loss: 0.8262, Accuracy: 0.8478, Precision: 0.8144, Recall: 0.7823, F1: 0.7959
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2374, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8078
Epoch 60/70
Train Loss: 0.0471, Accuracy: 0.9673, Precision: 0.9428, Recall: 0.9529, F1: 0.9475
Validation Loss: 1.0786, Accuracy: 0.8550, Precision: 0.8268, Recall: 0.7839, F1: 0.8010
Testing Loss: 0.9479, Accuracy: 0.8587, Precision: 0.8220, Recall: 0.7714, F1: 0.7873
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2148, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8716
Epoch 61/70
Train Loss: 0.0435, Accuracy: 0.9687, Precision: 0.9510, Recall: 0.9469, F1: 0.9489
Validation Loss: 1.0577, Accuracy: 0.8635, Precision: 0.8376, Recall: 0.8014, F1: 0.8169
Testing Loss: 0.9460, Accuracy: 0.8647, Precision: 0.8305, Recall: 0.7837, F1: 0.8006
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2152, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8716
Epoch 62/70
Train Loss: 0.0442, Accuracy: 0.9689, Precision: 0.9536, Recall: 0.9436, F1: 0.9483
Validation Loss: 1.0517, Accuracy: 0.8635, Precision: 0.8344, Recall: 0.8075, F1: 0.8194
Testing Loss: 0.9475, Accuracy: 0.8708, Precision: 0.8372, Recall: 0.8053, F1: 0.8189
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2219, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 63/70
Train Loss: 0.0439, Accuracy: 0.9640, Precision: 0.9408, Recall: 0.9462, F1: 0.9434
Validation Loss: 1.0583, Accuracy: 0.8678, Precision: 0.8393, Recall: 0.8122, F1: 0.8241
Testing Loss: 0.9398, Accuracy: 0.8684, Precision: 0.8311, Recall: 0.7964, F1: 0.8110
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2152, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 64/70
Train Loss: 0.0439, Accuracy: 0.9668, Precision: 0.9434, Recall: 0.9539, F1: 0.9482
Validation Loss: 1.1697, Accuracy: 0.8550, Precision: 0.8330, Recall: 0.7894, F1: 0.8069
Testing Loss: 0.9818, Accuracy: 0.8587, Precision: 0.8246, Recall: 0.7700, F1: 0.7850
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2608, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8078
Epoch 65/70
Train Loss: 0.0442, Accuracy: 0.9663, Precision: 0.9433, Recall: 0.9489, F1: 0.9460
Validation Loss: 1.1186, Accuracy: 0.8614, Precision: 0.8179, Recall: 0.7938, F1: 0.8042
Testing Loss: 0.9525, Accuracy: 0.8684, Precision: 0.8316, Recall: 0.7933, F1: 0.8072
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2207, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8333, F1: 0.8106
Epoch 66/70
Train Loss: 0.0652, Accuracy: 0.9637, Precision: 0.9476, Recall: 0.9382, F1: 0.9426
Validation Loss: 1.0308, Accuracy: 0.8571, Precision: 0.8248, Recall: 0.8038, F1: 0.8129
Testing Loss: 0.9392, Accuracy: 0.8539, Precision: 0.8128, Recall: 0.7917, F1: 0.8001
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2248, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 67/70
Train Loss: 0.0594, Accuracy: 0.9673, Precision: 0.9470, Recall: 0.9535, F1: 0.9499
Validation Loss: 1.0730, Accuracy: 0.8529, Precision: 0.8247, Recall: 0.7825, F1: 0.8000
Testing Loss: 0.9619, Accuracy: 0.8587, Precision: 0.8251, Recall: 0.7856, F1: 0.8010
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1903, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0571, Accuracy: 0.9670, Precision: 0.9497, Recall: 0.9430, F1: 0.9460
Validation Loss: 1.0327, Accuracy: 0.8635, Precision: 0.8344, Recall: 0.8055, F1: 0.8180
Testing Loss: 0.8742, Accuracy: 0.8696, Precision: 0.8342, Recall: 0.8029, F1: 0.8156
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2493, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 69/70
Train Loss: 0.0462, Accuracy: 0.9682, Precision: 0.9492, Recall: 0.9480, F1: 0.9486
Validation Loss: 1.0891, Accuracy: 0.8678, Precision: 0.8375, Recall: 0.8026, F1: 0.8170
Testing Loss: 0.9386, Accuracy: 0.8659, Precision: 0.8321, Recall: 0.7887, F1: 0.8046
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2319, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 70/70
Train Loss: 0.0437, Accuracy: 0.9675, Precision: 0.9492, Recall: 0.9452, F1: 0.9472
Validation Loss: 1.0826, Accuracy: 0.8678, Precision: 0.8360, Recall: 0.8029, F1: 0.8169
Testing Loss: 0.9583, Accuracy: 0.8635, Precision: 0.8301, Recall: 0.7944, F1: 0.8087
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2102, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Label Memorization Analysis: 
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2102, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700

