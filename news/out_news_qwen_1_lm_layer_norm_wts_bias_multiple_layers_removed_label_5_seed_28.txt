Model: Qwen/Qwen2-0.5B-Instruct, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
For early layers:  [0, 1, 2, 3, 4, 5, 6, 7]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 1.6096, Accuracy: 0.4150, Precision: 0.2993, Recall: 0.2954, F1: 0.2814
Validation Loss: 1.1191, Accuracy: 0.6055, Precision: 0.5628, Recall: 0.4889, F1: 0.4653
Testing Loss: 1.1069, Accuracy: 0.6171, Precision: 0.5568, Recall: 0.4971, F1: 0.4746
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 0, 4, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9336, Accuracy: 0.1905, Precision: 0.1235, Recall: 0.2400, F1: 0.1299
Epoch 2/70
Train Loss: 0.7126, Accuracy: 0.7527, Precision: 0.6695, Recall: 0.6418, F1: 0.6484
Validation Loss: 0.9358, Accuracy: 0.6887, Precision: 0.6764, Recall: 0.5646, F1: 0.5900
Testing Loss: 0.9197, Accuracy: 0.6848, Precision: 0.6962, Recall: 0.5643, F1: 0.5936
LM Predictions:  [0, 0, 2, 0, 1, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 4, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8813, Accuracy: 0.4524, Precision: 0.6975, Recall: 0.4227, F1: 0.4041
Epoch 3/70
Train Loss: 0.2734, Accuracy: 0.9184, Precision: 0.8833, Recall: 0.8653, F1: 0.8733
Validation Loss: 1.0306, Accuracy: 0.6908, Precision: 0.6853, Recall: 0.5887, F1: 0.6089
Testing Loss: 0.9656, Accuracy: 0.7331, Precision: 0.7055, Recall: 0.6273, F1: 0.6420
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 5, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5442, Accuracy: 0.8810, Precision: 0.7593, Recall: 0.7481, F1: 0.7377
Epoch 4/70
Train Loss: 0.1723, Accuracy: 0.9450, Precision: 0.9141, Recall: 0.9099, F1: 0.9119
Validation Loss: 1.1260, Accuracy: 0.7058, Precision: 0.6785, Recall: 0.6173, F1: 0.6349
Testing Loss: 1.0746, Accuracy: 0.7210, Precision: 0.6871, Recall: 0.6232, F1: 0.6358
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6462, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7661, F1: 0.7525
Epoch 5/70
Train Loss: 0.1351, Accuracy: 0.9542, Precision: 0.9272, Recall: 0.9305, F1: 0.9288
Validation Loss: 1.1737, Accuracy: 0.7292, Precision: 0.6361, Recall: 0.5972, F1: 0.6045
Testing Loss: 1.0902, Accuracy: 0.7210, Precision: 0.6269, Recall: 0.5922, F1: 0.5985
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 4, 3, 0, 0, 0, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7657, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7189, F1: 0.7109
Epoch 6/70
Train Loss: 0.1279, Accuracy: 0.9526, Precision: 0.9290, Recall: 0.9279, F1: 0.9284
Validation Loss: 1.3912, Accuracy: 0.7143, Precision: 0.6697, Recall: 0.6225, F1: 0.6308
Testing Loss: 1.2494, Accuracy: 0.7089, Precision: 0.6818, Recall: 0.6441, F1: 0.6495
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 5, 4, 0, 0, 0, 2, 5, 3, 5, 2, 4, 5, 3, 5, 5, 3, 1, 1, 2, 5, 4, 2, 0, 4, 3, 3, 2, 5, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6651, Accuracy: 0.7381, Precision: 0.7708, Recall: 0.6463, F1: 0.6768
Epoch 7/70
Train Loss: 0.1458, Accuracy: 0.9447, Precision: 0.9175, Recall: 0.9232, F1: 0.9202
Validation Loss: 1.4166, Accuracy: 0.7292, Precision: 0.6525, Recall: 0.6324, F1: 0.6341
Testing Loss: 1.4153, Accuracy: 0.7428, Precision: 0.6812, Recall: 0.6546, F1: 0.6585
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 3, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3537, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8711, F1: 0.8444
Epoch 8/70
Train Loss: 0.1456, Accuracy: 0.9466, Precision: 0.9226, Recall: 0.9217, F1: 0.9221
Validation Loss: 1.1046, Accuracy: 0.7207, Precision: 0.6486, Recall: 0.6114, F1: 0.6151
Testing Loss: 1.0670, Accuracy: 0.7186, Precision: 0.6687, Recall: 0.6289, F1: 0.6399
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 3, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4469, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8728, F1: 0.8487
Epoch 9/70
Train Loss: 0.0904, Accuracy: 0.9597, Precision: 0.9359, Recall: 0.9348, F1: 0.9353
Validation Loss: 1.8726, Accuracy: 0.7484, Precision: 0.6699, Recall: 0.6318, F1: 0.6401
Testing Loss: 1.8609, Accuracy: 0.7633, Precision: 0.7142, Recall: 0.6565, F1: 0.6704
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5318, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7683, F1: 0.7566
Epoch 10/70
Train Loss: 0.1025, Accuracy: 0.9557, Precision: 0.9318, Recall: 0.9319, F1: 0.9319
Validation Loss: 1.3204, Accuracy: 0.7399, Precision: 0.6726, Recall: 0.6405, F1: 0.6494
Testing Loss: 1.3115, Accuracy: 0.7633, Precision: 0.7163, Recall: 0.6762, F1: 0.6813
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6728, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7661, F1: 0.7542
Epoch 11/70
Train Loss: 0.0863, Accuracy: 0.9611, Precision: 0.9406, Recall: 0.9365, F1: 0.9385
Validation Loss: 1.7607, Accuracy: 0.7313, Precision: 0.6661, Recall: 0.6768, F1: 0.6649
Testing Loss: 1.6661, Accuracy: 0.7536, Precision: 0.6937, Recall: 0.7040, F1: 0.6917
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 5, 4, 3, 3, 3, 4, 3, 1, 5, 2, 4, 4, 2, 0, 4, 3, 2, 2, 5, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6409, Accuracy: 0.8571, Precision: 0.7649, Recall: 0.7255, F1: 0.7345
Epoch 12/70
Train Loss: 0.1040, Accuracy: 0.9547, Precision: 0.9302, Recall: 0.9312, F1: 0.9307
Validation Loss: 1.3743, Accuracy: 0.7505, Precision: 0.7078, Recall: 0.6629, F1: 0.6765
Testing Loss: 1.3041, Accuracy: 0.7524, Precision: 0.7091, Recall: 0.6727, F1: 0.6856
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 1, 2, 5, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4334, Accuracy: 0.8095, Precision: 0.7146, Recall: 0.6926, F1: 0.6771
Epoch 13/70
Train Loss: 0.1007, Accuracy: 0.9554, Precision: 0.9314, Recall: 0.9311, F1: 0.9312
Validation Loss: 1.4594, Accuracy: 0.7292, Precision: 0.6637, Recall: 0.6080, F1: 0.6225
Testing Loss: 1.3754, Accuracy: 0.7331, Precision: 0.6836, Recall: 0.6095, F1: 0.6265
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 0, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7543, Accuracy: 0.6667, Precision: 0.8526, Recall: 0.7011, F1: 0.6957
Epoch 14/70
Train Loss: 0.0860, Accuracy: 0.9632, Precision: 0.9432, Recall: 0.9383, F1: 0.9407
Validation Loss: 1.7923, Accuracy: 0.7761, Precision: 0.7006, Recall: 0.6969, F1: 0.6949
Testing Loss: 1.3997, Accuracy: 0.7729, Precision: 0.7024, Recall: 0.6970, F1: 0.6969
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3007, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8350, F1: 0.8125
Epoch 15/70
Train Loss: 0.0623, Accuracy: 0.9656, Precision: 0.9456, Recall: 0.9446, F1: 0.9451
Validation Loss: 2.1433, Accuracy: 0.7484, Precision: 0.7121, Recall: 0.6377, F1: 0.6548
Testing Loss: 1.6633, Accuracy: 0.7729, Precision: 0.7170, Recall: 0.6707, F1: 0.6874
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4904, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 16/70
Train Loss: 0.0575, Accuracy: 0.9687, Precision: 0.9494, Recall: 0.9496, F1: 0.9495
Validation Loss: 2.1511, Accuracy: 0.7569, Precision: 0.6929, Recall: 0.6856, F1: 0.6825
Testing Loss: 1.6706, Accuracy: 0.7597, Precision: 0.6846, Recall: 0.6827, F1: 0.6763
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5591, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 17/70
Train Loss: 0.0648, Accuracy: 0.9692, Precision: 0.9517, Recall: 0.9479, F1: 0.9497
Validation Loss: 1.9434, Accuracy: 0.7633, Precision: 0.6790, Recall: 0.6659, F1: 0.6656
Testing Loss: 1.5395, Accuracy: 0.7766, Precision: 0.7125, Recall: 0.6872, F1: 0.6875
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2409, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8528, F1: 0.8320
Epoch 18/70
Train Loss: 0.0602, Accuracy: 0.9704, Precision: 0.9545, Recall: 0.9528, F1: 0.9537
Validation Loss: 1.9168, Accuracy: 0.7356, Precision: 0.7099, Recall: 0.6525, F1: 0.6684
Testing Loss: 1.6824, Accuracy: 0.7548, Precision: 0.7123, Recall: 0.6754, F1: 0.6858
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2678, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 19/70
Train Loss: 0.1368, Accuracy: 0.9440, Precision: 0.9170, Recall: 0.9221, F1: 0.9195
Validation Loss: 1.7008, Accuracy: 0.7249, Precision: 0.6916, Recall: 0.6120, F1: 0.6197
Testing Loss: 1.5321, Accuracy: 0.7391, Precision: 0.6997, Recall: 0.6418, F1: 0.6551
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0439, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7683, F1: 0.7566
Epoch 20/70
Train Loss: 0.1334, Accuracy: 0.9519, Precision: 0.9258, Recall: 0.9260, F1: 0.9259
Validation Loss: 0.9512, Accuracy: 0.7591, Precision: 0.6909, Recall: 0.6423, F1: 0.6532
Testing Loss: 0.8847, Accuracy: 0.7560, Precision: 0.6914, Recall: 0.6550, F1: 0.6674
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4174, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7890
Epoch 21/70
Train Loss: 0.0715, Accuracy: 0.9649, Precision: 0.9455, Recall: 0.9392, F1: 0.9422
Validation Loss: 1.3556, Accuracy: 0.7761, Precision: 0.7175, Recall: 0.7070, F1: 0.7110
Testing Loss: 1.2414, Accuracy: 0.7693, Precision: 0.7114, Recall: 0.6993, F1: 0.7042
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1884, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 22/70
Train Loss: 0.0595, Accuracy: 0.9696, Precision: 0.9509, Recall: 0.9497, F1: 0.9503
Validation Loss: 1.4223, Accuracy: 0.7804, Precision: 0.7040, Recall: 0.6988, F1: 0.6977
Testing Loss: 1.3724, Accuracy: 0.7669, Precision: 0.6885, Recall: 0.6708, F1: 0.6700
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2532, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8883, F1: 0.8695
Epoch 23/70
Train Loss: 0.0534, Accuracy: 0.9713, Precision: 0.9541, Recall: 0.9506, F1: 0.9523
Validation Loss: 1.4633, Accuracy: 0.7761, Precision: 0.7081, Recall: 0.6961, F1: 0.6992
Testing Loss: 1.4113, Accuracy: 0.7754, Precision: 0.7218, Recall: 0.7086, F1: 0.7135
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2647, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 24/70
Train Loss: 0.0494, Accuracy: 0.9718, Precision: 0.9554, Recall: 0.9526, F1: 0.9540
Validation Loss: 1.5758, Accuracy: 0.7740, Precision: 0.7087, Recall: 0.6957, F1: 0.6985
Testing Loss: 1.5239, Accuracy: 0.7705, Precision: 0.7156, Recall: 0.7051, F1: 0.7086
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1956, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 25/70
Train Loss: 0.0511, Accuracy: 0.9706, Precision: 0.9522, Recall: 0.9525, F1: 0.9523
Validation Loss: 1.6272, Accuracy: 0.7761, Precision: 0.7031, Recall: 0.6809, F1: 0.6830
Testing Loss: 1.4712, Accuracy: 0.7705, Precision: 0.7309, Recall: 0.6939, F1: 0.7066
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3117, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7956, F1: 0.7750
Epoch 26/70
Train Loss: 0.0510, Accuracy: 0.9694, Precision: 0.9496, Recall: 0.9508, F1: 0.9502
Validation Loss: 1.7242, Accuracy: 0.7719, Precision: 0.6906, Recall: 0.6891, F1: 0.6858
Testing Loss: 1.6283, Accuracy: 0.7693, Precision: 0.7012, Recall: 0.6882, F1: 0.6886
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2392, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 27/70
Train Loss: 0.0564, Accuracy: 0.9668, Precision: 0.9481, Recall: 0.9409, F1: 0.9444
Validation Loss: 1.6114, Accuracy: 0.7271, Precision: 0.6871, Recall: 0.6604, F1: 0.6667
Testing Loss: 1.5525, Accuracy: 0.7500, Precision: 0.6973, Recall: 0.6715, F1: 0.6773
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5102, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 28/70
Train Loss: 0.1377, Accuracy: 0.9481, Precision: 0.9253, Recall: 0.9280, F1: 0.9266
Validation Loss: 1.2420, Accuracy: 0.7655, Precision: 0.7363, Recall: 0.6879, F1: 0.6992
Testing Loss: 1.1394, Accuracy: 0.7717, Precision: 0.7277, Recall: 0.6990, F1: 0.7098
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 0, 2, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6550, Accuracy: 0.7381, Precision: 0.6994, Recall: 0.6259, F1: 0.6231
Epoch 29/70
Train Loss: 0.0962, Accuracy: 0.9621, Precision: 0.9405, Recall: 0.9422, F1: 0.9413
Validation Loss: 1.0749, Accuracy: 0.7697, Precision: 0.6859, Recall: 0.6559, F1: 0.6631
Testing Loss: 1.0718, Accuracy: 0.7609, Precision: 0.6803, Recall: 0.6495, F1: 0.6541
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4559, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8111, F1: 0.7876
Epoch 30/70
Train Loss: 0.0739, Accuracy: 0.9604, Precision: 0.9391, Recall: 0.9347, F1: 0.9369
Validation Loss: 1.3614, Accuracy: 0.7761, Precision: 0.6909, Recall: 0.7021, F1: 0.6926
Testing Loss: 1.2613, Accuracy: 0.7742, Precision: 0.7263, Recall: 0.7219, F1: 0.7203
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1494, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 31/70
Train Loss: 0.0683, Accuracy: 0.9640, Precision: 0.9425, Recall: 0.9407, F1: 0.9416
Validation Loss: 1.4321, Accuracy: 0.7804, Precision: 0.7290, Recall: 0.6969, F1: 0.7050
Testing Loss: 1.2941, Accuracy: 0.7766, Precision: 0.7236, Recall: 0.6938, F1: 0.7041
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3219, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 32/70
Train Loss: 0.0540, Accuracy: 0.9699, Precision: 0.9507, Recall: 0.9501, F1: 0.9504
Validation Loss: 1.4677, Accuracy: 0.7761, Precision: 0.7424, Recall: 0.6841, F1: 0.7026
Testing Loss: 1.2987, Accuracy: 0.7790, Precision: 0.7424, Recall: 0.7047, F1: 0.7199
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3832, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 33/70
Train Loss: 0.0529, Accuracy: 0.9704, Precision: 0.9528, Recall: 0.9508, F1: 0.9518
Validation Loss: 1.4725, Accuracy: 0.7761, Precision: 0.7025, Recall: 0.7081, F1: 0.7020
Testing Loss: 1.2847, Accuracy: 0.7717, Precision: 0.7136, Recall: 0.7169, F1: 0.7147
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1597, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0523, Accuracy: 0.9680, Precision: 0.9492, Recall: 0.9494, F1: 0.9493
Validation Loss: 1.5283, Accuracy: 0.7761, Precision: 0.7111, Recall: 0.6782, F1: 0.6883
Testing Loss: 1.4250, Accuracy: 0.7778, Precision: 0.7258, Recall: 0.6998, F1: 0.7082
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3483, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 35/70
Train Loss: 0.0512, Accuracy: 0.9640, Precision: 0.9426, Recall: 0.9351, F1: 0.9386
Validation Loss: 1.5066, Accuracy: 0.7846, Precision: 0.7412, Recall: 0.7149, F1: 0.7255
Testing Loss: 1.4034, Accuracy: 0.7802, Precision: 0.7268, Recall: 0.7171, F1: 0.7204
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2805, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 36/70
Train Loss: 0.0490, Accuracy: 0.9706, Precision: 0.9544, Recall: 0.9516, F1: 0.9530
Validation Loss: 1.6340, Accuracy: 0.7804, Precision: 0.7191, Recall: 0.6855, F1: 0.6958
Testing Loss: 1.5383, Accuracy: 0.7742, Precision: 0.7285, Recall: 0.6947, F1: 0.7059
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3117, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 37/70
Train Loss: 0.0491, Accuracy: 0.9704, Precision: 0.9501, Recall: 0.9556, F1: 0.9527
Validation Loss: 1.6831, Accuracy: 0.7932, Precision: 0.7377, Recall: 0.7191, F1: 0.7262
Testing Loss: 1.6036, Accuracy: 0.7874, Precision: 0.7361, Recall: 0.7301, F1: 0.7313
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2539, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 38/70
Train Loss: 0.0511, Accuracy: 0.9701, Precision: 0.9507, Recall: 0.9542, F1: 0.9524
Validation Loss: 1.5635, Accuracy: 0.7953, Precision: 0.7513, Recall: 0.7009, F1: 0.7182
Testing Loss: 1.4952, Accuracy: 0.7826, Precision: 0.7463, Recall: 0.7029, F1: 0.7197
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1846, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 39/70
Train Loss: 0.0528, Accuracy: 0.9699, Precision: 0.9546, Recall: 0.9466, F1: 0.9505
Validation Loss: 1.5534, Accuracy: 0.7889, Precision: 0.7165, Recall: 0.7233, F1: 0.7186
Testing Loss: 1.4600, Accuracy: 0.7766, Precision: 0.7194, Recall: 0.7258, F1: 0.7213
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1258, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 40/70
Train Loss: 0.0477, Accuracy: 0.9673, Precision: 0.9482, Recall: 0.9475, F1: 0.9479
Validation Loss: 1.6838, Accuracy: 0.8017, Precision: 0.7526, Recall: 0.7261, F1: 0.7373
Testing Loss: 1.6134, Accuracy: 0.7754, Precision: 0.7288, Recall: 0.7129, F1: 0.7164
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3412, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 41/70
Train Loss: 0.0491, Accuracy: 0.9704, Precision: 0.9540, Recall: 0.9516, F1: 0.9528
Validation Loss: 1.7683, Accuracy: 0.8017, Precision: 0.7596, Recall: 0.7375, F1: 0.7465
Testing Loss: 1.7461, Accuracy: 0.7766, Precision: 0.7235, Recall: 0.7215, F1: 0.7199
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3430, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 42/70
Train Loss: 0.0594, Accuracy: 0.9644, Precision: 0.9399, Recall: 0.9464, F1: 0.9431
Validation Loss: 1.2267, Accuracy: 0.7846, Precision: 0.7240, Recall: 0.6726, F1: 0.6892
Testing Loss: 1.2636, Accuracy: 0.7669, Precision: 0.7271, Recall: 0.6772, F1: 0.6924
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2730, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8533, F1: 0.8276
Epoch 43/70
Train Loss: 0.1297, Accuracy: 0.9545, Precision: 0.9302, Recall: 0.9369, F1: 0.9334
Validation Loss: 1.3260, Accuracy: 0.7441, Precision: 0.6757, Recall: 0.6586, F1: 0.6591
Testing Loss: 1.3302, Accuracy: 0.7560, Precision: 0.7137, Recall: 0.6707, F1: 0.6802
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 0, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 3, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3129, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8628, F1: 0.8437
Epoch 44/70
Train Loss: 0.0897, Accuracy: 0.9621, Precision: 0.9430, Recall: 0.9365, F1: 0.9396
Validation Loss: 1.0792, Accuracy: 0.7633, Precision: 0.7248, Recall: 0.6906, F1: 0.7003
Testing Loss: 1.0891, Accuracy: 0.7693, Precision: 0.7334, Recall: 0.7075, F1: 0.7184
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1471, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9778, F1: 0.9701
Epoch 45/70
Train Loss: 0.0608, Accuracy: 0.9649, Precision: 0.9429, Recall: 0.9472, F1: 0.9450
Validation Loss: 1.5903, Accuracy: 0.7484, Precision: 0.7087, Recall: 0.6751, F1: 0.6779
Testing Loss: 1.4932, Accuracy: 0.7681, Precision: 0.7355, Recall: 0.7073, F1: 0.7133
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 0, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2307, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8667
Epoch 46/70
Train Loss: 0.0569, Accuracy: 0.9694, Precision: 0.9524, Recall: 0.9493, F1: 0.9508
Validation Loss: 1.4414, Accuracy: 0.7676, Precision: 0.7267, Recall: 0.7070, F1: 0.7100
Testing Loss: 1.4586, Accuracy: 0.7778, Precision: 0.7415, Recall: 0.7263, F1: 0.7303
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2685, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 47/70
Train Loss: 0.0510, Accuracy: 0.9682, Precision: 0.9483, Recall: 0.9501, F1: 0.9492
Validation Loss: 1.3972, Accuracy: 0.7804, Precision: 0.7139, Recall: 0.6888, F1: 0.6957
Testing Loss: 1.4295, Accuracy: 0.7850, Precision: 0.7531, Recall: 0.7079, F1: 0.7220
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2141, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 48/70
Train Loss: 0.0503, Accuracy: 0.9701, Precision: 0.9525, Recall: 0.9541, F1: 0.9533
Validation Loss: 1.3397, Accuracy: 0.7868, Precision: 0.7355, Recall: 0.7152, F1: 0.7212
Testing Loss: 1.3484, Accuracy: 0.7874, Precision: 0.7500, Recall: 0.7265, F1: 0.7362
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1102, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 49/70
Train Loss: 0.0483, Accuracy: 0.9696, Precision: 0.9516, Recall: 0.9516, F1: 0.9516
Validation Loss: 1.4820, Accuracy: 0.7846, Precision: 0.7314, Recall: 0.7207, F1: 0.7242
Testing Loss: 1.4878, Accuracy: 0.7790, Precision: 0.7372, Recall: 0.7254, F1: 0.7303
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2996, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 50/70
Train Loss: 0.0459, Accuracy: 0.9701, Precision: 0.9501, Recall: 0.9546, F1: 0.9522
Validation Loss: 1.5315, Accuracy: 0.7761, Precision: 0.6973, Recall: 0.6923, F1: 0.6920
Testing Loss: 1.5273, Accuracy: 0.7850, Precision: 0.7443, Recall: 0.7144, F1: 0.7240
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1382, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 51/70
Train Loss: 0.0451, Accuracy: 0.9708, Precision: 0.9548, Recall: 0.9511, F1: 0.9529
Validation Loss: 1.5882, Accuracy: 0.7889, Precision: 0.7283, Recall: 0.7007, F1: 0.7091
Testing Loss: 1.5899, Accuracy: 0.7802, Precision: 0.7443, Recall: 0.7043, F1: 0.7199
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4124, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 52/70
Train Loss: 0.0456, Accuracy: 0.9708, Precision: 0.9551, Recall: 0.9522, F1: 0.9536
Validation Loss: 1.6117, Accuracy: 0.7868, Precision: 0.7286, Recall: 0.7213, F1: 0.7230
Testing Loss: 1.6013, Accuracy: 0.7899, Precision: 0.7497, Recall: 0.7329, F1: 0.7395
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2037, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 53/70
Train Loss: 0.0494, Accuracy: 0.9696, Precision: 0.9514, Recall: 0.9545, F1: 0.9529
Validation Loss: 1.4709, Accuracy: 0.7804, Precision: 0.6873, Recall: 0.6887, F1: 0.6841
Testing Loss: 1.4571, Accuracy: 0.7729, Precision: 0.7169, Recall: 0.6833, F1: 0.6873
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1874, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 54/70
Train Loss: 0.0481, Accuracy: 0.9704, Precision: 0.9568, Recall: 0.9463, F1: 0.9512
Validation Loss: 1.5940, Accuracy: 0.7974, Precision: 0.7418, Recall: 0.7170, F1: 0.7252
Testing Loss: 1.5179, Accuracy: 0.7826, Precision: 0.7505, Recall: 0.7100, F1: 0.7258
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2476, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7911, F1: 0.7723
Epoch 55/70
Train Loss: 0.0470, Accuracy: 0.9689, Precision: 0.9517, Recall: 0.9479, F1: 0.9498
Validation Loss: 1.4271, Accuracy: 0.7868, Precision: 0.7120, Recall: 0.6926, F1: 0.6976
Testing Loss: 1.3621, Accuracy: 0.7874, Precision: 0.7540, Recall: 0.7085, F1: 0.7252
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2907, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 56/70
Train Loss: 0.0466, Accuracy: 0.9692, Precision: 0.9508, Recall: 0.9450, F1: 0.9478
Validation Loss: 1.5941, Accuracy: 0.7953, Precision: 0.7408, Recall: 0.7156, F1: 0.7244
Testing Loss: 1.5313, Accuracy: 0.7874, Precision: 0.7546, Recall: 0.7198, F1: 0.7340
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3562, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 57/70
Train Loss: 0.0504, Accuracy: 0.9666, Precision: 0.9464, Recall: 0.9471, F1: 0.9468
Validation Loss: 1.3486, Accuracy: 0.7783, Precision: 0.7239, Recall: 0.7066, F1: 0.7109
Testing Loss: 1.2394, Accuracy: 0.7790, Precision: 0.7262, Recall: 0.7054, F1: 0.7142
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3300, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7483, F1: 0.7376
Epoch 58/70
Train Loss: 0.0639, Accuracy: 0.9654, Precision: 0.9500, Recall: 0.9464, F1: 0.9482
Validation Loss: 1.7679, Accuracy: 0.7527, Precision: 0.6694, Recall: 0.7045, F1: 0.6794
Testing Loss: 1.4720, Accuracy: 0.7560, Precision: 0.6982, Recall: 0.7123, F1: 0.6964
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2684, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9356, F1: 0.9183
Epoch 59/70
Train Loss: 0.1291, Accuracy: 0.9493, Precision: 0.9272, Recall: 0.9237, F1: 0.9253
Validation Loss: 1.2415, Accuracy: 0.7633, Precision: 0.7319, Recall: 0.6788, F1: 0.6947
Testing Loss: 1.0980, Accuracy: 0.7838, Precision: 0.7504, Recall: 0.7095, F1: 0.7254
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 1, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2707, Accuracy: 0.9048, Precision: 0.9050, Recall: 0.9200, F1: 0.8933
Epoch 60/70
Train Loss: 0.0882, Accuracy: 0.9625, Precision: 0.9418, Recall: 0.9359, F1: 0.9388
Validation Loss: 1.3873, Accuracy: 0.7740, Precision: 0.7179, Recall: 0.6974, F1: 0.7029
Testing Loss: 1.1603, Accuracy: 0.7862, Precision: 0.7347, Recall: 0.7198, F1: 0.7257
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 3, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3775, Accuracy: 0.9048, Precision: 0.9068, Recall: 0.9106, F1: 0.8955
Epoch 61/70
Train Loss: 0.0667, Accuracy: 0.9677, Precision: 0.9515, Recall: 0.9461, F1: 0.9487
Validation Loss: 1.1451, Accuracy: 0.7676, Precision: 0.7181, Recall: 0.7016, F1: 0.7067
Testing Loss: 1.0033, Accuracy: 0.7899, Precision: 0.7417, Recall: 0.7269, F1: 0.7331
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1722, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 62/70
Train Loss: 0.0644, Accuracy: 0.9694, Precision: 0.9496, Recall: 0.9485, F1: 0.9490
Validation Loss: 1.4175, Accuracy: 0.7612, Precision: 0.7233, Recall: 0.6822, F1: 0.6946
Testing Loss: 1.2605, Accuracy: 0.7911, Precision: 0.7532, Recall: 0.7168, F1: 0.7307
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3253, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 63/70
Train Loss: 0.0641, Accuracy: 0.9677, Precision: 0.9482, Recall: 0.9522, F1: 0.9500
Validation Loss: 1.4859, Accuracy: 0.7719, Precision: 0.7232, Recall: 0.6745, F1: 0.6890
Testing Loss: 1.4450, Accuracy: 0.7766, Precision: 0.7330, Recall: 0.6678, F1: 0.6826
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6167, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7911, F1: 0.7706
Epoch 64/70
Train Loss: 0.0643, Accuracy: 0.9659, Precision: 0.9458, Recall: 0.9423, F1: 0.9440
Validation Loss: 1.4326, Accuracy: 0.7527, Precision: 0.7192, Recall: 0.6781, F1: 0.6914
Testing Loss: 1.3369, Accuracy: 0.7729, Precision: 0.7512, Recall: 0.6880, F1: 0.7084
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 3, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3160, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8756, F1: 0.8488
Epoch 65/70
Train Loss: 0.0653, Accuracy: 0.9692, Precision: 0.9510, Recall: 0.9508, F1: 0.9509
Validation Loss: 1.3328, Accuracy: 0.7633, Precision: 0.7042, Recall: 0.6705, F1: 0.6788
Testing Loss: 1.3418, Accuracy: 0.7705, Precision: 0.6882, Recall: 0.6602, F1: 0.6633
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4669, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8333, F1: 0.8106
Epoch 66/70
Train Loss: 0.0541, Accuracy: 0.9706, Precision: 0.9533, Recall: 0.9525, F1: 0.9529
Validation Loss: 1.5412, Accuracy: 0.7740, Precision: 0.7215, Recall: 0.7007, F1: 0.7070
Testing Loss: 1.4841, Accuracy: 0.7826, Precision: 0.7405, Recall: 0.6975, F1: 0.7102
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3451, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8556, F1: 0.8321
Epoch 67/70
Train Loss: 0.0539, Accuracy: 0.9680, Precision: 0.9470, Recall: 0.9502, F1: 0.9486
Validation Loss: 1.6891, Accuracy: 0.7441, Precision: 0.7204, Recall: 0.6500, F1: 0.6683
Testing Loss: 1.6074, Accuracy: 0.7766, Precision: 0.7529, Recall: 0.6927, F1: 0.7133
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3490, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8778, F1: 0.8527
Epoch 68/70
Train Loss: 0.0546, Accuracy: 0.9682, Precision: 0.9478, Recall: 0.9506, F1: 0.9491
Validation Loss: 1.6512, Accuracy: 0.7612, Precision: 0.6932, Recall: 0.6937, F1: 0.6901
Testing Loss: 1.5799, Accuracy: 0.7802, Precision: 0.7269, Recall: 0.7173, F1: 0.7186
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 3, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3085, Accuracy: 0.9286, Precision: 0.9247, Recall: 0.9306, F1: 0.9188
Epoch 69/70
Train Loss: 0.0513, Accuracy: 0.9706, Precision: 0.9508, Recall: 0.9561, F1: 0.9534
Validation Loss: 1.9965, Accuracy: 0.7569, Precision: 0.7159, Recall: 0.6649, F1: 0.6772
Testing Loss: 1.9750, Accuracy: 0.7609, Precision: 0.7324, Recall: 0.6576, F1: 0.6730
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3701, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 70/70
Train Loss: 0.0544, Accuracy: 0.9673, Precision: 0.9507, Recall: 0.9452, F1: 0.9479
Validation Loss: 1.5044, Accuracy: 0.7591, Precision: 0.7040, Recall: 0.6920, F1: 0.6945
Testing Loss: 1.4664, Accuracy: 0.7621, Precision: 0.7121, Recall: 0.6957, F1: 0.6993
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2214, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9378, F1: 0.9210
For middle layers:  [8, 9, 10, 11, 12, 13, 14, 15]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.6528, Accuracy: 0.8331, Precision: 0.7816, Recall: 0.7646, F1: 0.7723
Validation Loss: 0.3922, Accuracy: 0.8913, Precision: 0.8739, Recall: 0.8189, F1: 0.8384
Testing Loss: 0.3252, Accuracy: 0.8889, Precision: 0.8697, Recall: 0.8173, F1: 0.8339
LM Predictions:  [0, 5, 0, 0, 4, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 2, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3429, Accuracy: 0.1190, Precision: 0.0859, Recall: 0.1542, F1: 0.0797
Epoch 2/70
Train Loss: 0.2331, Accuracy: 0.9244, Precision: 0.8879, Recall: 0.8848, F1: 0.8863
Validation Loss: 0.5207, Accuracy: 0.8827, Precision: 0.8570, Recall: 0.7953, F1: 0.8090
Testing Loss: 0.4352, Accuracy: 0.8659, Precision: 0.8504, Recall: 0.7780, F1: 0.7946
LM Predictions:  [0, 1, 2, 1, 1, 5, 0, 4, 4, 5, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 2, 5, 0, 0, 0, 0, 2, 0, 4, 5, 0, 0, 0, 0, 1, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1738, Accuracy: 0.4762, Precision: 0.6740, Recall: 0.4282, F1: 0.4493
Epoch 3/70
Train Loss: 0.1687, Accuracy: 0.9407, Precision: 0.9097, Recall: 0.9035, F1: 0.9065
Validation Loss: 0.6847, Accuracy: 0.8849, Precision: 0.8531, Recall: 0.8304, F1: 0.8400
Testing Loss: 0.5163, Accuracy: 0.8732, Precision: 0.8488, Recall: 0.8239, F1: 0.8349
LM Predictions:  [0, 1, 2, 5, 1, 1, 0, 4, 4, 0, 5, 0, 2, 0, 3, 5, 2, 4, 0, 3, 0, 5, 3, 1, 5, 0, 5, 5, 2, 0, 4, 3, 3, 5, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9031, Accuracy: 0.6667, Precision: 0.7424, Recall: 0.5843, F1: 0.6161
Epoch 4/70
Train Loss: 0.1273, Accuracy: 0.9533, Precision: 0.9290, Recall: 0.9267, F1: 0.9278
Validation Loss: 0.7601, Accuracy: 0.8614, Precision: 0.8177, Recall: 0.7551, F1: 0.7692
Testing Loss: 0.5909, Accuracy: 0.8659, Precision: 0.8459, Recall: 0.7694, F1: 0.7823
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 0, 3, 0, 0, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9563, Accuracy: 0.6429, Precision: 0.7105, Recall: 0.5681, F1: 0.5567
Epoch 5/70
Train Loss: 0.1266, Accuracy: 0.9523, Precision: 0.9272, Recall: 0.9268, F1: 0.9269
Validation Loss: 0.9572, Accuracy: 0.8550, Precision: 0.8033, Recall: 0.7796, F1: 0.7888
Testing Loss: 0.7593, Accuracy: 0.8696, Precision: 0.8415, Recall: 0.8013, F1: 0.8155
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5410, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8083, F1: 0.7944
Epoch 6/70
Train Loss: 0.1055, Accuracy: 0.9557, Precision: 0.9321, Recall: 0.9336, F1: 0.9328
Validation Loss: 0.9034, Accuracy: 0.8422, Precision: 0.7877, Recall: 0.7315, F1: 0.7465
Testing Loss: 0.7456, Accuracy: 0.8490, Precision: 0.8527, Recall: 0.7479, F1: 0.7738
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 2, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7713, Accuracy: 0.8095, Precision: 0.8611, Recall: 0.8311, F1: 0.7987
Epoch 7/70
Train Loss: 0.0984, Accuracy: 0.9571, Precision: 0.9373, Recall: 0.9312, F1: 0.9342
Validation Loss: 1.1840, Accuracy: 0.8635, Precision: 0.8286, Recall: 0.7928, F1: 0.8075
Testing Loss: 0.8797, Accuracy: 0.8744, Precision: 0.8425, Recall: 0.8212, F1: 0.8309
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5147, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 8/70
Train Loss: 0.0716, Accuracy: 0.9630, Precision: 0.9396, Recall: 0.9389, F1: 0.9392
Validation Loss: 1.0544, Accuracy: 0.8678, Precision: 0.8306, Recall: 0.7841, F1: 0.8011
Testing Loss: 0.7803, Accuracy: 0.8708, Precision: 0.8408, Recall: 0.7793, F1: 0.7998
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6410, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7706, F1: 0.7569
Epoch 9/70
Train Loss: 0.0721, Accuracy: 0.9635, Precision: 0.9433, Recall: 0.9419, F1: 0.9426
Validation Loss: 1.0994, Accuracy: 0.8571, Precision: 0.8123, Recall: 0.8050, F1: 0.8081
Testing Loss: 0.9218, Accuracy: 0.8671, Precision: 0.8404, Recall: 0.8277, F1: 0.8322
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 5, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5323, Accuracy: 0.7381, Precision: 0.7262, Recall: 0.6421, F1: 0.6391
Epoch 10/70
Train Loss: 0.0869, Accuracy: 0.9618, Precision: 0.9418, Recall: 0.9441, F1: 0.9429
Validation Loss: 0.8526, Accuracy: 0.8486, Precision: 0.8155, Recall: 0.7717, F1: 0.7887
Testing Loss: 0.6327, Accuracy: 0.8684, Precision: 0.8489, Recall: 0.7880, F1: 0.8095
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4835, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 11/70
Train Loss: 0.0661, Accuracy: 0.9673, Precision: 0.9488, Recall: 0.9462, F1: 0.9475
Validation Loss: 1.4612, Accuracy: 0.8486, Precision: 0.8151, Recall: 0.7634, F1: 0.7814
Testing Loss: 1.1870, Accuracy: 0.8623, Precision: 0.8325, Recall: 0.7696, F1: 0.7823
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3914, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 12/70
Train Loss: 0.0635, Accuracy: 0.9685, Precision: 0.9535, Recall: 0.9448, F1: 0.9489
Validation Loss: 1.3121, Accuracy: 0.8614, Precision: 0.8276, Recall: 0.7992, F1: 0.8110
Testing Loss: 1.0409, Accuracy: 0.8684, Precision: 0.8293, Recall: 0.8039, F1: 0.8146
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 0, 4, 2, 2, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4413, Accuracy: 0.8095, Precision: 0.8477, Recall: 0.8128, F1: 0.8027
Epoch 13/70
Train Loss: 0.0626, Accuracy: 0.9687, Precision: 0.9496, Recall: 0.9528, F1: 0.9511
Validation Loss: 1.5083, Accuracy: 0.8593, Precision: 0.8272, Recall: 0.7828, F1: 0.8004
Testing Loss: 1.1017, Accuracy: 0.8732, Precision: 0.8415, Recall: 0.8003, F1: 0.8140
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2596, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9578, F1: 0.9444
Epoch 14/70
Train Loss: 0.0568, Accuracy: 0.9701, Precision: 0.9552, Recall: 0.9515, F1: 0.9533
Validation Loss: 1.7269, Accuracy: 0.8486, Precision: 0.8182, Recall: 0.7565, F1: 0.7773
Testing Loss: 1.3506, Accuracy: 0.8671, Precision: 0.8493, Recall: 0.7841, F1: 0.8038
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3093, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 15/70
Train Loss: 0.0590, Accuracy: 0.9687, Precision: 0.9504, Recall: 0.9477, F1: 0.9490
Validation Loss: 1.5583, Accuracy: 0.8529, Precision: 0.8209, Recall: 0.7785, F1: 0.7945
Testing Loss: 1.1871, Accuracy: 0.8684, Precision: 0.8333, Recall: 0.8133, F1: 0.8221
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4384, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 16/70
Train Loss: 0.0551, Accuracy: 0.9687, Precision: 0.9484, Recall: 0.9517, F1: 0.9500
Validation Loss: 1.6486, Accuracy: 0.8529, Precision: 0.8228, Recall: 0.7625, F1: 0.7833
Testing Loss: 1.3344, Accuracy: 0.8720, Precision: 0.8530, Recall: 0.7897, F1: 0.8073
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4529, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 17/70
Train Loss: 0.0544, Accuracy: 0.9677, Precision: 0.9447, Recall: 0.9510, F1: 0.9478
Validation Loss: 1.6899, Accuracy: 0.8550, Precision: 0.8170, Recall: 0.7641, F1: 0.7818
Testing Loss: 1.3960, Accuracy: 0.8659, Precision: 0.8395, Recall: 0.7811, F1: 0.7955
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2418, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 18/70
Train Loss: 0.0539, Accuracy: 0.9706, Precision: 0.9527, Recall: 0.9521, F1: 0.9524
Validation Loss: 1.7553, Accuracy: 0.8593, Precision: 0.8127, Recall: 0.7773, F1: 0.7896
Testing Loss: 1.4473, Accuracy: 0.8647, Precision: 0.8291, Recall: 0.7850, F1: 0.7966
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3882, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 19/70
Train Loss: 0.0546, Accuracy: 0.9689, Precision: 0.9520, Recall: 0.9466, F1: 0.9492
Validation Loss: 1.8466, Accuracy: 0.8507, Precision: 0.8063, Recall: 0.7543, F1: 0.7703
Testing Loss: 1.4845, Accuracy: 0.8684, Precision: 0.8354, Recall: 0.7828, F1: 0.7948
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2426, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 20/70
Train Loss: 0.0521, Accuracy: 0.9711, Precision: 0.9560, Recall: 0.9482, F1: 0.9519
Validation Loss: 1.6994, Accuracy: 0.8635, Precision: 0.8343, Recall: 0.7993, F1: 0.8137
Testing Loss: 1.3749, Accuracy: 0.8696, Precision: 0.8337, Recall: 0.8163, F1: 0.8243
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2241, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9800, F1: 0.9713
Epoch 21/70
Train Loss: 0.0514, Accuracy: 0.9685, Precision: 0.9477, Recall: 0.9514, F1: 0.9495
Validation Loss: 1.6896, Accuracy: 0.8593, Precision: 0.8301, Recall: 0.7756, F1: 0.7951
Testing Loss: 1.3583, Accuracy: 0.8659, Precision: 0.8407, Recall: 0.7842, F1: 0.8012
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3058, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 22/70
Train Loss: 0.0510, Accuracy: 0.9687, Precision: 0.9493, Recall: 0.9524, F1: 0.9509
Validation Loss: 1.8827, Accuracy: 0.8507, Precision: 0.7965, Recall: 0.7512, F1: 0.7650
Testing Loss: 1.4522, Accuracy: 0.8696, Precision: 0.8429, Recall: 0.7795, F1: 0.7953
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2571, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8533, F1: 0.8276
Epoch 23/70
Train Loss: 0.0515, Accuracy: 0.9661, Precision: 0.9466, Recall: 0.9405, F1: 0.9434
Validation Loss: 1.7805, Accuracy: 0.8550, Precision: 0.8212, Recall: 0.7712, F1: 0.7892
Testing Loss: 1.3580, Accuracy: 0.8768, Precision: 0.8471, Recall: 0.8000, F1: 0.8151
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4486, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8511, F1: 0.8252
Epoch 24/70
Train Loss: 0.0518, Accuracy: 0.9699, Precision: 0.9508, Recall: 0.9489, F1: 0.9498
Validation Loss: 1.9009, Accuracy: 0.8529, Precision: 0.7866, Recall: 0.7586, F1: 0.7658
Testing Loss: 1.4058, Accuracy: 0.8684, Precision: 0.8295, Recall: 0.7809, F1: 0.7911
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2906, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8511, F1: 0.8252
Epoch 25/70
Train Loss: 0.0512, Accuracy: 0.9708, Precision: 0.9520, Recall: 0.9544, F1: 0.9532
Validation Loss: 2.8441, Accuracy: 0.8529, Precision: 0.8181, Recall: 0.7568, F1: 0.7769
Testing Loss: 2.3271, Accuracy: 0.8623, Precision: 0.8390, Recall: 0.7667, F1: 0.7885
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2846, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 26/70
Train Loss: 0.2092, Accuracy: 0.9334, Precision: 0.9125, Recall: 0.9069, F1: 0.9096
Validation Loss: 1.0018, Accuracy: 0.8337, Precision: 0.7974, Recall: 0.7369, F1: 0.7500
Testing Loss: 0.8850, Accuracy: 0.8527, Precision: 0.8094, Recall: 0.7748, F1: 0.7880
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 0, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3103, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9106, F1: 0.8940
Epoch 27/70
Train Loss: 0.0945, Accuracy: 0.9597, Precision: 0.9356, Recall: 0.9357, F1: 0.9356
Validation Loss: 0.7672, Accuracy: 0.8657, Precision: 0.8262, Recall: 0.7797, F1: 0.7960
Testing Loss: 0.7905, Accuracy: 0.8599, Precision: 0.8371, Recall: 0.7798, F1: 0.7966
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3137, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 28/70
Train Loss: 0.0628, Accuracy: 0.9663, Precision: 0.9451, Recall: 0.9503, F1: 0.9475
Validation Loss: 1.0596, Accuracy: 0.8678, Precision: 0.8273, Recall: 0.8001, F1: 0.8113
Testing Loss: 0.9430, Accuracy: 0.8563, Precision: 0.8239, Recall: 0.7966, F1: 0.8081
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2654, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 29/70
Train Loss: 0.0508, Accuracy: 0.9677, Precision: 0.9459, Recall: 0.9524, F1: 0.9490
Validation Loss: 1.2367, Accuracy: 0.8550, Precision: 0.8247, Recall: 0.7741, F1: 0.7905
Testing Loss: 1.0618, Accuracy: 0.8551, Precision: 0.8210, Recall: 0.7890, F1: 0.8024
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2191, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 30/70
Train Loss: 0.0489, Accuracy: 0.9661, Precision: 0.9449, Recall: 0.9457, F1: 0.9453
Validation Loss: 1.2117, Accuracy: 0.8593, Precision: 0.8255, Recall: 0.7857, F1: 0.8009
Testing Loss: 1.0696, Accuracy: 0.8599, Precision: 0.8338, Recall: 0.7917, F1: 0.8091
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4969, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 31/70
Train Loss: 0.0472, Accuracy: 0.9696, Precision: 0.9527, Recall: 0.9532, F1: 0.9529
Validation Loss: 1.2383, Accuracy: 0.8593, Precision: 0.8224, Recall: 0.7784, F1: 0.7943
Testing Loss: 1.0829, Accuracy: 0.8527, Precision: 0.8134, Recall: 0.7715, F1: 0.7854
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3152, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 32/70
Train Loss: 0.0478, Accuracy: 0.9677, Precision: 0.9536, Recall: 0.9432, F1: 0.9481
Validation Loss: 1.2200, Accuracy: 0.8571, Precision: 0.8161, Recall: 0.7872, F1: 0.7987
Testing Loss: 1.0933, Accuracy: 0.8635, Precision: 0.8299, Recall: 0.8076, F1: 0.8172
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2825, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 33/70
Train Loss: 0.0463, Accuracy: 0.9720, Precision: 0.9557, Recall: 0.9553, F1: 0.9555
Validation Loss: 1.3277, Accuracy: 0.8507, Precision: 0.8275, Recall: 0.7638, F1: 0.7838
Testing Loss: 1.1724, Accuracy: 0.8527, Precision: 0.8157, Recall: 0.7684, F1: 0.7839
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3174, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 34/70
Train Loss: 0.0462, Accuracy: 0.9713, Precision: 0.9555, Recall: 0.9546, F1: 0.9551
Validation Loss: 1.3163, Accuracy: 0.8529, Precision: 0.8201, Recall: 0.7695, F1: 0.7873
Testing Loss: 1.1753, Accuracy: 0.8563, Precision: 0.8218, Recall: 0.7764, F1: 0.7917
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4162, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 35/70
Train Loss: 0.0475, Accuracy: 0.9668, Precision: 0.9448, Recall: 0.9469, F1: 0.9458
Validation Loss: 1.3378, Accuracy: 0.8593, Precision: 0.8194, Recall: 0.7770, F1: 0.7924
Testing Loss: 1.2241, Accuracy: 0.8599, Precision: 0.8203, Recall: 0.7813, F1: 0.7929
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2013, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 36/70
Train Loss: 0.0476, Accuracy: 0.9704, Precision: 0.9492, Recall: 0.9572, F1: 0.9531
Validation Loss: 1.3105, Accuracy: 0.8550, Precision: 0.8138, Recall: 0.7625, F1: 0.7794
Testing Loss: 1.2101, Accuracy: 0.8563, Precision: 0.8212, Recall: 0.7710, F1: 0.7867
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3823, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 37/70
Train Loss: 0.0496, Accuracy: 0.9666, Precision: 0.9482, Recall: 0.9439, F1: 0.9459
Validation Loss: 1.3173, Accuracy: 0.8529, Precision: 0.8187, Recall: 0.7770, F1: 0.7921
Testing Loss: 1.1334, Accuracy: 0.8539, Precision: 0.8178, Recall: 0.7937, F1: 0.8040
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2403, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 38/70
Train Loss: 0.0518, Accuracy: 0.9704, Precision: 0.9522, Recall: 0.9529, F1: 0.9525
Validation Loss: 1.1109, Accuracy: 0.8635, Precision: 0.8102, Recall: 0.8024, F1: 0.8049
Testing Loss: 1.0204, Accuracy: 0.8599, Precision: 0.8120, Recall: 0.8021, F1: 0.8059
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2016, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 39/70
Train Loss: 0.0523, Accuracy: 0.9718, Precision: 0.9544, Recall: 0.9557, F1: 0.9550
Validation Loss: 1.2902, Accuracy: 0.8571, Precision: 0.8344, Recall: 0.7914, F1: 0.8066
Testing Loss: 1.1361, Accuracy: 0.8587, Precision: 0.8189, Recall: 0.7962, F1: 0.8060
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1979, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8511, F1: 0.8252
Epoch 40/70
Train Loss: 0.0498, Accuracy: 0.9687, Precision: 0.9487, Recall: 0.9515, F1: 0.9501
Validation Loss: 1.3389, Accuracy: 0.8635, Precision: 0.8187, Recall: 0.7998, F1: 0.8077
Testing Loss: 1.1941, Accuracy: 0.8623, Precision: 0.8201, Recall: 0.7868, F1: 0.7950
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3641, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 41/70
Train Loss: 0.0484, Accuracy: 0.9694, Precision: 0.9507, Recall: 0.9489, F1: 0.9498
Validation Loss: 1.6653, Accuracy: 0.8635, Precision: 0.8308, Recall: 0.8092, F1: 0.8183
Testing Loss: 1.4571, Accuracy: 0.8671, Precision: 0.8317, Recall: 0.8139, F1: 0.8221
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2797, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 42/70
Train Loss: 0.1139, Accuracy: 0.9597, Precision: 0.9383, Recall: 0.9401, F1: 0.9392
Validation Loss: 1.0161, Accuracy: 0.7846, Precision: 0.7114, Recall: 0.6884, F1: 0.6792
Testing Loss: 0.9882, Accuracy: 0.7862, Precision: 0.7491, Recall: 0.6884, F1: 0.6789
LM Predictions:  [0, 4, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 4, 0, 2, 0, 4, 0, 4, 0, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5558, Accuracy: 0.6905, Precision: 0.8261, Recall: 0.7289, F1: 0.6821
Epoch 43/70
Train Loss: 0.1265, Accuracy: 0.9512, Precision: 0.9315, Recall: 0.9264, F1: 0.9289
Validation Loss: 0.7440, Accuracy: 0.8550, Precision: 0.8062, Recall: 0.7726, F1: 0.7848
Testing Loss: 0.8363, Accuracy: 0.8514, Precision: 0.8105, Recall: 0.7727, F1: 0.7824
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 3, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 0, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3438, Accuracy: 0.7619, Precision: 0.8514, Recall: 0.7861, F1: 0.7648
Epoch 44/70
Train Loss: 0.0735, Accuracy: 0.9604, Precision: 0.9360, Recall: 0.9373, F1: 0.9366
Validation Loss: 0.8729, Accuracy: 0.8443, Precision: 0.7941, Recall: 0.7785, F1: 0.7855
Testing Loss: 0.9491, Accuracy: 0.8551, Precision: 0.8148, Recall: 0.7876, F1: 0.7978
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2067, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 45/70
Train Loss: 0.0499, Accuracy: 0.9654, Precision: 0.9445, Recall: 0.9414, F1: 0.9430
Validation Loss: 1.1801, Accuracy: 0.8294, Precision: 0.7860, Recall: 0.7344, F1: 0.7540
Testing Loss: 1.2287, Accuracy: 0.8539, Precision: 0.8140, Recall: 0.7768, F1: 0.7904
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3188, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 46/70
Train Loss: 0.0504, Accuracy: 0.9673, Precision: 0.9451, Recall: 0.9500, F1: 0.9475
Validation Loss: 0.9075, Accuracy: 0.8422, Precision: 0.7805, Recall: 0.7642, F1: 0.7712
Testing Loss: 0.8528, Accuracy: 0.8623, Precision: 0.8205, Recall: 0.7904, F1: 0.8005
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1509, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 47/70
Train Loss: 0.0458, Accuracy: 0.9723, Precision: 0.9560, Recall: 0.9554, F1: 0.9557
Validation Loss: 1.0919, Accuracy: 0.8422, Precision: 0.7861, Recall: 0.7594, F1: 0.7694
Testing Loss: 1.0601, Accuracy: 0.8611, Precision: 0.8203, Recall: 0.7796, F1: 0.7904
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1464, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 48/70
Train Loss: 0.0452, Accuracy: 0.9689, Precision: 0.9492, Recall: 0.9491, F1: 0.9491
Validation Loss: 1.1194, Accuracy: 0.8422, Precision: 0.7960, Recall: 0.7685, F1: 0.7805
Testing Loss: 1.0714, Accuracy: 0.8575, Precision: 0.8200, Recall: 0.7877, F1: 0.8002
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2760, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 49/70
Train Loss: 0.0459, Accuracy: 0.9680, Precision: 0.9486, Recall: 0.9486, F1: 0.9486
Validation Loss: 1.1987, Accuracy: 0.8209, Precision: 0.7720, Recall: 0.7276, F1: 0.7440
Testing Loss: 1.1291, Accuracy: 0.8587, Precision: 0.8232, Recall: 0.7876, F1: 0.8016
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2705, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 50/70
Train Loss: 0.0465, Accuracy: 0.9685, Precision: 0.9485, Recall: 0.9483, F1: 0.9484
Validation Loss: 1.1748, Accuracy: 0.8358, Precision: 0.7828, Recall: 0.7521, F1: 0.7648
Testing Loss: 1.1373, Accuracy: 0.8599, Precision: 0.8234, Recall: 0.7910, F1: 0.8035
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3562, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 51/70
Train Loss: 0.0453, Accuracy: 0.9706, Precision: 0.9544, Recall: 0.9529, F1: 0.9537
Validation Loss: 1.1959, Accuracy: 0.8380, Precision: 0.7779, Recall: 0.7565, F1: 0.7648
Testing Loss: 1.1661, Accuracy: 0.8599, Precision: 0.8183, Recall: 0.7777, F1: 0.7883
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1798, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 52/70
Train Loss: 0.0449, Accuracy: 0.9694, Precision: 0.9499, Recall: 0.9540, F1: 0.9519
Validation Loss: 1.2641, Accuracy: 0.8358, Precision: 0.7914, Recall: 0.7473, F1: 0.7645
Testing Loss: 1.2227, Accuracy: 0.8551, Precision: 0.8213, Recall: 0.7733, F1: 0.7875
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3498, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 53/70
Train Loss: 0.0456, Accuracy: 0.9694, Precision: 0.9527, Recall: 0.9486, F1: 0.9506
Validation Loss: 1.2704, Accuracy: 0.8273, Precision: 0.7913, Recall: 0.7320, F1: 0.7530
Testing Loss: 1.2037, Accuracy: 0.8563, Precision: 0.8252, Recall: 0.7701, F1: 0.7874
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3884, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 54/70
Train Loss: 0.0459, Accuracy: 0.9723, Precision: 0.9591, Recall: 0.9512, F1: 0.9551
Validation Loss: 1.1946, Accuracy: 0.8422, Precision: 0.7927, Recall: 0.7697, F1: 0.7799
Testing Loss: 1.1798, Accuracy: 0.8599, Precision: 0.8236, Recall: 0.7943, F1: 0.8060
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1896, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 55/70
Train Loss: 0.0482, Accuracy: 0.9701, Precision: 0.9499, Recall: 0.9571, F1: 0.9533
Validation Loss: 1.3393, Accuracy: 0.8401, Precision: 0.7675, Recall: 0.7589, F1: 0.7606
Testing Loss: 1.2206, Accuracy: 0.8539, Precision: 0.8041, Recall: 0.7769, F1: 0.7833
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1340, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 56/70
Train Loss: 0.0467, Accuracy: 0.9737, Precision: 0.9587, Recall: 0.9527, F1: 0.9556
Validation Loss: 1.2155, Accuracy: 0.8443, Precision: 0.7876, Recall: 0.7703, F1: 0.7773
Testing Loss: 1.0533, Accuracy: 0.8587, Precision: 0.8185, Recall: 0.7990, F1: 0.8072
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2363, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 57/70
Train Loss: 0.0480, Accuracy: 0.9711, Precision: 0.9525, Recall: 0.9564, F1: 0.9544
Validation Loss: 1.5161, Accuracy: 0.8422, Precision: 0.8036, Recall: 0.7542, F1: 0.7729
Testing Loss: 1.3911, Accuracy: 0.8539, Precision: 0.8191, Recall: 0.7705, F1: 0.7874
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2941, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8528, F1: 0.8320
Epoch 58/70
Train Loss: 0.1102, Accuracy: 0.9559, Precision: 0.9345, Recall: 0.9301, F1: 0.9322
Validation Loss: 0.7357, Accuracy: 0.8486, Precision: 0.8205, Recall: 0.8016, F1: 0.8054
Testing Loss: 0.7816, Accuracy: 0.8394, Precision: 0.8084, Recall: 0.7966, F1: 0.8008
LM Predictions:  [0, 1, 0, 0, 1, 1, 0, 4, 4, 2, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 5, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3602, Accuracy: 0.7619, Precision: 0.6984, Recall: 0.6343, F1: 0.6369
Epoch 59/70
Train Loss: 0.1125, Accuracy: 0.9580, Precision: 0.9324, Recall: 0.9408, F1: 0.9364
Validation Loss: 0.7364, Accuracy: 0.8358, Precision: 0.8067, Recall: 0.7613, F1: 0.7746
Testing Loss: 0.6837, Accuracy: 0.8357, Precision: 0.8010, Recall: 0.7525, F1: 0.7702
LM Predictions:  [0, 1, 2, 1, 1, 1, 5, 5, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 5, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4559, Accuracy: 0.7143, Precision: 0.7179, Recall: 0.6125, F1: 0.6165
Epoch 60/70
Train Loss: 0.0630, Accuracy: 0.9663, Precision: 0.9458, Recall: 0.9475, F1: 0.9466
Validation Loss: 0.9628, Accuracy: 0.8337, Precision: 0.7948, Recall: 0.7634, F1: 0.7729
Testing Loss: 0.8769, Accuracy: 0.8442, Precision: 0.8143, Recall: 0.7839, F1: 0.7965
LM Predictions:  [0, 1, 2, 0, 1, 1, 2, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3323, Accuracy: 0.7857, Precision: 0.8444, Recall: 0.7911, F1: 0.7752
Epoch 61/70
Train Loss: 0.0535, Accuracy: 0.9647, Precision: 0.9416, Recall: 0.9453, F1: 0.9433
Validation Loss: 1.0538, Accuracy: 0.8316, Precision: 0.7638, Recall: 0.7565, F1: 0.7580
Testing Loss: 0.9462, Accuracy: 0.8575, Precision: 0.8235, Recall: 0.8039, F1: 0.8117
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1640, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 62/70
Train Loss: 0.0547, Accuracy: 0.9675, Precision: 0.9455, Recall: 0.9505, F1: 0.9479
Validation Loss: 1.1194, Accuracy: 0.8465, Precision: 0.8091, Recall: 0.7700, F1: 0.7848
Testing Loss: 1.0991, Accuracy: 0.8406, Precision: 0.8130, Recall: 0.7541, F1: 0.7712
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2016, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 63/70
Train Loss: 0.0508, Accuracy: 0.9727, Precision: 0.9561, Recall: 0.9566, F1: 0.9564
Validation Loss: 1.0422, Accuracy: 0.8422, Precision: 0.8033, Recall: 0.7621, F1: 0.7760
Testing Loss: 1.0238, Accuracy: 0.8297, Precision: 0.8056, Recall: 0.7456, F1: 0.7659
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2236, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 64/70
Train Loss: 0.0460, Accuracy: 0.9685, Precision: 0.9524, Recall: 0.9453, F1: 0.9487
Validation Loss: 1.0661, Accuracy: 0.8465, Precision: 0.7965, Recall: 0.7751, F1: 0.7831
Testing Loss: 1.0120, Accuracy: 0.8478, Precision: 0.8177, Recall: 0.7865, F1: 0.7994
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1752, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
Epoch 65/70
Train Loss: 0.0455, Accuracy: 0.9685, Precision: 0.9514, Recall: 0.9476, F1: 0.9494
Validation Loss: 1.1270, Accuracy: 0.8358, Precision: 0.8012, Recall: 0.7572, F1: 0.7721
Testing Loss: 1.0778, Accuracy: 0.8454, Precision: 0.8141, Recall: 0.7812, F1: 0.7953
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2447, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 66/70
Train Loss: 0.0452, Accuracy: 0.9685, Precision: 0.9463, Recall: 0.9500, F1: 0.9481
Validation Loss: 1.1222, Accuracy: 0.8529, Precision: 0.8049, Recall: 0.7749, F1: 0.7853
Testing Loss: 1.1099, Accuracy: 0.8454, Precision: 0.8124, Recall: 0.7639, F1: 0.7798
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2980, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 67/70
Train Loss: 0.0457, Accuracy: 0.9668, Precision: 0.9475, Recall: 0.9455, F1: 0.9465
Validation Loss: 1.1631, Accuracy: 0.8529, Precision: 0.8081, Recall: 0.7838, F1: 0.7930
Testing Loss: 1.1406, Accuracy: 0.8466, Precision: 0.8125, Recall: 0.7868, F1: 0.7976
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1755, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 68/70
Train Loss: 0.0457, Accuracy: 0.9687, Precision: 0.9515, Recall: 0.9473, F1: 0.9494
Validation Loss: 1.1389, Accuracy: 0.8550, Precision: 0.8103, Recall: 0.7806, F1: 0.7913
Testing Loss: 1.1159, Accuracy: 0.8442, Precision: 0.8087, Recall: 0.7649, F1: 0.7787
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2647, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 69/70
Train Loss: 0.0485, Accuracy: 0.9694, Precision: 0.9496, Recall: 0.9508, F1: 0.9502
Validation Loss: 1.1149, Accuracy: 0.8443, Precision: 0.8039, Recall: 0.7561, F1: 0.7706
Testing Loss: 1.2225, Accuracy: 0.8430, Precision: 0.8076, Recall: 0.7580, F1: 0.7724
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4714, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 70/70
Train Loss: 0.0618, Accuracy: 0.9647, Precision: 0.9443, Recall: 0.9431, F1: 0.9437
Validation Loss: 1.1970, Accuracy: 0.8380, Precision: 0.8024, Recall: 0.7492, F1: 0.7694
Testing Loss: 1.3079, Accuracy: 0.8261, Precision: 0.7918, Recall: 0.7396, F1: 0.7577
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1577, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
For later layers:  [16, 17, 18, 19, 20, 21, 22, 23]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.7576, Accuracy: 0.8226, Precision: 0.7602, Recall: 0.7518, F1: 0.7556
Validation Loss: 0.4857, Accuracy: 0.8614, Precision: 0.8464, Recall: 0.8035, F1: 0.8114
Testing Loss: 0.3788, Accuracy: 0.8865, Precision: 0.8733, Recall: 0.8105, F1: 0.8314
LM Predictions:  [0, 1, 0, 0, 4, 5, 5, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 3, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 4, 3, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2740, Accuracy: 0.1667, Precision: 0.3345, Recall: 0.1852, F1: 0.1344
Epoch 2/70
Train Loss: 0.2662, Accuracy: 0.9177, Precision: 0.8817, Recall: 0.8744, F1: 0.8779
Validation Loss: 0.5806, Accuracy: 0.8699, Precision: 0.8384, Recall: 0.7968, F1: 0.8099
Testing Loss: 0.4701, Accuracy: 0.8623, Precision: 0.8259, Recall: 0.7753, F1: 0.7801
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 2, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 2, 0, 0, 4, 4, 2, 0, 4, 3, 3, 0, 5, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8512, Accuracy: 0.7381, Precision: 0.6806, Recall: 0.6157, F1: 0.6120
Epoch 3/70
Train Loss: 0.1682, Accuracy: 0.9476, Precision: 0.9185, Recall: 0.9136, F1: 0.9160
Validation Loss: 0.6119, Accuracy: 0.8742, Precision: 0.8441, Recall: 0.8173, F1: 0.8257
Testing Loss: 0.4640, Accuracy: 0.8744, Precision: 0.8377, Recall: 0.8037, F1: 0.8123
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 5, 4, 2, 0, 4, 3, 3, 0, 5, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5124, Accuracy: 0.7619, Precision: 0.7273, Recall: 0.6421, F1: 0.6579
Epoch 4/70
Train Loss: 0.1297, Accuracy: 0.9554, Precision: 0.9271, Recall: 0.9231, F1: 0.9251
Validation Loss: 0.6321, Accuracy: 0.8763, Precision: 0.8239, Recall: 0.8111, F1: 0.8138
Testing Loss: 0.6017, Accuracy: 0.8720, Precision: 0.8361, Recall: 0.7962, F1: 0.8039
LM Predictions:  [0, 1, 2, 0, 3, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7015, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8639, F1: 0.8336
Epoch 5/70
Train Loss: 0.1126, Accuracy: 0.9578, Precision: 0.9307, Recall: 0.9331, F1: 0.9319
Validation Loss: 0.6438, Accuracy: 0.8550, Precision: 0.7672, Recall: 0.7403, F1: 0.7412
Testing Loss: 0.6023, Accuracy: 0.8671, Precision: 0.8570, Recall: 0.7635, F1: 0.7803
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6946, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8111, F1: 0.7876
Epoch 6/70
Train Loss: 0.0919, Accuracy: 0.9621, Precision: 0.9376, Recall: 0.9358, F1: 0.9367
Validation Loss: 0.9176, Accuracy: 0.8657, Precision: 0.8060, Recall: 0.7632, F1: 0.7737
Testing Loss: 1.0122, Accuracy: 0.8575, Precision: 0.8343, Recall: 0.7537, F1: 0.7679
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4825, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8306, F1: 0.8123
Epoch 7/70
Train Loss: 0.0930, Accuracy: 0.9609, Precision: 0.9383, Recall: 0.9381, F1: 0.9382
Validation Loss: 0.7456, Accuracy: 0.8827, Precision: 0.8433, Recall: 0.8303, F1: 0.8360
Testing Loss: 0.7217, Accuracy: 0.8708, Precision: 0.8398, Recall: 0.8251, F1: 0.8304
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 5, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5914, Accuracy: 0.8095, Precision: 0.7361, Recall: 0.6903, F1: 0.6786
Epoch 8/70
Train Loss: 0.0934, Accuracy: 0.9621, Precision: 0.9404, Recall: 0.9412, F1: 0.9408
Validation Loss: 0.7476, Accuracy: 0.8657, Precision: 0.8211, Recall: 0.7844, F1: 0.7971
Testing Loss: 0.6484, Accuracy: 0.8708, Precision: 0.8463, Recall: 0.7957, F1: 0.8138
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2486, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9378, F1: 0.9199
Epoch 9/70
Train Loss: 0.1620, Accuracy: 0.9464, Precision: 0.9239, Recall: 0.9232, F1: 0.9235
Validation Loss: 0.7380, Accuracy: 0.8699, Precision: 0.8341, Recall: 0.8185, F1: 0.8256
Testing Loss: 0.7960, Accuracy: 0.8502, Precision: 0.8202, Recall: 0.8036, F1: 0.8087
LM Predictions:  [0, 1, 2, 1, 1, 2, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 0, 0, 2, 4, 4, 2, 0, 4, 5, 3, 5, 0, 4, 0, 4, 3, 5, 1, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6560, Accuracy: 0.7619, Precision: 0.7222, Recall: 0.6449, F1: 0.6475
Epoch 10/70
Train Loss: 0.1353, Accuracy: 0.9504, Precision: 0.9278, Recall: 0.9273, F1: 0.9275
Validation Loss: 0.8167, Accuracy: 0.8614, Precision: 0.8209, Recall: 0.8200, F1: 0.8202
Testing Loss: 0.7701, Accuracy: 0.8527, Precision: 0.8083, Recall: 0.8036, F1: 0.8054
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 0, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 0, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3683, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9078, F1: 0.8920
Epoch 11/70
Train Loss: 0.1134, Accuracy: 0.9547, Precision: 0.9325, Recall: 0.9293, F1: 0.9309
Validation Loss: 0.7973, Accuracy: 0.8571, Precision: 0.8213, Recall: 0.7840, F1: 0.7979
Testing Loss: 0.8857, Accuracy: 0.8490, Precision: 0.8054, Recall: 0.7680, F1: 0.7768
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4610, Accuracy: 0.7381, Precision: 0.8444, Recall: 0.7711, F1: 0.7458
Epoch 12/70
Train Loss: 0.0931, Accuracy: 0.9604, Precision: 0.9421, Recall: 0.9398, F1: 0.9409
Validation Loss: 0.8966, Accuracy: 0.8529, Precision: 0.8119, Recall: 0.7869, F1: 0.7972
Testing Loss: 0.8701, Accuracy: 0.8478, Precision: 0.8102, Recall: 0.7883, F1: 0.7977
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3924, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8156, F1: 0.7950
Epoch 13/70
Train Loss: 0.0738, Accuracy: 0.9670, Precision: 0.9481, Recall: 0.9481, F1: 0.9481
Validation Loss: 1.0647, Accuracy: 0.8550, Precision: 0.8070, Recall: 0.7510, F1: 0.7650
Testing Loss: 1.1048, Accuracy: 0.8478, Precision: 0.8210, Recall: 0.7525, F1: 0.7695
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6374, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 14/70
Train Loss: 0.0630, Accuracy: 0.9680, Precision: 0.9480, Recall: 0.9507, F1: 0.9493
Validation Loss: 0.9572, Accuracy: 0.8699, Precision: 0.8385, Recall: 0.8097, F1: 0.8217
Testing Loss: 1.0294, Accuracy: 0.8490, Precision: 0.8142, Recall: 0.7851, F1: 0.7979
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5006, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 15/70
Train Loss: 0.0653, Accuracy: 0.9675, Precision: 0.9479, Recall: 0.9478, F1: 0.9479
Validation Loss: 1.0093, Accuracy: 0.8742, Precision: 0.8306, Recall: 0.8029, F1: 0.8128
Testing Loss: 1.0991, Accuracy: 0.8527, Precision: 0.8164, Recall: 0.7858, F1: 0.7985
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4314, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7483, F1: 0.7376
Epoch 16/70
Train Loss: 0.0663, Accuracy: 0.9675, Precision: 0.9489, Recall: 0.9484, F1: 0.9486
Validation Loss: 1.1452, Accuracy: 0.8550, Precision: 0.8018, Recall: 0.7556, F1: 0.7683
Testing Loss: 1.2035, Accuracy: 0.8514, Precision: 0.8132, Recall: 0.7613, F1: 0.7759
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3796, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8333, F1: 0.8106
Epoch 17/70
Train Loss: 0.0574, Accuracy: 0.9713, Precision: 0.9556, Recall: 0.9517, F1: 0.9536
Validation Loss: 1.3034, Accuracy: 0.8635, Precision: 0.8064, Recall: 0.7843, F1: 0.7890
Testing Loss: 1.3526, Accuracy: 0.8502, Precision: 0.8004, Recall: 0.7709, F1: 0.7744
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1539, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
Epoch 18/70
Train Loss: 0.0603, Accuracy: 0.9692, Precision: 0.9505, Recall: 0.9512, F1: 0.9508
Validation Loss: 1.2738, Accuracy: 0.8614, Precision: 0.8259, Recall: 0.7773, F1: 0.7941
Testing Loss: 1.3173, Accuracy: 0.8514, Precision: 0.8148, Recall: 0.7660, F1: 0.7833
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3709, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 19/70
Train Loss: 0.0618, Accuracy: 0.9675, Precision: 0.9484, Recall: 0.9476, F1: 0.9480
Validation Loss: 1.2634, Accuracy: 0.8678, Precision: 0.8249, Recall: 0.7956, F1: 0.8060
Testing Loss: 1.3258, Accuracy: 0.8539, Precision: 0.8197, Recall: 0.7738, F1: 0.7899
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4333, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 20/70
Train Loss: 0.0573, Accuracy: 0.9661, Precision: 0.9451, Recall: 0.9445, F1: 0.9448
Validation Loss: 1.3782, Accuracy: 0.8593, Precision: 0.8279, Recall: 0.7942, F1: 0.8064
Testing Loss: 1.3913, Accuracy: 0.8539, Precision: 0.8231, Recall: 0.7951, F1: 0.8074
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3213, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 21/70
Train Loss: 0.0572, Accuracy: 0.9708, Precision: 0.9537, Recall: 0.9527, F1: 0.9532
Validation Loss: 1.2552, Accuracy: 0.8699, Precision: 0.8260, Recall: 0.8030, F1: 0.8122
Testing Loss: 1.3154, Accuracy: 0.8551, Precision: 0.8206, Recall: 0.7991, F1: 0.8086
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1797, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9778, F1: 0.9701
Epoch 22/70
Train Loss: 0.0582, Accuracy: 0.9675, Precision: 0.9464, Recall: 0.9506, F1: 0.9484
Validation Loss: 1.3162, Accuracy: 0.8529, Precision: 0.7982, Recall: 0.7623, F1: 0.7725
Testing Loss: 1.3788, Accuracy: 0.8490, Precision: 0.8140, Recall: 0.7591, F1: 0.7746
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3175, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8083, F1: 0.7914
Epoch 23/70
Train Loss: 0.0546, Accuracy: 0.9663, Precision: 0.9442, Recall: 0.9410, F1: 0.9426
Validation Loss: 1.5496, Accuracy: 0.8742, Precision: 0.8391, Recall: 0.8099, F1: 0.8216
Testing Loss: 1.6338, Accuracy: 0.8539, Precision: 0.8164, Recall: 0.7793, F1: 0.7933
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3606, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8711, F1: 0.8468
Epoch 24/70
Train Loss: 0.0537, Accuracy: 0.9699, Precision: 0.9510, Recall: 0.9519, F1: 0.9515
Validation Loss: 1.5304, Accuracy: 0.8571, Precision: 0.8280, Recall: 0.7908, F1: 0.8058
Testing Loss: 1.6582, Accuracy: 0.8502, Precision: 0.8196, Recall: 0.7862, F1: 0.8006
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4914, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 25/70
Train Loss: 0.0559, Accuracy: 0.9680, Precision: 0.9506, Recall: 0.9454, F1: 0.9479
Validation Loss: 1.4095, Accuracy: 0.8507, Precision: 0.7964, Recall: 0.7551, F1: 0.7665
Testing Loss: 1.4611, Accuracy: 0.8490, Precision: 0.8101, Recall: 0.7531, F1: 0.7680
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4143, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8511, F1: 0.8278
Epoch 26/70
Train Loss: 0.0786, Accuracy: 0.9585, Precision: 0.9397, Recall: 0.9379, F1: 0.9388
Validation Loss: 0.6971, Accuracy: 0.8294, Precision: 0.8062, Recall: 0.7575, F1: 0.7736
Testing Loss: 0.7234, Accuracy: 0.8382, Precision: 0.8138, Recall: 0.7661, F1: 0.7779
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 2, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3985, Accuracy: 0.8333, Precision: 0.7315, Recall: 0.7167, F1: 0.6939
Epoch 27/70
Train Loss: 0.1524, Accuracy: 0.9410, Precision: 0.9201, Recall: 0.9155, F1: 0.9178
Validation Loss: 0.8806, Accuracy: 0.8358, Precision: 0.7936, Recall: 0.7477, F1: 0.7620
Testing Loss: 0.7520, Accuracy: 0.8490, Precision: 0.8181, Recall: 0.7651, F1: 0.7807
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5077, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7483, F1: 0.7376
Epoch 28/70
Train Loss: 0.0946, Accuracy: 0.9571, Precision: 0.9344, Recall: 0.9313, F1: 0.9328
Validation Loss: 1.0567, Accuracy: 0.8230, Precision: 0.7707, Recall: 0.7481, F1: 0.7562
Testing Loss: 0.9763, Accuracy: 0.8188, Precision: 0.7800, Recall: 0.7456, F1: 0.7543
LM Predictions:  [0, 3, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 5, 4, 0, 0, 0, 4, 3, 2, 1, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5142, Accuracy: 0.7381, Precision: 0.6986, Recall: 0.6324, F1: 0.6231
Epoch 29/70
Train Loss: 0.0807, Accuracy: 0.9625, Precision: 0.9404, Recall: 0.9384, F1: 0.9393
Validation Loss: 0.8909, Accuracy: 0.8635, Precision: 0.8074, Recall: 0.8055, F1: 0.8048
Testing Loss: 0.8036, Accuracy: 0.8454, Precision: 0.7976, Recall: 0.7900, F1: 0.7921
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2651, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8600, F1: 0.8324
Epoch 30/70
Train Loss: 0.0624, Accuracy: 0.9663, Precision: 0.9428, Recall: 0.9475, F1: 0.9450
Validation Loss: 1.2918, Accuracy: 0.8550, Precision: 0.8159, Recall: 0.7748, F1: 0.7888
Testing Loss: 1.1553, Accuracy: 0.8490, Precision: 0.8242, Recall: 0.7682, F1: 0.7858
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4316, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 31/70
Train Loss: 0.0585, Accuracy: 0.9647, Precision: 0.9442, Recall: 0.9395, F1: 0.9417
Validation Loss: 1.2460, Accuracy: 0.8657, Precision: 0.8270, Recall: 0.7995, F1: 0.8110
Testing Loss: 1.1207, Accuracy: 0.8502, Precision: 0.8197, Recall: 0.7884, F1: 0.8019
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4835, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 32/70
Train Loss: 0.0634, Accuracy: 0.9682, Precision: 0.9533, Recall: 0.9481, F1: 0.9506
Validation Loss: 1.0516, Accuracy: 0.8380, Precision: 0.7903, Recall: 0.7660, F1: 0.7732
Testing Loss: 0.8620, Accuracy: 0.8454, Precision: 0.8047, Recall: 0.7755, F1: 0.7857
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5021, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 33/70
Train Loss: 0.0594, Accuracy: 0.9694, Precision: 0.9496, Recall: 0.9517, F1: 0.9506
Validation Loss: 1.1540, Accuracy: 0.8486, Precision: 0.8118, Recall: 0.7855, F1: 0.7959
Testing Loss: 1.0064, Accuracy: 0.8527, Precision: 0.8188, Recall: 0.7794, F1: 0.7949
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3664, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 34/70
Train Loss: 0.0580, Accuracy: 0.9673, Precision: 0.9481, Recall: 0.9498, F1: 0.9489
Validation Loss: 1.2114, Accuracy: 0.8571, Precision: 0.8106, Recall: 0.7910, F1: 0.7968
Testing Loss: 1.0931, Accuracy: 0.8442, Precision: 0.7953, Recall: 0.7637, F1: 0.7725
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2394, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 35/70
Train Loss: 0.0517, Accuracy: 0.9689, Precision: 0.9504, Recall: 0.9484, F1: 0.9494
Validation Loss: 1.2598, Accuracy: 0.8507, Precision: 0.8111, Recall: 0.7975, F1: 0.8012
Testing Loss: 1.0712, Accuracy: 0.8466, Precision: 0.8040, Recall: 0.7941, F1: 0.7983
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2094, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 36/70
Train Loss: 0.0514, Accuracy: 0.9699, Precision: 0.9510, Recall: 0.9547, F1: 0.9528
Validation Loss: 1.2947, Accuracy: 0.8465, Precision: 0.8074, Recall: 0.7685, F1: 0.7841
Testing Loss: 1.0871, Accuracy: 0.8490, Precision: 0.8221, Recall: 0.7682, F1: 0.7884
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3037, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 37/70
Train Loss: 0.0511, Accuracy: 0.9687, Precision: 0.9507, Recall: 0.9458, F1: 0.9481
Validation Loss: 1.3979, Accuracy: 0.8486, Precision: 0.8155, Recall: 0.7950, F1: 0.8024
Testing Loss: 1.1942, Accuracy: 0.8466, Precision: 0.8083, Recall: 0.7974, F1: 0.8013
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1401, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 38/70
Train Loss: 0.0505, Accuracy: 0.9699, Precision: 0.9490, Recall: 0.9507, F1: 0.9498
Validation Loss: 1.3098, Accuracy: 0.8337, Precision: 0.7860, Recall: 0.7627, F1: 0.7716
Testing Loss: 1.1611, Accuracy: 0.8466, Precision: 0.8041, Recall: 0.7814, F1: 0.7909
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2065, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 39/70
Train Loss: 0.0495, Accuracy: 0.9699, Precision: 0.9513, Recall: 0.9532, F1: 0.9522
Validation Loss: 1.4114, Accuracy: 0.8507, Precision: 0.8051, Recall: 0.8037, F1: 0.8039
Testing Loss: 1.2387, Accuracy: 0.8418, Precision: 0.7912, Recall: 0.7859, F1: 0.7875
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1814, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 40/70
Train Loss: 0.0501, Accuracy: 0.9694, Precision: 0.9490, Recall: 0.9520, F1: 0.9505
Validation Loss: 1.3737, Accuracy: 0.8486, Precision: 0.8080, Recall: 0.7772, F1: 0.7900
Testing Loss: 1.1921, Accuracy: 0.8514, Precision: 0.8149, Recall: 0.7726, F1: 0.7859
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2359, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 41/70
Train Loss: 0.0485, Accuracy: 0.9692, Precision: 0.9498, Recall: 0.9540, F1: 0.9518
Validation Loss: 1.4107, Accuracy: 0.8550, Precision: 0.8185, Recall: 0.7860, F1: 0.7997
Testing Loss: 1.2109, Accuracy: 0.8514, Precision: 0.8142, Recall: 0.7748, F1: 0.7891
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3210, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 42/70
Train Loss: 0.0484, Accuracy: 0.9677, Precision: 0.9475, Recall: 0.9477, F1: 0.9476
Validation Loss: 1.4926, Accuracy: 0.8380, Precision: 0.8000, Recall: 0.7664, F1: 0.7798
Testing Loss: 1.2788, Accuracy: 0.8478, Precision: 0.8135, Recall: 0.7645, F1: 0.7818
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4438, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 43/70
Train Loss: 0.0489, Accuracy: 0.9687, Precision: 0.9500, Recall: 0.9488, F1: 0.9494
Validation Loss: 1.4274, Accuracy: 0.8443, Precision: 0.8062, Recall: 0.7834, F1: 0.7914
Testing Loss: 1.2030, Accuracy: 0.8490, Precision: 0.8035, Recall: 0.7828, F1: 0.7917
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.0987, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 44/70
Train Loss: 0.0503, Accuracy: 0.9687, Precision: 0.9503, Recall: 0.9480, F1: 0.9492
Validation Loss: 1.3483, Accuracy: 0.8507, Precision: 0.8134, Recall: 0.7962, F1: 0.8034
Testing Loss: 1.2143, Accuracy: 0.8514, Precision: 0.8089, Recall: 0.7864, F1: 0.7959
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1556, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 45/70
Train Loss: 0.0479, Accuracy: 0.9711, Precision: 0.9550, Recall: 0.9530, F1: 0.9540
Validation Loss: 1.4595, Accuracy: 0.8465, Precision: 0.8091, Recall: 0.7902, F1: 0.7966
Testing Loss: 1.2387, Accuracy: 0.8551, Precision: 0.8218, Recall: 0.7985, F1: 0.8085
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2461, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8111, F1: 0.7876
Epoch 46/70
Train Loss: 0.0535, Accuracy: 0.9718, Precision: 0.9528, Recall: 0.9605, F1: 0.9565
Validation Loss: 1.1387, Accuracy: 0.8380, Precision: 0.7988, Recall: 0.7739, F1: 0.7821
Testing Loss: 0.9888, Accuracy: 0.8297, Precision: 0.7789, Recall: 0.7579, F1: 0.7645
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2320, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 47/70
Train Loss: 0.1487, Accuracy: 0.9450, Precision: 0.9198, Recall: 0.9185, F1: 0.9192
Validation Loss: 0.7961, Accuracy: 0.8529, Precision: 0.8258, Recall: 0.7866, F1: 0.8004
Testing Loss: 0.8115, Accuracy: 0.8345, Precision: 0.7958, Recall: 0.7687, F1: 0.7803
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 5, 4, 0, 3, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3042, Accuracy: 0.7857, Precision: 0.7176, Recall: 0.6833, F1: 0.6541
Epoch 48/70
Train Loss: 0.0857, Accuracy: 0.9625, Precision: 0.9430, Recall: 0.9372, F1: 0.9400
Validation Loss: 0.8698, Accuracy: 0.8486, Precision: 0.8054, Recall: 0.7776, F1: 0.7878
Testing Loss: 0.9071, Accuracy: 0.8261, Precision: 0.7557, Recall: 0.7252, F1: 0.7286
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 0, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3900, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7233, F1: 0.7184
Epoch 49/70
Train Loss: 0.0620, Accuracy: 0.9659, Precision: 0.9439, Recall: 0.9483, F1: 0.9460
Validation Loss: 0.8154, Accuracy: 0.8529, Precision: 0.8108, Recall: 0.7997, F1: 0.8033
Testing Loss: 0.7702, Accuracy: 0.8466, Precision: 0.8006, Recall: 0.7778, F1: 0.7879
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3668, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 50/70
Train Loss: 0.0484, Accuracy: 0.9687, Precision: 0.9523, Recall: 0.9441, F1: 0.9481
Validation Loss: 1.0851, Accuracy: 0.8486, Precision: 0.8067, Recall: 0.7911, F1: 0.7973
Testing Loss: 0.9740, Accuracy: 0.8551, Precision: 0.8118, Recall: 0.7993, F1: 0.8051
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1481, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 51/70
Train Loss: 0.0470, Accuracy: 0.9666, Precision: 0.9467, Recall: 0.9462, F1: 0.9465
Validation Loss: 1.1061, Accuracy: 0.8529, Precision: 0.7991, Recall: 0.7745, F1: 0.7837
Testing Loss: 1.0222, Accuracy: 0.8502, Precision: 0.8094, Recall: 0.7724, F1: 0.7806
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2463, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8078
Epoch 52/70
Train Loss: 0.0485, Accuracy: 0.9677, Precision: 0.9498, Recall: 0.9445, F1: 0.9471
Validation Loss: 1.0930, Accuracy: 0.8486, Precision: 0.8086, Recall: 0.7853, F1: 0.7946
Testing Loss: 0.9993, Accuracy: 0.8514, Precision: 0.8062, Recall: 0.7810, F1: 0.7913
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2088, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 53/70
Train Loss: 0.0451, Accuracy: 0.9696, Precision: 0.9506, Recall: 0.9515, F1: 0.9511
Validation Loss: 1.1672, Accuracy: 0.8486, Precision: 0.8003, Recall: 0.7735, F1: 0.7830
Testing Loss: 1.0912, Accuracy: 0.8466, Precision: 0.8024, Recall: 0.7619, F1: 0.7737
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2274, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8716
Epoch 54/70
Train Loss: 0.0470, Accuracy: 0.9689, Precision: 0.9488, Recall: 0.9510, F1: 0.9499
Validation Loss: 1.1703, Accuracy: 0.8529, Precision: 0.8099, Recall: 0.7792, F1: 0.7912
Testing Loss: 1.0607, Accuracy: 0.8490, Precision: 0.8098, Recall: 0.7781, F1: 0.7915
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2540, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 55/70
Train Loss: 0.0476, Accuracy: 0.9711, Precision: 0.9528, Recall: 0.9528, F1: 0.9528
Validation Loss: 1.1299, Accuracy: 0.8550, Precision: 0.8171, Recall: 0.7830, F1: 0.7973
Testing Loss: 1.0635, Accuracy: 0.8502, Precision: 0.8114, Recall: 0.7863, F1: 0.7970
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2976, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 56/70
Train Loss: 0.0482, Accuracy: 0.9713, Precision: 0.9510, Recall: 0.9574, F1: 0.9541
Validation Loss: 0.9672, Accuracy: 0.8507, Precision: 0.7993, Recall: 0.7646, F1: 0.7759
Testing Loss: 0.9022, Accuracy: 0.8442, Precision: 0.8054, Recall: 0.7540, F1: 0.7673
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3168, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 57/70
Train Loss: 0.0471, Accuracy: 0.9687, Precision: 0.9485, Recall: 0.9525, F1: 0.9504
Validation Loss: 1.2497, Accuracy: 0.8507, Precision: 0.8104, Recall: 0.7659, F1: 0.7810
Testing Loss: 1.1308, Accuracy: 0.8418, Precision: 0.7965, Recall: 0.7442, F1: 0.7585
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2049, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 58/70
Train Loss: 0.0467, Accuracy: 0.9696, Precision: 0.9517, Recall: 0.9474, F1: 0.9494
Validation Loss: 1.2729, Accuracy: 0.8443, Precision: 0.8098, Recall: 0.7798, F1: 0.7918
Testing Loss: 1.1641, Accuracy: 0.8466, Precision: 0.8004, Recall: 0.7854, F1: 0.7918
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1378, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0460, Accuracy: 0.9720, Precision: 0.9555, Recall: 0.9530, F1: 0.9542
Validation Loss: 1.2154, Accuracy: 0.8571, Precision: 0.8217, Recall: 0.7863, F1: 0.8008
Testing Loss: 1.1251, Accuracy: 0.8454, Precision: 0.8049, Recall: 0.7682, F1: 0.7830
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2814, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 60/70
Train Loss: 0.0464, Accuracy: 0.9708, Precision: 0.9563, Recall: 0.9492, F1: 0.9526
Validation Loss: 1.3016, Accuracy: 0.8465, Precision: 0.8050, Recall: 0.7586, F1: 0.7752
Testing Loss: 1.1525, Accuracy: 0.8466, Precision: 0.8105, Recall: 0.7606, F1: 0.7793
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2658, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 61/70
Train Loss: 0.0464, Accuracy: 0.9704, Precision: 0.9534, Recall: 0.9497, F1: 0.9515
Validation Loss: 1.3074, Accuracy: 0.8507, Precision: 0.8220, Recall: 0.7966, F1: 0.8037
Testing Loss: 1.1603, Accuracy: 0.8478, Precision: 0.8135, Recall: 0.7939, F1: 0.8015
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3035, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 62/70
Train Loss: 0.0464, Accuracy: 0.9727, Precision: 0.9541, Recall: 0.9599, F1: 0.9568
Validation Loss: 1.3449, Accuracy: 0.8443, Precision: 0.8042, Recall: 0.7811, F1: 0.7894
Testing Loss: 1.1878, Accuracy: 0.8514, Precision: 0.8123, Recall: 0.7921, F1: 0.8011
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1891, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 63/70
Train Loss: 0.0481, Accuracy: 0.9670, Precision: 0.9447, Recall: 0.9494, F1: 0.9469
Validation Loss: 1.3899, Accuracy: 0.8486, Precision: 0.8042, Recall: 0.7678, F1: 0.7804
Testing Loss: 1.2365, Accuracy: 0.8502, Precision: 0.8041, Recall: 0.7621, F1: 0.7766
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2328, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 64/70
Train Loss: 0.0519, Accuracy: 0.9682, Precision: 0.9496, Recall: 0.9476, F1: 0.9486
Validation Loss: 1.1365, Accuracy: 0.8507, Precision: 0.8123, Recall: 0.7836, F1: 0.7961
Testing Loss: 0.9874, Accuracy: 0.8527, Precision: 0.8111, Recall: 0.7857, F1: 0.7947
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1441, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 65/70
Train Loss: 0.0918, Accuracy: 0.9613, Precision: 0.9426, Recall: 0.9432, F1: 0.9428
Validation Loss: 0.9426, Accuracy: 0.8230, Precision: 0.7227, Recall: 0.7404, F1: 0.7220
Testing Loss: 0.8963, Accuracy: 0.7947, Precision: 0.7432, Recall: 0.7180, F1: 0.7064
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5733, Accuracy: 0.7381, Precision: 0.8417, Recall: 0.7639, F1: 0.7376
Epoch 66/70
Train Loss: 0.1026, Accuracy: 0.9535, Precision: 0.9327, Recall: 0.9216, F1: 0.9269
Validation Loss: 0.8853, Accuracy: 0.8635, Precision: 0.8116, Recall: 0.7961, F1: 0.8020
Testing Loss: 0.9477, Accuracy: 0.8345, Precision: 0.8031, Recall: 0.7725, F1: 0.7828
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5730, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 67/70
Train Loss: 0.0669, Accuracy: 0.9628, Precision: 0.9398, Recall: 0.9401, F1: 0.9399
Validation Loss: 1.0564, Accuracy: 0.8465, Precision: 0.7900, Recall: 0.7874, F1: 0.7881
Testing Loss: 0.9311, Accuracy: 0.8454, Precision: 0.8024, Recall: 0.7957, F1: 0.7982
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1065, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 68/70
Train Loss: 0.0524, Accuracy: 0.9666, Precision: 0.9454, Recall: 0.9489, F1: 0.9471
Validation Loss: 1.1888, Accuracy: 0.8443, Precision: 0.7871, Recall: 0.7607, F1: 0.7687
Testing Loss: 1.1368, Accuracy: 0.8430, Precision: 0.8036, Recall: 0.7516, F1: 0.7639
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3037, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 69/70
Train Loss: 0.0460, Accuracy: 0.9680, Precision: 0.9500, Recall: 0.9435, F1: 0.9466
Validation Loss: 1.2018, Accuracy: 0.8486, Precision: 0.7892, Recall: 0.7805, F1: 0.7845
Testing Loss: 1.1373, Accuracy: 0.8478, Precision: 0.8079, Recall: 0.7881, F1: 0.7963
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3762, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 70/70
Train Loss: 0.0464, Accuracy: 0.9663, Precision: 0.9452, Recall: 0.9453, F1: 0.9452
Validation Loss: 1.2113, Accuracy: 0.8529, Precision: 0.7909, Recall: 0.7748, F1: 0.7812
Testing Loss: 1.1565, Accuracy: 0.8466, Precision: 0.8056, Recall: 0.7605, F1: 0.7736
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2346, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111

