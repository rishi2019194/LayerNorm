Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 2: 966
  Label 3: 495
  Label 1: 1011
  Label 4: 344
  Label 5: 260
Label counts for Validation:
  Label 3: 55
  Label 2: 107
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 2: 977
  Label 3: 505
  Label 1: 1016
  Label 4: 355
  Label 5: 218
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.6640, Accuracy: 0.2720, Precision: 0.1533, Recall: 0.1788, F1: 0.1512
Validation Loss: 1.6481, Accuracy: 0.3326, Precision: 0.1822, Recall: 0.2356, F1: 0.1572
Testing Loss: 1.6399, Accuracy: 0.3164, Precision: 0.1614, Recall: 0.2240, F1: 0.1486
LM Predictions:  [1, 2, 1, 2, 2, 1, 1, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 2, 2, 2, 1, 2]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8871, Accuracy: 0.1667, Precision: 0.0645, Recall: 0.1709, F1: 0.0922
Epoch 2/70
Train Loss: 1.6037, Accuracy: 0.3341, Precision: 0.1661, Recall: 0.2266, F1: 0.1906
Validation Loss: 1.4878, Accuracy: 0.4030, Precision: 0.2437, Recall: 0.2837, F1: 0.2295
Testing Loss: 1.4860, Accuracy: 0.3865, Precision: 0.2200, Recall: 0.2725, F1: 0.2172
LM Predictions:  [2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8987, Accuracy: 0.1905, Precision: 0.0500, Recall: 0.1455, F1: 0.0744
Epoch 3/70
Train Loss: 1.3710, Accuracy: 0.4702, Precision: 0.3270, Recall: 0.3153, F1: 0.2730
Validation Loss: 1.3201, Accuracy: 0.5394, Precision: 0.2711, Recall: 0.3668, F1: 0.3098
Testing Loss: 1.3142, Accuracy: 0.5821, Precision: 0.2921, Recall: 0.3967, F1: 0.3335
LM Predictions:  [1, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 1, 1, 1, 2, 2, 1, 2, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1246, Accuracy: 0.2143, Precision: 0.1595, Recall: 0.3164, F1: 0.1718
Epoch 4/70
Train Loss: 1.1214, Accuracy: 0.6033, Precision: 0.4502, Recall: 0.4343, F1: 0.4058
Validation Loss: 0.9079, Accuracy: 0.6930, Precision: 0.4628, Recall: 0.5368, F1: 0.4944
Testing Loss: 0.8682, Accuracy: 0.7150, Precision: 0.4861, Recall: 0.5512, F1: 0.5126
LM Predictions:  [0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 4, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4089, Accuracy: 0.1190, Precision: 0.0576, Recall: 0.1782, F1: 0.0656
Epoch 5/70
Train Loss: 0.9446, Accuracy: 0.6701, Precision: 0.4630, Recall: 0.5189, F1: 0.4769
Validation Loss: 0.8015, Accuracy: 0.7377, Precision: 0.5088, Recall: 0.5842, F1: 0.5399
Testing Loss: 0.7686, Accuracy: 0.7452, Precision: 0.6840, Recall: 0.5993, F1: 0.5542
LM Predictions:  [4, 0, 0, 0, 0, 0, 0, 2, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6763, Accuracy: 0.2143, Precision: 0.2170, Recall: 0.2727, F1: 0.1562
Epoch 6/70
Train Loss: 0.8192, Accuracy: 0.7197, Precision: 0.5546, Recall: 0.5652, F1: 0.5234
Validation Loss: 0.7536, Accuracy: 0.7377, Precision: 0.5432, Recall: 0.6029, F1: 0.5659
Testing Loss: 0.6837, Accuracy: 0.7585, Precision: 0.5831, Recall: 0.6276, F1: 0.5932
LM Predictions:  [4, 4, 0, 0, 0, 0, 0, 2, 4, 0, 1, 0, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 4, 0, 0, 0, 2, 4, 3, 0, 3, 0, 3, 1, 0, 3, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5606, Accuracy: 0.2619, Precision: 0.2988, Recall: 0.2709, F1: 0.2248
Epoch 7/70
Train Loss: 0.7420, Accuracy: 0.7534, Precision: 0.6108, Recall: 0.6245, F1: 0.6075
Validation Loss: 0.6860, Accuracy: 0.7996, Precision: 0.6585, Recall: 0.6937, F1: 0.6722
Testing Loss: 0.5970, Accuracy: 0.8176, Precision: 0.6753, Recall: 0.7053, F1: 0.6881
LM Predictions:  [3, 3, 3, 0, 0, 0, 0, 2, 3, 0, 0, 3, 3, 3, 3, 3, 3, 0, 3, 4, 3, 0, 3, 3, 3, 0, 0, 2, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9150, Accuracy: 0.3095, Precision: 0.2198, Recall: 0.3382, F1: 0.2035
Epoch 8/70
Train Loss: 0.6686, Accuracy: 0.7958, Precision: 0.6901, Recall: 0.6802, F1: 0.6665
Validation Loss: 0.6434, Accuracy: 0.8188, Precision: 0.7167, Recall: 0.7236, F1: 0.7083
Testing Loss: 0.5735, Accuracy: 0.8297, Precision: 0.7865, Recall: 0.7286, F1: 0.7170
LM Predictions:  [4, 3, 3, 0, 0, 0, 0, 2, 4, 0, 0, 5, 4, 4, 3, 4, 4, 0, 0, 4, 3, 0, 5, 4, 0, 0, 0, 2, 4, 0, 0, 4, 3, 0, 4, 0, 0, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4836, Accuracy: 0.3095, Precision: 0.2651, Recall: 0.2727, F1: 0.2083
Epoch 9/70
Train Loss: 0.6037, Accuracy: 0.8148, Precision: 0.7107, Recall: 0.7109, F1: 0.7013
Validation Loss: 0.6379, Accuracy: 0.8273, Precision: 0.7360, Recall: 0.7241, F1: 0.7087
Testing Loss: 0.5263, Accuracy: 0.8490, Precision: 0.7818, Recall: 0.7436, F1: 0.7380
LM Predictions:  [5, 3, 3, 0, 0, 0, 0, 2, 5, 0, 0, 0, 5, 0, 0, 5, 5, 0, 0, 0, 3, 0, 5, 0, 0, 0, 0, 2, 5, 0, 0, 5, 3, 0, 5, 0, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.1179, Accuracy: 0.1667, Precision: 0.1923, Recall: 0.1818, F1: 0.1163
Epoch 10/70
Train Loss: 0.6576, Accuracy: 0.7996, Precision: 0.7218, Recall: 0.7039, F1: 0.7007
Validation Loss: 0.5544, Accuracy: 0.8166, Precision: 0.7384, Recall: 0.7403, F1: 0.7363
Testing Loss: 0.5054, Accuracy: 0.8575, Precision: 0.8295, Recall: 0.7910, F1: 0.7964
LM Predictions:  [4, 3, 3, 0, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 3, 4, 5, 0, 5, 4, 3, 0, 4, 5, 5, 5, 0, 2, 5, 0, 5, 4, 4, 5, 1, 0, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7282, Accuracy: 0.2143, Precision: 0.2833, Recall: 0.1939, F1: 0.1821
Epoch 11/70
Train Loss: 0.5354, Accuracy: 0.8411, Precision: 0.7793, Recall: 0.7583, F1: 0.7641
Validation Loss: 0.6583, Accuracy: 0.8188, Precision: 0.7422, Recall: 0.7268, F1: 0.7243
Testing Loss: 0.4903, Accuracy: 0.8684, Precision: 0.8465, Recall: 0.7824, F1: 0.7916
LM Predictions:  [5, 3, 3, 0, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 3, 4, 5, 0, 5, 4, 3, 0, 4, 5, 5, 5, 0, 2, 5, 0, 5, 4, 3, 5, 1, 0, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0461, Accuracy: 0.1905, Precision: 0.2667, Recall: 0.1788, F1: 0.1645
Epoch 12/70
Train Loss: 0.5216, Accuracy: 0.8442, Precision: 0.7821, Recall: 0.7672, F1: 0.7720
Validation Loss: 0.5843, Accuracy: 0.8294, Precision: 0.7516, Recall: 0.7444, F1: 0.7388
Testing Loss: 0.4617, Accuracy: 0.8659, Precision: 0.8423, Recall: 0.7884, F1: 0.7949
LM Predictions:  [5, 3, 3, 0, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 0, 4, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 2, 5, 0, 0, 5, 5, 5, 1, 0, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9352, Accuracy: 0.1905, Precision: 0.3905, Recall: 0.1970, F1: 0.1956
Epoch 13/70
Train Loss: 0.4847, Accuracy: 0.8511, Precision: 0.7809, Recall: 0.7755, F1: 0.7771
Validation Loss: 0.5552, Accuracy: 0.8316, Precision: 0.7831, Recall: 0.7635, F1: 0.7696
Testing Loss: 0.4944, Accuracy: 0.8684, Precision: 0.8463, Recall: 0.8189, F1: 0.8275
LM Predictions:  [5, 3, 4, 0, 0, 0, 0, 2, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 2, 5, 0, 5, 5, 5, 5, 1, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7256, Accuracy: 0.2143, Precision: 0.5385, Recall: 0.2121, F1: 0.2357
Epoch 14/70
Train Loss: 0.4656, Accuracy: 0.8618, Precision: 0.7992, Recall: 0.7898, F1: 0.7938
Validation Loss: 0.4847, Accuracy: 0.8422, Precision: 0.7767, Recall: 0.7998, F1: 0.7864
Testing Loss: 0.4639, Accuracy: 0.8732, Precision: 0.8250, Recall: 0.8316, F1: 0.8251
LM Predictions:  [4, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 4, 5, 0, 5, 4, 5, 4, 1, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5507, Accuracy: 0.1905, Precision: 0.3718, Recall: 0.1955, F1: 0.1923
Epoch 15/70
Train Loss: 0.4399, Accuracy: 0.8670, Precision: 0.8128, Recall: 0.8116, F1: 0.8120
Validation Loss: 0.5374, Accuracy: 0.8465, Precision: 0.7992, Recall: 0.8100, F1: 0.8031
Testing Loss: 0.4300, Accuracy: 0.8744, Precision: 0.8389, Recall: 0.8392, F1: 0.8385
LM Predictions:  [5, 3, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 1, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7302, Accuracy: 0.1429, Precision: 0.3000, Recall: 0.1667, F1: 0.1698
Epoch 16/70
Train Loss: 0.4227, Accuracy: 0.8753, Precision: 0.8205, Recall: 0.8268, F1: 0.8230
Validation Loss: 0.5635, Accuracy: 0.8316, Precision: 0.7995, Recall: 0.8320, F1: 0.8037
Testing Loss: 0.4752, Accuracy: 0.8696, Precision: 0.8316, Recall: 0.8590, F1: 0.8383
LM Predictions:  [5, 3, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6697, Accuracy: 0.1190, Precision: 0.4167, Recall: 0.1333, F1: 0.1852
Epoch 17/70
Train Loss: 0.4240, Accuracy: 0.8743, Precision: 0.8231, Recall: 0.8289, F1: 0.8252
Validation Loss: 0.5434, Accuracy: 0.8465, Precision: 0.7955, Recall: 0.8126, F1: 0.8023
Testing Loss: 0.4804, Accuracy: 0.8829, Precision: 0.8432, Recall: 0.8545, F1: 0.8479
LM Predictions:  [5, 3, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 4, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7315, Accuracy: 0.1429, Precision: 0.3718, Recall: 0.1667, F1: 0.1667
Epoch 18/70
Train Loss: 0.3896, Accuracy: 0.8826, Precision: 0.8289, Recall: 0.8313, F1: 0.8299
Validation Loss: 0.5223, Accuracy: 0.8422, Precision: 0.7852, Recall: 0.7683, F1: 0.7749
Testing Loss: 0.4514, Accuracy: 0.8829, Precision: 0.8634, Recall: 0.8326, F1: 0.8450
LM Predictions:  [5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 4, 5, 5, 4, 5, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9655, Accuracy: 0.1429, Precision: 0.4524, Recall: 0.1652, F1: 0.1641
Epoch 19/70
Train Loss: 0.3899, Accuracy: 0.8833, Precision: 0.8327, Recall: 0.8330, F1: 0.8326
Validation Loss: 0.5457, Accuracy: 0.8380, Precision: 0.7803, Recall: 0.7631, F1: 0.7668
Testing Loss: 0.4796, Accuracy: 0.8708, Precision: 0.8564, Recall: 0.8056, F1: 0.8202
LM Predictions:  [5, 3, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 0, 5, 0, 0, 4, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9793, Accuracy: 0.1905, Precision: 0.5370, Recall: 0.2152, F1: 0.1969
Epoch 20/70
Train Loss: 0.3579, Accuracy: 0.8928, Precision: 0.8411, Recall: 0.8425, F1: 0.8414
Validation Loss: 0.5310, Accuracy: 0.8380, Precision: 0.7735, Recall: 0.7652, F1: 0.7634
Testing Loss: 0.4708, Accuracy: 0.8744, Precision: 0.8517, Recall: 0.8156, F1: 0.8270
LM Predictions:  [5, 3, 5, 0, 0, 0, 0, 2, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 4, 0, 5, 1, 5, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7253, Accuracy: 0.2143, Precision: 0.4611, Recall: 0.2303, F1: 0.2211
Epoch 21/70
Train Loss: 0.3611, Accuracy: 0.8978, Precision: 0.8520, Recall: 0.8567, F1: 0.8539
Validation Loss: 0.5202, Accuracy: 0.8422, Precision: 0.7909, Recall: 0.7903, F1: 0.7902
Testing Loss: 0.5086, Accuracy: 0.8671, Precision: 0.8289, Recall: 0.8010, F1: 0.8104
LM Predictions:  [4, 5, 5, 0, 5, 0, 0, 2, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 4, 0, 5, 4, 5, 5, 4, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7295, Accuracy: 0.2381, Precision: 0.5139, Recall: 0.2258, F1: 0.2427
Epoch 22/70
Train Loss: 0.3436, Accuracy: 0.9002, Precision: 0.8547, Recall: 0.8659, F1: 0.8593
Validation Loss: 0.5913, Accuracy: 0.8380, Precision: 0.7847, Recall: 0.7544, F1: 0.7601
Testing Loss: 0.5144, Accuracy: 0.8732, Precision: 0.8570, Recall: 0.8046, F1: 0.8199
LM Predictions:  [5, 3, 5, 0, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 4, 0, 5, 4, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0954, Accuracy: 0.2143, Precision: 0.4795, Recall: 0.2303, F1: 0.2143
Epoch 23/70
Train Loss: 0.3434, Accuracy: 0.9009, Precision: 0.8546, Recall: 0.8637, F1: 0.8581
Validation Loss: 0.5518, Accuracy: 0.8443, Precision: 0.8026, Recall: 0.8273, F1: 0.8106
Testing Loss: 0.5056, Accuracy: 0.8732, Precision: 0.8333, Recall: 0.8425, F1: 0.8364
LM Predictions:  [4, 3, 5, 5, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 4, 5, 5, 4, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4914, Accuracy: 0.2143, Precision: 0.5417, Recall: 0.2121, F1: 0.2687
Epoch 24/70
Train Loss: 0.3301, Accuracy: 0.9044, Precision: 0.8601, Recall: 0.8703, F1: 0.8641
Validation Loss: 0.5838, Accuracy: 0.8358, Precision: 0.7748, Recall: 0.7791, F1: 0.7740
Testing Loss: 0.5041, Accuracy: 0.8708, Precision: 0.8357, Recall: 0.8237, F1: 0.8279
LM Predictions:  [4, 3, 5, 0, 0, 0, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 5, 0, 5, 0, 5, 4, 5, 5, 4, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8058, Accuracy: 0.2381, Precision: 0.4343, Recall: 0.2273, F1: 0.2478
Epoch 25/70
Train Loss: 0.3113, Accuracy: 0.9113, Precision: 0.8648, Recall: 0.8696, F1: 0.8669
Validation Loss: 0.6145, Accuracy: 0.8380, Precision: 0.7879, Recall: 0.8078, F1: 0.7944
Testing Loss: 0.4959, Accuracy: 0.8732, Precision: 0.8307, Recall: 0.8398, F1: 0.8342
LM Predictions:  [5, 3, 4, 5, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4360, Accuracy: 0.2619, Precision: 0.6667, Recall: 0.2424, F1: 0.3166
Epoch 26/70
Train Loss: 0.3019, Accuracy: 0.9118, Precision: 0.8683, Recall: 0.8749, F1: 0.8711
Validation Loss: 0.5813, Accuracy: 0.8571, Precision: 0.8173, Recall: 0.8388, F1: 0.8264
Testing Loss: 0.5193, Accuracy: 0.8696, Precision: 0.8339, Recall: 0.8317, F1: 0.8311
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1930, Accuracy: 0.3095, Precision: 0.7381, Recall: 0.2727, F1: 0.3399
Epoch 27/70
Train Loss: 0.2899, Accuracy: 0.9146, Precision: 0.8668, Recall: 0.8815, F1: 0.8727
Validation Loss: 0.5948, Accuracy: 0.8443, Precision: 0.7975, Recall: 0.8027, F1: 0.7994
Testing Loss: 0.5006, Accuracy: 0.8792, Precision: 0.8411, Recall: 0.8352, F1: 0.8376
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3533, Accuracy: 0.2381, Precision: 0.7222, Recall: 0.2273, F1: 0.2817
Epoch 28/70
Train Loss: 0.3277, Accuracy: 0.9042, Precision: 0.8569, Recall: 0.8648, F1: 0.8603
Validation Loss: 0.5987, Accuracy: 0.8273, Precision: 0.7775, Recall: 0.7730, F1: 0.7731
Testing Loss: 0.5339, Accuracy: 0.8659, Precision: 0.8216, Recall: 0.8190, F1: 0.8180
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 0, 4, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 4, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1922, Accuracy: 0.3571, Precision: 0.6926, Recall: 0.3030, F1: 0.3434
Epoch 29/70
Train Loss: 0.2684, Accuracy: 0.9248, Precision: 0.8839, Recall: 0.8927, F1: 0.8876
Validation Loss: 0.6023, Accuracy: 0.8422, Precision: 0.7897, Recall: 0.7759, F1: 0.7817
Testing Loss: 0.5287, Accuracy: 0.8684, Precision: 0.8324, Recall: 0.8189, F1: 0.8244
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 5, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6993, Accuracy: 0.2143, Precision: 0.5417, Recall: 0.2121, F1: 0.2414
Epoch 30/70
Train Loss: 0.2718, Accuracy: 0.9208, Precision: 0.8789, Recall: 0.8869, F1: 0.8821
Validation Loss: 0.5601, Accuracy: 0.8507, Precision: 0.8042, Recall: 0.8101, F1: 0.8063
Testing Loss: 0.5100, Accuracy: 0.8671, Precision: 0.8294, Recall: 0.8276, F1: 0.8275
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4889, Accuracy: 0.2619, Precision: 0.7407, Recall: 0.2606, F1: 0.3056
Epoch 31/70
Train Loss: 0.2526, Accuracy: 0.9303, Precision: 0.8909, Recall: 0.9074, F1: 0.8970
Validation Loss: 0.6675, Accuracy: 0.8358, Precision: 0.7764, Recall: 0.7852, F1: 0.7786
Testing Loss: 0.5585, Accuracy: 0.8671, Precision: 0.8201, Recall: 0.8188, F1: 0.8191
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5005, Accuracy: 0.2619, Precision: 0.7407, Recall: 0.2606, F1: 0.3056
Epoch 32/70
Train Loss: 0.2545, Accuracy: 0.9253, Precision: 0.8835, Recall: 0.8996, F1: 0.8896
Validation Loss: 0.6264, Accuracy: 0.8401, Precision: 0.7942, Recall: 0.8022, F1: 0.7973
Testing Loss: 0.5638, Accuracy: 0.8647, Precision: 0.8250, Recall: 0.8239, F1: 0.8230
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5813, Accuracy: 0.2619, Precision: 0.7500, Recall: 0.2606, F1: 0.3129
Epoch 33/70
Train Loss: 0.2619, Accuracy: 0.9227, Precision: 0.8780, Recall: 0.8921, F1: 0.8834
Validation Loss: 0.6471, Accuracy: 0.8465, Precision: 0.7934, Recall: 0.8181, F1: 0.8026
Testing Loss: 0.5499, Accuracy: 0.8671, Precision: 0.8164, Recall: 0.8309, F1: 0.8228
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1522, Accuracy: 0.3571, Precision: 0.7196, Recall: 0.3030, F1: 0.3556
Epoch 34/70
Train Loss: 0.2566, Accuracy: 0.9296, Precision: 0.8911, Recall: 0.9060, F1: 0.8974
Validation Loss: 0.9626, Accuracy: 0.7655, Precision: 0.7372, Recall: 0.6992, F1: 0.6954
Testing Loss: 0.8650, Accuracy: 0.7947, Precision: 0.7667, Recall: 0.7254, F1: 0.7326
LM Predictions:  [5, 3, 3, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0312, Accuracy: 0.2143, Precision: 0.6667, Recall: 0.2303, F1: 0.2408
Epoch 35/70
Train Loss: 0.3181, Accuracy: 0.9082, Precision: 0.8642, Recall: 0.8794, F1: 0.8697
Validation Loss: 0.6279, Accuracy: 0.8443, Precision: 0.8034, Recall: 0.8214, F1: 0.8097
Testing Loss: 0.5393, Accuracy: 0.8732, Precision: 0.8406, Recall: 0.8417, F1: 0.8394
LM Predictions:  [5, 3, 4, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3451, Accuracy: 0.3333, Precision: 0.7500, Recall: 0.2879, F1: 0.3594
Epoch 36/70
Train Loss: 0.2791, Accuracy: 0.9210, Precision: 0.8832, Recall: 0.8990, F1: 0.8889
Validation Loss: 0.5731, Accuracy: 0.8486, Precision: 0.8124, Recall: 0.8185, F1: 0.8120
Testing Loss: 0.5309, Accuracy: 0.8647, Precision: 0.8287, Recall: 0.8249, F1: 0.8243
LM Predictions:  [5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5516, Accuracy: 0.2381, Precision: 0.7381, Recall: 0.2273, F1: 0.2937
Epoch 37/70
Train Loss: 0.2417, Accuracy: 0.9305, Precision: 0.8881, Recall: 0.9065, F1: 0.8955
Validation Loss: 0.6672, Accuracy: 0.8337, Precision: 0.7963, Recall: 0.7721, F1: 0.7823
Testing Loss: 0.5976, Accuracy: 0.8587, Precision: 0.8299, Recall: 0.8034, F1: 0.8127
LM Predictions:  [5, 5, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8263, Accuracy: 0.2381, Precision: 0.7407, Recall: 0.2439, F1: 0.2803
Epoch 38/70
Train Loss: 0.2269, Accuracy: 0.9360, Precision: 0.8986, Recall: 0.9194, F1: 0.9062
Validation Loss: 0.6639, Accuracy: 0.8465, Precision: 0.7986, Recall: 0.8142, F1: 0.8044
Testing Loss: 0.5190, Accuracy: 0.8708, Precision: 0.8302, Recall: 0.8330, F1: 0.8299
LM Predictions:  [4, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1357, Accuracy: 0.3333, Precision: 0.7667, Recall: 0.2879, F1: 0.3685
Epoch 39/70
Train Loss: 0.2118, Accuracy: 0.9391, Precision: 0.9002, Recall: 0.9205, F1: 0.9075
Validation Loss: 0.7050, Accuracy: 0.8443, Precision: 0.8000, Recall: 0.8061, F1: 0.8008
Testing Loss: 0.5996, Accuracy: 0.8708, Precision: 0.8345, Recall: 0.8306, F1: 0.8303
LM Predictions:  [5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5511, Accuracy: 0.2619, Precision: 0.7667, Recall: 0.2424, F1: 0.3278
Epoch 40/70
Train Loss: 0.2232, Accuracy: 0.9357, Precision: 0.8968, Recall: 0.9169, F1: 0.9035
Validation Loss: 0.6551, Accuracy: 0.8486, Precision: 0.8012, Recall: 0.8049, F1: 0.8029
Testing Loss: 0.5888, Accuracy: 0.8611, Precision: 0.8144, Recall: 0.8082, F1: 0.8088
LM Predictions:  [4, 3, 4, 0, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 4, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 0, 2, 4, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3073, Accuracy: 0.3810, Precision: 0.7148, Recall: 0.3364, F1: 0.3611
Epoch 41/70
Train Loss: 0.2095, Accuracy: 0.9379, Precision: 0.8988, Recall: 0.9115, F1: 0.9037
Validation Loss: 0.6752, Accuracy: 0.8465, Precision: 0.7934, Recall: 0.8042, F1: 0.7975
Testing Loss: 0.5585, Accuracy: 0.8708, Precision: 0.8250, Recall: 0.8243, F1: 0.8240
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0914, Accuracy: 0.3810, Precision: 0.7500, Recall: 0.3364, F1: 0.3818
Epoch 42/70
Train Loss: 0.2059, Accuracy: 0.9424, Precision: 0.9040, Recall: 0.9187, F1: 0.9097
Validation Loss: 0.7293, Accuracy: 0.8273, Precision: 0.7802, Recall: 0.7814, F1: 0.7786
Testing Loss: 0.6452, Accuracy: 0.8587, Precision: 0.8186, Recall: 0.8063, F1: 0.8099
LM Predictions:  [5, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 5, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5714, Accuracy: 0.2619, Precision: 0.7500, Recall: 0.2606, F1: 0.3129
Epoch 43/70
Train Loss: 0.2094, Accuracy: 0.9412, Precision: 0.9024, Recall: 0.9233, F1: 0.9106
Validation Loss: 0.6859, Accuracy: 0.8422, Precision: 0.7884, Recall: 0.8028, F1: 0.7943
Testing Loss: 0.5628, Accuracy: 0.8671, Precision: 0.8179, Recall: 0.8245, F1: 0.8202
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2068, Accuracy: 0.3810, Precision: 0.7315, Recall: 0.3364, F1: 0.3748
Epoch 44/70
Train Loss: 0.1978, Accuracy: 0.9457, Precision: 0.9085, Recall: 0.9278, F1: 0.9163
Validation Loss: 0.6807, Accuracy: 0.8401, Precision: 0.7939, Recall: 0.7859, F1: 0.7894
Testing Loss: 0.6135, Accuracy: 0.8635, Precision: 0.8300, Recall: 0.8073, F1: 0.8169
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 1, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1890, Accuracy: 0.3571, Precision: 0.6667, Recall: 0.3212, F1: 0.3631
Epoch 45/70
Train Loss: 0.2042, Accuracy: 0.9412, Precision: 0.9046, Recall: 0.9237, F1: 0.9117
Validation Loss: 0.7285, Accuracy: 0.8273, Precision: 0.7743, Recall: 0.7843, F1: 0.7765
Testing Loss: 0.5930, Accuracy: 0.8732, Precision: 0.8289, Recall: 0.8320, F1: 0.8287
LM Predictions:  [5, 3, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 1, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4060, Accuracy: 0.3095, Precision: 0.6786, Recall: 0.2909, F1: 0.3462
Epoch 46/70
Train Loss: 0.1973, Accuracy: 0.9450, Precision: 0.9069, Recall: 0.9263, F1: 0.9143
Validation Loss: 0.6880, Accuracy: 0.8401, Precision: 0.7816, Recall: 0.7983, F1: 0.7885
Testing Loss: 0.6322, Accuracy: 0.8659, Precision: 0.8171, Recall: 0.8179, F1: 0.8166
LM Predictions:  [4, 3, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1831, Accuracy: 0.3810, Precision: 0.7315, Recall: 0.3364, F1: 0.3748
Epoch 47/70
Train Loss: 0.1977, Accuracy: 0.9443, Precision: 0.9077, Recall: 0.9245, F1: 0.9141
Validation Loss: 0.6887, Accuracy: 0.8465, Precision: 0.8029, Recall: 0.8053, F1: 0.8035
Testing Loss: 0.6528, Accuracy: 0.8539, Precision: 0.8112, Recall: 0.7971, F1: 0.8017
LM Predictions:  [4, 3, 4, 0, 0, 5, 5, 5, 0, 5, 5, 5, 4, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2144, Accuracy: 0.3810, Precision: 0.7222, Recall: 0.3364, F1: 0.3675
Epoch 48/70
Train Loss: 0.1879, Accuracy: 0.9443, Precision: 0.9083, Recall: 0.9270, F1: 0.9153
Validation Loss: 0.7283, Accuracy: 0.8422, Precision: 0.7860, Recall: 0.7929, F1: 0.7880
Testing Loss: 0.6552, Accuracy: 0.8635, Precision: 0.8095, Recall: 0.8074, F1: 0.8076
LM Predictions:  [4, 3, 4, 0, 5, 0, 0, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 0, 5, 1, 3, 5, 4, 4, 5, 5, 0, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0254, Accuracy: 0.4048, Precision: 0.7030, Recall: 0.3515, F1: 0.3641
Epoch 49/70
Train Loss: 0.1829, Accuracy: 0.9462, Precision: 0.9127, Recall: 0.9296, F1: 0.9193
Validation Loss: 0.7317, Accuracy: 0.8316, Precision: 0.7703, Recall: 0.7818, F1: 0.7741
Testing Loss: 0.6384, Accuracy: 0.8623, Precision: 0.8161, Recall: 0.8143, F1: 0.8149
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 3, 4, 5, 5, 5, 4, 5, 5, 4, 4, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 3, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0155, Accuracy: 0.4286, Precision: 0.6899, Recall: 0.3682, F1: 0.4022
Epoch 50/70
Train Loss: 0.2214, Accuracy: 0.9355, Precision: 0.8939, Recall: 0.9110, F1: 0.9004
Validation Loss: 0.6729, Accuracy: 0.8486, Precision: 0.7891, Recall: 0.7817, F1: 0.7788
Testing Loss: 0.6560, Accuracy: 0.8563, Precision: 0.8035, Recall: 0.7728, F1: 0.7809
LM Predictions:  [4, 3, 4, 0, 4, 0, 0, 5, 4, 5, 0, 5, 4, 5, 5, 4, 5, 0, 5, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 0, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0789, Accuracy: 0.4286, Precision: 0.6884, Recall: 0.3848, F1: 0.3546
Epoch 51/70
Train Loss: 0.1933, Accuracy: 0.9440, Precision: 0.9088, Recall: 0.9197, F1: 0.9134
Validation Loss: 0.7139, Accuracy: 0.8337, Precision: 0.7805, Recall: 0.7886, F1: 0.7831
Testing Loss: 0.6532, Accuracy: 0.8611, Precision: 0.8201, Recall: 0.8215, F1: 0.8190
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3021, Accuracy: 0.4048, Precision: 0.7619, Recall: 0.3515, F1: 0.4000
Epoch 52/70
Train Loss: 0.1858, Accuracy: 0.9450, Precision: 0.9089, Recall: 0.9278, F1: 0.9159
Validation Loss: 0.7151, Accuracy: 0.8273, Precision: 0.7677, Recall: 0.7777, F1: 0.7708
Testing Loss: 0.6719, Accuracy: 0.8671, Precision: 0.8235, Recall: 0.8199, F1: 0.8210
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 5, 4, 5, 5, 5, 4, 5, 5, 3, 4, 0, 3, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 3, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7633, Accuracy: 0.4524, Precision: 0.7000, Recall: 0.3848, F1: 0.4176
Epoch 53/70
Train Loss: 0.1867, Accuracy: 0.9438, Precision: 0.9055, Recall: 0.9221, F1: 0.9118
Validation Loss: 0.7615, Accuracy: 0.8337, Precision: 0.7807, Recall: 0.7957, F1: 0.7864
Testing Loss: 0.6416, Accuracy: 0.8696, Precision: 0.8216, Recall: 0.8215, F1: 0.8207
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0593, Accuracy: 0.4048, Precision: 0.7316, Recall: 0.3515, F1: 0.3864
Epoch 54/70
Train Loss: 0.1785, Accuracy: 0.9459, Precision: 0.9099, Recall: 0.9325, F1: 0.9183
Validation Loss: 0.6882, Accuracy: 0.8273, Precision: 0.7756, Recall: 0.7861, F1: 0.7794
Testing Loss: 0.6258, Accuracy: 0.8659, Precision: 0.8238, Recall: 0.8176, F1: 0.8199
LM Predictions:  [4, 3, 4, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 5, 4, 5, 0, 5, 1, 3, 0, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0504, Accuracy: 0.4048, Precision: 0.7475, Recall: 0.3515, F1: 0.3965
Epoch 55/70
Train Loss: 0.1705, Accuracy: 0.9521, Precision: 0.9164, Recall: 0.9362, F1: 0.9238
Validation Loss: 0.7802, Accuracy: 0.8273, Precision: 0.7770, Recall: 0.7759, F1: 0.7744
Testing Loss: 0.6950, Accuracy: 0.8599, Precision: 0.8183, Recall: 0.8110, F1: 0.8137
LM Predictions:  [4, 3, 4, 5, 5, 5, 0, 5, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 3, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8389, Accuracy: 0.4286, Precision: 0.7778, Recall: 0.3682, F1: 0.4315
Epoch 56/70
Train Loss: 0.1666, Accuracy: 0.9514, Precision: 0.9147, Recall: 0.9359, F1: 0.9226
Validation Loss: 0.7631, Accuracy: 0.8337, Precision: 0.7798, Recall: 0.7963, F1: 0.7857
Testing Loss: 0.6966, Accuracy: 0.8635, Precision: 0.8182, Recall: 0.8191, F1: 0.8176
LM Predictions:  [4, 3, 4, 5, 5, 5, 0, 5, 4, 5, 3, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 3, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8890, Accuracy: 0.4286, Precision: 0.7361, Recall: 0.3682, F1: 0.4260
Epoch 57/70
Train Loss: 0.1774, Accuracy: 0.9488, Precision: 0.9129, Recall: 0.9339, F1: 0.9213
Validation Loss: 0.7500, Accuracy: 0.8380, Precision: 0.7795, Recall: 0.7861, F1: 0.7822
Testing Loss: 0.6841, Accuracy: 0.8539, Precision: 0.8111, Recall: 0.7915, F1: 0.7997
LM Predictions:  [4, 3, 4, 0, 5, 0, 0, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 0, 5, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0913, Accuracy: 0.4286, Precision: 0.7194, Recall: 0.3848, F1: 0.3798
Epoch 58/70
Train Loss: 0.1697, Accuracy: 0.9507, Precision: 0.9159, Recall: 0.9340, F1: 0.9234
Validation Loss: 0.8468, Accuracy: 0.8230, Precision: 0.7631, Recall: 0.7605, F1: 0.7596
Testing Loss: 0.7087, Accuracy: 0.8659, Precision: 0.8172, Recall: 0.7972, F1: 0.8042
LM Predictions:  [4, 3, 4, 5, 5, 0, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 4, 0, 4, 1, 3, 4, 4, 4, 0, 0, 0, 2, 4, 0, 5, 4, 0, 5, 3, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2178, Accuracy: 0.4524, Precision: 0.6111, Recall: 0.4015, F1: 0.3866
Epoch 59/70
Train Loss: 0.1772, Accuracy: 0.9471, Precision: 0.9133, Recall: 0.9272, F1: 0.9191
Validation Loss: 0.8803, Accuracy: 0.8294, Precision: 0.7735, Recall: 0.7589, F1: 0.7639
Testing Loss: 0.7051, Accuracy: 0.8623, Precision: 0.8218, Recall: 0.7954, F1: 0.8072
LM Predictions:  [4, 3, 4, 5, 5, 0, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 5, 4, 0, 5, 3, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1126, Accuracy: 0.4524, Precision: 0.6667, Recall: 0.4015, F1: 0.4192
Epoch 60/70
Train Loss: 0.1865, Accuracy: 0.9481, Precision: 0.9117, Recall: 0.9209, F1: 0.9158
Validation Loss: 0.7345, Accuracy: 0.8273, Precision: 0.7712, Recall: 0.7734, F1: 0.7715
Testing Loss: 0.5858, Accuracy: 0.8732, Precision: 0.8260, Recall: 0.8182, F1: 0.8217
LM Predictions:  [4, 3, 4, 5, 5, 0, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 0, 0, 4, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 5, 4, 0, 5, 3, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9960, Accuracy: 0.4524, Precision: 0.6308, Recall: 0.4015, F1: 0.3936
Epoch 61/70
Train Loss: 0.1731, Accuracy: 0.9500, Precision: 0.9194, Recall: 0.9224, F1: 0.9208
Validation Loss: 0.8421, Accuracy: 0.8273, Precision: 0.7773, Recall: 0.7845, F1: 0.7783
Testing Loss: 0.6659, Accuracy: 0.8635, Precision: 0.8147, Recall: 0.8171, F1: 0.8154
LM Predictions:  [4, 3, 4, 5, 5, 5, 5, 2, 4, 5, 5, 5, 4, 5, 5, 5, 4, 0, 4, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9740, Accuracy: 0.4048, Precision: 0.6528, Recall: 0.3515, F1: 0.3884
Epoch 62/70
Train Loss: 0.1697, Accuracy: 0.9493, Precision: 0.9147, Recall: 0.9330, F1: 0.9222
Validation Loss: 0.9074, Accuracy: 0.8188, Precision: 0.7613, Recall: 0.7783, F1: 0.7666
Testing Loss: 0.7384, Accuracy: 0.8539, Precision: 0.8056, Recall: 0.8080, F1: 0.8063
LM Predictions:  [4, 3, 4, 5, 5, 0, 5, 5, 4, 5, 5, 5, 4, 5, 5, 5, 4, 0, 4, 1, 3, 5, 4, 4, 5, 0, 5, 2, 4, 0, 5, 4, 0, 5, 3, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9413, Accuracy: 0.4524, Precision: 0.7405, Recall: 0.4015, F1: 0.4248
Epoch 63/70
Train Loss: 0.1664, Accuracy: 0.9509, Precision: 0.9172, Recall: 0.9337, F1: 0.9240
Validation Loss: 0.7941, Accuracy: 0.8337, Precision: 0.7807, Recall: 0.7975, F1: 0.7879
Testing Loss: 0.6195, Accuracy: 0.8623, Precision: 0.8108, Recall: 0.8104, F1: 0.8102
LM Predictions:  [4, 3, 4, 5, 5, 5, 0, 5, 4, 4, 5, 5, 4, 5, 5, 4, 4, 0, 1, 1, 3, 4, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 5, 4, 2, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0086, Accuracy: 0.4286, Precision: 0.6432, Recall: 0.3848, F1: 0.4226
Epoch 64/70
Train Loss: 0.1640, Accuracy: 0.9516, Precision: 0.9189, Recall: 0.9306, F1: 0.9239
Validation Loss: 0.7528, Accuracy: 0.8443, Precision: 0.7976, Recall: 0.7731, F1: 0.7788
Testing Loss: 0.6212, Accuracy: 0.8708, Precision: 0.8347, Recall: 0.7877, F1: 0.8007
LM Predictions:  [0, 3, 4, 0, 5, 0, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 5, 2, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9549, Accuracy: 0.4048, Precision: 0.6019, Recall: 0.3879, F1: 0.3767
Epoch 65/70
Train Loss: 0.1634, Accuracy: 0.9483, Precision: 0.9114, Recall: 0.9168, F1: 0.9136
Validation Loss: 0.8895, Accuracy: 0.8252, Precision: 0.7728, Recall: 0.7640, F1: 0.7663
Testing Loss: 0.6926, Accuracy: 0.8599, Precision: 0.8224, Recall: 0.7929, F1: 0.8054
LM Predictions:  [4, 3, 4, 0, 5, 0, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 1, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 5, 3, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8572, Accuracy: 0.4762, Precision: 0.6474, Recall: 0.4348, F1: 0.4404
Epoch 66/70
Train Loss: 0.1536, Accuracy: 0.9540, Precision: 0.9223, Recall: 0.9379, F1: 0.9283
Validation Loss: 0.9505, Accuracy: 0.8273, Precision: 0.7783, Recall: 0.7632, F1: 0.7680
Testing Loss: 0.7547, Accuracy: 0.8575, Precision: 0.8107, Recall: 0.7881, F1: 0.7973
LM Predictions:  [4, 3, 4, 5, 5, 5, 0, 1, 4, 4, 1, 5, 4, 5, 1, 5, 4, 0, 1, 1, 3, 1, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 5, 1, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7133, Accuracy: 0.4762, Precision: 0.6169, Recall: 0.4515, F1: 0.4072
Epoch 67/70
Train Loss: 0.1780, Accuracy: 0.9488, Precision: 0.9171, Recall: 0.9228, F1: 0.9195
Validation Loss: 0.8507, Accuracy: 0.8294, Precision: 0.7742, Recall: 0.7842, F1: 0.7755
Testing Loss: 0.7263, Accuracy: 0.8599, Precision: 0.8096, Recall: 0.8101, F1: 0.8093
LM Predictions:  [4, 3, 4, 5, 5, 5, 0, 2, 4, 5, 5, 5, 4, 5, 5, 5, 0, 0, 4, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 5, 4, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0196, Accuracy: 0.4286, Precision: 0.6361, Recall: 0.3848, F1: 0.3776
Epoch 68/70
Train Loss: 0.1549, Accuracy: 0.9509, Precision: 0.9176, Recall: 0.9250, F1: 0.9208
Validation Loss: 0.8237, Accuracy: 0.8380, Precision: 0.7961, Recall: 0.8142, F1: 0.8024
Testing Loss: 0.7725, Accuracy: 0.8502, Precision: 0.8128, Recall: 0.8170, F1: 0.8103
LM Predictions:  [4, 3, 4, 5, 5, 5, 5, 2, 4, 5, 5, 5, 4, 5, 5, 5, 5, 0, 1, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 0, 5, 4, 4, 5, 5, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9746, Accuracy: 0.4286, Precision: 0.7333, Recall: 0.3848, F1: 0.4674
Epoch 69/70
Train Loss: 0.1553, Accuracy: 0.9557, Precision: 0.9257, Recall: 0.9352, F1: 0.9299
Validation Loss: 0.7644, Accuracy: 0.8273, Precision: 0.7815, Recall: 0.7807, F1: 0.7796
Testing Loss: 0.7139, Accuracy: 0.8599, Precision: 0.8144, Recall: 0.8038, F1: 0.8078
LM Predictions:  [0, 3, 4, 5, 5, 5, 5, 2, 0, 5, 1, 5, 0, 5, 5, 5, 5, 0, 1, 1, 3, 5, 0, 4, 5, 0, 0, 2, 0, 0, 5, 0, 0, 5, 4, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2211, Accuracy: 0.3095, Precision: 0.5456, Recall: 0.3273, F1: 0.3189
Epoch 70/70
Train Loss: 0.1529, Accuracy: 0.9571, Precision: 0.9265, Recall: 0.9400, F1: 0.9323
Validation Loss: 0.9057, Accuracy: 0.8401, Precision: 0.7918, Recall: 0.7803, F1: 0.7846
Testing Loss: 0.7893, Accuracy: 0.8563, Precision: 0.8075, Recall: 0.7761, F1: 0.7881
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 2, 4, 5, 1, 5, 4, 5, 5, 5, 5, 0, 1, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2859, Accuracy: 0.4524, Precision: 0.5919, Recall: 0.4182, F1: 0.4071
Label Memorization Analysis: 
LM Predictions:  [4, 3, 4, 0, 5, 5, 0, 2, 4, 5, 1, 5, 4, 5, 5, 5, 5, 0, 1, 1, 3, 5, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2859, Accuracy: 0.4524, Precision: 0.5919, Recall: 0.4182, F1: 0.4071

