Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 1: 1011
  Label 0: 1141
  Label 2: 966
  Label 4: 344
  Label 3: 495
  Label 5: 260
Label counts for Validation:
  Label 2: 107
  Label 0: 127
  Label 1: 113
  Label 5: 29
  Label 4: 38
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 1: 1025
  Label 0: 1150
  Label 2: 972
  Label 4: 351
  Label 3: 501
  Label 5: 218
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.5206, Accuracy: 0.3785, Precision: 0.2281, Recall: 0.2485, F1: 0.2120
Validation Loss: 1.2653, Accuracy: 0.5373, Precision: 0.3871, Recall: 0.3682, F1: 0.3260
Testing Loss: 1.2542, Accuracy: 0.5507, Precision: 0.3665, Recall: 0.3754, F1: 0.3289
LM Predictions:  [1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1737, Accuracy: 0.2381, Precision: 0.1067, Recall: 0.1984, F1: 0.1218
Epoch 2/70
Train Loss: 1.1075, Accuracy: 0.6026, Precision: 0.4139, Recall: 0.4463, F1: 0.4167
Validation Loss: 0.9314, Accuracy: 0.6951, Precision: 0.4859, Recall: 0.5327, F1: 0.4999
Testing Loss: 0.8641, Accuracy: 0.7234, Precision: 0.5045, Recall: 0.5540, F1: 0.5201
LM Predictions:  [4, 0, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9274, Accuracy: 0.2381, Precision: 0.3212, Recall: 0.2270, F1: 0.1584
Epoch 3/70
Train Loss: 0.8507, Accuracy: 0.7055, Precision: 0.5875, Recall: 0.5585, F1: 0.5378
Validation Loss: 0.7776, Accuracy: 0.7505, Precision: 0.6933, Recall: 0.6450, F1: 0.6499
Testing Loss: 0.7040, Accuracy: 0.7621, Precision: 0.7117, Recall: 0.6569, F1: 0.6547
LM Predictions:  [5, 5, 3, 4, 3, 4, 5, 5, 0, 0, 2, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 3, 4, 3, 0, 3, 4, 4, 0, 4, 0, 0, 5, 3, 3, 1, 2, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6612, Accuracy: 0.1905, Precision: 0.3310, Recall: 0.1548, F1: 0.1726
Epoch 4/70
Train Loss: 0.7424, Accuracy: 0.7406, Precision: 0.6384, Recall: 0.6216, F1: 0.6178
Validation Loss: 0.6646, Accuracy: 0.7953, Precision: 0.8345, Recall: 0.6663, F1: 0.6716
Testing Loss: 0.6135, Accuracy: 0.7911, Precision: 0.6613, Recall: 0.6619, F1: 0.6465
LM Predictions:  [0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 4, 3, 0, 0, 5, 4, 0, 4, 0, 0, 0, 4, 0, 4, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8784, Accuracy: 0.1429, Precision: 0.0778, Recall: 0.1217, F1: 0.0855
Epoch 5/70
Train Loss: 0.6710, Accuracy: 0.7854, Precision: 0.7075, Recall: 0.6871, F1: 0.6873
Validation Loss: 0.6258, Accuracy: 0.8017, Precision: 0.7493, Recall: 0.7037, F1: 0.7121
Testing Loss: 0.5677, Accuracy: 0.8092, Precision: 0.7285, Recall: 0.6999, F1: 0.6977
LM Predictions:  [0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9625, Accuracy: 0.1190, Precision: 0.0278, Recall: 0.0926, F1: 0.0427
Epoch 6/70
Train Loss: 0.6018, Accuracy: 0.8093, Precision: 0.7315, Recall: 0.7191, F1: 0.7221
Validation Loss: 0.6207, Accuracy: 0.8145, Precision: 0.7925, Recall: 0.7091, F1: 0.7183
Testing Loss: 0.5459, Accuracy: 0.8225, Precision: 0.7018, Recall: 0.7049, F1: 0.6978
LM Predictions:  [0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 3, 0, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.4979, Accuracy: 0.1429, Precision: 0.0303, Recall: 0.1111, F1: 0.0476
Epoch 7/70
Train Loss: 0.5716, Accuracy: 0.8214, Precision: 0.7454, Recall: 0.7241, F1: 0.7285
Validation Loss: 0.5592, Accuracy: 0.8465, Precision: 0.8209, Recall: 0.8072, F1: 0.8132
Testing Loss: 0.4978, Accuracy: 0.8430, Precision: 0.8091, Recall: 0.8069, F1: 0.8036
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 3, 5, 0, 5, 5, 5, 5, 5, 5, 3, 5, 5, 0, 5, 5, 5, 4, 5, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7421, Accuracy: 0.0476, Precision: 0.1905, Recall: 0.0423, F1: 0.0625
Epoch 8/70
Train Loss: 0.5423, Accuracy: 0.8335, Precision: 0.7749, Recall: 0.7561, F1: 0.7630
Validation Loss: 0.6100, Accuracy: 0.8038, Precision: 0.7764, Recall: 0.7017, F1: 0.7055
Testing Loss: 0.5129, Accuracy: 0.8394, Precision: 0.8315, Recall: 0.7379, F1: 0.7371
LM Predictions:  [0, 5, 5, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 4, 0, 0, 0, 0, 0, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.3937, Accuracy: 0.1905, Precision: 0.0781, Recall: 0.1534, F1: 0.0872
Epoch 9/70
Train Loss: 0.4994, Accuracy: 0.8487, Precision: 0.7934, Recall: 0.7775, F1: 0.7833
Validation Loss: 0.5322, Accuracy: 0.8507, Precision: 0.8372, Recall: 0.7964, F1: 0.8127
Testing Loss: 0.4572, Accuracy: 0.8539, Precision: 0.8273, Recall: 0.8068, F1: 0.8145
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2582, Accuracy: 0.0476, Precision: 0.0303, Recall: 0.0370, F1: 0.0333
Epoch 10/70
Train Loss: 0.4756, Accuracy: 0.8558, Precision: 0.8093, Recall: 0.7977, F1: 0.8030
Validation Loss: 0.5384, Accuracy: 0.8443, Precision: 0.8094, Recall: 0.7805, F1: 0.7897
Testing Loss: 0.4621, Accuracy: 0.8647, Precision: 0.8431, Recall: 0.8077, F1: 0.8216
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 3, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 4, 5, 0, 5, 5, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0232, Accuracy: 0.0476, Precision: 0.0256, Recall: 0.0370, F1: 0.0303
Epoch 11/70
Train Loss: 0.4515, Accuracy: 0.8648, Precision: 0.8195, Recall: 0.8141, F1: 0.8166
Validation Loss: 0.5151, Accuracy: 0.8273, Precision: 0.8053, Recall: 0.7794, F1: 0.7891
Testing Loss: 0.4890, Accuracy: 0.8539, Precision: 0.8344, Recall: 0.8142, F1: 0.8185
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7298, Accuracy: 0.0714, Precision: 0.2037, Recall: 0.0489, F1: 0.0593
Epoch 12/70
Train Loss: 0.4343, Accuracy: 0.8644, Precision: 0.8158, Recall: 0.8027, F1: 0.8088
Validation Loss: 0.5240, Accuracy: 0.8465, Precision: 0.8066, Recall: 0.8036, F1: 0.8045
Testing Loss: 0.4718, Accuracy: 0.8720, Precision: 0.8421, Recall: 0.8466, F1: 0.8422
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 2, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0072, Accuracy: 0.0714, Precision: 0.2083, Recall: 0.0489, F1: 0.0614
Epoch 13/70
Train Loss: 0.4224, Accuracy: 0.8731, Precision: 0.8260, Recall: 0.8166, F1: 0.8209
Validation Loss: 0.6058, Accuracy: 0.8443, Precision: 0.8326, Recall: 0.7768, F1: 0.7977
Testing Loss: 0.5227, Accuracy: 0.8635, Precision: 0.8475, Recall: 0.7881, F1: 0.8090
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 2, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7496, Accuracy: 0.0952, Precision: 0.1961, Recall: 0.0675, F1: 0.0607
Epoch 14/70
Train Loss: 0.4061, Accuracy: 0.8765, Precision: 0.8252, Recall: 0.8244, F1: 0.8246
Validation Loss: 0.5876, Accuracy: 0.8443, Precision: 0.8225, Recall: 0.7714, F1: 0.7896
Testing Loss: 0.5040, Accuracy: 0.8647, Precision: 0.8469, Recall: 0.7901, F1: 0.8103
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 2, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1096, Accuracy: 0.1190, Precision: 0.2111, Recall: 0.0860, F1: 0.0778
Epoch 15/70
Train Loss: 0.3994, Accuracy: 0.8767, Precision: 0.8272, Recall: 0.8162, F1: 0.8211
Validation Loss: 0.5216, Accuracy: 0.8380, Precision: 0.8171, Recall: 0.7852, F1: 0.7982
Testing Loss: 0.4600, Accuracy: 0.8611, Precision: 0.8349, Recall: 0.7961, F1: 0.8102
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8215, Accuracy: 0.0952, Precision: 0.0444, Recall: 0.0741, F1: 0.0556
Epoch 16/70
Train Loss: 0.3688, Accuracy: 0.8885, Precision: 0.8410, Recall: 0.8383, F1: 0.8394
Validation Loss: 0.5499, Accuracy: 0.8358, Precision: 0.8213, Recall: 0.7575, F1: 0.7786
Testing Loss: 0.4969, Accuracy: 0.8599, Precision: 0.8335, Recall: 0.7766, F1: 0.7965
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9740, Accuracy: 0.1190, Precision: 0.0397, Recall: 0.0926, F1: 0.0556
Epoch 17/70
Train Loss: 0.3678, Accuracy: 0.8871, Precision: 0.8393, Recall: 0.8393, F1: 0.8392
Validation Loss: 0.5507, Accuracy: 0.8401, Precision: 0.8099, Recall: 0.8087, F1: 0.8082
Testing Loss: 0.4906, Accuracy: 0.8563, Precision: 0.8284, Recall: 0.8236, F1: 0.8205
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2470, Accuracy: 0.0476, Precision: 0.0667, Recall: 0.0370, F1: 0.0476
Epoch 18/70
Train Loss: 0.3643, Accuracy: 0.8890, Precision: 0.8426, Recall: 0.8493, F1: 0.8456
Validation Loss: 0.5492, Accuracy: 0.8465, Precision: 0.8188, Recall: 0.7915, F1: 0.7988
Testing Loss: 0.4436, Accuracy: 0.8696, Precision: 0.8492, Recall: 0.8125, F1: 0.8234
LM Predictions:  [5, 5, 4, 5, 0, 4, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 4, 5, 0, 4, 5, 5, 5, 5, 5, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7699, Accuracy: 0.1190, Precision: 0.0861, Recall: 0.0979, F1: 0.0859
Epoch 19/70
Train Loss: 0.3560, Accuracy: 0.8926, Precision: 0.8470, Recall: 0.8456, F1: 0.8461
Validation Loss: 0.5310, Accuracy: 0.8486, Precision: 0.8190, Recall: 0.7808, F1: 0.7923
Testing Loss: 0.4598, Accuracy: 0.8720, Precision: 0.8525, Recall: 0.8073, F1: 0.8226
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 5, 5, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8451, Accuracy: 0.0952, Precision: 0.0370, Recall: 0.0741, F1: 0.0494
Epoch 20/70
Train Loss: 0.3444, Accuracy: 0.8987, Precision: 0.8564, Recall: 0.8551, F1: 0.8555
Validation Loss: 0.6076, Accuracy: 0.8507, Precision: 0.8126, Recall: 0.7903, F1: 0.7998
Testing Loss: 0.5026, Accuracy: 0.8684, Precision: 0.8471, Recall: 0.8055, F1: 0.8200
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0064, Accuracy: 0.0714, Precision: 0.0455, Recall: 0.0556, F1: 0.0500
Epoch 21/70
Train Loss: 0.3408, Accuracy: 0.8931, Precision: 0.8478, Recall: 0.8480, F1: 0.8476
Validation Loss: 0.6199, Accuracy: 0.8443, Precision: 0.8179, Recall: 0.7672, F1: 0.7818
Testing Loss: 0.4870, Accuracy: 0.8708, Precision: 0.8566, Recall: 0.7873, F1: 0.8075
LM Predictions:  [5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9879, Accuracy: 0.1190, Precision: 0.0379, Recall: 0.0926, F1: 0.0538
Epoch 22/70
Train Loss: 0.3294, Accuracy: 0.8923, Precision: 0.8505, Recall: 0.8459, F1: 0.8479
Validation Loss: 0.5465, Accuracy: 0.8380, Precision: 0.8183, Recall: 0.7624, F1: 0.7831
Testing Loss: 0.4555, Accuracy: 0.8684, Precision: 0.8483, Recall: 0.7946, F1: 0.8157
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5296, Accuracy: 0.1190, Precision: 0.0595, Recall: 0.0926, F1: 0.0725
Epoch 23/70
Train Loss: 0.3203, Accuracy: 0.9028, Precision: 0.8638, Recall: 0.8569, F1: 0.8600
Validation Loss: 0.5141, Accuracy: 0.8486, Precision: 0.8177, Recall: 0.7875, F1: 0.8007
Testing Loss: 0.4393, Accuracy: 0.8635, Precision: 0.8382, Recall: 0.8109, F1: 0.8218
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9527, Accuracy: 0.0952, Precision: 0.0667, Recall: 0.0741, F1: 0.0702
Epoch 24/70
Train Loss: 0.2940, Accuracy: 0.9163, Precision: 0.8788, Recall: 0.8799, F1: 0.8791
Validation Loss: 0.5946, Accuracy: 0.8465, Precision: 0.8206, Recall: 0.7642, F1: 0.7813
Testing Loss: 0.4888, Accuracy: 0.8732, Precision: 0.8554, Recall: 0.7943, F1: 0.8144
LM Predictions:  [5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1184, Accuracy: 0.1190, Precision: 0.0379, Recall: 0.0926, F1: 0.0538
Epoch 25/70
Train Loss: 0.2932, Accuracy: 0.9130, Precision: 0.8733, Recall: 0.8740, F1: 0.8734
Validation Loss: 0.5264, Accuracy: 0.8422, Precision: 0.8074, Recall: 0.7896, F1: 0.7969
Testing Loss: 0.4353, Accuracy: 0.8708, Precision: 0.8377, Recall: 0.8157, F1: 0.8254
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7639, Accuracy: 0.0952, Precision: 0.0606, Recall: 0.0741, F1: 0.0667
Epoch 26/70
Train Loss: 0.2801, Accuracy: 0.9163, Precision: 0.8782, Recall: 0.8792, F1: 0.8784
Validation Loss: 0.6446, Accuracy: 0.8380, Precision: 0.8097, Recall: 0.7447, F1: 0.7644
Testing Loss: 0.5463, Accuracy: 0.8539, Precision: 0.8452, Recall: 0.7543, F1: 0.7779
LM Predictions:  [5, 0, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0395, Accuracy: 0.1905, Precision: 0.0513, Recall: 0.1481, F1: 0.0762
Epoch 27/70
Train Loss: 0.2897, Accuracy: 0.9125, Precision: 0.8736, Recall: 0.8686, F1: 0.8709
Validation Loss: 0.5876, Accuracy: 0.8401, Precision: 0.8107, Recall: 0.7586, F1: 0.7717
Testing Loss: 0.5153, Accuracy: 0.8659, Precision: 0.8585, Recall: 0.7784, F1: 0.7966
LM Predictions:  [5, 0, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 2, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7926, Accuracy: 0.1905, Precision: 0.0533, Recall: 0.1481, F1: 0.0784
Epoch 28/70
Train Loss: 0.2779, Accuracy: 0.9172, Precision: 0.8776, Recall: 0.8774, F1: 0.8774
Validation Loss: 0.6235, Accuracy: 0.8550, Precision: 0.8167, Recall: 0.7972, F1: 0.8026
Testing Loss: 0.4708, Accuracy: 0.8720, Precision: 0.8343, Recall: 0.8112, F1: 0.8203
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 2, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8261, Accuracy: 0.1190, Precision: 0.0556, Recall: 0.0926, F1: 0.0694
Epoch 29/70
Train Loss: 0.2669, Accuracy: 0.9156, Precision: 0.8766, Recall: 0.8688, F1: 0.8725
Validation Loss: 0.6041, Accuracy: 0.8593, Precision: 0.8235, Recall: 0.8291, F1: 0.8247
Testing Loss: 0.4358, Accuracy: 0.8816, Precision: 0.8506, Recall: 0.8531, F1: 0.8498
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9084, Accuracy: 0.0952, Precision: 0.0952, Recall: 0.0741, F1: 0.0833
Epoch 30/70
Train Loss: 0.2695, Accuracy: 0.9198, Precision: 0.8808, Recall: 0.8765, F1: 0.8784
Validation Loss: 0.6238, Accuracy: 0.8401, Precision: 0.8093, Recall: 0.7669, F1: 0.7818
Testing Loss: 0.4903, Accuracy: 0.8647, Precision: 0.8527, Recall: 0.7907, F1: 0.8117
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0121, Accuracy: 0.1905, Precision: 0.0667, Recall: 0.1481, F1: 0.0920
Epoch 31/70
Train Loss: 0.2508, Accuracy: 0.9246, Precision: 0.8860, Recall: 0.8849, F1: 0.8853
Validation Loss: 0.6608, Accuracy: 0.8529, Precision: 0.8127, Recall: 0.8114, F1: 0.8118
Testing Loss: 0.4979, Accuracy: 0.8792, Precision: 0.8482, Recall: 0.8328, F1: 0.8399
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7064, Accuracy: 0.0952, Precision: 0.0833, Recall: 0.0741, F1: 0.0784
Epoch 32/70
Train Loss: 0.2484, Accuracy: 0.9251, Precision: 0.8835, Recall: 0.8842, F1: 0.8837
Validation Loss: 0.6895, Accuracy: 0.8614, Precision: 0.8381, Recall: 0.8070, F1: 0.8191
Testing Loss: 0.6199, Accuracy: 0.8587, Precision: 0.8364, Recall: 0.7928, F1: 0.8087
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 4, 5, 0, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2196, Accuracy: 0.1429, Precision: 0.2424, Recall: 0.1164, F1: 0.1250
Epoch 33/70
Train Loss: 0.2614, Accuracy: 0.9239, Precision: 0.8843, Recall: 0.8832, F1: 0.8836
Validation Loss: 0.6550, Accuracy: 0.8358, Precision: 0.8210, Recall: 0.7534, F1: 0.7736
Testing Loss: 0.5584, Accuracy: 0.8527, Precision: 0.8537, Recall: 0.7611, F1: 0.7841
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9535, Accuracy: 0.1905, Precision: 0.0606, Recall: 0.1481, F1: 0.0860
Epoch 34/70
Train Loss: 0.2465, Accuracy: 0.9324, Precision: 0.8968, Recall: 0.8991, F1: 0.8975
Validation Loss: 0.6727, Accuracy: 0.8422, Precision: 0.8230, Recall: 0.7578, F1: 0.7612
Testing Loss: 0.6063, Accuracy: 0.8514, Precision: 0.8322, Recall: 0.7615, F1: 0.7704
LM Predictions:  [5, 5, 0, 5, 0, 4, 5, 5, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 0, 5, 5, 5, 5, 0, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9850, Accuracy: 0.2381, Precision: 0.2663, Recall: 0.1997, F1: 0.1613
Epoch 35/70
Train Loss: 0.2385, Accuracy: 0.9281, Precision: 0.8894, Recall: 0.8904, F1: 0.8896
Validation Loss: 0.6588, Accuracy: 0.8422, Precision: 0.8007, Recall: 0.7889, F1: 0.7926
Testing Loss: 0.4914, Accuracy: 0.8696, Precision: 0.8306, Recall: 0.8015, F1: 0.8105
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 2, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5585, Accuracy: 0.1905, Precision: 0.2353, Recall: 0.1574, F1: 0.1374
Epoch 36/70
Train Loss: 0.2288, Accuracy: 0.9293, Precision: 0.8915, Recall: 0.8883, F1: 0.8897
Validation Loss: 0.6230, Accuracy: 0.8529, Precision: 0.8368, Recall: 0.7884, F1: 0.8021
Testing Loss: 0.5202, Accuracy: 0.8599, Precision: 0.8393, Recall: 0.7743, F1: 0.7907
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8789, Accuracy: 0.2381, Precision: 0.3939, Recall: 0.1997, F1: 0.1753
Epoch 37/70
Train Loss: 0.2350, Accuracy: 0.9270, Precision: 0.8861, Recall: 0.8813, F1: 0.8836
Validation Loss: 0.7571, Accuracy: 0.8443, Precision: 0.8115, Recall: 0.7817, F1: 0.7945
Testing Loss: 0.5894, Accuracy: 0.8611, Precision: 0.8339, Recall: 0.7884, F1: 0.8059
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7536, Accuracy: 0.1667, Precision: 0.0729, Recall: 0.1296, F1: 0.0933
Epoch 38/70
Train Loss: 0.2462, Accuracy: 0.9284, Precision: 0.8907, Recall: 0.9034, F1: 0.8961
Validation Loss: 0.6912, Accuracy: 0.8465, Precision: 0.8185, Recall: 0.7848, F1: 0.7980
Testing Loss: 0.5334, Accuracy: 0.8696, Precision: 0.8514, Recall: 0.7965, F1: 0.8145
LM Predictions:  [5, 0, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9478, Accuracy: 0.2143, Precision: 0.2333, Recall: 0.1601, F1: 0.1142
Epoch 39/70
Train Loss: 0.2439, Accuracy: 0.9272, Precision: 0.8913, Recall: 0.8894, F1: 0.8901
Validation Loss: 0.5882, Accuracy: 0.8507, Precision: 0.8235, Recall: 0.7919, F1: 0.8038
Testing Loss: 0.4475, Accuracy: 0.8720, Precision: 0.8463, Recall: 0.8073, F1: 0.8222
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6819, Accuracy: 0.1905, Precision: 0.0702, Recall: 0.1481, F1: 0.0952
Epoch 40/70
Train Loss: 0.2124, Accuracy: 0.9343, Precision: 0.8987, Recall: 0.8958, F1: 0.8971
Validation Loss: 0.7228, Accuracy: 0.8465, Precision: 0.8131, Recall: 0.7800, F1: 0.7924
Testing Loss: 0.6055, Accuracy: 0.8684, Precision: 0.8342, Recall: 0.8010, F1: 0.8149
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 2, 0, 0, 0, 0, 5, 5, 0, 5, 3, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4884, Accuracy: 0.2143, Precision: 0.4306, Recall: 0.1693, F1: 0.1810
Epoch 41/70
Train Loss: 0.2337, Accuracy: 0.9310, Precision: 0.8931, Recall: 0.8998, F1: 0.8960
Validation Loss: 0.6325, Accuracy: 0.8571, Precision: 0.8203, Recall: 0.8219, F1: 0.8201
Testing Loss: 0.5371, Accuracy: 0.8696, Precision: 0.8413, Recall: 0.8361, F1: 0.8359
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 0, 5, 5, 5, 5, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5322, Accuracy: 0.1667, Precision: 0.2917, Recall: 0.1389, F1: 0.1653
Epoch 42/70
Train Loss: 0.2240, Accuracy: 0.9315, Precision: 0.8923, Recall: 0.8946, F1: 0.8932
Validation Loss: 0.6440, Accuracy: 0.8507, Precision: 0.8083, Recall: 0.7962, F1: 0.8015
Testing Loss: 0.5294, Accuracy: 0.8611, Precision: 0.8262, Recall: 0.8137, F1: 0.8187
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 2, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4744, Accuracy: 0.1905, Precision: 0.4444, Recall: 0.1508, F1: 0.1810
Epoch 43/70
Train Loss: 0.2136, Accuracy: 0.9324, Precision: 0.8923, Recall: 0.9023, F1: 0.8963
Validation Loss: 0.6828, Accuracy: 0.8486, Precision: 0.8256, Recall: 0.7630, F1: 0.7836
Testing Loss: 0.5842, Accuracy: 0.8623, Precision: 0.8310, Recall: 0.7778, F1: 0.7968
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3612, Accuracy: 0.2381, Precision: 0.4035, Recall: 0.1878, F1: 0.1651
Epoch 44/70
Train Loss: 0.2047, Accuracy: 0.9391, Precision: 0.9031, Recall: 0.9100, F1: 0.9058
Validation Loss: 0.8040, Accuracy: 0.8401, Precision: 0.8031, Recall: 0.7704, F1: 0.7826
Testing Loss: 0.6609, Accuracy: 0.8647, Precision: 0.8360, Recall: 0.8004, F1: 0.8142
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4840, Accuracy: 0.2143, Precision: 0.2556, Recall: 0.1759, F1: 0.1587
Epoch 45/70
Train Loss: 0.1974, Accuracy: 0.9424, Precision: 0.9076, Recall: 0.9175, F1: 0.9116
Validation Loss: 0.7037, Accuracy: 0.8443, Precision: 0.8078, Recall: 0.7820, F1: 0.7915
Testing Loss: 0.6227, Accuracy: 0.8635, Precision: 0.8308, Recall: 0.7955, F1: 0.8087
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 3, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1599, Accuracy: 0.2619, Precision: 0.4359, Recall: 0.1997, F1: 0.2105
Epoch 46/70
Train Loss: 0.2234, Accuracy: 0.9374, Precision: 0.9023, Recall: 0.9110, F1: 0.9060
Validation Loss: 0.6663, Accuracy: 0.8380, Precision: 0.7883, Recall: 0.7722, F1: 0.7787
Testing Loss: 0.5662, Accuracy: 0.8454, Precision: 0.8113, Recall: 0.7959, F1: 0.8029
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 3, 0, 5, 5, 0, 5, 5, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 2, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4325, Accuracy: 0.2381, Precision: 0.3796, Recall: 0.1812, F1: 0.2130
Epoch 47/70
Train Loss: 0.2346, Accuracy: 0.9303, Precision: 0.8922, Recall: 0.9012, F1: 0.8962
Validation Loss: 0.7216, Accuracy: 0.8316, Precision: 0.7956, Recall: 0.7659, F1: 0.7738
Testing Loss: 0.5991, Accuracy: 0.8611, Precision: 0.8315, Recall: 0.7912, F1: 0.8046
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6570, Accuracy: 0.2143, Precision: 0.2407, Recall: 0.1759, F1: 0.1464
Epoch 48/70
Train Loss: 0.1994, Accuracy: 0.9400, Precision: 0.9036, Recall: 0.9113, F1: 0.9069
Validation Loss: 0.6379, Accuracy: 0.8465, Precision: 0.8000, Recall: 0.8017, F1: 0.8000
Testing Loss: 0.6563, Accuracy: 0.8563, Precision: 0.8244, Recall: 0.8165, F1: 0.8185
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3706, Accuracy: 0.1905, Precision: 0.3125, Recall: 0.1574, F1: 0.1849
Epoch 49/70
Train Loss: 0.1972, Accuracy: 0.9417, Precision: 0.9065, Recall: 0.9148, F1: 0.9099
Validation Loss: 0.6551, Accuracy: 0.8486, Precision: 0.8024, Recall: 0.8107, F1: 0.8061
Testing Loss: 0.5619, Accuracy: 0.8671, Precision: 0.8312, Recall: 0.8267, F1: 0.8279
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2027, Accuracy: 0.2619, Precision: 0.6667, Recall: 0.2209, F1: 0.2931
Epoch 50/70
Train Loss: 0.1928, Accuracy: 0.9400, Precision: 0.9036, Recall: 0.9173, F1: 0.9089
Validation Loss: 0.7389, Accuracy: 0.8550, Precision: 0.8243, Recall: 0.7934, F1: 0.8053
Testing Loss: 0.6721, Accuracy: 0.8539, Precision: 0.8243, Recall: 0.7897, F1: 0.8043
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6442, Accuracy: 0.2619, Precision: 0.4167, Recall: 0.2063, F1: 0.1810
Epoch 51/70
Train Loss: 0.2175, Accuracy: 0.9346, Precision: 0.9030, Recall: 0.9077, F1: 0.9047
Validation Loss: 0.7106, Accuracy: 0.8550, Precision: 0.8214, Recall: 0.7892, F1: 0.8007
Testing Loss: 0.6591, Accuracy: 0.8599, Precision: 0.8280, Recall: 0.8013, F1: 0.8107
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 5, 5, 5, 2, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3820, Accuracy: 0.2381, Precision: 0.4167, Recall: 0.1839, F1: 0.1706
Epoch 52/70
Train Loss: 0.1818, Accuracy: 0.9469, Precision: 0.9123, Recall: 0.9160, F1: 0.9140
Validation Loss: 0.7280, Accuracy: 0.8465, Precision: 0.8082, Recall: 0.7725, F1: 0.7805
Testing Loss: 0.6390, Accuracy: 0.8599, Precision: 0.8214, Recall: 0.7832, F1: 0.7952
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 3, 5, 5, 5, 5, 5, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4369, Accuracy: 0.2619, Precision: 0.4118, Recall: 0.2156, F1: 0.2081
Epoch 53/70
Train Loss: 0.1952, Accuracy: 0.9395, Precision: 0.9055, Recall: 0.9090, F1: 0.9070
Validation Loss: 0.6864, Accuracy: 0.8443, Precision: 0.8163, Recall: 0.7705, F1: 0.7880
Testing Loss: 0.5671, Accuracy: 0.8659, Precision: 0.8404, Recall: 0.8021, F1: 0.8174
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 5, 3, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3420, Accuracy: 0.2857, Precision: 0.4405, Recall: 0.2460, F1: 0.2554
Epoch 54/70
Train Loss: 0.1911, Accuracy: 0.9398, Precision: 0.9027, Recall: 0.9134, F1: 0.9072
Validation Loss: 0.6931, Accuracy: 0.8571, Precision: 0.8255, Recall: 0.7978, F1: 0.8097
Testing Loss: 0.5835, Accuracy: 0.8671, Precision: 0.8368, Recall: 0.7996, F1: 0.8132
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3623, Accuracy: 0.2619, Precision: 0.4123, Recall: 0.2183, F1: 0.1964
Epoch 55/70
Train Loss: 0.1806, Accuracy: 0.9445, Precision: 0.9100, Recall: 0.9170, F1: 0.9128
Validation Loss: 0.7093, Accuracy: 0.8550, Precision: 0.8229, Recall: 0.7861, F1: 0.7981
Testing Loss: 0.6181, Accuracy: 0.8623, Precision: 0.8291, Recall: 0.7966, F1: 0.8083
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2673, Accuracy: 0.2619, Precision: 0.4123, Recall: 0.2183, F1: 0.1964
Epoch 56/70
Train Loss: 0.1823, Accuracy: 0.9485, Precision: 0.9168, Recall: 0.9217, F1: 0.9189
Validation Loss: 0.7083, Accuracy: 0.8529, Precision: 0.8243, Recall: 0.7800, F1: 0.7969
Testing Loss: 0.6316, Accuracy: 0.8575, Precision: 0.8289, Recall: 0.7853, F1: 0.8025
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1447, Accuracy: 0.2857, Precision: 0.4167, Recall: 0.2460, F1: 0.2361
Epoch 57/70
Train Loss: 0.1784, Accuracy: 0.9476, Precision: 0.9160, Recall: 0.9231, F1: 0.9190
Validation Loss: 0.6550, Accuracy: 0.8443, Precision: 0.8044, Recall: 0.7893, F1: 0.7955
Testing Loss: 0.5648, Accuracy: 0.8647, Precision: 0.8332, Recall: 0.8123, F1: 0.8206
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 3, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6790, Accuracy: 0.2381, Precision: 0.3167, Recall: 0.1944, F1: 0.2055
Epoch 58/70
Train Loss: 0.1703, Accuracy: 0.9471, Precision: 0.9132, Recall: 0.9261, F1: 0.9183
Validation Loss: 0.6965, Accuracy: 0.8422, Precision: 0.8046, Recall: 0.7954, F1: 0.7991
Testing Loss: 0.5778, Accuracy: 0.8659, Precision: 0.8312, Recall: 0.8111, F1: 0.8200
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5653, Accuracy: 0.2619, Precision: 0.4833, Recall: 0.2183, F1: 0.2472
Epoch 59/70
Train Loss: 0.1666, Accuracy: 0.9490, Precision: 0.9123, Recall: 0.9254, F1: 0.9175
Validation Loss: 0.7444, Accuracy: 0.8443, Precision: 0.8098, Recall: 0.7877, F1: 0.7974
Testing Loss: 0.6267, Accuracy: 0.8527, Precision: 0.8228, Recall: 0.8018, F1: 0.8095
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2199, Accuracy: 0.2619, Precision: 0.3571, Recall: 0.2183, F1: 0.2151
Epoch 60/70
Train Loss: 0.1788, Accuracy: 0.9419, Precision: 0.9049, Recall: 0.9171, F1: 0.9100
Validation Loss: 0.7649, Accuracy: 0.8529, Precision: 0.8230, Recall: 0.7803, F1: 0.7957
Testing Loss: 0.6890, Accuracy: 0.8514, Precision: 0.8231, Recall: 0.7769, F1: 0.7920
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 2, 0, 0, 0, 0, 5, 0, 0, 5, 5, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 3, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1430, Accuracy: 0.2857, Precision: 0.4216, Recall: 0.2460, F1: 0.2417
Epoch 61/70
Train Loss: 0.1776, Accuracy: 0.9481, Precision: 0.9148, Recall: 0.9219, F1: 0.9178
Validation Loss: 0.8950, Accuracy: 0.8038, Precision: 0.7882, Recall: 0.7202, F1: 0.7310
Testing Loss: 0.8464, Accuracy: 0.8068, Precision: 0.7895, Recall: 0.7008, F1: 0.7130
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 2, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5953, Accuracy: 0.2143, Precision: 0.2879, Recall: 0.1601, F1: 0.1556
Epoch 62/70
Train Loss: 0.1995, Accuracy: 0.9412, Precision: 0.9053, Recall: 0.9148, F1: 0.9093
Validation Loss: 0.7189, Accuracy: 0.8465, Precision: 0.8153, Recall: 0.7755, F1: 0.7913
Testing Loss: 0.6942, Accuracy: 0.8418, Precision: 0.8157, Recall: 0.7677, F1: 0.7861
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5696, Accuracy: 0.2619, Precision: 0.4167, Recall: 0.2024, F1: 0.1750
Epoch 63/70
Train Loss: 0.1794, Accuracy: 0.9471, Precision: 0.9137, Recall: 0.9198, F1: 0.9162
Validation Loss: 0.7499, Accuracy: 0.8380, Precision: 0.7987, Recall: 0.7825, F1: 0.7894
Testing Loss: 0.6199, Accuracy: 0.8611, Precision: 0.8251, Recall: 0.8152, F1: 0.8197
LM Predictions:  [5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4492, Accuracy: 0.2381, Precision: 0.2917, Recall: 0.1905, F1: 0.1845
Epoch 64/70
Train Loss: 0.1659, Accuracy: 0.9462, Precision: 0.9104, Recall: 0.9226, F1: 0.9153
Validation Loss: 0.7736, Accuracy: 0.8401, Precision: 0.7945, Recall: 0.7814, F1: 0.7867
Testing Loss: 0.6463, Accuracy: 0.8611, Precision: 0.8215, Recall: 0.8134, F1: 0.8171
LM Predictions:  [1, 5, 3, 5, 0, 5, 5, 5, 2, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 1, 0, 5, 0, 5, 0, 0, 5, 2, 0, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2647, Accuracy: 0.2381, Precision: 0.2821, Recall: 0.1905, F1: 0.1780
Epoch 65/70
Train Loss: 0.1823, Accuracy: 0.9450, Precision: 0.9098, Recall: 0.9185, F1: 0.9135
Validation Loss: 0.7867, Accuracy: 0.8401, Precision: 0.8059, Recall: 0.7716, F1: 0.7851
Testing Loss: 0.6842, Accuracy: 0.8551, Precision: 0.8189, Recall: 0.7794, F1: 0.7935
LM Predictions:  [5, 4, 0, 5, 0, 5, 5, 5, 2, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4643, Accuracy: 0.2381, Precision: 0.1583, Recall: 0.1905, F1: 0.1405
Epoch 66/70
Train Loss: 0.1591, Accuracy: 0.9500, Precision: 0.9147, Recall: 0.9233, F1: 0.9185
Validation Loss: 0.7056, Accuracy: 0.8443, Precision: 0.8054, Recall: 0.7997, F1: 0.8019
Testing Loss: 0.6339, Accuracy: 0.8551, Precision: 0.8240, Recall: 0.8125, F1: 0.8156
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5197, Accuracy: 0.2143, Precision: 0.1250, Recall: 0.1667, F1: 0.1429
Epoch 67/70
Train Loss: 0.1510, Accuracy: 0.9483, Precision: 0.9126, Recall: 0.9269, F1: 0.9184
Validation Loss: 0.8146, Accuracy: 0.8465, Precision: 0.8112, Recall: 0.7892, F1: 0.7987
Testing Loss: 0.7338, Accuracy: 0.8587, Precision: 0.8362, Recall: 0.8026, F1: 0.8162
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 2, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2130, Accuracy: 0.2619, Precision: 0.4697, Recall: 0.2024, F1: 0.2139
Epoch 68/70
Train Loss: 0.1578, Accuracy: 0.9474, Precision: 0.9112, Recall: 0.9282, F1: 0.9176
Validation Loss: 0.7185, Accuracy: 0.8443, Precision: 0.8118, Recall: 0.7720, F1: 0.7808
Testing Loss: 0.6787, Accuracy: 0.8587, Precision: 0.8211, Recall: 0.7838, F1: 0.7950
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3275, Accuracy: 0.2619, Precision: 0.4048, Recall: 0.2183, F1: 0.1893
Epoch 69/70
Train Loss: 0.1645, Accuracy: 0.9495, Precision: 0.9154, Recall: 0.9253, F1: 0.9196
Validation Loss: 0.7546, Accuracy: 0.8507, Precision: 0.8368, Recall: 0.7717, F1: 0.7907
Testing Loss: 0.6839, Accuracy: 0.8671, Precision: 0.8449, Recall: 0.7852, F1: 0.8020
LM Predictions:  [5, 4, 1, 5, 0, 5, 5, 5, 1, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 1, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1743, Accuracy: 0.3333, Precision: 0.4500, Recall: 0.2540, F1: 0.2437
Epoch 70/70
Train Loss: 0.1642, Accuracy: 0.9483, Precision: 0.9169, Recall: 0.9173, F1: 0.9170
Validation Loss: 0.8147, Accuracy: 0.8465, Precision: 0.8298, Recall: 0.7743, F1: 0.7943
Testing Loss: 0.6996, Accuracy: 0.8635, Precision: 0.8456, Recall: 0.7865, F1: 0.8084
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4804, Accuracy: 0.2857, Precision: 0.5714, Recall: 0.2302, F1: 0.2115
Label Memorization Analysis: 
LM Predictions:  [5, 5, 1, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4804, Accuracy: 0.2857, Precision: 0.5714, Recall: 0.2302, F1: 0.2115

