Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4848, Accuracy: 0.3993, Precision: 0.2778, Recall: 0.2670, F1: 0.2344
Validation Loss: 1.1000, Accuracy: 0.5970, Precision: 0.3920, Recall: 0.4374, F1: 0.4029
Testing Loss: 1.0824, Accuracy: 0.6111, Precision: 0.3796, Recall: 0.4381, F1: 0.3990
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1214, Accuracy: 0.0952, Precision: 0.0216, Recall: 0.1600, F1: 0.0381
Epoch 2/70
Train Loss: 1.0065, Accuracy: 0.6490, Precision: 0.4639, Recall: 0.4813, F1: 0.4529
Validation Loss: 0.8743, Accuracy: 0.7228, Precision: 0.5650, Recall: 0.5890, F1: 0.5740
Testing Loss: 0.8066, Accuracy: 0.7355, Precision: 0.5590, Recall: 0.6015, F1: 0.5720
LM Predictions:  [0, 0, 0, 4, 4, 0, 0, 0, 0, 2, 0, 0, 3, 0, 3, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3082, Accuracy: 0.1905, Precision: 0.1958, Recall: 0.2400, F1: 0.1530
Epoch 3/70
Train Loss: 0.7892, Accuracy: 0.7230, Precision: 0.6250, Recall: 0.5956, F1: 0.5894
Validation Loss: 0.6688, Accuracy: 0.7846, Precision: 0.7563, Recall: 0.6955, F1: 0.7107
Testing Loss: 0.6330, Accuracy: 0.8007, Precision: 0.7318, Recall: 0.7006, F1: 0.7070
LM Predictions:  [0, 3, 0, 5, 4, 0, 0, 5, 5, 2, 5, 0, 3, 0, 3, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 5, 5, 0, 0, 1, 5, 3, 5, 3, 0, 0, 3, 0, 3, 0, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9484, Accuracy: 0.1667, Precision: 0.0873, Recall: 0.1833, F1: 0.1039
Epoch 4/70
Train Loss: 0.6839, Accuracy: 0.7785, Precision: 0.6994, Recall: 0.6658, F1: 0.6658
Validation Loss: 0.6974, Accuracy: 0.8038, Precision: 0.6853, Recall: 0.6708, F1: 0.6711
Testing Loss: 0.6063, Accuracy: 0.8092, Precision: 0.6828, Recall: 0.6901, F1: 0.6838
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3525, Accuracy: 0.1190, Precision: 0.0440, Recall: 0.1500, F1: 0.0547
Epoch 5/70
Train Loss: 0.5966, Accuracy: 0.8127, Precision: 0.7314, Recall: 0.7178, F1: 0.7180
Validation Loss: 0.6108, Accuracy: 0.8273, Precision: 0.8141, Recall: 0.7388, F1: 0.7609
Testing Loss: 0.5146, Accuracy: 0.8454, Precision: 0.8107, Recall: 0.7744, F1: 0.7890
LM Predictions:  [0, 3, 0, 5, 4, 5, 5, 5, 5, 4, 5, 0, 0, 0, 3, 5, 0, 2, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 0, 5, 1, 5, 0, 5, 0, 5, 0, 2, 0, 3, 3, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3036, Accuracy: 0.0714, Precision: 0.0529, Recall: 0.0833, F1: 0.0525
Epoch 6/70
Train Loss: 0.5708, Accuracy: 0.8226, Precision: 0.7540, Recall: 0.7221, F1: 0.7268
Validation Loss: 0.6236, Accuracy: 0.8230, Precision: 0.7898, Recall: 0.7023, F1: 0.7128
Testing Loss: 0.4860, Accuracy: 0.8454, Precision: 0.7344, Recall: 0.7273, F1: 0.7259
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1758, Accuracy: 0.0952, Precision: 0.0180, Recall: 0.1333, F1: 0.0317
Epoch 7/70
Train Loss: 0.5249, Accuracy: 0.8323, Precision: 0.7733, Recall: 0.7479, F1: 0.7573
Validation Loss: 0.5937, Accuracy: 0.8294, Precision: 0.8651, Recall: 0.7227, F1: 0.7297
Testing Loss: 0.4781, Accuracy: 0.8490, Precision: 0.8073, Recall: 0.7381, F1: 0.7289
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2643, Accuracy: 0.1190, Precision: 0.0746, Recall: 0.1500, F1: 0.0590
Epoch 8/70
Train Loss: 0.4858, Accuracy: 0.8416, Precision: 0.7882, Recall: 0.7641, F1: 0.7740
Validation Loss: 0.5757, Accuracy: 0.8294, Precision: 0.7548, Recall: 0.7187, F1: 0.7196
Testing Loss: 0.5098, Accuracy: 0.8466, Precision: 0.7192, Recall: 0.7348, F1: 0.7243
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.9275, Accuracy: 0.0952, Precision: 0.0180, Recall: 0.1333, F1: 0.0317
Epoch 9/70
Train Loss: 0.4582, Accuracy: 0.8580, Precision: 0.8105, Recall: 0.8046, F1: 0.8073
Validation Loss: 0.5716, Accuracy: 0.8401, Precision: 0.8059, Recall: 0.8026, F1: 0.8000
Testing Loss: 0.4357, Accuracy: 0.8708, Precision: 0.8362, Recall: 0.8459, F1: 0.8391
LM Predictions:  [0, 3, 0, 5, 4, 5, 5, 5, 5, 0, 5, 0, 3, 5, 3, 5, 5, 2, 0, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1605, Accuracy: 0.0952, Precision: 0.0833, Recall: 0.1167, F1: 0.0826
Epoch 10/70
Train Loss: 0.4432, Accuracy: 0.8594, Precision: 0.8094, Recall: 0.7947, F1: 0.8015
Validation Loss: 0.5695, Accuracy: 0.8529, Precision: 0.8514, Recall: 0.7637, F1: 0.7818
Testing Loss: 0.4488, Accuracy: 0.8587, Precision: 0.8094, Recall: 0.7592, F1: 0.7608
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 5, 0, 5, 0, 0, 0, 0, 3, 0, 0, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8686, Accuracy: 0.1190, Precision: 0.1048, Recall: 0.1500, F1: 0.0648
Epoch 11/70
Train Loss: 0.4291, Accuracy: 0.8655, Precision: 0.8111, Recall: 0.8035, F1: 0.8071
Validation Loss: 0.5591, Accuracy: 0.8422, Precision: 0.7156, Recall: 0.7340, F1: 0.7223
Testing Loss: 0.4539, Accuracy: 0.8611, Precision: 0.7331, Recall: 0.7500, F1: 0.7384
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1191, Accuracy: 0.1190, Precision: 0.0244, Recall: 0.2000, F1: 0.0435
Epoch 12/70
Train Loss: 0.4124, Accuracy: 0.8677, Precision: 0.8216, Recall: 0.8098, F1: 0.8150
Validation Loss: 0.6216, Accuracy: 0.8380, Precision: 0.8404, Recall: 0.7288, F1: 0.7398
Testing Loss: 0.4529, Accuracy: 0.8539, Precision: 0.8151, Recall: 0.7430, F1: 0.7471
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4475, Accuracy: 0.1190, Precision: 0.1857, Recall: 0.1542, F1: 0.0704
Epoch 13/70
Train Loss: 0.4022, Accuracy: 0.8769, Precision: 0.8289, Recall: 0.8272, F1: 0.8278
Validation Loss: 0.5710, Accuracy: 0.8486, Precision: 0.8226, Recall: 0.8150, F1: 0.8115
Testing Loss: 0.4146, Accuracy: 0.8780, Precision: 0.8427, Recall: 0.8592, F1: 0.8474
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 3, 5, 5, 2, 0, 5, 0, 5, 2, 5, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9774, Accuracy: 0.0714, Precision: 0.1167, Recall: 0.0833, F1: 0.0722
Epoch 14/70
Train Loss: 0.3900, Accuracy: 0.8772, Precision: 0.8289, Recall: 0.8289, F1: 0.8288
Validation Loss: 0.5440, Accuracy: 0.8635, Precision: 0.8427, Recall: 0.8185, F1: 0.8282
Testing Loss: 0.4380, Accuracy: 0.8720, Precision: 0.8473, Recall: 0.8189, F1: 0.8305
LM Predictions:  [0, 5, 0, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3343, Accuracy: 0.0952, Precision: 0.1167, Recall: 0.1208, F1: 0.0833
Epoch 15/70
Train Loss: 0.3923, Accuracy: 0.8772, Precision: 0.8347, Recall: 0.8282, F1: 0.8310
Validation Loss: 0.5693, Accuracy: 0.8657, Precision: 0.8432, Recall: 0.8138, F1: 0.8268
Testing Loss: 0.4489, Accuracy: 0.8841, Precision: 0.8652, Recall: 0.8284, F1: 0.8433
LM Predictions:  [0, 5, 0, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 0, 5, 2, 0, 5, 0, 0, 2, 0, 0, 0, 5, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4360, Accuracy: 0.0952, Precision: 0.0819, Recall: 0.1208, F1: 0.0720
Epoch 16/70
Train Loss: 0.3583, Accuracy: 0.8912, Precision: 0.8491, Recall: 0.8501, F1: 0.8492
Validation Loss: 0.5674, Accuracy: 0.8486, Precision: 0.8252, Recall: 0.7951, F1: 0.8055
Testing Loss: 0.4767, Accuracy: 0.8671, Precision: 0.8466, Recall: 0.8146, F1: 0.8255
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3496, Accuracy: 0.0952, Precision: 0.2024, Recall: 0.1208, F1: 0.0897
Epoch 17/70
Train Loss: 0.3474, Accuracy: 0.8926, Precision: 0.8483, Recall: 0.8506, F1: 0.8492
Validation Loss: 0.5425, Accuracy: 0.8763, Precision: 0.8810, Recall: 0.8261, F1: 0.8467
Testing Loss: 0.4008, Accuracy: 0.8816, Precision: 0.8545, Recall: 0.8129, F1: 0.8272
LM Predictions:  [0, 0, 0, 0, 4, 5, 5, 5, 0, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1544, Accuracy: 0.1190, Precision: 0.1917, Recall: 0.1375, F1: 0.1011
Epoch 18/70
Train Loss: 0.3280, Accuracy: 0.8971, Precision: 0.8546, Recall: 0.8504, F1: 0.8523
Validation Loss: 0.5902, Accuracy: 0.8593, Precision: 0.8266, Recall: 0.8207, F1: 0.8217
Testing Loss: 0.4378, Accuracy: 0.8720, Precision: 0.8443, Recall: 0.8292, F1: 0.8331
LM Predictions:  [0, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.6132, Accuracy: 0.0952, Precision: 0.2051, Recall: 0.1208, F1: 0.0926
Epoch 19/70
Train Loss: 0.3455, Accuracy: 0.8964, Precision: 0.8609, Recall: 0.8503, F1: 0.8550
Validation Loss: 0.6376, Accuracy: 0.8635, Precision: 0.8708, Recall: 0.8092, F1: 0.8336
Testing Loss: 0.4907, Accuracy: 0.8671, Precision: 0.8453, Recall: 0.8091, F1: 0.8232
LM Predictions:  [0, 5, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 5, 0, 0, 5, 3, 0, 5, 0, 0, 3, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9795, Accuracy: 0.0952, Precision: 0.0806, Recall: 0.1167, F1: 0.0656
Epoch 20/70
Train Loss: 0.3247, Accuracy: 0.8983, Precision: 0.8594, Recall: 0.8621, F1: 0.8605
Validation Loss: 0.6123, Accuracy: 0.8657, Precision: 0.8445, Recall: 0.8216, F1: 0.8314
Testing Loss: 0.4601, Accuracy: 0.8756, Precision: 0.8415, Recall: 0.8250, F1: 0.8320
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 1, 0, 0, 0, 5, 5, 5, 0, 5, 5, 4, 5, 5, 0, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8157, Accuracy: 0.1190, Precision: 0.2885, Recall: 0.1375, F1: 0.1204
Epoch 21/70
Train Loss: 0.3178, Accuracy: 0.8978, Precision: 0.8574, Recall: 0.8442, F1: 0.8504
Validation Loss: 0.5515, Accuracy: 0.8678, Precision: 0.8377, Recall: 0.8336, F1: 0.8327
Testing Loss: 0.4369, Accuracy: 0.8768, Precision: 0.8391, Recall: 0.8388, F1: 0.8387
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9070, Accuracy: 0.1190, Precision: 0.2885, Recall: 0.1375, F1: 0.1204
Epoch 22/70
Train Loss: 0.2933, Accuracy: 0.9137, Precision: 0.8763, Recall: 0.8778, F1: 0.8767
Validation Loss: 0.5781, Accuracy: 0.8614, Precision: 0.8784, Recall: 0.7960, F1: 0.8204
Testing Loss: 0.4572, Accuracy: 0.8756, Precision: 0.8662, Recall: 0.7869, F1: 0.7962
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 3, 0, 0, 0, 0, 5, 5, 0, 5, 0, 3, 0, 0, 0, 0, 0, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9046, Accuracy: 0.1667, Precision: 0.3125, Recall: 0.1708, F1: 0.1429
Epoch 23/70
Train Loss: 0.2875, Accuracy: 0.9113, Precision: 0.8730, Recall: 0.8694, F1: 0.8711
Validation Loss: 0.5924, Accuracy: 0.8721, Precision: 0.8463, Recall: 0.8226, F1: 0.8328
Testing Loss: 0.4349, Accuracy: 0.8744, Precision: 0.8463, Recall: 0.8276, F1: 0.8342
LM Predictions:  [5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 3, 0, 5, 0, 5, 3, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0373, Accuracy: 0.1190, Precision: 0.2778, Recall: 0.1250, F1: 0.1337
Epoch 24/70
Train Loss: 0.2988, Accuracy: 0.9070, Precision: 0.8679, Recall: 0.8618, F1: 0.8647
Validation Loss: 0.5147, Accuracy: 0.8721, Precision: 0.8592, Recall: 0.8126, F1: 0.8319
Testing Loss: 0.3959, Accuracy: 0.8756, Precision: 0.8473, Recall: 0.8177, F1: 0.8293
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 2, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8637, Accuracy: 0.1429, Precision: 0.3072, Recall: 0.1583, F1: 0.1364
Epoch 25/70
Train Loss: 0.2932, Accuracy: 0.9127, Precision: 0.8720, Recall: 0.8757, F1: 0.8735
Validation Loss: 0.5487, Accuracy: 0.8678, Precision: 0.8692, Recall: 0.8037, F1: 0.8269
Testing Loss: 0.4072, Accuracy: 0.8829, Precision: 0.8651, Recall: 0.7989, F1: 0.8158
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8648, Accuracy: 0.1667, Precision: 0.4384, Recall: 0.1750, F1: 0.1519
Epoch 26/70
Train Loss: 0.2951, Accuracy: 0.9106, Precision: 0.8731, Recall: 0.8698, F1: 0.8713
Validation Loss: 0.5460, Accuracy: 0.8742, Precision: 0.8714, Recall: 0.8264, F1: 0.8438
Testing Loss: 0.4295, Accuracy: 0.8792, Precision: 0.8543, Recall: 0.8107, F1: 0.8256
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 1, 0, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1922, Accuracy: 0.1667, Precision: 0.5294, Recall: 0.1750, F1: 0.1727
Epoch 27/70
Train Loss: 0.2619, Accuracy: 0.9189, Precision: 0.8816, Recall: 0.8725, F1: 0.8768
Validation Loss: 0.5454, Accuracy: 0.8699, Precision: 0.8346, Recall: 0.8311, F1: 0.8315
Testing Loss: 0.4063, Accuracy: 0.8804, Precision: 0.8439, Recall: 0.8367, F1: 0.8387
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 1, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1554, Accuracy: 0.1667, Precision: 0.4944, Recall: 0.1750, F1: 0.1879
Epoch 28/70
Train Loss: 0.2610, Accuracy: 0.9201, Precision: 0.8837, Recall: 0.8802, F1: 0.8818
Validation Loss: 0.6327, Accuracy: 0.8721, Precision: 0.8796, Recall: 0.7989, F1: 0.8262
Testing Loss: 0.4665, Accuracy: 0.8744, Precision: 0.8605, Recall: 0.7970, F1: 0.8135
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 0, 2, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2757, Accuracy: 0.1667, Precision: 0.4405, Recall: 0.1750, F1: 0.1546
Epoch 29/70
Train Loss: 0.2636, Accuracy: 0.9220, Precision: 0.8828, Recall: 0.8881, F1: 0.8851
Validation Loss: 0.5929, Accuracy: 0.8678, Precision: 0.8747, Recall: 0.7911, F1: 0.8211
Testing Loss: 0.4501, Accuracy: 0.8708, Precision: 0.8545, Recall: 0.7965, F1: 0.8168
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 0, 5, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9943, Accuracy: 0.1429, Precision: 0.3028, Recall: 0.1583, F1: 0.1309
Epoch 30/70
Train Loss: 0.2479, Accuracy: 0.9274, Precision: 0.8933, Recall: 0.8964, F1: 0.8942
Validation Loss: 0.6626, Accuracy: 0.8742, Precision: 0.8499, Recall: 0.8323, F1: 0.8381
Testing Loss: 0.4719, Accuracy: 0.8853, Precision: 0.8532, Recall: 0.8399, F1: 0.8459
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 3, 0, 2, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 0, 5, 5, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7994, Accuracy: 0.1905, Precision: 0.4125, Recall: 0.1875, F1: 0.2072
Epoch 31/70
Train Loss: 0.2576, Accuracy: 0.9203, Precision: 0.8827, Recall: 0.8888, F1: 0.8855
Validation Loss: 0.6650, Accuracy: 0.8699, Precision: 0.8819, Recall: 0.7916, F1: 0.8186
Testing Loss: 0.5176, Accuracy: 0.8720, Precision: 0.8676, Recall: 0.7760, F1: 0.7890
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 2, 0, 0, 2, 0, 0, 0, 0, 5, 2, 0, 4, 0, 2, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7957, Accuracy: 0.1905, Precision: 0.4384, Recall: 0.1958, F1: 0.1677
Epoch 32/70
Train Loss: 0.2389, Accuracy: 0.9274, Precision: 0.8906, Recall: 0.8885, F1: 0.8894
Validation Loss: 0.6053, Accuracy: 0.8699, Precision: 0.8604, Recall: 0.8106, F1: 0.8296
Testing Loss: 0.4645, Accuracy: 0.8816, Precision: 0.8620, Recall: 0.8058, F1: 0.8220
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 1, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0587, Accuracy: 0.1667, Precision: 0.5250, Recall: 0.1750, F1: 0.1673
Epoch 33/70
Train Loss: 0.2199, Accuracy: 0.9362, Precision: 0.9052, Recall: 0.9068, F1: 0.9057
Validation Loss: 0.5956, Accuracy: 0.8742, Precision: 0.8636, Recall: 0.8174, F1: 0.8355
Testing Loss: 0.4511, Accuracy: 0.8792, Precision: 0.8486, Recall: 0.8074, F1: 0.8203
LM Predictions:  [0, 1, 3, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 5, 3, 5, 5, 5, 0, 2, 3, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 0, 1, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7104, Accuracy: 0.2143, Precision: 0.4739, Recall: 0.2102, F1: 0.2129
Epoch 34/70
Train Loss: 0.2247, Accuracy: 0.9324, Precision: 0.8993, Recall: 0.9043, F1: 0.9010
Validation Loss: 0.6477, Accuracy: 0.8550, Precision: 0.8142, Recall: 0.8128, F1: 0.8128
Testing Loss: 0.5044, Accuracy: 0.8708, Precision: 0.8455, Recall: 0.8149, F1: 0.8246
LM Predictions:  [0, 1, 3, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 1, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7830, Accuracy: 0.1905, Precision: 0.4829, Recall: 0.1935, F1: 0.2045
Epoch 35/70
Train Loss: 0.2390, Accuracy: 0.9260, Precision: 0.8907, Recall: 0.8953, F1: 0.8924
Validation Loss: 0.5766, Accuracy: 0.8806, Precision: 0.8759, Recall: 0.8149, F1: 0.8382
Testing Loss: 0.4929, Accuracy: 0.8780, Precision: 0.8698, Recall: 0.7935, F1: 0.8142
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 3, 0, 0, 5, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4722, Accuracy: 0.2381, Precision: 0.6930, Recall: 0.2269, F1: 0.2489
Epoch 36/70
Train Loss: 0.2167, Accuracy: 0.9353, Precision: 0.8989, Recall: 0.9030, F1: 0.9006
Validation Loss: 0.6327, Accuracy: 0.8721, Precision: 0.8578, Recall: 0.8136, F1: 0.8296
Testing Loss: 0.5130, Accuracy: 0.8647, Precision: 0.8340, Recall: 0.7896, F1: 0.8013
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 3, 0, 0, 5, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7670, Accuracy: 0.2381, Precision: 0.6567, Recall: 0.2394, F1: 0.2234
Epoch 37/70
Train Loss: 0.2129, Accuracy: 0.9372, Precision: 0.9049, Recall: 0.9043, F1: 0.9042
Validation Loss: 0.6099, Accuracy: 0.8699, Precision: 0.8425, Recall: 0.8107, F1: 0.8229
Testing Loss: 0.4655, Accuracy: 0.8792, Precision: 0.8584, Recall: 0.8035, F1: 0.8131
LM Predictions:  [0, 1, 2, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 0, 1, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3304, Accuracy: 0.2619, Precision: 0.5763, Recall: 0.2519, F1: 0.2604
Epoch 38/70
Train Loss: 0.2241, Accuracy: 0.9327, Precision: 0.9012, Recall: 0.9009, F1: 0.9008
Validation Loss: 0.5864, Accuracy: 0.8678, Precision: 0.8615, Recall: 0.8023, F1: 0.8204
Testing Loss: 0.4938, Accuracy: 0.8696, Precision: 0.8420, Recall: 0.7858, F1: 0.7953
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 5, 0, 0, 2, 5, 3, 0, 2, 5, 0, 5, 0, 0, 3, 0, 0, 5, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5231, Accuracy: 0.3333, Precision: 0.6970, Recall: 0.3227, F1: 0.3181
Epoch 39/70
Train Loss: 0.2404, Accuracy: 0.9258, Precision: 0.8892, Recall: 0.8984, F1: 0.8930
Validation Loss: 0.6505, Accuracy: 0.8507, Precision: 0.8147, Recall: 0.8006, F1: 0.8065
Testing Loss: 0.5242, Accuracy: 0.8792, Precision: 0.8449, Recall: 0.8302, F1: 0.8369
LM Predictions:  [0, 1, 3, 5, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 5, 5, 0, 0, 3, 0, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4922, Accuracy: 0.2619, Precision: 0.6917, Recall: 0.2602, F1: 0.2906
Epoch 40/70
Train Loss: 0.2310, Accuracy: 0.9291, Precision: 0.8949, Recall: 0.8883, F1: 0.8914
Validation Loss: 0.5825, Accuracy: 0.8635, Precision: 0.8350, Recall: 0.8090, F1: 0.8194
Testing Loss: 0.4647, Accuracy: 0.8780, Precision: 0.8483, Recall: 0.8378, F1: 0.8408
LM Predictions:  [5, 1, 2, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 5, 5, 3, 0, 5, 2, 5, 0, 0, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4159, Accuracy: 0.2619, Precision: 0.6889, Recall: 0.2352, F1: 0.3037
Epoch 41/70
Train Loss: 0.2256, Accuracy: 0.9329, Precision: 0.9001, Recall: 0.9008, F1: 0.9001
Validation Loss: 0.6252, Accuracy: 0.8763, Precision: 0.8815, Recall: 0.7981, F1: 0.8211
Testing Loss: 0.5539, Accuracy: 0.8708, Precision: 0.8512, Recall: 0.7833, F1: 0.7961
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 2, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5995, Accuracy: 0.2619, Precision: 0.6458, Recall: 0.2477, F1: 0.2584
Epoch 42/70
Train Loss: 0.1960, Accuracy: 0.9369, Precision: 0.9028, Recall: 0.9102, F1: 0.9057
Validation Loss: 0.6309, Accuracy: 0.8806, Precision: 0.8506, Recall: 0.8208, F1: 0.8326
Testing Loss: 0.5523, Accuracy: 0.8744, Precision: 0.8367, Recall: 0.8077, F1: 0.8161
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 5, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2037, Accuracy: 0.2857, Precision: 0.6930, Recall: 0.2644, F1: 0.2915
Epoch 43/70
Train Loss: 0.2111, Accuracy: 0.9350, Precision: 0.8970, Recall: 0.9051, F1: 0.9004
Validation Loss: 0.5918, Accuracy: 0.8657, Precision: 0.8272, Recall: 0.8355, F1: 0.8278
Testing Loss: 0.4699, Accuracy: 0.8792, Precision: 0.8471, Recall: 0.8575, F1: 0.8492
LM Predictions:  [5, 1, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 4, 3, 5, 5, 5, 5, 3, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3046, Accuracy: 0.2381, Precision: 0.7333, Recall: 0.2102, F1: 0.2991
Epoch 44/70
Train Loss: 0.2061, Accuracy: 0.9362, Precision: 0.9019, Recall: 0.9074, F1: 0.9038
Validation Loss: 0.6058, Accuracy: 0.8785, Precision: 0.8471, Recall: 0.8231, F1: 0.8338
Testing Loss: 0.5368, Accuracy: 0.8744, Precision: 0.8401, Recall: 0.8080, F1: 0.8182
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 5, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 0, 3, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4210, Accuracy: 0.2619, Precision: 0.6528, Recall: 0.2477, F1: 0.2713
Epoch 45/70
Train Loss: 0.1860, Accuracy: 0.9429, Precision: 0.9106, Recall: 0.9172, F1: 0.9132
Validation Loss: 0.6809, Accuracy: 0.8593, Precision: 0.8252, Recall: 0.8109, F1: 0.8162
Testing Loss: 0.5584, Accuracy: 0.8575, Precision: 0.8204, Recall: 0.7948, F1: 0.8030
LM Predictions:  [0, 1, 3, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 0, 0, 1, 5, 0, 1, 0, 5, 1, 0, 4, 5, 3, 5, 0, 0, 0, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3209, Accuracy: 0.2381, Precision: 0.4979, Recall: 0.2269, F1: 0.2351
Epoch 46/70
Train Loss: 0.1869, Accuracy: 0.9485, Precision: 0.9194, Recall: 0.9223, F1: 0.9204
Validation Loss: 0.6531, Accuracy: 0.8742, Precision: 0.8466, Recall: 0.8221, F1: 0.8327
Testing Loss: 0.5379, Accuracy: 0.8744, Precision: 0.8366, Recall: 0.8193, F1: 0.8262
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3705, Accuracy: 0.2857, Precision: 0.6667, Recall: 0.2644, F1: 0.3080
Epoch 47/70
Train Loss: 0.1968, Accuracy: 0.9410, Precision: 0.9064, Recall: 0.9198, F1: 0.9117
Validation Loss: 0.7633, Accuracy: 0.8678, Precision: 0.8570, Recall: 0.8093, F1: 0.8261
Testing Loss: 0.6252, Accuracy: 0.8659, Precision: 0.8375, Recall: 0.7947, F1: 0.8065
LM Predictions:  [0, 1, 3, 0, 5, 5, 0, 0, 0, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3552, Accuracy: 0.3333, Precision: 0.6429, Recall: 0.3144, F1: 0.3144
Epoch 48/70
Train Loss: 0.1939, Accuracy: 0.9407, Precision: 0.9087, Recall: 0.9114, F1: 0.9099
Validation Loss: 0.6322, Accuracy: 0.8721, Precision: 0.8481, Recall: 0.8089, F1: 0.8241
Testing Loss: 0.5084, Accuracy: 0.8744, Precision: 0.8461, Recall: 0.8057, F1: 0.8194
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 3, 5, 5, 5, 0, 4, 0, 3, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2163, Accuracy: 0.3333, Precision: 0.6704, Recall: 0.3144, F1: 0.3267
Epoch 49/70
Train Loss: 0.1903, Accuracy: 0.9360, Precision: 0.9009, Recall: 0.9053, F1: 0.9028
Validation Loss: 0.6605, Accuracy: 0.8614, Precision: 0.8380, Recall: 0.8081, F1: 0.8198
Testing Loss: 0.5808, Accuracy: 0.8671, Precision: 0.8362, Recall: 0.8221, F1: 0.8277
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 3, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4973, Accuracy: 0.3333, Precision: 0.6846, Recall: 0.3144, F1: 0.3428
Epoch 50/70
Train Loss: 0.1863, Accuracy: 0.9414, Precision: 0.9092, Recall: 0.9129, F1: 0.9106
Validation Loss: 0.7088, Accuracy: 0.8550, Precision: 0.8212, Recall: 0.8146, F1: 0.8142
Testing Loss: 0.5823, Accuracy: 0.8635, Precision: 0.8252, Recall: 0.8286, F1: 0.8257
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 3, 0, 5, 3, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6020, Accuracy: 0.2619, Precision: 0.6889, Recall: 0.2435, F1: 0.2906
Epoch 51/70
Train Loss: 0.1853, Accuracy: 0.9421, Precision: 0.9113, Recall: 0.9061, F1: 0.9085
Validation Loss: 0.5990, Accuracy: 0.8721, Precision: 0.8400, Recall: 0.8175, F1: 0.8264
Testing Loss: 0.5152, Accuracy: 0.8659, Precision: 0.8297, Recall: 0.8064, F1: 0.8166
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 2, 0, 0, 2, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4382, Accuracy: 0.2619, Precision: 0.5861, Recall: 0.2644, F1: 0.2553
Epoch 52/70
Train Loss: 0.1810, Accuracy: 0.9421, Precision: 0.9069, Recall: 0.9194, F1: 0.9117
Validation Loss: 0.6252, Accuracy: 0.8678, Precision: 0.8406, Recall: 0.8144, F1: 0.8254
Testing Loss: 0.6407, Accuracy: 0.8575, Precision: 0.8290, Recall: 0.7942, F1: 0.8070
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 0, 5, 5, 0, 3, 0, 0, 3, 0, 0, 3, 5, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3025, Accuracy: 0.3810, Precision: 0.6704, Recall: 0.3560, F1: 0.3640
Epoch 53/70
Train Loss: 0.1693, Accuracy: 0.9414, Precision: 0.9117, Recall: 0.9075, F1: 0.9095
Validation Loss: 0.5994, Accuracy: 0.8657, Precision: 0.8354, Recall: 0.8140, F1: 0.8232
Testing Loss: 0.5366, Accuracy: 0.8611, Precision: 0.8288, Recall: 0.8167, F1: 0.8204
LM Predictions:  [0, 1, 3, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 0, 5, 0, 5, 0, 0, 3, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3102, Accuracy: 0.2857, Precision: 0.6476, Recall: 0.2769, F1: 0.2924
Epoch 54/70
Train Loss: 0.1758, Accuracy: 0.9429, Precision: 0.9108, Recall: 0.9113, F1: 0.9108
Validation Loss: 0.7272, Accuracy: 0.8657, Precision: 0.8413, Recall: 0.8018, F1: 0.8176
Testing Loss: 0.6584, Accuracy: 0.8635, Precision: 0.8264, Recall: 0.7976, F1: 0.8086
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 2, 4, 3, 0, 0, 5, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 5, 0, 4, 0, 3, 5, 0, 0, 0, 5, 0, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5168, Accuracy: 0.3095, Precision: 0.6444, Recall: 0.2935, F1: 0.2922
Epoch 55/70
Train Loss: 0.1748, Accuracy: 0.9464, Precision: 0.9185, Recall: 0.9197, F1: 0.9188
Validation Loss: 0.7472, Accuracy: 0.8507, Precision: 0.8195, Recall: 0.7854, F1: 0.7970
Testing Loss: 0.7185, Accuracy: 0.8527, Precision: 0.8140, Recall: 0.7779, F1: 0.7856
LM Predictions:  [0, 1, 3, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5445, Accuracy: 0.3333, Precision: 0.6270, Recall: 0.3144, F1: 0.3095
Epoch 56/70
Train Loss: 0.1734, Accuracy: 0.9450, Precision: 0.9164, Recall: 0.9131, F1: 0.9145
Validation Loss: 0.6715, Accuracy: 0.8657, Precision: 0.8440, Recall: 0.8291, F1: 0.8320
Testing Loss: 0.6186, Accuracy: 0.8599, Precision: 0.8312, Recall: 0.8201, F1: 0.8231
LM Predictions:  [0, 1, 3, 5, 5, 5, 0, 5, 5, 5, 0, 0, 2, 5, 3, 5, 2, 5, 5, 3, 0, 5, 3, 5, 5, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4373, Accuracy: 0.3095, Precision: 0.7063, Recall: 0.2977, F1: 0.3490
Epoch 57/70
Train Loss: 0.1705, Accuracy: 0.9471, Precision: 0.9157, Recall: 0.9257, F1: 0.9198
Validation Loss: 0.6722, Accuracy: 0.8699, Precision: 0.8461, Recall: 0.8088, F1: 0.8239
Testing Loss: 0.6669, Accuracy: 0.8635, Precision: 0.8331, Recall: 0.7832, F1: 0.7986
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4518, Accuracy: 0.3095, Precision: 0.6623, Recall: 0.2977, F1: 0.2911
Epoch 58/70
Train Loss: 0.1693, Accuracy: 0.9452, Precision: 0.9161, Recall: 0.9112, F1: 0.9136
Validation Loss: 0.6896, Accuracy: 0.8678, Precision: 0.8431, Recall: 0.8131, F1: 0.8251
Testing Loss: 0.6827, Accuracy: 0.8623, Precision: 0.8343, Recall: 0.8114, F1: 0.8195
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 5, 3, 5, 0, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3840, Accuracy: 0.3333, Precision: 0.6889, Recall: 0.3144, F1: 0.3471
Epoch 59/70
Train Loss: 0.1801, Accuracy: 0.9414, Precision: 0.9079, Recall: 0.9123, F1: 0.9095
Validation Loss: 0.6904, Accuracy: 0.8699, Precision: 0.8465, Recall: 0.8246, F1: 0.8335
Testing Loss: 0.6354, Accuracy: 0.8647, Precision: 0.8279, Recall: 0.8107, F1: 0.8178
LM Predictions:  [0, 1, 3, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 0, 2, 5, 5, 3, 3, 0, 3, 0, 0, 3, 0, 5, 3, 0, 4, 0, 3, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3185, Accuracy: 0.3571, Precision: 0.6486, Recall: 0.3310, F1: 0.3391
Epoch 60/70
Train Loss: 0.1794, Accuracy: 0.9455, Precision: 0.9123, Recall: 0.9231, F1: 0.9170
Validation Loss: 0.6527, Accuracy: 0.8550, Precision: 0.8233, Recall: 0.8060, F1: 0.8127
Testing Loss: 0.6124, Accuracy: 0.8684, Precision: 0.8304, Recall: 0.8261, F1: 0.8277
LM Predictions:  [0, 1, 3, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 0, 5, 5, 3, 3, 0, 3, 5, 4, 3, 5, 5, 3, 0, 4, 5, 3, 5, 0, 5, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8664, Accuracy: 0.3333, Precision: 0.6037, Recall: 0.3102, F1: 0.3279
Epoch 61/70
Train Loss: 0.1648, Accuracy: 0.9474, Precision: 0.9217, Recall: 0.9162, F1: 0.9189
Validation Loss: 0.6502, Accuracy: 0.8593, Precision: 0.8412, Recall: 0.7981, F1: 0.8135
Testing Loss: 0.5998, Accuracy: 0.8635, Precision: 0.8331, Recall: 0.7896, F1: 0.8018
LM Predictions:  [0, 1, 3, 0, 5, 0, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 3, 0, 3, 0, 0, 3, 0, 5, 3, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0881, Accuracy: 0.3333, Precision: 0.6345, Recall: 0.3144, F1: 0.2965
Epoch 62/70
Train Loss: 0.1584, Accuracy: 0.9481, Precision: 0.9190, Recall: 0.9186, F1: 0.9186
Validation Loss: 0.6735, Accuracy: 0.8785, Precision: 0.8518, Recall: 0.8281, F1: 0.8377
Testing Loss: 0.6559, Accuracy: 0.8720, Precision: 0.8378, Recall: 0.8201, F1: 0.8278
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 5, 5, 3, 0, 0, 3, 5, 0, 3, 5, 5, 2, 0, 4, 5, 3, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4916, Accuracy: 0.3571, Precision: 0.6846, Recall: 0.3394, F1: 0.3548
Epoch 63/70
Train Loss: 0.1536, Accuracy: 0.9504, Precision: 0.9220, Recall: 0.9261, F1: 0.9236
Validation Loss: 0.6681, Accuracy: 0.8763, Precision: 0.8478, Recall: 0.8300, F1: 0.8364
Testing Loss: 0.6416, Accuracy: 0.8671, Precision: 0.8329, Recall: 0.8252, F1: 0.8284
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 2, 5, 5, 3, 0, 5, 3, 5, 0, 3, 5, 5, 2, 0, 4, 5, 3, 5, 5, 5, 5, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3413, Accuracy: 0.4048, Precision: 0.7074, Recall: 0.3769, F1: 0.4159
Epoch 64/70
Train Loss: 0.1582, Accuracy: 0.9476, Precision: 0.9161, Recall: 0.9177, F1: 0.9167
Validation Loss: 0.6506, Accuracy: 0.8486, Precision: 0.8067, Recall: 0.8056, F1: 0.8035
Testing Loss: 0.6761, Accuracy: 0.8514, Precision: 0.8225, Recall: 0.8093, F1: 0.8085
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 2, 4, 5, 3, 0, 5, 3, 5, 5, 2, 5, 5, 2, 0, 4, 5, 3, 5, 5, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0988, Accuracy: 0.4286, Precision: 0.7500, Recall: 0.3935, F1: 0.4509
Epoch 65/70
Train Loss: 0.1722, Accuracy: 0.9457, Precision: 0.9172, Recall: 0.9163, F1: 0.9164
Validation Loss: 0.7540, Accuracy: 0.8550, Precision: 0.8241, Recall: 0.8116, F1: 0.8169
Testing Loss: 0.7209, Accuracy: 0.8635, Precision: 0.8284, Recall: 0.8071, F1: 0.8151
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 2, 4, 5, 3, 0, 0, 3, 5, 4, 3, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0843, Accuracy: 0.3810, Precision: 0.6657, Recall: 0.3519, F1: 0.4000
Epoch 66/70
Train Loss: 0.1678, Accuracy: 0.9443, Precision: 0.9116, Recall: 0.9241, F1: 0.9166
Validation Loss: 0.6493, Accuracy: 0.8507, Precision: 0.8199, Recall: 0.7809, F1: 0.7961
Testing Loss: 0.6615, Accuracy: 0.8551, Precision: 0.8204, Recall: 0.7787, F1: 0.7930
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 5, 3, 0, 2, 5, 0, 3, 3, 0, 3, 0, 0, 2, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8150, Accuracy: 0.4048, Precision: 0.7000, Recall: 0.3769, F1: 0.3709
Epoch 67/70
Train Loss: 0.1529, Accuracy: 0.9488, Precision: 0.9207, Recall: 0.9220, F1: 0.9212
Validation Loss: 0.7112, Accuracy: 0.8742, Precision: 0.8543, Recall: 0.8158, F1: 0.8315
Testing Loss: 0.6610, Accuracy: 0.8563, Precision: 0.8283, Recall: 0.7857, F1: 0.8026
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 2, 0, 0, 3, 3, 0, 3, 5, 0, 3, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1001, Accuracy: 0.3810, Precision: 0.6429, Recall: 0.3560, F1: 0.3381
Epoch 68/70
Train Loss: 0.1884, Accuracy: 0.9393, Precision: 0.9092, Recall: 0.9048, F1: 0.9067
Validation Loss: 0.7459, Accuracy: 0.8742, Precision: 0.8581, Recall: 0.8140, F1: 0.8317
Testing Loss: 0.7464, Accuracy: 0.8575, Precision: 0.8281, Recall: 0.7813, F1: 0.7960
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1400, Accuracy: 0.4048, Precision: 0.6636, Recall: 0.3769, F1: 0.3700
Epoch 69/70
Train Loss: 0.1512, Accuracy: 0.9500, Precision: 0.9206, Recall: 0.9239, F1: 0.9219
Validation Loss: 0.7164, Accuracy: 0.8721, Precision: 0.8542, Recall: 0.8126, F1: 0.8298
Testing Loss: 0.7658, Accuracy: 0.8514, Precision: 0.8254, Recall: 0.7711, F1: 0.7890
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 3, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0076, Accuracy: 0.3810, Precision: 0.5803, Recall: 0.3602, F1: 0.3417
Epoch 70/70
Train Loss: 0.1541, Accuracy: 0.9526, Precision: 0.9271, Recall: 0.9267, F1: 0.9266
Validation Loss: 0.7115, Accuracy: 0.8699, Precision: 0.8399, Recall: 0.8281, F1: 0.8326
Testing Loss: 0.7168, Accuracy: 0.8635, Precision: 0.8271, Recall: 0.8237, F1: 0.8248
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 2, 1, 5, 3, 0, 5, 3, 5, 0, 3, 5, 5, 2, 0, 4, 5, 3, 5, 5, 5, 5, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1014, Accuracy: 0.4048, Precision: 0.6241, Recall: 0.3769, F1: 0.4128
Label Memorization Analysis: 
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 0, 0, 2, 4, 3, 5, 2, 1, 5, 3, 0, 5, 3, 5, 0, 3, 5, 5, 2, 0, 4, 5, 3, 5, 5, 5, 5, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1014, Accuracy: 0.4048, Precision: 0.6241, Recall: 0.3769, F1: 0.4128

