Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.1073, Accuracy: 0.6381, Precision: 0.5160, Recall: 0.4817, F1: 0.4777
Testing Loss: 0.6781, Accuracy: 0.8273, Precision: 0.6690, Recall: 0.7179, F1: 0.6921
LM Predictions:  [0, 3, 0, 0, 3, 0, 3, 3, 0, 2, 3, 0, 3, 0, 3, 0, 3, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8745, Accuracy: 0.1667, Precision: 0.1731, Recall: 0.2000, F1: 0.1266
Epoch 2/70
Train Loss: 0.6924, Accuracy: 0.7963, Precision: 0.6980, Recall: 0.6952, F1: 0.6855
Validation Loss: 0.6726, Accuracy: 0.8273, Precision: 0.8285, Recall: 0.7103, F1: 0.7206
Testing Loss: 0.6100, Accuracy: 0.8357, Precision: 0.7567, Recall: 0.7226, F1: 0.7142
LM Predictions:  [0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4071, Accuracy: 0.1190, Precision: 0.1852, Recall: 0.1500, F1: 0.0628
Epoch 3/70
Train Loss: 0.5743, Accuracy: 0.8314, Precision: 0.7555, Recall: 0.7431, F1: 0.7466
Validation Loss: 0.6194, Accuracy: 0.8422, Precision: 0.8151, Recall: 0.7877, F1: 0.7949
Testing Loss: 0.5108, Accuracy: 0.8611, Precision: 0.8282, Recall: 0.8001, F1: 0.8076
LM Predictions:  [0, 3, 0, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 3, 0, 0, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 3, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1385, Accuracy: 0.0952, Precision: 0.0609, Recall: 0.1167, F1: 0.0561
Epoch 4/70
Train Loss: 0.4937, Accuracy: 0.8544, Precision: 0.7968, Recall: 0.7947, F1: 0.7953
Validation Loss: 0.5835, Accuracy: 0.8486, Precision: 0.8151, Recall: 0.8288, F1: 0.8167
Testing Loss: 0.4757, Accuracy: 0.8732, Precision: 0.8285, Recall: 0.8497, F1: 0.8378
LM Predictions:  [0, 3, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9752, Accuracy: 0.0714, Precision: 0.1923, Recall: 0.0833, F1: 0.0673
Epoch 5/70
Train Loss: 0.4494, Accuracy: 0.8698, Precision: 0.8149, Recall: 0.8122, F1: 0.8134
Validation Loss: 0.5211, Accuracy: 0.8550, Precision: 0.8276, Recall: 0.7886, F1: 0.8041
Testing Loss: 0.4674, Accuracy: 0.8684, Precision: 0.8618, Recall: 0.7908, F1: 0.8065
LM Predictions:  [0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0946, Accuracy: 0.1190, Precision: 0.1923, Recall: 0.1500, F1: 0.0733
Epoch 6/70
Train Loss: 0.4005, Accuracy: 0.8881, Precision: 0.8398, Recall: 0.8469, F1: 0.8426
Validation Loss: 0.5857, Accuracy: 0.8507, Precision: 0.8358, Recall: 0.7835, F1: 0.8042
Testing Loss: 0.5108, Accuracy: 0.8708, Precision: 0.8491, Recall: 0.7998, F1: 0.8196
LM Predictions:  [0, 5, 0, 0, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.5299, Accuracy: 0.1190, Precision: 0.1957, Recall: 0.1500, F1: 0.0779
Epoch 7/70
Train Loss: 0.3751, Accuracy: 0.8938, Precision: 0.8460, Recall: 0.8595, F1: 0.8511
Validation Loss: 0.5596, Accuracy: 0.8529, Precision: 0.8200, Recall: 0.7918, F1: 0.8031
Testing Loss: 0.5423, Accuracy: 0.8804, Precision: 0.8752, Recall: 0.8144, F1: 0.8311
LM Predictions:  [0, 3, 0, 0, 4, 0, 0, 5, 0, 5, 5, 0, 2, 0, 5, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4220, Accuracy: 0.1429, Precision: 0.1944, Recall: 0.1708, F1: 0.1071
Epoch 8/70
Train Loss: 0.3607, Accuracy: 0.9002, Precision: 0.8558, Recall: 0.8669, F1: 0.8595
Validation Loss: 0.5725, Accuracy: 0.8742, Precision: 0.8292, Recall: 0.8507, F1: 0.8387
Testing Loss: 0.5296, Accuracy: 0.8756, Precision: 0.8311, Recall: 0.8418, F1: 0.8359
LM Predictions:  [0, 3, 0, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1744, Accuracy: 0.0952, Precision: 0.1190, Recall: 0.1167, F1: 0.0804
Epoch 9/70
Train Loss: 0.3452, Accuracy: 0.9030, Precision: 0.8564, Recall: 0.8681, F1: 0.8601
Validation Loss: 0.5505, Accuracy: 0.8486, Precision: 0.8158, Recall: 0.8185, F1: 0.8115
Testing Loss: 0.5005, Accuracy: 0.8732, Precision: 0.8425, Recall: 0.8575, F1: 0.8442
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1424, Accuracy: 0.0952, Precision: 0.3167, Recall: 0.1042, F1: 0.1303
Epoch 10/70
Train Loss: 0.3219, Accuracy: 0.9108, Precision: 0.8680, Recall: 0.8844, F1: 0.8733
Validation Loss: 0.5483, Accuracy: 0.8529, Precision: 0.8163, Recall: 0.8109, F1: 0.8122
Testing Loss: 0.5194, Accuracy: 0.8647, Precision: 0.8341, Recall: 0.8376, F1: 0.8325
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7268, Accuracy: 0.0952, Precision: 0.2333, Recall: 0.1042, F1: 0.1278
Epoch 11/70
Train Loss: 0.3360, Accuracy: 0.9132, Precision: 0.8661, Recall: 0.8815, F1: 0.8718
Validation Loss: 0.6037, Accuracy: 0.8635, Precision: 0.8218, Recall: 0.8087, F1: 0.8148
Testing Loss: 0.5709, Accuracy: 0.8611, Precision: 0.8209, Recall: 0.8146, F1: 0.8169
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 4, 5, 0, 2, 0, 5, 0, 5, 2, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 4, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2021, Accuracy: 0.1190, Precision: 0.1705, Recall: 0.1375, F1: 0.1196
Epoch 12/70
Train Loss: 0.3820, Accuracy: 0.8874, Precision: 0.8386, Recall: 0.8498, F1: 0.8425
Validation Loss: 0.6119, Accuracy: 0.8635, Precision: 0.8211, Recall: 0.8264, F1: 0.8219
Testing Loss: 0.5521, Accuracy: 0.8623, Precision: 0.8220, Recall: 0.8315, F1: 0.8256
LM Predictions:  [0, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 2, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9728, Accuracy: 0.1190, Precision: 0.1843, Recall: 0.1375, F1: 0.1215
Epoch 13/70
Train Loss: 0.2918, Accuracy: 0.9194, Precision: 0.8775, Recall: 0.8891, F1: 0.8819
Validation Loss: 0.5950, Accuracy: 0.8486, Precision: 0.8284, Recall: 0.7896, F1: 0.8053
Testing Loss: 0.5840, Accuracy: 0.8587, Precision: 0.8339, Recall: 0.8002, F1: 0.8132
LM Predictions:  [0, 1, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 0, 5, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0179, Accuracy: 0.1429, Precision: 0.3690, Recall: 0.1560, F1: 0.1471
Epoch 14/70
Train Loss: 0.2659, Accuracy: 0.9298, Precision: 0.8890, Recall: 0.9045, F1: 0.8948
Validation Loss: 0.5999, Accuracy: 0.8507, Precision: 0.8022, Recall: 0.8224, F1: 0.8087
Testing Loss: 0.5944, Accuracy: 0.8659, Precision: 0.8315, Recall: 0.8356, F1: 0.8305
LM Predictions:  [0, 1, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0748, Accuracy: 0.1429, Precision: 0.5000, Recall: 0.1560, F1: 0.1891
Epoch 15/70
Train Loss: 0.2525, Accuracy: 0.9315, Precision: 0.8925, Recall: 0.9152, F1: 0.8994
Validation Loss: 0.6013, Accuracy: 0.8571, Precision: 0.8238, Recall: 0.8054, F1: 0.8138
Testing Loss: 0.5386, Accuracy: 0.8587, Precision: 0.8280, Recall: 0.8191, F1: 0.8219
LM Predictions:  [0, 1, 5, 5, 4, 5, 0, 5, 0, 5, 5, 0, 2, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8414, Accuracy: 0.1667, Precision: 0.4611, Recall: 0.1894, F1: 0.1648
Epoch 16/70
Train Loss: 0.2619, Accuracy: 0.9322, Precision: 0.8933, Recall: 0.9171, F1: 0.9019
Validation Loss: 0.7327, Accuracy: 0.8486, Precision: 0.8236, Recall: 0.7998, F1: 0.8081
Testing Loss: 0.5822, Accuracy: 0.8635, Precision: 0.8310, Recall: 0.8137, F1: 0.8211
LM Predictions:  [0, 1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 2, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 1, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0913, Accuracy: 0.1190, Precision: 0.2885, Recall: 0.1394, F1: 0.1229
Epoch 17/70
Train Loss: 0.2454, Accuracy: 0.9324, Precision: 0.8938, Recall: 0.9148, F1: 0.9004
Validation Loss: 0.7561, Accuracy: 0.8422, Precision: 0.8080, Recall: 0.7949, F1: 0.8008
Testing Loss: 0.5909, Accuracy: 0.8696, Precision: 0.8361, Recall: 0.8145, F1: 0.8226
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 2, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0944, Accuracy: 0.1667, Precision: 0.5370, Recall: 0.1894, F1: 0.1586
Epoch 18/70
Train Loss: 0.2430, Accuracy: 0.9303, Precision: 0.8913, Recall: 0.9054, F1: 0.8954
Validation Loss: 0.7007, Accuracy: 0.8486, Precision: 0.8158, Recall: 0.8093, F1: 0.8087
Testing Loss: 0.6171, Accuracy: 0.8563, Precision: 0.8248, Recall: 0.8289, F1: 0.8242
LM Predictions:  [0, 1, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4517, Accuracy: 0.1667, Precision: 0.5741, Recall: 0.1894, F1: 0.1959
Epoch 19/70
Train Loss: 0.2346, Accuracy: 0.9353, Precision: 0.8973, Recall: 0.9155, F1: 0.9025
Validation Loss: 0.7172, Accuracy: 0.8593, Precision: 0.8126, Recall: 0.8212, F1: 0.8156
Testing Loss: 0.6559, Accuracy: 0.8599, Precision: 0.8240, Recall: 0.8290, F1: 0.8252
LM Predictions:  [0, 3, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8052, Accuracy: 0.1429, Precision: 0.3106, Recall: 0.1708, F1: 0.1481
Epoch 20/70
Train Loss: 0.2398, Accuracy: 0.9364, Precision: 0.8976, Recall: 0.9164, F1: 0.9045
Validation Loss: 0.7268, Accuracy: 0.8550, Precision: 0.8320, Recall: 0.8062, F1: 0.8161
Testing Loss: 0.6422, Accuracy: 0.8659, Precision: 0.8376, Recall: 0.8259, F1: 0.8307
LM Predictions:  [0, 1, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0390, Accuracy: 0.1429, Precision: 0.3939, Recall: 0.1727, F1: 0.1537
Epoch 21/70
Train Loss: 0.2177, Accuracy: 0.9407, Precision: 0.9021, Recall: 0.9256, F1: 0.9101
Validation Loss: 0.6406, Accuracy: 0.8614, Precision: 0.8238, Recall: 0.8177, F1: 0.8207
Testing Loss: 0.6306, Accuracy: 0.8539, Precision: 0.8162, Recall: 0.8035, F1: 0.8078
LM Predictions:  [0, 1, 5, 0, 4, 5, 0, 5, 0, 5, 5, 0, 2, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8853, Accuracy: 0.1667, Precision: 0.4559, Recall: 0.1894, F1: 0.1588
Epoch 22/70
Train Loss: 0.2200, Accuracy: 0.9410, Precision: 0.9021, Recall: 0.9188, F1: 0.9079
Validation Loss: 0.6784, Accuracy: 0.8635, Precision: 0.8341, Recall: 0.8117, F1: 0.8206
Testing Loss: 0.5908, Accuracy: 0.8635, Precision: 0.8315, Recall: 0.8209, F1: 0.8249
LM Predictions:  [0, 1, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 2, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7123, Accuracy: 0.1905, Precision: 0.4124, Recall: 0.2102, F1: 0.1958
Epoch 23/70
Train Loss: 0.2271, Accuracy: 0.9374, Precision: 0.8985, Recall: 0.9155, F1: 0.9046
Validation Loss: 0.7233, Accuracy: 0.8571, Precision: 0.8173, Recall: 0.8197, F1: 0.8161
Testing Loss: 0.6198, Accuracy: 0.8671, Precision: 0.8290, Recall: 0.8301, F1: 0.8283
LM Predictions:  [0, 1, 1, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 0, 5, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7322, Accuracy: 0.1905, Precision: 0.4402, Recall: 0.2102, F1: 0.1988
Epoch 24/70
Train Loss: 0.2238, Accuracy: 0.9383, Precision: 0.9003, Recall: 0.9190, F1: 0.9066
Validation Loss: 0.7747, Accuracy: 0.8507, Precision: 0.8107, Recall: 0.8045, F1: 0.8067
Testing Loss: 0.6750, Accuracy: 0.8635, Precision: 0.8243, Recall: 0.8189, F1: 0.8210
LM Predictions:  [0, 1, 5, 0, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 2, 5, 5, 2, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7453, Accuracy: 0.1905, Precision: 0.3889, Recall: 0.2102, F1: 0.1951
Epoch 25/70
Train Loss: 0.2041, Accuracy: 0.9474, Precision: 0.9112, Recall: 0.9337, F1: 0.9190
Validation Loss: 0.7502, Accuracy: 0.8443, Precision: 0.7998, Recall: 0.8057, F1: 0.8002
Testing Loss: 0.6084, Accuracy: 0.8575, Precision: 0.8225, Recall: 0.8205, F1: 0.8191
LM Predictions:  [0, 5, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7942, Accuracy: 0.1905, Precision: 0.5556, Recall: 0.2042, F1: 0.2398
Epoch 26/70
Train Loss: 0.2284, Accuracy: 0.9364, Precision: 0.8986, Recall: 0.9177, F1: 0.9061
Validation Loss: 0.9476, Accuracy: 0.8294, Precision: 0.7956, Recall: 0.7778, F1: 0.7820
Testing Loss: 0.6936, Accuracy: 0.8587, Precision: 0.8259, Recall: 0.8088, F1: 0.8162
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 2, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6133, Accuracy: 0.2143, Precision: 0.5754, Recall: 0.2269, F1: 0.2217
Epoch 27/70
Train Loss: 0.2391, Accuracy: 0.9379, Precision: 0.9033, Recall: 0.9186, F1: 0.9079
Validation Loss: 0.7390, Accuracy: 0.8294, Precision: 0.7829, Recall: 0.7851, F1: 0.7805
Testing Loss: 0.6963, Accuracy: 0.8442, Precision: 0.8166, Recall: 0.7959, F1: 0.7989
LM Predictions:  [0, 1, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 2, 0, 5, 4, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 4, 4, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6822, Accuracy: 0.2143, Precision: 0.4111, Recall: 0.2269, F1: 0.2273
Epoch 28/70
Train Loss: 0.3306, Accuracy: 0.9108, Precision: 0.8827, Recall: 0.8996, F1: 0.8884
Validation Loss: 0.7011, Accuracy: 0.8401, Precision: 0.7972, Recall: 0.7934, F1: 0.7936
Testing Loss: 0.6053, Accuracy: 0.8635, Precision: 0.8290, Recall: 0.8217, F1: 0.8242
LM Predictions:  [0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9334, Accuracy: 0.1429, Precision: 0.3810, Recall: 0.1708, F1: 0.1375
Epoch 29/70
Train Loss: 0.2077, Accuracy: 0.9436, Precision: 0.9068, Recall: 0.9308, F1: 0.9141
Validation Loss: 0.7258, Accuracy: 0.8465, Precision: 0.8165, Recall: 0.7842, F1: 0.7960
Testing Loss: 0.6307, Accuracy: 0.8587, Precision: 0.8333, Recall: 0.8094, F1: 0.8182
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7120, Accuracy: 0.1905, Precision: 0.5606, Recall: 0.2102, F1: 0.2136
Epoch 30/70
Train Loss: 0.2335, Accuracy: 0.9331, Precision: 0.8950, Recall: 0.9113, F1: 0.8983
Validation Loss: 0.6922, Accuracy: 0.8486, Precision: 0.8085, Recall: 0.8091, F1: 0.8037
Testing Loss: 0.6903, Accuracy: 0.8490, Precision: 0.8110, Recall: 0.8209, F1: 0.8124
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7402, Accuracy: 0.1905, Precision: 0.5667, Recall: 0.2102, F1: 0.2192
Epoch 31/70
Train Loss: 0.1985, Accuracy: 0.9452, Precision: 0.9079, Recall: 0.9248, F1: 0.9139
Validation Loss: 0.7414, Accuracy: 0.8635, Precision: 0.8396, Recall: 0.7936, F1: 0.8105
Testing Loss: 0.6815, Accuracy: 0.8563, Precision: 0.8169, Recall: 0.7794, F1: 0.7905
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6691, Accuracy: 0.2381, Precision: 0.6167, Recall: 0.2602, F1: 0.2131
Epoch 32/70
Train Loss: 0.1990, Accuracy: 0.9471, Precision: 0.9135, Recall: 0.9280, F1: 0.9190
Validation Loss: 0.7030, Accuracy: 0.8507, Precision: 0.8112, Recall: 0.8002, F1: 0.8041
Testing Loss: 0.6611, Accuracy: 0.8587, Precision: 0.8245, Recall: 0.8032, F1: 0.8123
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7036, Accuracy: 0.2143, Precision: 0.6346, Recall: 0.2269, F1: 0.2316
Epoch 33/70
Train Loss: 0.1890, Accuracy: 0.9481, Precision: 0.9155, Recall: 0.9258, F1: 0.9189
Validation Loss: 0.7391, Accuracy: 0.8422, Precision: 0.7956, Recall: 0.8027, F1: 0.7935
Testing Loss: 0.6903, Accuracy: 0.8587, Precision: 0.8171, Recall: 0.8248, F1: 0.8189
LM Predictions:  [0, 3, 3, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7864, Accuracy: 0.1905, Precision: 0.4619, Recall: 0.2083, F1: 0.2303
Epoch 34/70
Train Loss: 0.2028, Accuracy: 0.9433, Precision: 0.9071, Recall: 0.9174, F1: 0.9105
Validation Loss: 0.8389, Accuracy: 0.8507, Precision: 0.8082, Recall: 0.7989, F1: 0.8014
Testing Loss: 0.7358, Accuracy: 0.8599, Precision: 0.8234, Recall: 0.8084, F1: 0.8149
LM Predictions:  [0, 3, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 5, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6011, Accuracy: 0.1905, Precision: 0.4333, Recall: 0.2083, F1: 0.1893
Epoch 35/70
Train Loss: 0.2760, Accuracy: 0.9215, Precision: 0.8814, Recall: 0.8975, F1: 0.8878
Validation Loss: 0.8100, Accuracy: 0.8358, Precision: 0.8108, Recall: 0.7940, F1: 0.7987
Testing Loss: 0.7019, Accuracy: 0.8502, Precision: 0.8175, Recall: 0.8019, F1: 0.8063
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5168, Accuracy: 0.2143, Precision: 0.7179, Recall: 0.2269, F1: 0.2347
Epoch 36/70
Train Loss: 0.2140, Accuracy: 0.9395, Precision: 0.9073, Recall: 0.9231, F1: 0.9125
Validation Loss: 0.7326, Accuracy: 0.8443, Precision: 0.8055, Recall: 0.7997, F1: 0.8001
Testing Loss: 0.6756, Accuracy: 0.8539, Precision: 0.8119, Recall: 0.7985, F1: 0.8044
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 0, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3515, Accuracy: 0.2381, Precision: 0.7105, Recall: 0.2602, F1: 0.2301
Epoch 37/70
Train Loss: 0.1992, Accuracy: 0.9440, Precision: 0.9103, Recall: 0.9219, F1: 0.9140
Validation Loss: 0.8078, Accuracy: 0.8337, Precision: 0.7842, Recall: 0.8094, F1: 0.7928
Testing Loss: 0.7337, Accuracy: 0.8430, Precision: 0.7957, Recall: 0.8062, F1: 0.7972
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 3, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7029, Accuracy: 0.2143, Precision: 0.6389, Recall: 0.2269, F1: 0.2365
Epoch 38/70
Train Loss: 0.2350, Accuracy: 0.9353, Precision: 0.8970, Recall: 0.9072, F1: 0.8996
Validation Loss: 0.8742, Accuracy: 0.8273, Precision: 0.8019, Recall: 0.7622, F1: 0.7733
Testing Loss: 0.7076, Accuracy: 0.8527, Precision: 0.8339, Recall: 0.8036, F1: 0.8122
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 0, 5, 0, 2, 5, 3, 5, 3, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 5, 5, 5, 5, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4449, Accuracy: 0.2619, Precision: 0.6250, Recall: 0.2810, F1: 0.2782
Epoch 39/70
Train Loss: 0.2089, Accuracy: 0.9414, Precision: 0.9065, Recall: 0.9204, F1: 0.9101
Validation Loss: 0.6736, Accuracy: 0.8529, Precision: 0.8207, Recall: 0.7997, F1: 0.8079
Testing Loss: 0.6853, Accuracy: 0.8527, Precision: 0.8103, Recall: 0.7903, F1: 0.7990
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3359, Accuracy: 0.2619, Precision: 0.5518, Recall: 0.2810, F1: 0.2343
Epoch 40/70
Train Loss: 0.1916, Accuracy: 0.9466, Precision: 0.9150, Recall: 0.9229, F1: 0.9173
Validation Loss: 0.7837, Accuracy: 0.8443, Precision: 0.8213, Recall: 0.7917, F1: 0.8023
Testing Loss: 0.7081, Accuracy: 0.8502, Precision: 0.8134, Recall: 0.7830, F1: 0.7963
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 5, 5, 0, 5, 5, 0, 5, 0, 5, 2, 0, 4, 0, 0, 5, 0, 0, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2591, Accuracy: 0.2619, Precision: 0.4702, Recall: 0.2810, F1: 0.2342
Epoch 41/70
Train Loss: 0.1848, Accuracy: 0.9485, Precision: 0.9179, Recall: 0.9260, F1: 0.9202
Validation Loss: 0.6977, Accuracy: 0.8635, Precision: 0.8490, Recall: 0.7992, F1: 0.8153
Testing Loss: 0.7006, Accuracy: 0.8490, Precision: 0.8050, Recall: 0.7694, F1: 0.7831
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 3, 5, 0, 5, 0, 0, 2, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 3, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2894, Accuracy: 0.3333, Precision: 0.5792, Recall: 0.3352, F1: 0.2830
Epoch 42/70
Train Loss: 0.1717, Accuracy: 0.9521, Precision: 0.9218, Recall: 0.9322, F1: 0.9256
Validation Loss: 0.8093, Accuracy: 0.8571, Precision: 0.8473, Recall: 0.7853, F1: 0.8065
Testing Loss: 0.8108, Accuracy: 0.8490, Precision: 0.8112, Recall: 0.7654, F1: 0.7821
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3456, Accuracy: 0.2619, Precision: 0.4892, Recall: 0.2810, F1: 0.2243
Epoch 43/70
Train Loss: 0.1787, Accuracy: 0.9509, Precision: 0.9217, Recall: 0.9242, F1: 0.9222
Validation Loss: 0.7728, Accuracy: 0.8550, Precision: 0.8157, Recall: 0.8146, F1: 0.8128
Testing Loss: 0.7421, Accuracy: 0.8539, Precision: 0.8180, Recall: 0.8038, F1: 0.8099
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6057, Accuracy: 0.2619, Precision: 0.5397, Recall: 0.2810, F1: 0.2439
Epoch 44/70
Train Loss: 0.1633, Accuracy: 0.9545, Precision: 0.9257, Recall: 0.9377, F1: 0.9302
Validation Loss: 0.7179, Accuracy: 0.8465, Precision: 0.8037, Recall: 0.7856, F1: 0.7900
Testing Loss: 0.7040, Accuracy: 0.8466, Precision: 0.8013, Recall: 0.7789, F1: 0.7876
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 2, 0, 0, 2, 5, 0, 2, 0, 5, 1, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0409, Accuracy: 0.3095, Precision: 0.3904, Recall: 0.3227, F1: 0.2495
Epoch 45/70
Train Loss: 0.1889, Accuracy: 0.9455, Precision: 0.9155, Recall: 0.9211, F1: 0.9174
Validation Loss: 0.8312, Accuracy: 0.8443, Precision: 0.8278, Recall: 0.7484, F1: 0.7735
Testing Loss: 0.7976, Accuracy: 0.8394, Precision: 0.8041, Recall: 0.7426, F1: 0.7621
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1645, Accuracy: 0.3571, Precision: 0.5864, Recall: 0.3602, F1: 0.2846
Epoch 46/70
Train Loss: 0.1824, Accuracy: 0.9485, Precision: 0.9201, Recall: 0.9281, F1: 0.9226
Validation Loss: 0.7636, Accuracy: 0.8529, Precision: 0.8101, Recall: 0.8141, F1: 0.8079
Testing Loss: 0.7408, Accuracy: 0.8502, Precision: 0.8115, Recall: 0.8029, F1: 0.8063
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 5, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 1, 0, 4, 5, 0, 5, 0, 0, 0, 5, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4051, Accuracy: 0.2619, Precision: 0.4306, Recall: 0.2810, F1: 0.2409
Train Loss: 0.1732, Accuracy: 0.9535, Precision: 0.9279, Recall: 0.9302, F1: 0.9283
Validation Loss: 0.8838, Accuracy: 0.8380, Precision: 0.7901, Recall: 0.7704, F1: 0.7770
Testing Loss: 0.7911, Accuracy: 0.8502, Precision: 0.8131, Recall: 0.7838, F1: 0.7965
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 1, 0, 3, 0, 5, 1, 5, 0, 1, 0, 5, 1, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3611, Accuracy: 0.2857, Precision: 0.5240, Recall: 0.2954, F1: 0.2492
Epoch 47/70
Train Loss: 0.1732, Accuracy: 0.9535, Precision: 0.9279, Recall: 0.9302, F1: 0.9283
Validation Loss: 0.8838, Accuracy: 0.8380, Precision: 0.7901, Recall: 0.7704, F1: 0.7770
Testing Loss: 0.7911, Accuracy: 0.8502, Precision: 0.8131, Recall: 0.7838, F1: 0.7965
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 1, 0, 3, 0, 5, 1, 5, 0, 1, 0, 5, 1, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 1, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3611, Accuracy: 0.2857, Precision: 0.5240, Recall: 0.2954, F1: 0.2492
Epoch 48/70
Train Loss: 0.1730, Accuracy: 0.9471, Precision: 0.9203, Recall: 0.9164, F1: 0.9178
Validation Loss: 0.8155, Accuracy: 0.8422, Precision: 0.7993, Recall: 0.7723, F1: 0.7814
Testing Loss: 0.7497, Accuracy: 0.8418, Precision: 0.8039, Recall: 0.7716, F1: 0.7856
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 2, 0, 0, 2, 5, 0, 2, 0, 5, 2, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9868, Accuracy: 0.3333, Precision: 0.5529, Recall: 0.3435, F1: 0.2620
Epoch 49/70
Train Loss: 0.1736, Accuracy: 0.9493, Precision: 0.9206, Recall: 0.9272, F1: 0.9225
Validation Loss: 0.9166, Accuracy: 0.8443, Precision: 0.8183, Recall: 0.7822, F1: 0.7912
Testing Loss: 0.8410, Accuracy: 0.8466, Precision: 0.8173, Recall: 0.7742, F1: 0.7918
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 0, 0, 5, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1386, Accuracy: 0.2619, Precision: 0.5657, Recall: 0.2769, F1: 0.2403
Epoch 50/70
Train Loss: 0.1820, Accuracy: 0.9497, Precision: 0.9218, Recall: 0.9219, F1: 0.9211
Validation Loss: 0.8012, Accuracy: 0.8529, Precision: 0.8103, Recall: 0.8042, F1: 0.8060
Testing Loss: 0.7728, Accuracy: 0.8514, Precision: 0.8082, Recall: 0.7909, F1: 0.7988
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 5, 2, 5, 0, 2, 5, 5, 2, 0, 4, 0, 0, 5, 5, 0, 0, 0, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3020, Accuracy: 0.3571, Precision: 0.5300, Recall: 0.3602, F1: 0.3068
Epoch 51/70
Train Loss: 0.1655, Accuracy: 0.9502, Precision: 0.9216, Recall: 0.9205, F1: 0.9207
Validation Loss: 0.9152, Accuracy: 0.8401, Precision: 0.8005, Recall: 0.7853, F1: 0.7915
Testing Loss: 0.7964, Accuracy: 0.8527, Precision: 0.8205, Recall: 0.7935, F1: 0.8046
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 1, 0, 5, 2, 5, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6936, Accuracy: 0.2381, Precision: 0.3990, Recall: 0.2602, F1: 0.2082
Epoch 52/70
 Train Loss: 0.1644, Accuracy: 0.9533, Precision: 0.9224, Recall: 0.9346, F1: 0.9266
Validation Loss: 0.8861, Accuracy: 0.8571, Precision: 0.8173, Recall: 0.8046, F1: 0.8102
Testing Loss: 0.8156, Accuracy: 0.8514, Precision: 0.8197, Recall: 0.7834, F1: 0.7987
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 1, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6428, Accuracy: 0.2619, Precision: 0.4792, Recall: 0.2769, F1: 0.2335
Epoch 53/70
Train Loss: 0.1641, Accuracy: 0.9490, Precision: 0.9226, Recall: 0.9247, F1: 0.9231
Validation Loss: 0.7920, Accuracy: 0.8486, Precision: 0.8108, Recall: 0.7883, F1: 0.7958
Testing Loss: 0.7517, Accuracy: 0.8478, Precision: 0.8157, Recall: 0.7806, F1: 0.7957
LM Predictions:  [0, 2, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 0, 0, 2, 0, 0, 2, 0, 5, 2, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 3, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3729, Accuracy: 0.3095, Precision: 0.3889, Recall: 0.3208, F1: 0.2396
Epoch 54/70
Train Loss: 0.1630, Accuracy: 0.9535, Precision: 0.9267, Recall: 0.9368, F1: 0.9301
Validation Loss: 0.9189, Accuracy: 0.8358, Precision: 0.7993, Recall: 0.7924, F1: 0.7934
Testing Loss: 0.8242, Accuracy: 0.8478, Precision: 0.8167, Recall: 0.7930, F1: 0.8035
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 5, 2, 0, 0, 2, 0, 5, 2, 0, 4, 0, 0, 5, 0, 0, 0, 5, 0, 3, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5691, Accuracy: 0.3333, Precision: 0.5587, Recall: 0.3394, F1: 0.2772
Epoch 55/70
Train Loss: 0.1608, Accuracy: 0.9545, Precision: 0.9280, Recall: 0.9375, F1: 0.9315
Validation Loss: 0.9585, Accuracy: 0.8358, Precision: 0.8064, Recall: 0.7508, F1: 0.7705
Testing Loss: 0.8644, Accuracy: 0.8466, Precision: 0.8327, Recall: 0.7661, F1: 0.7888
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3779, Accuracy: 0.2857, Precision: 0.4376, Recall: 0.2977, F1: 0.2364
Epoch 55/70
Train Loss: 0.1608, Accuracy: 0.9545, Precision: 0.9280, Recall: 0.9375, F1: 0.9315
Validation Loss: 0.9585, Accuracy: 0.8358, Precision: 0.8064, Recall: 0.7508, F1: 0.7705
Testing Loss: 0.8644, Accuracy: 0.8466, Precision: 0.8327, Recall: 0.7661, F1: 0.7888
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3779, Accuracy: 0.2857, Precision: 0.4376, Recall: 0.2977, F1: 0.2364
Epoch 56/70
Train Loss: 0.1711, Accuracy: 0.9512, Precision: 0.9273, Recall: 0.9282, F1: 0.9274
Validation Loss: 0.9071, Accuracy: 0.8230, Precision: 0.7850, Recall: 0.7827, F1: 0.7799
Testing Loss: 0.7668, Accuracy: 0.8418, Precision: 0.8012, Recall: 0.8025, F1: 0.7991
LM Predictions:  [0, 1, 1, 0, 5, 5, 0, 5, 0, 0, 0, 0, 2, 5, 3, 0, 3, 2, 5, 3, 5, 5, 2, 5, 0, 1, 5, 5, 2, 0, 4, 5, 5, 5, 5, 0, 0, 5, 5, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9804, Accuracy: 0.2857, Precision: 0.4750, Recall: 0.2977, F1: 0.2807
Epoch 57/70
Train Loss: 0.1839, Accuracy: 0.9521, Precision: 0.9270, Recall: 0.9373, F1: 0.9310
Validation Loss: 0.8682, Accuracy: 0.8337, Precision: 0.8021, Recall: 0.7979, F1: 0.7984
Testing Loss: 0.7654, Accuracy: 0.8466, Precision: 0.8088, Recall: 0.7766, F1: 0.7888
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 5, 0, 0, 4, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4944, Accuracy: 0.2619, Precision: 0.4361, Recall: 0.2769, F1: 0.2209
Epoch 58/70
Train Loss: 0.1913, Accuracy: 0.9469, Precision: 0.9228, Recall: 0.9307, F1: 0.9258
Validation Loss: 1.0015, Accuracy: 0.7910, Precision: 0.7825, Recall: 0.7450, F1: 0.7552
Testing Loss: 0.7628, Accuracy: 0.8249, Precision: 0.8218, Recall: 0.7627, F1: 0.7835
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 0, 0, 5, 0, 0, 1, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2689, Accuracy: 0.2381, Precision: 0.4209, Recall: 0.2560, F1: 0.1940
Epoch 59/70
Train Loss: 0.2019, Accuracy: 0.9476, Precision: 0.9219, Recall: 0.9342, F1: 0.9269
Validation Loss: 0.9546, Accuracy: 0.8358, Precision: 0.8083, Recall: 0.7720, F1: 0.7874
Testing Loss: 0.7888, Accuracy: 0.8478, Precision: 0.8139, Recall: 0.7728, F1: 0.7881
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5367, Accuracy: 0.2857, Precision: 0.3543, Recall: 0.2977, F1: 0.2339
Epoch 60/70
Train Loss: 0.1672, Accuracy: 0.9535, Precision: 0.9318, Recall: 0.9335, F1: 0.9322
Validation Loss: 0.8831, Accuracy: 0.8550, Precision: 0.8219, Recall: 0.7840, F1: 0.7977
Testing Loss: 0.8099, Accuracy: 0.8539, Precision: 0.8201, Recall: 0.7653, F1: 0.7818
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 4, 0, 3, 0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9252, Accuracy: 0.2857, Precision: 0.4951, Recall: 0.2935, F1: 0.2389
Epoch 61/70
Train Loss: 0.1535, Accuracy: 0.9542, Precision: 0.9350, Recall: 0.9297, F1: 0.9321
Validation Loss: 0.9134, Accuracy: 0.8358, Precision: 0.8037, Recall: 0.7645, F1: 0.7807
Testing Loss: 0.8178, Accuracy: 0.8478, Precision: 0.8149, Recall: 0.7635, F1: 0.7820
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 4, 0, 3, 0, 0, 2, 0, 0, 4, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9078, Accuracy: 0.3095, Precision: 0.4737, Recall: 0.3144, F1: 0.2625
Epoch 62/70
Train Loss: 0.1469, Accuracy: 0.9542, Precision: 0.9311, Recall: 0.9334, F1: 0.9317
Validation Loss: 0.9169, Accuracy: 0.8529, Precision: 0.8309, Recall: 0.7677, F1: 0.7843
Testing Loss: 0.8363, Accuracy: 0.8539, Precision: 0.8187, Recall: 0.7563, F1: 0.7708
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 4, 0, 3, 0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0246, Accuracy: 0.2857, Precision: 0.4951, Recall: 0.2935, F1: 0.2389
Epoch 63/70
Train Loss: 0.1510, Accuracy: 0.9526, Precision: 0.9288, Recall: 0.9259, F1: 0.9271
Validation Loss: 0.8729, Accuracy: 0.8529, Precision: 0.8153, Recall: 0.7909, F1: 0.8009
Testing Loss: 0.7807, Accuracy: 0.8539, Precision: 0.8129, Recall: 0.7743, F1: 0.7890
LM Predictions:  [0, 1, 4, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 4, 0, 3, 0, 0, 4, 0, 0, 4, 0, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1137, Accuracy: 0.2857, Precision: 0.4963, Recall: 0.2935, F1: 0.2406
Epoch 64/70
Train Loss: 0.1471, Accuracy: 0.9564, Precision: 0.9389, Recall: 0.9326, F1: 0.9356
Validation Loss: 0.8721, Accuracy: 0.8529, Precision: 0.8151, Recall: 0.7863, F1: 0.7984
Testing Loss: 0.8039, Accuracy: 0.8527, Precision: 0.8168, Recall: 0.7707, F1: 0.7883
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7256, Accuracy: 0.3571, Precision: 0.5864, Recall: 0.3602, F1: 0.2846
Epoch 65/70
Train Loss: 0.1624, Accuracy: 0.9542, Precision: 0.9306, Recall: 0.9413, F1: 0.9345
Validation Loss: 0.8424, Accuracy: 0.8571, Precision: 0.8205, Recall: 0.7924, F1: 0.8038
Testing Loss: 0.8073, Accuracy: 0.8514, Precision: 0.8184, Recall: 0.7740, F1: 0.7917
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8871, Accuracy: 0.3571, Precision: 0.5864, Recall: 0.3602, F1: 0.2846
Epoch 66/70
Train Loss: 0.1570, Accuracy: 0.9542, Precision: 0.9336, Recall: 0.9362, F1: 0.9342
Validation Loss: 0.9854, Accuracy: 0.8081, Precision: 0.7972, Recall: 0.7450, F1: 0.7644
Testing Loss: 0.7589, Accuracy: 0.8406, Precision: 0.8207, Recall: 0.7616, F1: 0.7844
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8235, Accuracy: 0.3333, Precision: 0.5795, Recall: 0.3394, F1: 0.2712
Epoch 67/70
Train Loss: 0.1695, Accuracy: 0.9495, Precision: 0.9299, Recall: 0.9277, F1: 0.9283
Validation Loss: 0.8409, Accuracy: 0.8358, Precision: 0.7946, Recall: 0.7790, F1: 0.7854
Testing Loss: 0.7085, Accuracy: 0.8527, Precision: 0.8074, Recall: 0.7987, F1: 0.8017
LM Predictions:  [0, 1, 2, 4, 5, 0, 0, 5, 4, 0, 4, 0, 2, 4, 3, 0, 3, 2, 4, 3, 4, 4, 2, 4, 0, 2, 4, 4, 2, 0, 4, 0, 0, 0, 4, 4, 0, 0, 4, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5797, Accuracy: 0.5000, Precision: 0.5363, Recall: 0.4602, F1: 0.3921
Epoch 68/70
Train Loss: 0.1562, Accuracy: 0.9509, Precision: 0.9278, Recall: 0.9335, F1: 0.9301
Validation Loss: 0.8176, Accuracy: 0.8294, Precision: 0.8289, Recall: 0.7268, F1: 0.7328
Testing Loss: 0.7244, Accuracy: 0.8490, Precision: 0.8414, Recall: 0.7454, F1: 0.7517
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5876, Accuracy: 0.3333, Precision: 0.5784, Recall: 0.3394, F1: 0.2696
Epoch 69/70
Train Loss: 0.1823, Accuracy: 0.9433, Precision: 0.9231, Recall: 0.9299, F1: 0.9251
Validation Loss: 0.8300, Accuracy: 0.8507, Precision: 0.8068, Recall: 0.7990, F1: 0.8018
Testing Loss: 0.8161, Accuracy: 0.8539, Precision: 0.8159, Recall: 0.7822, F1: 0.7950
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 1, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 1, 0, 2, 1, 0, 2, 0, 0, 5, 0, 4, 0, 1, 1, 0, 1, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9483, Accuracy: 0.3571, Precision: 0.4692, Recall: 0.3579, F1: 0.2915
Epoch 70/70
Train Loss: 0.1534, Accuracy: 0.9557, Precision: 0.9336, Recall: 0.9336, F1: 0.9333
Validation Loss: 0.8232, Accuracy: 0.8550, Precision: 0.8147, Recall: 0.7962, F1: 0.8043
Testing Loss: 0.7735, Accuracy: 0.8490, Precision: 0.8170, Recall: 0.7692, F1: 0.7865
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6706, Accuracy: 0.3571, Precision: 0.5864, Recall: 0.3602, F1: 0.2846
Label Memorization Analysis: 
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6706, Accuracy: 0.3571, Precision: 0.5864, Recall: 0.3602, F1: 0.2846