Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.0958, Accuracy: 0.6327, Precision: 0.5218, Recall: 0.4840, F1: 0.4776
Validation Loss: 0.8104, Accuracy: 0.7420, Precision: 0.7102, Recall: 0.5958, F1: 0.5652
Testing Loss: 0.7900, Accuracy: 0.7488, Precision: 0.6415, Recall: 0.6022, F1: 0.5641
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8856, Accuracy: 0.0952, Precision: 0.0222, Recall: 0.1600, F1: 0.0390
Epoch 2/70
Train Loss: 0.7840, Accuracy: 0.7657, Precision: 0.7266, Recall: 0.6510, F1: 0.6422
Validation Loss: 0.7047, Accuracy: 0.7974, Precision: 0.7384, Recall: 0.6869, F1: 0.6936
Testing Loss: 0.6550, Accuracy: 0.8116, Precision: 0.7509, Recall: 0.6904, F1: 0.6995
LM Predictions:  [0, 0, 0, 0, 4, 0, 5, 4, 5, 0, 4, 0, 5, 0, 0, 0, 5, 2, 0, 4, 0, 0, 2, 0, 0, 0, 0, 4, 5, 0, 0, 4, 0, 5, 0, 0, 0, 0, 0, 4, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1909, Accuracy: 0.1429, Precision: 0.0673, Recall: 0.1667, F1: 0.0800
Epoch 3/70
Train Loss: 0.5877, Accuracy: 0.8264, Precision: 0.7525, Recall: 0.7274, F1: 0.7339
Validation Loss: 0.5665, Accuracy: 0.8401, Precision: 0.7997, Recall: 0.8406, F1: 0.8082
Testing Loss: 0.5282, Accuracy: 0.8406, Precision: 0.7934, Recall: 0.8314, F1: 0.8043
LM Predictions:  [0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 2, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 3, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8003, Accuracy: 0.0476, Precision: 0.1905, Recall: 0.0500, F1: 0.0581
Epoch 4/70
Train Loss: 0.5116, Accuracy: 0.8513, Precision: 0.7982, Recall: 0.7960, F1: 0.7961
Validation Loss: 0.5931, Accuracy: 0.8401, Precision: 0.8026, Recall: 0.7883, F1: 0.7931
Testing Loss: 0.5094, Accuracy: 0.8635, Precision: 0.8419, Recall: 0.7992, F1: 0.8094
LM Predictions:  [0, 3, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 0, 3, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 4, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1676, Accuracy: 0.0952, Precision: 0.2659, Recall: 0.1000, F1: 0.0837
Epoch 5/70
Train Loss: 0.4532, Accuracy: 0.8724, Precision: 0.8219, Recall: 0.8253, F1: 0.8225
Validation Loss: 0.6411, Accuracy: 0.8358, Precision: 0.8119, Recall: 0.7873, F1: 0.7961
Testing Loss: 0.5410, Accuracy: 0.8647, Precision: 0.8707, Recall: 0.8051, F1: 0.8262
LM Predictions:  [0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.6836, Accuracy: 0.1190, Precision: 0.1923, Recall: 0.1500, F1: 0.0733
Epoch 6/70
Train Loss: 0.4133, Accuracy: 0.8821, Precision: 0.8381, Recall: 0.8333, F1: 0.8352
Validation Loss: 0.5916, Accuracy: 0.8465, Precision: 0.8089, Recall: 0.8193, F1: 0.8133
Testing Loss: 0.4875, Accuracy: 0.8720, Precision: 0.8352, Recall: 0.8298, F1: 0.8314
LM Predictions:  [0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 4, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8882, Accuracy: 0.0952, Precision: 0.1970, Recall: 0.1000, F1: 0.0972
Epoch 7/70
Train Loss: 0.3734, Accuracy: 0.8938, Precision: 0.8507, Recall: 0.8541, F1: 0.8518
Validation Loss: 0.5287, Accuracy: 0.8550, Precision: 0.8215, Recall: 0.8183, F1: 0.8188
Testing Loss: 0.4886, Accuracy: 0.8744, Precision: 0.8409, Recall: 0.8173, F1: 0.8262
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 4, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3383, Accuracy: 0.0714, Precision: 0.1863, Recall: 0.0833, F1: 0.0606
Epoch 8/70
Train Loss: 0.3832, Accuracy: 0.8968, Precision: 0.8574, Recall: 0.8749, F1: 0.8638
Validation Loss: 0.5756, Accuracy: 0.8614, Precision: 0.8291, Recall: 0.8253, F1: 0.8258
Testing Loss: 0.4825, Accuracy: 0.8744, Precision: 0.8405, Recall: 0.8275, F1: 0.8332
LM Predictions:  [0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 4, 5, 2, 5, 5, 5, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0527, Accuracy: 0.0952, Precision: 0.3590, Recall: 0.1000, F1: 0.0976
Epoch 9/70
Train Loss: 0.3334, Accuracy: 0.9080, Precision: 0.8682, Recall: 0.8794, F1: 0.8721
Validation Loss: 0.6252, Accuracy: 0.8465, Precision: 0.8098, Recall: 0.7981, F1: 0.8023
Testing Loss: 0.5346, Accuracy: 0.8696, Precision: 0.8463, Recall: 0.8132, F1: 0.8254
LM Predictions:  [0, 5, 0, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 4, 4, 4, 5, 5, 0, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0604, Accuracy: 0.0952, Precision: 0.2279, Recall: 0.1000, F1: 0.0844
Epoch 10/70
Train Loss: 0.3396, Accuracy: 0.9032, Precision: 0.8584, Recall: 0.8670, F1: 0.8617
Validation Loss: 0.5638, Accuracy: 0.8593, Precision: 0.8424, Recall: 0.8211, F1: 0.8296
Testing Loss: 0.4936, Accuracy: 0.8647, Precision: 0.8324, Recall: 0.8127, F1: 0.8212
LM Predictions:  [0, 2, 2, 5, 4, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 2, 0, 5, 5, 5, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5688, Accuracy: 0.1905, Precision: 0.3444, Recall: 0.2083, F1: 0.1857
Epoch 11/70
Train Loss: 0.2983, Accuracy: 0.9203, Precision: 0.8814, Recall: 0.8941, F1: 0.8856
Validation Loss: 0.6483, Accuracy: 0.8550, Precision: 0.8116, Recall: 0.8216, F1: 0.8156
Testing Loss: 0.5458, Accuracy: 0.8635, Precision: 0.8241, Recall: 0.8210, F1: 0.8202
LM Predictions:  [0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 4, 0, 5, 0, 5, 4, 0, 0, 5, 0, 5, 5, 5, 4, 4, 4, 5, 5, 5, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0648, Accuracy: 0.1190, Precision: 0.2611, Recall: 0.1167, F1: 0.1140
Epoch 12/70
Train Loss: 0.2929, Accuracy: 0.9220, Precision: 0.8837, Recall: 0.8963, F1: 0.8883
Validation Loss: 0.6718, Accuracy: 0.8465, Precision: 0.8243, Recall: 0.7889, F1: 0.8032
Testing Loss: 0.5350, Accuracy: 0.8696, Precision: 0.8480, Recall: 0.8070, F1: 0.8228
LM Predictions:  [0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2975, Accuracy: 0.1190, Precision: 0.1957, Recall: 0.1500, F1: 0.0779
Epoch 13/70
Train Loss: 0.2909, Accuracy: 0.9189, Precision: 0.8766, Recall: 0.8918, F1: 0.8822
Validation Loss: 0.6396, Accuracy: 0.8529, Precision: 0.8188, Recall: 0.8260, F1: 0.8213
Testing Loss: 0.5152, Accuracy: 0.8696, Precision: 0.8229, Recall: 0.8268, F1: 0.8248
LM Predictions:  [0, 3, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 4, 0, 5, 5, 5, 5, 0, 0, 5, 0, 5, 5, 5, 4, 5, 4, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6603, Accuracy: 0.1905, Precision: 0.4066, Recall: 0.1917, F1: 0.2082
Epoch 14/70
Train Loss: 0.2665, Accuracy: 0.9277, Precision: 0.8907, Recall: 0.9025, F1: 0.8947
Validation Loss: 0.6840, Accuracy: 0.8486, Precision: 0.8173, Recall: 0.8173, F1: 0.8150
Testing Loss: 0.5410, Accuracy: 0.8720, Precision: 0.8371, Recall: 0.8360, F1: 0.8354
LM Predictions:  [0, 3, 3, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 3, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7824, Accuracy: 0.1429, Precision: 0.3542, Recall: 0.1542, F1: 0.1655
Epoch 15/70
Train Loss: 0.2524, Accuracy: 0.9329, Precision: 0.8936, Recall: 0.9094, F1: 0.8997
Validation Loss: 0.6910, Accuracy: 0.8550, Precision: 0.8202, Recall: 0.8267, F1: 0.8230
Testing Loss: 0.6104, Accuracy: 0.8684, Precision: 0.8314, Recall: 0.8225, F1: 0.8257
LM Predictions:  [0, 3, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 4, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6765, Accuracy: 0.1905, Precision: 0.3194, Recall: 0.2083, F1: 0.1797
Epoch 16/70
Train Loss: 0.2473, Accuracy: 0.9327, Precision: 0.8965, Recall: 0.9132, F1: 0.9021
Validation Loss: 0.6497, Accuracy: 0.8699, Precision: 0.8426, Recall: 0.8313, F1: 0.8361
Testing Loss: 0.5919, Accuracy: 0.8684, Precision: 0.8318, Recall: 0.8186, F1: 0.8243
LM Predictions:  [0, 1, 1, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 5, 4, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8681, Accuracy: 0.1905, Precision: 0.5513, Recall: 0.2060, F1: 0.1995
Epoch 17/70
Train Loss: 0.2651, Accuracy: 0.9286, Precision: 0.8888, Recall: 0.9071, F1: 0.8948
Validation Loss: 0.7183, Accuracy: 0.8657, Precision: 0.8410, Recall: 0.8365, F1: 0.8357
Testing Loss: 0.6088, Accuracy: 0.8708, Precision: 0.8312, Recall: 0.8225, F1: 0.8266
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 1, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 2, 5, 5, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9819, Accuracy: 0.2143, Precision: 0.5111, Recall: 0.2269, F1: 0.2379
Epoch 18/70
Train Loss: 0.2468, Accuracy: 0.9376, Precision: 0.8990, Recall: 0.9231, F1: 0.9074
Validation Loss: 0.6623, Accuracy: 0.8507, Precision: 0.8297, Recall: 0.8049, F1: 0.8145
Testing Loss: 0.6192, Accuracy: 0.8623, Precision: 0.8434, Recall: 0.8130, F1: 0.8258
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 1, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5825, Accuracy: 0.2143, Precision: 0.6439, Recall: 0.2269, F1: 0.2409
Epoch 19/70
Train Loss: 0.2387, Accuracy: 0.9362, Precision: 0.9002, Recall: 0.9182, F1: 0.9064
Validation Loss: 0.6815, Accuracy: 0.8401, Precision: 0.8069, Recall: 0.8062, F1: 0.8011
Testing Loss: 0.5823, Accuracy: 0.8635, Precision: 0.8317, Recall: 0.8298, F1: 0.8296
LM Predictions:  [0, 1, 2, 5, 4, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 1, 5, 5, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6863, Accuracy: 0.1905, Precision: 0.5347, Recall: 0.1935, F1: 0.2298
Epoch 20/70
Train Loss: 0.2352, Accuracy: 0.9405, Precision: 0.9024, Recall: 0.9135, F1: 0.9067
Validation Loss: 0.6937, Accuracy: 0.8465, Precision: 0.8019, Recall: 0.8153, F1: 0.8056
Testing Loss: 0.5967, Accuracy: 0.8696, Precision: 0.8365, Recall: 0.8348, F1: 0.8348
LM Predictions:  [0, 1, 2, 5, 5, 5, 0, 5, 5, 5, 5, 0, 1, 5, 3, 0, 5, 1, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 4, 5, 5, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7560, Accuracy: 0.1905, Precision: 0.5159, Recall: 0.1935, F1: 0.2337
Epoch 21/70
Train Loss: 0.2689, Accuracy: 0.9272, Precision: 0.8865, Recall: 0.9104, F1: 0.8951
Validation Loss: 0.6504, Accuracy: 0.8571, Precision: 0.8162, Recall: 0.8251, F1: 0.8177
Testing Loss: 0.5996, Accuracy: 0.8611, Precision: 0.8227, Recall: 0.8158, F1: 0.8174
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 2, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 4, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7750, Accuracy: 0.2143, Precision: 0.5884, Recall: 0.2269, F1: 0.2354
Epoch 22/70
Train Loss: 0.2503, Accuracy: 0.9317, Precision: 0.8900, Recall: 0.9126, F1: 0.8984
Validation Loss: 0.7277, Accuracy: 0.8401, Precision: 0.8036, Recall: 0.7991, F1: 0.7992
Testing Loss: 0.6294, Accuracy: 0.8696, Precision: 0.8337, Recall: 0.8289, F1: 0.8303
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 1, 5, 3, 0, 5, 2, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 4, 5, 0, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6010, Accuracy: 0.2143, Precision: 0.4722, Recall: 0.2269, F1: 0.2253
Epoch 23/70
Train Loss: 0.2346, Accuracy: 0.9367, Precision: 0.8993, Recall: 0.9254, F1: 0.9089
Validation Loss: 0.6487, Accuracy: 0.8465, Precision: 0.8118, Recall: 0.8112, F1: 0.8086
Testing Loss: 0.5427, Accuracy: 0.8659, Precision: 0.8332, Recall: 0.8250, F1: 0.8274
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 5, 4, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6794, Accuracy: 0.2381, Precision: 0.6389, Recall: 0.2477, F1: 0.2608
Epoch 24/70
Train Loss: 0.2238, Accuracy: 0.9414, Precision: 0.9076, Recall: 0.9265, F1: 0.9148
Validation Loss: 0.7307, Accuracy: 0.8380, Precision: 0.8022, Recall: 0.8054, F1: 0.7967
Testing Loss: 0.5795, Accuracy: 0.8756, Precision: 0.8401, Recall: 0.8442, F1: 0.8386
LM Predictions:  [5, 1, 2, 5, 5, 5, 0, 5, 5, 5, 5, 0, 1, 5, 3, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 4, 5, 4, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9090, Accuracy: 0.1667, Precision: 0.5556, Recall: 0.1602, F1: 0.2157
Epoch 25/70
Train Loss: 0.2257, Accuracy: 0.9405, Precision: 0.9048, Recall: 0.9264, F1: 0.9122
Validation Loss: 0.6907, Accuracy: 0.8358, Precision: 0.7959, Recall: 0.7972, F1: 0.7903
Testing Loss: 0.5517, Accuracy: 0.8659, Precision: 0.8214, Recall: 0.8233, F1: 0.8219
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 5, 5, 5, 0, 1, 5, 3, 5, 5, 3, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 5, 2, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4745, Accuracy: 0.2143, Precision: 0.5051, Recall: 0.2269, F1: 0.2323
Epoch 26/70
Train Loss: 0.2096, Accuracy: 0.9388, Precision: 0.9043, Recall: 0.9155, F1: 0.9088
Validation Loss: 0.7298, Accuracy: 0.8401, Precision: 0.8039, Recall: 0.7850, F1: 0.7919
Testing Loss: 0.5968, Accuracy: 0.8696, Precision: 0.8418, Recall: 0.8110, F1: 0.8236
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 0, 5, 0, 0, 3, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 1, 5, 0, 0, 3, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5941, Accuracy: 0.2143, Precision: 0.5056, Recall: 0.2269, F1: 0.2062
Epoch 27/70
Train Loss: 0.2383, Accuracy: 0.9362, Precision: 0.9018, Recall: 0.9204, F1: 0.9090
Validation Loss: 0.8296, Accuracy: 0.8422, Precision: 0.8223, Recall: 0.7778, F1: 0.7956
Testing Loss: 0.6900, Accuracy: 0.8575, Precision: 0.8303, Recall: 0.7817, F1: 0.8000
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 2, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1862, Accuracy: 0.2381, Precision: 0.6567, Recall: 0.2477, F1: 0.2286
Epoch 28/70
Train Loss: 0.2008, Accuracy: 0.9431, Precision: 0.9088, Recall: 0.9272, F1: 0.9153
Validation Loss: 0.7647, Accuracy: 0.8401, Precision: 0.8036, Recall: 0.7974, F1: 0.7986
Testing Loss: 0.6595, Accuracy: 0.8684, Precision: 0.8339, Recall: 0.8203, F1: 0.8258
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 5, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 2, 5, 0, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5348, Accuracy: 0.2381, Precision: 0.5809, Recall: 0.2477, F1: 0.2354
Epoch 29/70
Train Loss: 0.1996, Accuracy: 0.9436, Precision: 0.9083, Recall: 0.9258, F1: 0.9150
Validation Loss: 0.7406, Accuracy: 0.8443, Precision: 0.8124, Recall: 0.7921, F1: 0.7995
Testing Loss: 0.6601, Accuracy: 0.8647, Precision: 0.8374, Recall: 0.8100, F1: 0.8223
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 2, 0, 5, 0, 5, 2, 0, 0, 5, 5, 5, 5, 0, 4, 0, 2, 5, 0, 5, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7990, Accuracy: 0.2619, Precision: 0.6369, Recall: 0.2685, F1: 0.2463
Epoch 30/70
Train Loss: 0.1988, Accuracy: 0.9443, Precision: 0.9119, Recall: 0.9235, F1: 0.9157
Validation Loss: 0.7560, Accuracy: 0.8465, Precision: 0.8197, Recall: 0.7829, F1: 0.7981
Testing Loss: 0.6888, Accuracy: 0.8575, Precision: 0.8371, Recall: 0.7802, F1: 0.8015
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 3, 1, 0, 5, 0, 0, 1, 0, 0, 5, 0, 5, 5, 0, 4, 0, 1, 5, 0, 0, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3125, Accuracy: 0.2381, Precision: 0.4886, Recall: 0.2477, F1: 0.2240
Epoch 31/70
Train Loss: 0.1964, Accuracy: 0.9462, Precision: 0.9140, Recall: 0.9296, F1: 0.9198
Validation Loss: 0.7478, Accuracy: 0.8337, Precision: 0.8091, Recall: 0.7762, F1: 0.7890
Testing Loss: 0.6355, Accuracy: 0.8527, Precision: 0.8136, Recall: 0.7802, F1: 0.7931
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 1, 0, 3, 0, 3, 3, 0, 5, 0, 0, 3, 0, 0, 5, 0, 5, 3, 0, 4, 0, 1, 5, 0, 0, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5695, Accuracy: 0.2381, Precision: 0.4762, Recall: 0.2435, F1: 0.2177
Epoch 32/70
Train Loss: 0.2388, Accuracy: 0.9338, Precision: 0.8993, Recall: 0.9129, F1: 0.9032
Validation Loss: 0.6802, Accuracy: 0.8529, Precision: 0.8242, Recall: 0.8176, F1: 0.8199
Testing Loss: 0.5857, Accuracy: 0.8659, Precision: 0.8278, Recall: 0.8212, F1: 0.8238
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 5, 5, 0, 0, 1, 5, 3, 0, 1, 5, 0, 5, 0, 5, 1, 0, 0, 5, 5, 5, 5, 0, 4, 0, 1, 5, 0, 5, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2647, Accuracy: 0.2143, Precision: 0.5704, Recall: 0.2269, F1: 0.2091
Epoch 33/70
Train Loss: 0.2148, Accuracy: 0.9407, Precision: 0.9043, Recall: 0.9187, F1: 0.9095
Validation Loss: 0.7151, Accuracy: 0.8507, Precision: 0.8270, Recall: 0.8106, F1: 0.8176
Testing Loss: 0.5710, Accuracy: 0.8659, Precision: 0.8235, Recall: 0.8027, F1: 0.8119
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 1, 1, 0, 5, 0, 0, 1, 0, 0, 5, 5, 5, 1, 0, 4, 0, 2, 5, 0, 0, 0, 5, 5, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3640, Accuracy: 0.2381, Precision: 0.5179, Recall: 0.2477, F1: 0.2174
Epoch 34/70
Train Loss: 0.2087, Accuracy: 0.9438, Precision: 0.9132, Recall: 0.9249, F1: 0.9175
Validation Loss: 0.7331, Accuracy: 0.8422, Precision: 0.8004, Recall: 0.8001, F1: 0.7971
Testing Loss: 0.6194, Accuracy: 0.8684, Precision: 0.8349, Recall: 0.8275, F1: 0.8308
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 3, 1, 5, 5, 5, 5, 2, 5, 0, 5, 5, 5, 1, 0, 4, 0, 2, 5, 0, 5, 0, 5, 5, 1, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3475, Accuracy: 0.2381, Precision: 0.3412, Recall: 0.2477, F1: 0.2393
Epoch 35/70
Train Loss: 0.2895, Accuracy: 0.9172, Precision: 0.8924, Recall: 0.9026, F1: 0.8956
Validation Loss: 0.7153, Accuracy: 0.8380, Precision: 0.8088, Recall: 0.7635, F1: 0.7819
Testing Loss: 0.6434, Accuracy: 0.8466, Precision: 0.8201, Recall: 0.7632, F1: 0.7839
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 3, 2, 0, 5, 0, 0, 2, 0, 0, 5, 0, 0, 5, 0, 4, 0, 2, 0, 0, 0, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1490, Accuracy: 0.2381, Precision: 0.5267, Recall: 0.2477, F1: 0.2073
Epoch 36/70
Train Loss: 0.3126, Accuracy: 0.9149, Precision: 0.8892, Recall: 0.8961, F1: 0.8906
Validation Loss: 0.7323, Accuracy: 0.8422, Precision: 0.8070, Recall: 0.8080, F1: 0.8030
Testing Loss: 0.6440, Accuracy: 0.8659, Precision: 0.8346, Recall: 0.8337, F1: 0.8313
LM Predictions:  [0, 1, 2, 0, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4643, Accuracy: 0.2381, Precision: 0.6162, Recall: 0.2477, F1: 0.2635
Epoch 37/70
Train Loss: 0.1999, Accuracy: 0.9410, Precision: 0.9056, Recall: 0.9203, F1: 0.9114
Validation Loss: 0.7591, Accuracy: 0.8358, Precision: 0.8000, Recall: 0.7802, F1: 0.7866
Testing Loss: 0.6762, Accuracy: 0.8527, Precision: 0.8090, Recall: 0.7859, F1: 0.7960
LM Predictions:  [0, 0, 2, 0, 5, 0, 0, 5, 0, 5, 0, 0, 1, 5, 3, 0, 5, 5, 0, 5, 0, 5, 2, 0, 0, 5, 0, 5, 5, 0, 4, 0, 1, 5, 0, 0, 3, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4887, Accuracy: 0.1905, Precision: 0.3962, Recall: 0.2083, F1: 0.1742
Epoch 38/70
Train Loss: 0.1897, Accuracy: 0.9455, Precision: 0.9136, Recall: 0.9275, F1: 0.9192
Validation Loss: 0.8226, Accuracy: 0.8422, Precision: 0.8082, Recall: 0.8028, F1: 0.8002
Testing Loss: 0.6923, Accuracy: 0.8635, Precision: 0.8299, Recall: 0.8101, F1: 0.8183
LM Predictions:  [0, 0, 2, 0, 4, 0, 0, 5, 5, 5, 0, 0, 1, 5, 3, 0, 2, 2, 0, 5, 0, 5, 2, 0, 0, 5, 5, 5, 2, 0, 4, 5, 1, 5, 0, 5, 0, 5, 5, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5716, Accuracy: 0.2381, Precision: 0.4028, Recall: 0.2500, F1: 0.2168
Epoch 39/70
Train Loss: 0.1841, Accuracy: 0.9493, Precision: 0.9226, Recall: 0.9304, F1: 0.9258
Validation Loss: 0.7703, Accuracy: 0.8422, Precision: 0.8117, Recall: 0.7956, F1: 0.7984
Testing Loss: 0.6573, Accuracy: 0.8599, Precision: 0.8306, Recall: 0.7979, F1: 0.8118
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 5, 5, 0, 0, 1, 5, 3, 0, 1, 5, 0, 5, 0, 5, 2, 0, 0, 5, 5, 5, 2, 0, 4, 5, 1, 5, 0, 5, 0, 5, 0, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2156, Accuracy: 0.2143, Precision: 0.4954, Recall: 0.2292, F1: 0.2019
Epoch 40/70
Train Loss: 0.1747, Accuracy: 0.9495, Precision: 0.9203, Recall: 0.9342, F1: 0.9256
Validation Loss: 0.7662, Accuracy: 0.8380, Precision: 0.8059, Recall: 0.7937, F1: 0.7961
Testing Loss: 0.6506, Accuracy: 0.8611, Precision: 0.8347, Recall: 0.8035, F1: 0.8163
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 5, 0, 5, 0, 5, 2, 0, 0, 5, 0, 5, 2, 0, 4, 0, 1, 5, 0, 5, 0, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1809, Accuracy: 0.2619, Precision: 0.5073, Recall: 0.2708, F1: 0.2352
Epoch 41/70
Train Loss: 0.1698, Accuracy: 0.9500, Precision: 0.9228, Recall: 0.9297, F1: 0.9254
Validation Loss: 0.8943, Accuracy: 0.8507, Precision: 0.8174, Recall: 0.7967, F1: 0.8048
Testing Loss: 0.7494, Accuracy: 0.8551, Precision: 0.8215, Recall: 0.7832, F1: 0.7988
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 1, 0, 4, 0, 1, 5, 0, 0, 0, 5, 0, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3198, Accuracy: 0.2619, Precision: 0.4778, Recall: 0.2833, F1: 0.2114
Epoch 42/70
Train Loss: 0.1688, Accuracy: 0.9542, Precision: 0.9262, Recall: 0.9386, F1: 0.9307
Validation Loss: 0.8022, Accuracy: 0.8465, Precision: 0.8061, Recall: 0.7966, F1: 0.8000
Testing Loss: 0.7098, Accuracy: 0.8647, Precision: 0.8259, Recall: 0.8059, F1: 0.8143
LM Predictions:  [0, 0, 2, 0, 4, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 1, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 2, 0, 4, 0, 1, 5, 0, 0, 0, 0, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0195, Accuracy: 0.3095, Precision: 0.3998, Recall: 0.3250, F1: 0.2467
Epoch 43/70
Train Loss: 0.1671, Accuracy: 0.9519, Precision: 0.9271, Recall: 0.9318, F1: 0.9286
Validation Loss: 0.8107, Accuracy: 0.8422, Precision: 0.8003, Recall: 0.7696, F1: 0.7822
Testing Loss: 0.7062, Accuracy: 0.8490, Precision: 0.8101, Recall: 0.7692, F1: 0.7845
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 3, 1, 0, 5, 0, 0, 1, 0, 0, 5, 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0673, Accuracy: 0.2381, Precision: 0.4464, Recall: 0.2625, F1: 0.1995
Epoch 44/70
Train Loss: 0.1720, Accuracy: 0.9516, Precision: 0.9257, Recall: 0.9318, F1: 0.9277
Validation Loss: 0.8015, Accuracy: 0.8422, Precision: 0.8076, Recall: 0.7630, F1: 0.7771
Testing Loss: 0.6619, Accuracy: 0.8478, Precision: 0.8062, Recall: 0.7518, F1: 0.7649
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 1, 1, 0, 5, 0, 0, 2, 0, 0, 5, 0, 0, 1, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8270, Accuracy: 0.2143, Precision: 0.4732, Recall: 0.2417, F1: 0.1702
Epoch 45/70
Train Loss: 0.1742, Accuracy: 0.9528, Precision: 0.9326, Recall: 0.9285, F1: 0.9302
Validation Loss: 0.8050, Accuracy: 0.8486, Precision: 0.8161, Recall: 0.7791, F1: 0.7927
Testing Loss: 0.6851, Accuracy: 0.8514, Precision: 0.8250, Recall: 0.7676, F1: 0.7886
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 5, 0, 0, 2, 0, 0, 5, 0, 0, 2, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0653, Accuracy: 0.3095, Precision: 0.4881, Recall: 0.3250, F1: 0.2361
Epoch 46/70
Train Loss: 0.1643, Accuracy: 0.9504, Precision: 0.9209, Recall: 0.9321, F1: 0.9254
Validation Loss: 0.7755, Accuracy: 0.8507, Precision: 0.8102, Recall: 0.8033, F1: 0.8056
Testing Loss: 0.6952, Accuracy: 0.8575, Precision: 0.8185, Recall: 0.7921, F1: 0.8033
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 5, 3, 0, 2, 1, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 1, 0, 4, 0, 1, 0, 0, 5, 0, 5, 0, 1, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9972, Accuracy: 0.2619, Precision: 0.5347, Recall: 0.2833, F1: 0.2292
Epoch 47/70
Train Loss: 0.1602, Accuracy: 0.9557, Precision: 0.9316, Recall: 0.9391, F1: 0.9341
Validation Loss: 0.8125, Accuracy: 0.8529, Precision: 0.8083, Recall: 0.8082, F1: 0.8077
Testing Loss: 0.7674, Accuracy: 0.8611, Precision: 0.8194, Recall: 0.7989, F1: 0.8064
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 5, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 2, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2230, Accuracy: 0.3095, Precision: 0.5427, Recall: 0.3227, F1: 0.2579
Epoch 48/70
Train Loss: 0.1663, Accuracy: 0.9504, Precision: 0.9201, Recall: 0.9330, F1: 0.9245
Validation Loss: 0.8273, Accuracy: 0.8529, Precision: 0.8130, Recall: 0.8083, F1: 0.8095
Testing Loss: 0.7297, Accuracy: 0.8647, Precision: 0.8212, Recall: 0.8074, F1: 0.8133
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 0, 4, 0, 0, 2, 4, 3, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 5, 0, 5, 1, 0, 4, 0, 2, 5, 0, 5, 0, 0, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9647, Accuracy: 0.3095, Precision: 0.4426, Recall: 0.3060, F1: 0.2564
Epoch 49/70
Train Loss: 0.1590, Accuracy: 0.9490, Precision: 0.9175, Recall: 0.9239, F1: 0.9200
Validation Loss: 0.8075, Accuracy: 0.8529, Precision: 0.8094, Recall: 0.8026, F1: 0.8037
Testing Loss: 0.7035, Accuracy: 0.8587, Precision: 0.8194, Recall: 0.7979, F1: 0.8072
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 1, 2, 0, 5, 0, 0, 3, 0, 0, 5, 5, 5, 1, 0, 4, 0, 1, 0, 0, 5, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8440, Accuracy: 0.2619, Precision: 0.5351, Recall: 0.2644, F1: 0.2504
Epoch 50/70
Train Loss: 0.1672, Accuracy: 0.9502, Precision: 0.9204, Recall: 0.9290, F1: 0.9237
Validation Loss: 0.7426, Accuracy: 0.8465, Precision: 0.8193, Recall: 0.7698, F1: 0.7893
Testing Loss: 0.6517, Accuracy: 0.8539, Precision: 0.8338, Recall: 0.7730, F1: 0.7959
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2374, Accuracy: 0.2381, Precision: 0.4881, Recall: 0.2625, F1: 0.1944
Epoch 51/70
Train Loss: 0.1578, Accuracy: 0.9547, Precision: 0.9411, Recall: 0.9215, F1: 0.9307
Validation Loss: 0.7828, Accuracy: 0.8529, Precision: 0.8186, Recall: 0.8055, F1: 0.8092
Testing Loss: 0.7207, Accuracy: 0.8587, Precision: 0.8237, Recall: 0.7981, F1: 0.8094
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 2, 0, 5, 0, 5, 2, 0, 0, 5, 0, 5, 2, 0, 4, 0, 2, 5, 0, 5, 5, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9844, Accuracy: 0.2857, Precision: 0.5600, Recall: 0.2894, F1: 0.2562
Epoch 52/70
Train Loss: 0.1506, Accuracy: 0.9568, Precision: 0.9325, Recall: 0.9351, F1: 0.9333
Validation Loss: 0.8046, Accuracy: 0.8507, Precision: 0.8116, Recall: 0.8085, F1: 0.8064
Testing Loss: 0.7065, Accuracy: 0.8563, Precision: 0.8123, Recall: 0.7985, F1: 0.8046
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 5, 0, 0, 0, 2, 0, 3, 5, 2, 2, 0, 5, 0, 5, 2, 0, 0, 5, 5, 5, 2, 0, 4, 5, 1, 5, 0, 5, 5, 5, 5, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5077, Accuracy: 0.3333, Precision: 0.5139, Recall: 0.3435, F1: 0.2967
Epoch 53/70
Train Loss: 0.1500, Accuracy: 0.9523, Precision: 0.9281, Recall: 0.9313, F1: 0.9288
Validation Loss: 0.7841, Accuracy: 0.8507, Precision: 0.8308, Recall: 0.7837, F1: 0.8026
Testing Loss: 0.7447, Accuracy: 0.8551, Precision: 0.8226, Recall: 0.7713, F1: 0.7895
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0, 5, 0, 0, 2, 0, 4, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5100, Accuracy: 0.3333, Precision: 0.6298, Recall: 0.3435, F1: 0.2556
Epoch 54/70
Train Loss: 0.1598, Accuracy: 0.9469, Precision: 0.9225, Recall: 0.9173, F1: 0.9195
Validation Loss: 0.8423, Accuracy: 0.8230, Precision: 0.7762, Recall: 0.7838, F1: 0.7734
Testing Loss: 0.7484, Accuracy: 0.8502, Precision: 0.8042, Recall: 0.7944, F1: 0.7985
LM Predictions:  [0, 0, 2, 3, 0, 3, 0, 5, 5, 5, 0, 0, 2, 5, 3, 3, 3, 3, 3, 5, 3, 5, 3, 3, 0, 3, 3, 5, 3, 0, 4, 3, 3, 5, 3, 5, 3, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0448, Accuracy: 0.3571, Precision: 0.4686, Recall: 0.3292, F1: 0.2965
Epoch 55/70
Train Loss: 0.1771, Accuracy: 0.9471, Precision: 0.9227, Recall: 0.9202, F1: 0.9214
Validation Loss: 0.7728, Accuracy: 0.8443, Precision: 0.8106, Recall: 0.7878, F1: 0.7957
Testing Loss: 0.7114, Accuracy: 0.8611, Precision: 0.8351, Recall: 0.7974, F1: 0.8136
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 5, 0, 0, 0, 2, 0, 3, 0, 3, 1, 0, 3, 0, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8777, Accuracy: 0.3810, Precision: 0.5833, Recall: 0.3727, F1: 0.3333
Epoch 56/70
Train Loss: 0.1502, Accuracy: 0.9549, Precision: 0.9372, Recall: 0.9273, F1: 0.9321
Validation Loss: 0.8503, Accuracy: 0.8443, Precision: 0.8136, Recall: 0.7878, F1: 0.7971
Testing Loss: 0.7423, Accuracy: 0.8587, Precision: 0.8265, Recall: 0.7900, F1: 0.8056
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 0, 3, 0, 0, 2, 5, 5, 2, 0, 4, 0, 3, 5, 0, 0, 0, 0, 0, 3, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4540, Accuracy: 0.4048, Precision: 0.6442, Recall: 0.3935, F1: 0.3433
Epoch 57/70
Train Loss: 0.1464, Accuracy: 0.9542, Precision: 0.9327, Recall: 0.9284, F1: 0.9304
Validation Loss: 0.8245, Accuracy: 0.8443, Precision: 0.8092, Recall: 0.7762, F1: 0.7890
Testing Loss: 0.7492, Accuracy: 0.8587, Precision: 0.8349, Recall: 0.7743, F1: 0.7965
LM Predictions:  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 2, 0, 5, 0, 0, 5, 0, 0, 2, 0, 0, 1, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1029, Accuracy: 0.2857, Precision: 0.4398, Recall: 0.3000, F1: 0.2332
Epoch 58/70
Train Loss: 0.1418, Accuracy: 0.9559, Precision: 0.9338, Recall: 0.9320, F1: 0.9326
Validation Loss: 0.8498, Accuracy: 0.8337, Precision: 0.7982, Recall: 0.7784, F1: 0.7830
Testing Loss: 0.7226, Accuracy: 0.8575, Precision: 0.8225, Recall: 0.8002, F1: 0.8099
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 5, 5, 0, 0, 0, 2, 0, 3, 5, 1, 1, 0, 3, 0, 5, 2, 0, 0, 1, 5, 5, 1, 0, 4, 5, 3, 5, 0, 5, 0, 5, 0, 3, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0551, Accuracy: 0.3095, Precision: 0.4157, Recall: 0.3144, F1: 0.2821
Epoch 59/70
Train Loss: 0.1384, Accuracy: 0.9547, Precision: 0.9336, Recall: 0.9322, F1: 0.9324
Validation Loss: 0.7890, Accuracy: 0.8401, Precision: 0.8045, Recall: 0.7691, F1: 0.7830
Testing Loss: 0.7297, Accuracy: 0.8563, Precision: 0.8236, Recall: 0.7753, F1: 0.7926
LM Predictions:  [0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6606, Accuracy: 0.3571, Precision: 0.4357, Recall: 0.4300, F1: 0.3297
Epoch 60/70
Train Loss: 0.1357, Accuracy: 0.9576, Precision: 0.9396, Recall: 0.9330, F1: 0.9361
Validation Loss: 0.8281, Accuracy: 0.8465, Precision: 0.8069, Recall: 0.7859, F1: 0.7947
Testing Loss: 0.7393, Accuracy: 0.8635, Precision: 0.8328, Recall: 0.7948, F1: 0.8085
LM Predictions:  [0, 1, 2, 0, 4, 5, 0, 0, 0, 4, 0, 0, 2, 4, 3, 0, 2, 2, 0, 0, 0, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6054, Accuracy: 0.3810, Precision: 0.5862, Recall: 0.3602, F1: 0.3357
Epoch 61/70
Train Loss: 0.1339, Accuracy: 0.9568, Precision: 0.9340, Recall: 0.9348, F1: 0.9339
Validation Loss: 0.8683, Accuracy: 0.8358, Precision: 0.8049, Recall: 0.7634, F1: 0.7792
Testing Loss: 0.7444, Accuracy: 0.8623, Precision: 0.8391, Recall: 0.7852, F1: 0.8042
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 3, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4662, Accuracy: 0.3571, Precision: 0.6071, Recall: 0.4322, F1: 0.3511
Epoch 62/70
Train Loss: 0.1332, Accuracy: 0.9597, Precision: 0.9392, Recall: 0.9397, F1: 0.9392
Validation Loss: 0.8504, Accuracy: 0.8443, Precision: 0.8016, Recall: 0.7952, F1: 0.7971
Testing Loss: 0.7758, Accuracy: 0.8563, Precision: 0.8311, Recall: 0.7931, F1: 0.8090
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7728, Accuracy: 0.3810, Precision: 0.6617, Recall: 0.3810, F1: 0.3102
Epoch 63/70
Train Loss: 0.1409, Accuracy: 0.9554, Precision: 0.9371, Recall: 0.9314, F1: 0.9340
Validation Loss: 0.8934, Accuracy: 0.8337, Precision: 0.7975, Recall: 0.7587, F1: 0.7734
Testing Loss: 0.7533, Accuracy: 0.8539, Precision: 0.8326, Recall: 0.7700, F1: 0.7922
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 2, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3370, Accuracy: 0.3333, Precision: 0.6954, Recall: 0.3394, F1: 0.2964
Epoch 64/70
Train Loss: 0.1432, Accuracy: 0.9559, Precision: 0.9375, Recall: 0.9290, F1: 0.9330
Validation Loss: 1.0283, Accuracy: 0.8316, Precision: 0.7901, Recall: 0.7422, F1: 0.7561
Testing Loss: 0.8742, Accuracy: 0.8527, Precision: 0.8231, Recall: 0.7657, F1: 0.7821
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 3, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.7141, Accuracy: 0.3571, Precision: 0.6119, Recall: 0.4272, F1: 0.3585
Epoch 65/70
Train Loss: 0.1502, Accuracy: 0.9559, Precision: 0.9408, Recall: 0.9249, F1: 0.9323
Validation Loss: 0.7912, Accuracy: 0.8443, Precision: 0.7949, Recall: 0.7921, F1: 0.7928
Testing Loss: 0.6805, Accuracy: 0.8490, Precision: 0.8087, Recall: 0.7890, F1: 0.7965
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 4, 0, 4, 0, 0, 2, 4, 3, 0, 2, 2, 0, 5, 0, 0, 2, 0, 0, 2, 5, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8266, Accuracy: 0.3810, Precision: 0.5720, Recall: 0.3602, F1: 0.3258
Epoch 66/70
Train Loss: 0.1301, Accuracy: 0.9561, Precision: 0.9339, Recall: 0.9424, F1: 0.9370
Validation Loss: 0.8032, Accuracy: 0.8337, Precision: 0.7844, Recall: 0.7534, F1: 0.7650
Testing Loss: 0.7467, Accuracy: 0.8575, Precision: 0.8265, Recall: 0.7828, F1: 0.7983
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 2, 4, 3, 0, 5, 2, 0, 5, 0, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5894, Accuracy: 0.3810, Precision: 0.5889, Recall: 0.3727, F1: 0.3325
Epoch 67/70
Train Loss: 0.1355, Accuracy: 0.9523, Precision: 0.9307, Recall: 0.9185, F1: 0.9242
Validation Loss: 0.9654, Accuracy: 0.8316, Precision: 0.7922, Recall: 0.7749, F1: 0.7809
Testing Loss: 0.8300, Accuracy: 0.8575, Precision: 0.8237, Recall: 0.7888, F1: 0.8021
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 0, 0, 5, 0, 0, 2, 4, 3, 0, 2, 1, 0, 5, 0, 0, 2, 0, 0, 2, 5, 5, 1, 0, 4, 0, 3, 0, 0, 5, 0, 0, 0, 1, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0320, Accuracy: 0.3333, Precision: 0.4623, Recall: 0.3227, F1: 0.2991
Epoch 68/70
Train Loss: 0.1342, Accuracy: 0.9564, Precision: 0.9341, Recall: 0.9388, F1: 0.9359
Validation Loss: 0.8628, Accuracy: 0.8422, Precision: 0.7899, Recall: 0.7741, F1: 0.7806
Testing Loss: 0.7814, Accuracy: 0.8599, Precision: 0.8344, Recall: 0.7918, F1: 0.8068
LM Predictions:  [0, 1, 2, 0, 4, 0, 0, 4, 0, 4, 0, 0, 2, 4, 3, 0, 2, 1, 0, 5, 0, 0, 5, 0, 0, 2, 5, 0, 1, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5465, Accuracy: 0.3810, Precision: 0.5025, Recall: 0.3602, F1: 0.3381
Epoch 69/70
Train Loss: 0.1322, Accuracy: 0.9547, Precision: 0.9276, Recall: 0.9359, F1: 0.9312
Validation Loss: 0.8831, Accuracy: 0.8337, Precision: 0.7685, Recall: 0.7734, F1: 0.7689
Testing Loss: 0.7866, Accuracy: 0.8394, Precision: 0.7935, Recall: 0.7707, F1: 0.7796
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 0, 3, 0, 0, 2, 5, 5, 2, 0, 4, 0, 3, 0, 0, 0, 0, 0, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4476, Accuracy: 0.4524, Precision: 0.6792, Recall: 0.4310, F1: 0.3761
Epoch 70/70
Train Loss: 0.1290, Accuracy: 0.9573, Precision: 0.9344, Recall: 0.9382, F1: 0.9359
Validation Loss: 0.8552, Accuracy: 0.8358, Precision: 0.7990, Recall: 0.7856, F1: 0.7887
Testing Loss: 0.7807, Accuracy: 0.8514, Precision: 0.8201, Recall: 0.7880, F1: 0.8017
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 5, 0, 0, 5, 0, 0, 2, 5, 5, 2, 0, 4, 0, 3, 0, 0, 5, 0, 5, 5, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1344, Accuracy: 0.3810, Precision: 0.6821, Recall: 0.3810, F1: 0.3245
Label Memorization Analysis: 
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 2, 0, 5, 0, 0, 5, 0, 0, 2, 5, 5, 2, 0, 4, 0, 3, 0, 0, 5, 0, 5, 5, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1344, Accuracy: 0.3810, Precision: 0.6821, Recall: 0.3810, F1: 0.3245

