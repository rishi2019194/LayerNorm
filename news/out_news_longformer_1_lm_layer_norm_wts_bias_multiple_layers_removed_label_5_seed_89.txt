Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 1: 1011
  Label 0: 1141
  Label 2: 966
  Label 4: 344
  Label 3: 495
  Label 5: 260
Label counts for Validation:
  Label 2: 107
  Label 0: 127
  Label 1: 113
  Label 5: 29
  Label 4: 38
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 1: 1025
  Label 0: 1150
  Label 2: 972
  Label 4: 351
  Label 3: 501
  Label 5: 218
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.0715, Accuracy: 0.6094, Precision: 0.5543, Recall: 0.4721, F1: 0.4695
Validation Loss: 0.6710, Accuracy: 0.7889, Precision: 0.7566, Recall: 0.6825, F1: 0.6891
Testing Loss: 0.6191, Accuracy: 0.8031, Precision: 0.7755, Recall: 0.6965, F1: 0.7012
LM Predictions:  [3, 5, 0, 3, 3, 3, 0, 3, 0, 0, 0, 0, 0, 5, 0, 3, 0, 3, 0, 0, 0, 0, 0, 5, 3, 3, 0, 0, 4, 5, 0, 4, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0500, Accuracy: 0.1429, Precision: 0.1252, Recall: 0.1257, F1: 0.0959
Epoch 2/70
Train Loss: 0.6437, Accuracy: 0.7961, Precision: 0.7072, Recall: 0.6866, F1: 0.6834
Validation Loss: 0.5976, Accuracy: 0.8273, Precision: 0.8271, Recall: 0.7386, F1: 0.7570
Testing Loss: 0.5061, Accuracy: 0.8502, Precision: 0.8432, Recall: 0.7531, F1: 0.7575
LM Predictions:  [0, 4, 0, 4, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 3, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 1, 5, 0, 5, 0, 0, 0, 3, 0, 0, 2, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2950, Accuracy: 0.1429, Precision: 0.1131, Recall: 0.1045, F1: 0.0659
Epoch 3/70
Train Loss: 0.5256, Accuracy: 0.8371, Precision: 0.7769, Recall: 0.7635, F1: 0.7687
Validation Loss: 0.5475, Accuracy: 0.8337, Precision: 0.8713, Recall: 0.7258, F1: 0.7343
Testing Loss: 0.4563, Accuracy: 0.8502, Precision: 0.8551, Recall: 0.7507, F1: 0.7602
LM Predictions:  [5, 5, 2, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 0, 5, 0, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1014, Accuracy: 0.1190, Precision: 0.0287, Recall: 0.0926, F1: 0.0439
Epoch 4/70
Train Loss: 0.4552, Accuracy: 0.8594, Precision: 0.8030, Recall: 0.7853, F1: 0.7922
Validation Loss: 0.4723, Accuracy: 0.8593, Precision: 0.8318, Recall: 0.8199, F1: 0.8249
Testing Loss: 0.4106, Accuracy: 0.8804, Precision: 0.8567, Recall: 0.8420, F1: 0.8485
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 0, 5, 5, 3, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0661, Accuracy: 0.0238, Precision: 0.0152, Recall: 0.0185, F1: 0.0167
Epoch 5/70
Train Loss: 0.4168, Accuracy: 0.8729, Precision: 0.8251, Recall: 0.8187, F1: 0.8217
Validation Loss: 0.5156, Accuracy: 0.8614, Precision: 0.8584, Recall: 0.7697, F1: 0.7883
Testing Loss: 0.4514, Accuracy: 0.8647, Precision: 0.8426, Recall: 0.7754, F1: 0.7866
LM Predictions:  [5, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1167, Accuracy: 0.1667, Precision: 0.0467, Recall: 0.1296, F1: 0.0686
Epoch 6/70
Train Loss: 0.3842, Accuracy: 0.8848, Precision: 0.8404, Recall: 0.8284, F1: 0.8340
Validation Loss: 0.4726, Accuracy: 0.8614, Precision: 0.8354, Recall: 0.8152, F1: 0.8236
Testing Loss: 0.4412, Accuracy: 0.8816, Precision: 0.8563, Recall: 0.8336, F1: 0.8422
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 3, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9938, Accuracy: 0.0714, Precision: 0.1944, Recall: 0.0648, F1: 0.0794
Epoch 7/70
Train Loss: 0.3655, Accuracy: 0.8895, Precision: 0.8404, Recall: 0.8342, F1: 0.8371
Validation Loss: 0.5281, Accuracy: 0.8486, Precision: 0.8315, Recall: 0.7723, F1: 0.7883
Testing Loss: 0.4714, Accuracy: 0.8720, Precision: 0.8596, Recall: 0.7962, F1: 0.8165
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 5, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1266, Accuracy: 0.0952, Precision: 0.0317, Recall: 0.0741, F1: 0.0444
Epoch 8/70
Train Loss: 0.3485, Accuracy: 0.8968, Precision: 0.8545, Recall: 0.8493, F1: 0.8516
Validation Loss: 0.4759, Accuracy: 0.8699, Precision: 0.8365, Recall: 0.8470, F1: 0.8410
Testing Loss: 0.4537, Accuracy: 0.8696, Precision: 0.8344, Recall: 0.8476, F1: 0.8384
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1195, Accuracy: 0.0238, Precision: 0.0417, Recall: 0.0185, F1: 0.0256
Epoch 9/70
Train Loss: 0.3277, Accuracy: 0.9014, Precision: 0.8590, Recall: 0.8554, F1: 0.8571
Validation Loss: 0.4302, Accuracy: 0.8785, Precision: 0.8592, Recall: 0.8067, F1: 0.8212
Testing Loss: 0.4039, Accuracy: 0.8780, Precision: 0.8467, Recall: 0.8113, F1: 0.8183
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 3, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 5, 5, 2, 2, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6164, Accuracy: 0.1667, Precision: 0.2255, Recall: 0.1389, F1: 0.1245
Epoch 10/70
Train Loss: 0.3058, Accuracy: 0.9080, Precision: 0.8676, Recall: 0.8652, F1: 0.8662
Validation Loss: 0.6123, Accuracy: 0.8529, Precision: 0.8398, Recall: 0.7924, F1: 0.8071
Testing Loss: 0.4707, Accuracy: 0.8804, Precision: 0.8619, Recall: 0.8163, F1: 0.8299
LM Predictions:  [5, 1, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 5, 5, 5, 2, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9864, Accuracy: 0.1429, Precision: 0.1296, Recall: 0.1045, F1: 0.0826
Epoch 11/70
Train Loss: 0.2922, Accuracy: 0.9115, Precision: 0.8717, Recall: 0.8746, F1: 0.8729
Validation Loss: 0.4420, Accuracy: 0.8657, Precision: 0.8317, Recall: 0.8433, F1: 0.8356
Testing Loss: 0.3943, Accuracy: 0.8841, Precision: 0.8481, Recall: 0.8718, F1: 0.8564
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8105, Accuracy: 0.0238, Precision: 0.1667, Recall: 0.0185, F1: 0.0333
Epoch 12/70
Train Loss: 0.2798, Accuracy: 0.9198, Precision: 0.8810, Recall: 0.8860, F1: 0.8831
Validation Loss: 0.5291, Accuracy: 0.8593, Precision: 0.8488, Recall: 0.7884, F1: 0.8041
Testing Loss: 0.4495, Accuracy: 0.8780, Precision: 0.8600, Recall: 0.8091, F1: 0.8249
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 5, 5, 5, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0862, Accuracy: 0.1667, Precision: 0.2222, Recall: 0.1230, F1: 0.0963
Epoch 13/70
Train Loss: 0.2668, Accuracy: 0.9201, Precision: 0.8819, Recall: 0.8772, F1: 0.8794
Validation Loss: 0.5826, Accuracy: 0.8486, Precision: 0.8284, Recall: 0.7910, F1: 0.8045
Testing Loss: 0.4734, Accuracy: 0.8684, Precision: 0.8556, Recall: 0.8116, F1: 0.8281
LM Predictions:  [5, 1, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6723, Accuracy: 0.1667, Precision: 0.1458, Recall: 0.1230, F1: 0.1008
Epoch 14/70
Train Loss: 0.2558, Accuracy: 0.9220, Precision: 0.8813, Recall: 0.8899, F1: 0.8850
Validation Loss: 0.6185, Accuracy: 0.8358, Precision: 0.8510, Recall: 0.7596, F1: 0.7860
Testing Loss: 0.6109, Accuracy: 0.8357, Precision: 0.8602, Recall: 0.7528, F1: 0.7794
LM Predictions:  [5, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8483, Accuracy: 0.1667, Precision: 0.0486, Recall: 0.1296, F1: 0.0707
Epoch 15/70
Train Loss: 0.2483, Accuracy: 0.9255, Precision: 0.8884, Recall: 0.8916, F1: 0.8893
Validation Loss: 0.5983, Accuracy: 0.8550, Precision: 0.8360, Recall: 0.7832, F1: 0.7967
Testing Loss: 0.5230, Accuracy: 0.8744, Precision: 0.8492, Recall: 0.7947, F1: 0.8054
LM Predictions:  [5, 1, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 1, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 4, 0, 0, 0, 5, 5, 5, 5, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3509, Accuracy: 0.1667, Precision: 0.2222, Recall: 0.1349, F1: 0.1157
Epoch 16/70
Train Loss: 0.2414, Accuracy: 0.9277, Precision: 0.8916, Recall: 0.8856, F1: 0.8884
Validation Loss: 0.6168, Accuracy: 0.8571, Precision: 0.8333, Recall: 0.7944, F1: 0.8092
Testing Loss: 0.4770, Accuracy: 0.8804, Precision: 0.8675, Recall: 0.8244, F1: 0.8415
LM Predictions:  [5, 2, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5527, Accuracy: 0.1667, Precision: 0.2255, Recall: 0.1389, F1: 0.1245
Epoch 17/70
Train Loss: 0.2283, Accuracy: 0.9289, Precision: 0.8898, Recall: 0.8931, F1: 0.8910
Validation Loss: 0.5961, Accuracy: 0.8614, Precision: 0.8238, Recall: 0.8110, F1: 0.8168
Testing Loss: 0.4991, Accuracy: 0.8792, Precision: 0.8550, Recall: 0.8496, F1: 0.8518
LM Predictions:  [5, 2, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2, 0, 5, 0, 5, 0, 0, 5, 5, 5, 1, 5, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4691, Accuracy: 0.1429, Precision: 0.2500, Recall: 0.1071, F1: 0.1364
Epoch 18/70
Train Loss: 0.2299, Accuracy: 0.9286, Precision: 0.8893, Recall: 0.8880, F1: 0.8886
Validation Loss: 0.5396, Accuracy: 0.8678, Precision: 0.8578, Recall: 0.8139, F1: 0.8282
Testing Loss: 0.4541, Accuracy: 0.8732, Precision: 0.8454, Recall: 0.8135, F1: 0.8237
LM Predictions:  [5, 2, 5, 5, 5, 5, 4, 5, 5, 0, 0, 5, 1, 5, 4, 0, 4, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5, 1, 0, 5, 0, 4, 0, 0, 5, 5, 1, 1, 1, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9966, Accuracy: 0.3571, Precision: 0.4944, Recall: 0.2817, F1: 0.3206
Epoch 19/70
Train Loss: 0.2028, Accuracy: 0.9367, Precision: 0.8977, Recall: 0.9029, F1: 0.8999
Validation Loss: 0.6423, Accuracy: 0.8571, Precision: 0.8294, Recall: 0.7718, F1: 0.7848
Testing Loss: 0.4977, Accuracy: 0.8768, Precision: 0.8611, Recall: 0.8038, F1: 0.8200
LM Predictions:  [5, 2, 5, 5, 5, 2, 5, 5, 5, 0, 0, 0, 1, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 1, 0, 5, 0, 5, 0, 0, 0, 5, 5, 1, 5, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6963, Accuracy: 0.2619, Precision: 0.2770, Recall: 0.1931, F1: 0.1870
Epoch 20/70
Train Loss: 0.2110, Accuracy: 0.9338, Precision: 0.8984, Recall: 0.8923, F1: 0.8952
Validation Loss: 0.5912, Accuracy: 0.8593, Precision: 0.8299, Recall: 0.8235, F1: 0.8264
Testing Loss: 0.4879, Accuracy: 0.8756, Precision: 0.8501, Recall: 0.8497, F1: 0.8475
LM Predictions:  [5, 2, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7049, Accuracy: 0.1190, Precision: 0.1944, Recall: 0.1019, F1: 0.1306
Epoch 21/70
Train Loss: 0.2068, Accuracy: 0.9369, Precision: 0.9004, Recall: 0.9177, F1: 0.9072
Validation Loss: 0.6005, Accuracy: 0.8678, Precision: 0.8382, Recall: 0.8313, F1: 0.8334
Testing Loss: 0.5175, Accuracy: 0.8804, Precision: 0.8598, Recall: 0.8615, F1: 0.8572
LM Predictions:  [5, 2, 5, 5, 5, 2, 5, 5, 5, 0, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 5, 4, 5, 0, 5, 5, 1, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2098, Accuracy: 0.1905, Precision: 0.4944, Recall: 0.1362, F1: 0.2091
Epoch 22/70
Train Loss: 0.1965, Accuracy: 0.9343, Precision: 0.8965, Recall: 0.9004, F1: 0.8982
Validation Loss: 0.5975, Accuracy: 0.8550, Precision: 0.8220, Recall: 0.8034, F1: 0.8119
Testing Loss: 0.4879, Accuracy: 0.8732, Precision: 0.8470, Recall: 0.8362, F1: 0.8411
LM Predictions:  [1, 2, 5, 5, 5, 1, 5, 5, 5, 0, 0, 5, 1, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 0, 5, 0, 5, 0, 0, 5, 3, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1724, Accuracy: 0.3095, Precision: 0.3829, Recall: 0.2169, F1: 0.2381
Epoch 23/70
Train Loss: 0.1878, Accuracy: 0.9412, Precision: 0.9066, Recall: 0.9074, F1: 0.9068
Validation Loss: 0.6865, Accuracy: 0.8614, Precision: 0.8297, Recall: 0.7998, F1: 0.8116
Testing Loss: 0.5603, Accuracy: 0.8659, Precision: 0.8445, Recall: 0.7998, F1: 0.8144
LM Predictions:  [1, 2, 0, 5, 0, 1, 5, 5, 5, 0, 0, 0, 1, 5, 0, 0, 5, 3, 1, 5, 0, 5, 0, 5, 0, 0, 5, 1, 0, 0, 0, 5, 0, 0, 5, 5, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8763, Accuracy: 0.3810, Precision: 0.5208, Recall: 0.2817, F1: 0.2821
Epoch 24/70
Train Loss: 0.1927, Accuracy: 0.9410, Precision: 0.9066, Recall: 0.9105, F1: 0.9079
Validation Loss: 0.7720, Accuracy: 0.8358, Precision: 0.8186, Recall: 0.7542, F1: 0.7709
Testing Loss: 0.5919, Accuracy: 0.8623, Precision: 0.8568, Recall: 0.7762, F1: 0.7988
LM Predictions:  [5, 2, 5, 5, 0, 1, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3550, Accuracy: 0.3095, Precision: 0.4167, Recall: 0.2302, F1: 0.2176
Epoch 25/70
Train Loss: 0.1745, Accuracy: 0.9419, Precision: 0.9056, Recall: 0.9105, F1: 0.9074
Validation Loss: 0.7599, Accuracy: 0.8443, Precision: 0.8385, Recall: 0.7528, F1: 0.7557
Testing Loss: 0.6079, Accuracy: 0.8671, Precision: 0.8429, Recall: 0.7671, F1: 0.7712
LM Predictions:  [1, 2, 1, 1, 0, 1, 0, 5, 1, 0, 0, 0, 1, 5, 0, 0, 0, 3, 0, 1, 0, 5, 0, 5, 0, 0, 5, 1, 0, 0, 0, 4, 0, 0, 0, 3, 1, 1, 0, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8030, Accuracy: 0.4762, Precision: 0.6061, Recall: 0.3413, F1: 0.3344
Epoch 26/70
Train Loss: 0.1746, Accuracy: 0.9438, Precision: 0.9125, Recall: 0.9151, F1: 0.9136
Validation Loss: 0.7129, Accuracy: 0.8657, Precision: 0.8312, Recall: 0.8068, F1: 0.8164
Testing Loss: 0.5700, Accuracy: 0.8780, Precision: 0.8569, Recall: 0.8308, F1: 0.8423
LM Predictions:  [1, 2, 1, 1, 0, 5, 5, 5, 1, 0, 3, 5, 1, 5, 5, 0, 5, 3, 1, 5, 5, 5, 0, 5, 0, 0, 5, 1, 0, 0, 0, 0, 0, 3, 5, 3, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7846, Accuracy: 0.4286, Precision: 0.4692, Recall: 0.3148, F1: 0.3327
Epoch 27/70
Train Loss: 0.1787, Accuracy: 0.9429, Precision: 0.9079, Recall: 0.9204, F1: 0.9127
Validation Loss: 0.6993, Accuracy: 0.8635, Precision: 0.8377, Recall: 0.8058, F1: 0.8189
Testing Loss: 0.7054, Accuracy: 0.8514, Precision: 0.8181, Recall: 0.7973, F1: 0.8055
LM Predictions:  [3, 2, 0, 0, 0, 1, 4, 5, 1, 0, 0, 5, 1, 5, 5, 0, 4, 0, 5, 1, 5, 5, 5, 5, 0, 0, 5, 2, 0, 4, 0, 4, 0, 5, 5, 0, 0, 2, 2, 1, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2017, Accuracy: 0.3571, Precision: 0.3583, Recall: 0.2884, F1: 0.2985
Epoch 28/70
Train Loss: 0.1736, Accuracy: 0.9474, Precision: 0.9171, Recall: 0.9161, F1: 0.9165
Validation Loss: 0.6866, Accuracy: 0.8699, Precision: 0.8425, Recall: 0.8057, F1: 0.8187
Testing Loss: 0.6004, Accuracy: 0.8768, Precision: 0.8598, Recall: 0.8112, F1: 0.8273
LM Predictions:  [1, 2, 1, 2, 0, 1, 5, 5, 5, 0, 0, 0, 1, 5, 5, 0, 5, 0, 1, 1, 5, 5, 0, 5, 0, 0, 5, 1, 0, 0, 0, 4, 0, 0, 5, 3, 1, 2, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9556, Accuracy: 0.3571, Precision: 0.3981, Recall: 0.2593, F1: 0.2578
Epoch 29/70
Train Loss: 0.1624, Accuracy: 0.9414, Precision: 0.9067, Recall: 0.9123, F1: 0.9091
Validation Loss: 0.7302, Accuracy: 0.8593, Precision: 0.8376, Recall: 0.7831, F1: 0.8005
Testing Loss: 0.6146, Accuracy: 0.8732, Precision: 0.8647, Recall: 0.7899, F1: 0.8099
LM Predictions:  [2, 2, 1, 2, 0, 1, 4, 2, 1, 0, 0, 0, 1, 5, 0, 0, 4, 0, 1, 2, 0, 2, 0, 5, 0, 0, 5, 1, 0, 4, 0, 4, 0, 0, 5, 3, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6549, Accuracy: 0.5714, Precision: 0.4956, Recall: 0.4563, F1: 0.4492
Epoch 30/70
Train Loss: 0.1757, Accuracy: 0.9455, Precision: 0.9161, Recall: 0.9205, F1: 0.9180
Validation Loss: 0.7061, Accuracy: 0.8593, Precision: 0.8260, Recall: 0.8047, F1: 0.8132
Testing Loss: 0.5959, Accuracy: 0.8804, Precision: 0.8428, Recall: 0.8328, F1: 0.8373
LM Predictions:  [5, 2, 5, 5, 0, 5, 4, 5, 1, 0, 0, 5, 1, 5, 4, 0, 4, 3, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 0, 4, 5, 4, 0, 0, 5, 3, 1, 5, 0, 1, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7100, Accuracy: 0.4286, Precision: 0.6472, Recall: 0.3585, F1: 0.4134
Epoch 31/70
Train Loss: 0.1561, Accuracy: 0.9500, Precision: 0.9177, Recall: 0.9285, F1: 0.9225
Validation Loss: 0.7184, Accuracy: 0.8614, Precision: 0.8269, Recall: 0.8067, F1: 0.8144
Testing Loss: 0.6336, Accuracy: 0.8768, Precision: 0.8396, Recall: 0.8224, F1: 0.8293
LM Predictions:  [1, 2, 1, 1, 0, 1, 5, 1, 3, 0, 3, 5, 1, 5, 4, 0, 5, 3, 1, 1, 5, 5, 5, 5, 0, 0, 1, 1, 0, 4, 5, 4, 0, 3, 5, 5, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.3680, Accuracy: 0.5476, Precision: 0.6802, Recall: 0.4101, F1: 0.4735
Epoch 32/70
Train Loss: 0.1470, Accuracy: 0.9514, Precision: 0.9199, Recall: 0.9276, F1: 0.9234
Validation Loss: 0.7460, Accuracy: 0.8465, Precision: 0.8150, Recall: 0.7846, F1: 0.7965
Testing Loss: 0.6177, Accuracy: 0.8659, Precision: 0.8382, Recall: 0.8050, F1: 0.8189
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 5, 1, 5, 0, 0, 4, 0, 1, 5, 5, 5, 5, 5, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 5, 5, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1825, Accuracy: 0.5476, Precision: 0.5568, Recall: 0.3902, F1: 0.4082
Epoch 33/70
Train Loss: 0.1454, Accuracy: 0.9523, Precision: 0.9210, Recall: 0.9342, F1: 0.9268
Validation Loss: 0.6970, Accuracy: 0.8635, Precision: 0.8458, Recall: 0.7949, F1: 0.8128
Testing Loss: 0.6145, Accuracy: 0.8671, Precision: 0.8286, Recall: 0.7837, F1: 0.7972
LM Predictions:  [1, 2, 1, 2, 0, 1, 4, 5, 1, 0, 0, 0, 1, 5, 0, 0, 4, 0, 0, 5, 0, 5, 5, 5, 0, 0, 5, 1, 0, 4, 0, 4, 0, 0, 5, 5, 1, 1, 0, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6005, Accuracy: 0.5000, Precision: 0.4734, Recall: 0.3730, F1: 0.3843
Epoch 34/70
Train Loss: 0.1372, Accuracy: 0.9535, Precision: 0.9246, Recall: 0.9285, F1: 0.9263
Validation Loss: 0.7432, Accuracy: 0.8550, Precision: 0.8092, Recall: 0.7695, F1: 0.7792
Testing Loss: 0.6264, Accuracy: 0.8720, Precision: 0.8434, Recall: 0.7925, F1: 0.8030
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 1, 1, 4, 0, 4, 0, 0, 1, 0, 1, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 5, 1, 1, 1, 0, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0730, Accuracy: 0.6667, Precision: 0.5228, Recall: 0.4802, F1: 0.4392
Epoch 35/70
Train Loss: 0.1440, Accuracy: 0.9507, Precision: 0.9220, Recall: 0.9255, F1: 0.9236
Validation Loss: 0.7164, Accuracy: 0.8614, Precision: 0.8399, Recall: 0.7948, F1: 0.8123
Testing Loss: 0.6563, Accuracy: 0.8671, Precision: 0.8429, Recall: 0.7967, F1: 0.8131
LM Predictions:  [1, 2, 1, 2, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 5, 1, 5, 5, 0, 0, 1, 1, 0, 0, 5, 0, 0, 0, 5, 1, 1, 1, 0, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4505, Accuracy: 0.4762, Precision: 0.2796, Recall: 0.3069, F1: 0.2669
Epoch 36/70
Train Loss: 0.1545, Accuracy: 0.9512, Precision: 0.9230, Recall: 0.9259, F1: 0.9243
Validation Loss: 0.8069, Accuracy: 0.8529, Precision: 0.8345, Recall: 0.7727, F1: 0.7887
Testing Loss: 0.7117, Accuracy: 0.8647, Precision: 0.8359, Recall: 0.7705, F1: 0.7845
LM Predictions:  [1, 2, 3, 2, 0, 1, 5, 1, 3, 0, 3, 0, 1, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 3, 1, 0, 0, 3, 0, 0, 3, 5, 1, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.3972, Accuracy: 0.4524, Precision: 0.3543, Recall: 0.3333, F1: 0.3144
Epoch 37/70
Train Loss: 0.1485, Accuracy: 0.9474, Precision: 0.9211, Recall: 0.9145, F1: 0.9178
Validation Loss: 0.7408, Accuracy: 0.8593, Precision: 0.8263, Recall: 0.8141, F1: 0.8191
Testing Loss: 0.6570, Accuracy: 0.8575, Precision: 0.8212, Recall: 0.8012, F1: 0.8095
LM Predictions:  [5, 2, 1, 1, 0, 1, 4, 5, 1, 0, 0, 5, 1, 5, 4, 0, 4, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 0, 4, 5, 4, 0, 5, 5, 5, 1, 1, 0, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5327, Accuracy: 0.5238, Precision: 0.6148, Recall: 0.3902, F1: 0.4428
Epoch 38/70
Train Loss: 0.1320, Accuracy: 0.9535, Precision: 0.9208, Recall: 0.9321, F1: 0.9260
Validation Loss: 0.8176, Accuracy: 0.8571, Precision: 0.8263, Recall: 0.7892, F1: 0.8045
Testing Loss: 0.7239, Accuracy: 0.8659, Precision: 0.8317, Recall: 0.7947, F1: 0.8099
LM Predictions:  [1, 2, 0, 1, 0, 1, 5, 1, 1, 0, 0, 5, 0, 1, 5, 0, 5, 0, 0, 5, 5, 1, 5, 5, 0, 0, 1, 1, 0, 5, 0, 5, 0, 3, 5, 1, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1920, Accuracy: 0.5000, Precision: 0.5632, Recall: 0.3347, F1: 0.3422
Epoch 39/70
Train Loss: 0.1288, Accuracy: 0.9549, Precision: 0.9275, Recall: 0.9328, F1: 0.9297
Validation Loss: 0.7820, Accuracy: 0.8593, Precision: 0.8304, Recall: 0.7935, F1: 0.8057
Testing Loss: 0.6959, Accuracy: 0.8696, Precision: 0.8338, Recall: 0.7861, F1: 0.7973
LM Predictions:  [2, 2, 0, 2, 0, 1, 4, 1, 0, 0, 0, 0, 0, 5, 4, 0, 4, 0, 0, 5, 0, 5, 5, 5, 0, 0, 0, 1, 0, 4, 3, 4, 0, 3, 5, 0, 1, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.2663, Accuracy: 0.5714, Precision: 0.6389, Recall: 0.4683, F1: 0.4981
Epoch 40/70
Train Loss: 0.1255, Accuracy: 0.9533, Precision: 0.9214, Recall: 0.9311, F1: 0.9258
Validation Loss: 0.7374, Accuracy: 0.8635, Precision: 0.8282, Recall: 0.8111, F1: 0.8173
Testing Loss: 0.6265, Accuracy: 0.8744, Precision: 0.8375, Recall: 0.8271, F1: 0.8315
LM Predictions:  [2, 2, 0, 5, 0, 1, 4, 1, 1, 0, 0, 0, 1, 5, 4, 0, 4, 0, 5, 2, 5, 5, 5, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 0, 5, 5, 1, 1, 0, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0631, Accuracy: 0.5952, Precision: 0.5529, Recall: 0.4696, F1: 0.4859
Epoch 41/70
Train Loss: 0.1167, Accuracy: 0.9576, Precision: 0.9319, Recall: 0.9294, F1: 0.9306
Validation Loss: 0.7590, Accuracy: 0.8550, Precision: 0.8247, Recall: 0.7900, F1: 0.8026
Testing Loss: 0.7109, Accuracy: 0.8635, Precision: 0.8224, Recall: 0.7995, F1: 0.8083
LM Predictions:  [1, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 4, 1, 1, 4, 0, 4, 3, 1, 2, 5, 1, 1, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7490, Accuracy: 0.7857, Precision: 0.7060, Recall: 0.6085, F1: 0.6225
Epoch 42/70
Train Loss: 0.1208, Accuracy: 0.9549, Precision: 0.9247, Recall: 0.9316, F1: 0.9279
Validation Loss: 0.7997, Accuracy: 0.8465, Precision: 0.8052, Recall: 0.7695, F1: 0.7784
Testing Loss: 0.7144, Accuracy: 0.8671, Precision: 0.8292, Recall: 0.7919, F1: 0.8012
LM Predictions:  [1, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 1, 1, 4, 0, 4, 0, 1, 5, 0, 1, 1, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8749, Accuracy: 0.7619, Precision: 0.7454, Recall: 0.5714, F1: 0.5759
Epoch 43/70
Train Loss: 0.1234, Accuracy: 0.9578, Precision: 0.9313, Recall: 0.9334, F1: 0.9322
Validation Loss: 0.7959, Accuracy: 0.8635, Precision: 0.8293, Recall: 0.8280, F1: 0.8285
Testing Loss: 0.6879, Accuracy: 0.8696, Precision: 0.8376, Recall: 0.8170, F1: 0.8262
LM Predictions:  [1, 2, 0, 5, 0, 1, 4, 1, 1, 0, 0, 0, 3, 3, 4, 0, 4, 3, 0, 2, 5, 5, 0, 5, 0, 0, 1, 1, 0, 4, 3, 4, 0, 0, 5, 5, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1640, Accuracy: 0.5952, Precision: 0.6537, Recall: 0.4735, F1: 0.5159
Epoch 44/70
Train Loss: 0.1587, Accuracy: 0.9478, Precision: 0.9210, Recall: 0.9203, F1: 0.9206
Validation Loss: 0.7699, Accuracy: 0.8486, Precision: 0.8156, Recall: 0.7673, F1: 0.7799
Testing Loss: 0.7111, Accuracy: 0.8696, Precision: 0.8455, Recall: 0.7808, F1: 0.7927
LM Predictions:  [1, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 0, 3, 4, 0, 4, 0, 0, 1, 0, 1, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0333, Accuracy: 0.6667, Precision: 0.6467, Recall: 0.5905, F1: 0.5422
Epoch 45/70
Train Loss: 0.1216, Accuracy: 0.9557, Precision: 0.9288, Recall: 0.9345, F1: 0.9315
Validation Loss: 0.7877, Accuracy: 0.8507, Precision: 0.8290, Recall: 0.7833, F1: 0.7982
Testing Loss: 0.6865, Accuracy: 0.8659, Precision: 0.8434, Recall: 0.7810, F1: 0.7955
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 1, 1, 5, 0, 5, 0, 0, 2, 0, 1, 0, 5, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9859, Accuracy: 0.6905, Precision: 0.5643, Recall: 0.4881, F1: 0.4816
Epoch 46/70
Train Loss: 0.1246, Accuracy: 0.9576, Precision: 0.9286, Recall: 0.9376, F1: 0.9328
Validation Loss: 0.7965, Accuracy: 0.8614, Precision: 0.8417, Recall: 0.8019, F1: 0.8149
Testing Loss: 0.7392, Accuracy: 0.8514, Precision: 0.8075, Recall: 0.7874, F1: 0.7920
LM Predictions:  [1, 2, 0, 5, 0, 1, 4, 1, 1, 0, 0, 4, 1, 5, 4, 0, 4, 3, 0, 1, 5, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0249, Accuracy: 0.6905, Precision: 0.7202, Recall: 0.5608, F1: 0.5924
Epoch 47/70
Train Loss: 0.1265, Accuracy: 0.9514, Precision: 0.9233, Recall: 0.9283, F1: 0.9257
Validation Loss: 0.7409, Accuracy: 0.8635, Precision: 0.8405, Recall: 0.7926, F1: 0.8093
Testing Loss: 0.6688, Accuracy: 0.8611, Precision: 0.8294, Recall: 0.7919, F1: 0.8070
LM Predictions:  [1, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 5, 1, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9875, Accuracy: 0.5476, Precision: 0.5218, Recall: 0.3664, F1: 0.3439
Epoch 48/70
Train Loss: 0.1166, Accuracy: 0.9580, Precision: 0.9317, Recall: 0.9408, F1: 0.9360
Validation Loss: 0.8021, Accuracy: 0.8550, Precision: 0.8285, Recall: 0.7718, F1: 0.7879
Testing Loss: 0.7087, Accuracy: 0.8659, Precision: 0.8403, Recall: 0.7864, F1: 0.8015
LM Predictions:  [1, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 3, 0, 2, 0, 2, 1, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7295, Accuracy: 0.7143, Precision: 0.8600, Recall: 0.6524, F1: 0.6614
Epoch 49/70
Train Loss: 0.1685, Accuracy: 0.9433, Precision: 0.9132, Recall: 0.9240, F1: 0.9184
Validation Loss: 0.7596, Accuracy: 0.8571, Precision: 0.8302, Recall: 0.7980, F1: 0.8112
Testing Loss: 0.6835, Accuracy: 0.8587, Precision: 0.8289, Recall: 0.8027, F1: 0.8145
LM Predictions:  [2, 2, 5, 5, 0, 2, 0, 2, 5, 0, 0, 0, 5, 5, 0, 0, 0, 3, 5, 2, 5, 2, 5, 4, 0, 0, 5, 1, 0, 0, 0, 4, 0, 0, 2, 5, 2, 1, 0, 1, 5, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1778, Accuracy: 0.4524, Precision: 0.6759, Recall: 0.3981, F1: 0.3983
Epoch 50/70
Train Loss: 0.1115, Accuracy: 0.9587, Precision: 0.9343, Recall: 0.9344, F1: 0.9343
Validation Loss: 0.7444, Accuracy: 0.8614, Precision: 0.8303, Recall: 0.8186, F1: 0.8232
Testing Loss: 0.7053, Accuracy: 0.8647, Precision: 0.8315, Recall: 0.8177, F1: 0.8239
LM Predictions:  [5, 2, 0, 2, 0, 1, 4, 1, 3, 0, 0, 4, 2, 2, 4, 0, 4, 3, 0, 2, 5, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 2, 1, 1, 2, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1055, Accuracy: 0.6429, Precision: 0.5796, Recall: 0.5370, F1: 0.5194
Epoch 51/70
Train Loss: 0.1166, Accuracy: 0.9564, Precision: 0.9299, Recall: 0.9333, F1: 0.9316
Validation Loss: 0.8426, Accuracy: 0.8571, Precision: 0.8179, Recall: 0.7880, F1: 0.7988
Testing Loss: 0.7084, Accuracy: 0.8659, Precision: 0.8292, Recall: 0.8133, F1: 0.8198
LM Predictions:  [5, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 4, 1, 1, 4, 0, 4, 3, 1, 2, 5, 5, 1, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5750, Accuracy: 0.7381, Precision: 0.6907, Recall: 0.5688, F1: 0.5907
Epoch 52/70
Train Loss: 0.1143, Accuracy: 0.9573, Precision: 0.9294, Recall: 0.9377, F1: 0.9333
Validation Loss: 0.7352, Accuracy: 0.8657, Precision: 0.8367, Recall: 0.8026, F1: 0.8145
Testing Loss: 0.6941, Accuracy: 0.8696, Precision: 0.8408, Recall: 0.8019, F1: 0.8146
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 4, 5, 1, 4, 0, 4, 3, 0, 2, 0, 5, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7364, Accuracy: 0.7857, Precision: 0.6833, Recall: 0.6151, F1: 0.6205
Epoch 53/70
Train Loss: 0.1088, Accuracy: 0.9637, Precision: 0.9404, Recall: 0.9446, F1: 0.9425
Validation Loss: 0.7202, Accuracy: 0.8699, Precision: 0.8373, Recall: 0.8213, F1: 0.8288
Testing Loss: 0.6826, Accuracy: 0.8635, Precision: 0.8310, Recall: 0.8229, F1: 0.8264
LM Predictions:  [5, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 4, 5, 1, 4, 0, 5, 3, 0, 2, 5, 5, 0, 4, 0, 0, 1, 1, 0, 4, 3, 5, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6942, Accuracy: 0.6905, Precision: 0.6581, Recall: 0.5212, F1: 0.5621
Epoch 54/70
Train Loss: 0.1113, Accuracy: 0.9566, Precision: 0.9289, Recall: 0.9342, F1: 0.9315
Validation Loss: 0.7923, Accuracy: 0.8614, Precision: 0.8313, Recall: 0.7923, F1: 0.8066
Testing Loss: 0.7763, Accuracy: 0.8539, Precision: 0.8294, Recall: 0.7806, F1: 0.7973
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 1, 1, 4, 0, 4, 3, 1, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5116, Accuracy: 0.8571, Precision: 0.9118, Recall: 0.8238, F1: 0.8427
Epoch 55/70
Train Loss: 0.1033, Accuracy: 0.9590, Precision: 0.9337, Recall: 0.9329, F1: 0.9332
Validation Loss: 0.8433, Accuracy: 0.8614, Precision: 0.8290, Recall: 0.7998, F1: 0.8116
Testing Loss: 0.7662, Accuracy: 0.8659, Precision: 0.8390, Recall: 0.7995, F1: 0.8144
LM Predictions:  [5, 2, 0, 1, 0, 1, 5, 1, 1, 0, 0, 5, 3, 1, 4, 0, 5, 0, 0, 2, 0, 2, 3, 5, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7044, Accuracy: 0.7143, Precision: 0.7604, Recall: 0.5476, F1: 0.5905
Epoch 56/70
Train Loss: 0.1094, Accuracy: 0.9597, Precision: 0.9329, Recall: 0.9432, F1: 0.9377
Validation Loss: 0.8137, Accuracy: 0.8550, Precision: 0.8302, Recall: 0.7773, F1: 0.7930
Testing Loss: 0.7970, Accuracy: 0.8514, Precision: 0.8303, Recall: 0.7718, F1: 0.7873
LM Predictions:  [5, 2, 1, 1, 0, 1, 4, 1, 1, 0, 3, 4, 1, 1, 0, 0, 4, 0, 1, 2, 0, 1, 1, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7615, Accuracy: 0.6905, Precision: 0.5537, Recall: 0.4960, F1: 0.4826
Epoch 57/70
Train Loss: 0.1201, Accuracy: 0.9587, Precision: 0.9365, Recall: 0.9317, F1: 0.9340
Validation Loss: 0.7470, Accuracy: 0.8571, Precision: 0.8160, Recall: 0.8001, F1: 0.8057
Testing Loss: 0.7362, Accuracy: 0.8635, Precision: 0.8291, Recall: 0.8170, F1: 0.8222
LM Predictions:  [5, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 4, 0, 1, 0, 0, 4, 3, 0, 5, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8199, Accuracy: 0.7857, Precision: 0.7183, Recall: 0.6230, F1: 0.6454
Epoch 58/70
Train Loss: 0.1095, Accuracy: 0.9578, Precision: 0.9339, Recall: 0.9346, F1: 0.9342
Validation Loss: 0.7021, Accuracy: 0.8571, Precision: 0.8235, Recall: 0.7832, F1: 0.7945
Testing Loss: 0.6499, Accuracy: 0.8659, Precision: 0.8404, Recall: 0.7909, F1: 0.8052
LM Predictions:  [2, 2, 0, 3, 0, 1, 4, 1, 1, 0, 3, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5568, Accuracy: 0.9048, Precision: 0.9133, Recall: 0.9238, F1: 0.9073
Epoch 59/70
Train Loss: 0.0980, Accuracy: 0.9644, Precision: 0.9399, Recall: 0.9427, F1: 0.9413
Validation Loss: 0.8326, Accuracy: 0.8593, Precision: 0.8300, Recall: 0.7914, F1: 0.8015
Testing Loss: 0.8196, Accuracy: 0.8611, Precision: 0.8277, Recall: 0.7793, F1: 0.7896
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 3, 0, 3, 1, 0, 0, 0, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6141, Accuracy: 0.8333, Precision: 0.8786, Recall: 0.8238, F1: 0.8144
Epoch 60/70
Train Loss: 0.1107, Accuracy: 0.9557, Precision: 0.9322, Recall: 0.9253, F1: 0.9286
Validation Loss: 0.6881, Accuracy: 0.8635, Precision: 0.8316, Recall: 0.8103, F1: 0.8182
Testing Loss: 0.6432, Accuracy: 0.8635, Precision: 0.8264, Recall: 0.8020, F1: 0.8124
LM Predictions:  [5, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 5, 1, 4, 0, 4, 0, 0, 1, 0, 1, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 5, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8907, Accuracy: 0.6667, Precision: 0.5549, Recall: 0.4802, F1: 0.4548
Epoch 61/70
Train Loss: 0.1056, Accuracy: 0.9635, Precision: 0.9425, Recall: 0.9460, F1: 0.9441
Validation Loss: 0.8094, Accuracy: 0.8657, Precision: 0.8337, Recall: 0.8097, F1: 0.8201
Testing Loss: 0.7225, Accuracy: 0.8623, Precision: 0.8345, Recall: 0.8105, F1: 0.8211
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 1, 1, 0, 0, 4, 3, 0, 2, 5, 2, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6200, Accuracy: 0.7143, Precision: 0.7213, Recall: 0.5529, F1: 0.5658
Epoch 62/70
Train Loss: 0.0964, Accuracy: 0.9625, Precision: 0.9363, Recall: 0.9417, F1: 0.9389
Validation Loss: 0.8510, Accuracy: 0.8678, Precision: 0.8477, Recall: 0.8027, F1: 0.8200
Testing Loss: 0.7914, Accuracy: 0.8502, Precision: 0.8177, Recall: 0.7750, F1: 0.7910
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6497, Accuracy: 0.6905, Precision: 0.8703, Recall: 0.6286, F1: 0.6256
Epoch 63/70
Train Loss: 0.0996, Accuracy: 0.9613, Precision: 0.9358, Recall: 0.9439, F1: 0.9395
Validation Loss: 0.7522, Accuracy: 0.8635, Precision: 0.8297, Recall: 0.8299, F1: 0.8294
Testing Loss: 0.6903, Accuracy: 0.8659, Precision: 0.8356, Recall: 0.8370, F1: 0.8341
LM Predictions:  [2, 2, 5, 1, 0, 2, 4, 1, 1, 0, 3, 0, 0, 1, 0, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 2, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 1, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8412, Accuracy: 0.6667, Precision: 0.5206, Recall: 0.5198, F1: 0.4939
Epoch 64/70
Train Loss: 0.0966, Accuracy: 0.9642, Precision: 0.9390, Recall: 0.9503, F1: 0.9439
Validation Loss: 0.8087, Accuracy: 0.8550, Precision: 0.8124, Recall: 0.8034, F1: 0.8068
Testing Loss: 0.7562, Accuracy: 0.8587, Precision: 0.8119, Recall: 0.8045, F1: 0.8075
LM Predictions:  [2, 2, 5, 1, 0, 1, 4, 1, 1, 0, 3, 4, 1, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 1, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4270, Accuracy: 0.8810, Precision: 0.7540, Recall: 0.7262, F1: 0.7388
Epoch 65/70
Train Loss: 0.1064, Accuracy: 0.9571, Precision: 0.9318, Recall: 0.9316, F1: 0.9317
Validation Loss: 0.7740, Accuracy: 0.8657, Precision: 0.8375, Recall: 0.8170, F1: 0.8261
Testing Loss: 0.6964, Accuracy: 0.8647, Precision: 0.8268, Recall: 0.8116, F1: 0.8187
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 0, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 2, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6112, Accuracy: 0.8095, Precision: 0.8792, Recall: 0.7905, F1: 0.7867
Epoch 66/70
Train Loss: 0.0971, Accuracy: 0.9637, Precision: 0.9390, Recall: 0.9475, F1: 0.9429
Validation Loss: 0.7669, Accuracy: 0.8614, Precision: 0.8302, Recall: 0.8356, F1: 0.8301
Testing Loss: 0.6971, Accuracy: 0.8635, Precision: 0.8316, Recall: 0.8293, F1: 0.8263
LM Predictions:  [2, 2, 5, 1, 0, 1, 5, 1, 1, 0, 0, 5, 0, 1, 4, 0, 5, 0, 3, 2, 5, 2, 3, 5, 0, 0, 1, 2, 0, 4, 5, 5, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8096, Accuracy: 0.7143, Precision: 0.7601, Recall: 0.5767, F1: 0.6263
Epoch 67/70
Train Loss: 0.0999, Accuracy: 0.9606, Precision: 0.9383, Recall: 0.9322, F1: 0.9350
Validation Loss: 0.8441, Accuracy: 0.8550, Precision: 0.8324, Recall: 0.7819, F1: 0.7977
Testing Loss: 0.7867, Accuracy: 0.8671, Precision: 0.8369, Recall: 0.7993, F1: 0.8134
LM Predictions:  [2, 2, 3, 1, 0, 1, 4, 1, 1, 0, 3, 0, 0, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 2, 0, 0, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5330, Accuracy: 0.8095, Precision: 0.8301, Recall: 0.8048, F1: 0.7946
Epoch 68/70
Train Loss: 0.1287, Accuracy: 0.9573, Precision: 0.9336, Recall: 0.9340, F1: 0.9338
Validation Loss: 0.8123, Accuracy: 0.8678, Precision: 0.8301, Recall: 0.8257, F1: 0.8276
Testing Loss: 0.7367, Accuracy: 0.8659, Precision: 0.8206, Recall: 0.8141, F1: 0.8170
LM Predictions:  [5, 2, 0, 1, 0, 5, 4, 1, 1, 0, 0, 4, 0, 1, 4, 0, 4, 0, 0, 5, 0, 5, 0, 4, 0, 0, 1, 2, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8216, Accuracy: 0.6429, Precision: 0.5000, Recall: 0.4802, F1: 0.4583
Epoch 69/70
Train Loss: 0.1244, Accuracy: 0.9547, Precision: 0.9264, Recall: 0.9303, F1: 0.9283
Validation Loss: 0.8599, Accuracy: 0.8443, Precision: 0.8091, Recall: 0.7913, F1: 0.7976
Testing Loss: 0.8417, Accuracy: 0.8527, Precision: 0.8116, Recall: 0.8043, F1: 0.8072
LM Predictions:  [2, 2, 3, 1, 0, 3, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3722, Accuracy: 0.8810, Precision: 0.8886, Recall: 0.8905, F1: 0.8807
Epoch 70/70
Train Loss: 0.1130, Accuracy: 0.9618, Precision: 0.9346, Recall: 0.9430, F1: 0.9386
Validation Loss: 0.8223, Accuracy: 0.8550, Precision: 0.8263, Recall: 0.8131, F1: 0.8187
Testing Loss: 0.7320, Accuracy: 0.8599, Precision: 0.8226, Recall: 0.8110, F1: 0.8163
LM Predictions:  [2, 2, 0, 1, 0, 0, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 1, 0, 5, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6494, Accuracy: 0.7619, Precision: 0.7410, Recall: 0.6032, F1: 0.6175
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.8985, Accuracy: 0.6837, Precision: 0.6587, Recall: 0.5590, F1: 0.5735
Validation Loss: 0.4983, Accuracy: 0.8401, Precision: 0.8261, Recall: 0.7589, F1: 0.7660
Testing Loss: 0.4108, Accuracy: 0.8647, Precision: 0.8529, Recall: 0.7727, F1: 0.7904
LM Predictions:  [2, 5, 1, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7837, Accuracy: 0.1905, Precision: 0.2857, Recall: 0.1508, F1: 0.1179
Epoch 2/70
Train Loss: 0.4431, Accuracy: 0.8584, Precision: 0.7935, Recall: 0.7853, F1: 0.7883
Validation Loss: 0.4625, Accuracy: 0.8571, Precision: 0.8323, Recall: 0.8028, F1: 0.8157
Testing Loss: 0.3568, Accuracy: 0.8937, Precision: 0.8810, Recall: 0.8421, F1: 0.8583
LM Predictions:  [5, 5, 4, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 3, 5, 0, 5, 0, 5, 0, 5, 5, 5, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8984, Accuracy: 0.1190, Precision: 0.0439, Recall: 0.0926, F1: 0.0595
Epoch 3/70
Train Loss: 0.3656, Accuracy: 0.8845, Precision: 0.8382, Recall: 0.8371, F1: 0.8375
Validation Loss: 0.4813, Accuracy: 0.8614, Precision: 0.8307, Recall: 0.8071, F1: 0.8141
Testing Loss: 0.3812, Accuracy: 0.8792, Precision: 0.8440, Recall: 0.8263, F1: 0.8338
LM Predictions:  [2, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 2, 5, 0, 5, 0, 5, 0, 5, 5, 5, 5, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6848, Accuracy: 0.1190, Precision: 0.2955, Recall: 0.0952, F1: 0.1139
Epoch 4/70
Train Loss: 0.3296, Accuracy: 0.8973, Precision: 0.8548, Recall: 0.8584, F1: 0.8561
Validation Loss: 0.4711, Accuracy: 0.8507, Precision: 0.8516, Recall: 0.7568, F1: 0.7633
Testing Loss: 0.3812, Accuracy: 0.8780, Precision: 0.8999, Recall: 0.7897, F1: 0.8039
LM Predictions:  [5, 0, 4, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 3, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5750, Accuracy: 0.1905, Precision: 0.2056, Recall: 0.1415, F1: 0.0821
Epoch 5/70
Train Loss: 0.2941, Accuracy: 0.9082, Precision: 0.8658, Recall: 0.8650, F1: 0.8653
Validation Loss: 0.4259, Accuracy: 0.8827, Precision: 0.8569, Recall: 0.8622, F1: 0.8583
Testing Loss: 0.3836, Accuracy: 0.8877, Precision: 0.8679, Recall: 0.8666, F1: 0.8623
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5986, Accuracy: 0.0714, Precision: 0.2500, Recall: 0.0489, F1: 0.0735
Epoch 6/70
Train Loss: 0.2522, Accuracy: 0.9189, Precision: 0.8771, Recall: 0.8822, F1: 0.8792
Validation Loss: 0.4653, Accuracy: 0.8614, Precision: 0.8488, Recall: 0.8049, F1: 0.8222
Testing Loss: 0.4267, Accuracy: 0.8551, Precision: 0.8509, Recall: 0.7762, F1: 0.7998
LM Predictions:  [2, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7785, Accuracy: 0.1429, Precision: 0.3889, Recall: 0.1138, F1: 0.1333
Epoch 7/70
Train Loss: 0.2338, Accuracy: 0.9272, Precision: 0.8880, Recall: 0.8938, F1: 0.8904
Validation Loss: 0.4955, Accuracy: 0.8657, Precision: 0.8360, Recall: 0.8272, F1: 0.8307
Testing Loss: 0.4063, Accuracy: 0.8829, Precision: 0.8480, Recall: 0.8359, F1: 0.8410
LM Predictions:  [2, 5, 5, 5, 5, 5, 4, 5, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 0, 4, 0, 5, 5, 5, 5, 1, 3, 2, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3521, Accuracy: 0.1905, Precision: 0.4630, Recall: 0.1614, F1: 0.2074
Epoch 8/70
Train Loss: 0.2327, Accuracy: 0.9232, Precision: 0.8835, Recall: 0.8915, F1: 0.8868
Validation Loss: 0.4603, Accuracy: 0.8678, Precision: 0.8364, Recall: 0.8451, F1: 0.8394
Testing Loss: 0.4301, Accuracy: 0.8804, Precision: 0.8506, Recall: 0.8491, F1: 0.8473
LM Predictions:  [2, 4, 4, 1, 1, 5, 4, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 1, 4, 5, 5, 5, 5, 5, 1, 5, 1, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9405, Accuracy: 0.2619, Precision: 0.4802, Recall: 0.1958, F1: 0.2595
Epoch 9/70
Train Loss: 0.1989, Accuracy: 0.9334, Precision: 0.8915, Recall: 0.8993, F1: 0.8949
Validation Loss: 0.4992, Accuracy: 0.8635, Precision: 0.8256, Recall: 0.8330, F1: 0.8281
Testing Loss: 0.3991, Accuracy: 0.8744, Precision: 0.8366, Recall: 0.8383, F1: 0.8367
LM Predictions:  [2, 4, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 5, 4, 0, 5, 5, 5, 5, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2466, Accuracy: 0.2381, Precision: 0.5185, Recall: 0.1852, F1: 0.2472
Epoch 10/70
Train Loss: 0.1851, Accuracy: 0.9405, Precision: 0.9038, Recall: 0.9110, F1: 0.9070
Validation Loss: 0.5490, Accuracy: 0.8593, Precision: 0.8297, Recall: 0.8229, F1: 0.8255
Testing Loss: 0.3898, Accuracy: 0.8865, Precision: 0.8557, Recall: 0.8477, F1: 0.8508
LM Predictions:  [2, 2, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 5, 4, 0, 5, 5, 5, 1, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7355, Accuracy: 0.2619, Precision: 0.5741, Recall: 0.2011, F1: 0.2731
Epoch 11/70
Train Loss: 0.1786, Accuracy: 0.9391, Precision: 0.9026, Recall: 0.9106, F1: 0.9060
Validation Loss: 0.5560, Accuracy: 0.8550, Precision: 0.8265, Recall: 0.8429, F1: 0.8309
Testing Loss: 0.5084, Accuracy: 0.8599, Precision: 0.8441, Recall: 0.8407, F1: 0.8338
LM Predictions:  [2, 2, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 0, 5, 5, 4, 5, 5, 5, 5, 5, 1, 5, 0, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3201, Accuracy: 0.1667, Precision: 0.5667, Recall: 0.1402, F1: 0.2143
Epoch 12/70
Train Loss: 0.1580, Accuracy: 0.9459, Precision: 0.9115, Recall: 0.9232, F1: 0.9162
Validation Loss: 0.5771, Accuracy: 0.8529, Precision: 0.8162, Recall: 0.7792, F1: 0.7911
Testing Loss: 0.4846, Accuracy: 0.8587, Precision: 0.8303, Recall: 0.7830, F1: 0.7989
LM Predictions:  [2, 2, 1, 5, 0, 5, 4, 5, 5, 0, 0, 0, 0, 5, 4, 0, 4, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 1, 0, 0, 1, 4, 0, 0, 5, 5, 2, 1, 0, 0, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5637, Accuracy: 0.4524, Precision: 0.5278, Recall: 0.3651, F1: 0.3805
Epoch 13/70
Train Loss: 0.1609, Accuracy: 0.9429, Precision: 0.9058, Recall: 0.9143, F1: 0.9096
Validation Loss: 0.5871, Accuracy: 0.8614, Precision: 0.8292, Recall: 0.8284, F1: 0.8277
Testing Loss: 0.4596, Accuracy: 0.8816, Precision: 0.8620, Recall: 0.8427, F1: 0.8493
LM Predictions:  [2, 2, 5, 5, 5, 1, 0, 5, 5, 0, 5, 0, 0, 1, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 1, 0, 5, 1, 4, 0, 5, 5, 5, 2, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7999, Accuracy: 0.3095, Precision: 0.5111, Recall: 0.2249, F1: 0.2859
Epoch 14/70
Train Loss: 0.1446, Accuracy: 0.9509, Precision: 0.9186, Recall: 0.9281, F1: 0.9227
Validation Loss: 0.5675, Accuracy: 0.8529, Precision: 0.8170, Recall: 0.8153, F1: 0.8155
Testing Loss: 0.4815, Accuracy: 0.8684, Precision: 0.8363, Recall: 0.8244, F1: 0.8286
LM Predictions:  [2, 2, 1, 1, 5, 5, 4, 5, 1, 0, 1, 0, 0, 1, 0, 0, 0, 5, 5, 2, 5, 5, 0, 5, 5, 5, 1, 1, 0, 5, 1, 4, 0, 5, 1, 1, 2, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7743, Accuracy: 0.4762, Precision: 0.5185, Recall: 0.3360, F1: 0.3892
Epoch 15/70
Train Loss: 0.1519, Accuracy: 0.9469, Precision: 0.9124, Recall: 0.9165, F1: 0.9141
Validation Loss: 0.6453, Accuracy: 0.8422, Precision: 0.8071, Recall: 0.7794, F1: 0.7776
Testing Loss: 0.5172, Accuracy: 0.8587, Precision: 0.8283, Recall: 0.7753, F1: 0.7796
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 5, 1, 0, 5, 4, 0, 1, 4, 0, 4, 3, 0, 5, 0, 5, 5, 5, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 2, 1, 0, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0181, Accuracy: 0.6667, Precision: 0.7183, Recall: 0.5000, F1: 0.5377
Epoch 16/70
Train Loss: 0.1341, Accuracy: 0.9502, Precision: 0.9212, Recall: 0.9227, F1: 0.9219
Validation Loss: 0.6773, Accuracy: 0.8614, Precision: 0.8308, Recall: 0.7972, F1: 0.8073
Testing Loss: 0.5679, Accuracy: 0.8720, Precision: 0.8316, Recall: 0.7960, F1: 0.8063
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 2, 0, 2, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 5, 0, 0, 1, 5, 0, 0, 1, 4, 0, 0, 1, 1, 2, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7577, Accuracy: 0.7857, Precision: 0.7344, Recall: 0.6389, F1: 0.6612
Epoch 17/70
Train Loss: 0.1186, Accuracy: 0.9516, Precision: 0.9193, Recall: 0.9212, F1: 0.9202
Validation Loss: 0.6967, Accuracy: 0.8401, Precision: 0.8305, Recall: 0.7636, F1: 0.7831
Testing Loss: 0.6524, Accuracy: 0.8514, Precision: 0.8502, Recall: 0.7678, F1: 0.7889
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 1, 4, 0, 1, 4, 0, 4, 0, 0, 2, 0, 5, 0, 5, 0, 0, 5, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7278, Accuracy: 0.7143, Precision: 0.5818, Recall: 0.5238, F1: 0.5248
Epoch 18/70
Train Loss: 0.1129, Accuracy: 0.9533, Precision: 0.9242, Recall: 0.9237, F1: 0.9239
Validation Loss: 0.7183, Accuracy: 0.8614, Precision: 0.8258, Recall: 0.7954, F1: 0.8058
Testing Loss: 0.6277, Accuracy: 0.8684, Precision: 0.8463, Recall: 0.7902, F1: 0.8053
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 2, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 5, 0, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 1, 1, 4, 1, 0, 1, 5, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7863, Accuracy: 0.7857, Precision: 0.7305, Recall: 0.6190, F1: 0.6196
Epoch 19/70
Train Loss: 0.1030, Accuracy: 0.9566, Precision: 0.9266, Recall: 0.9333, F1: 0.9296
Validation Loss: 0.7532, Accuracy: 0.8465, Precision: 0.8176, Recall: 0.7842, F1: 0.7957
Testing Loss: 0.5362, Accuracy: 0.8780, Precision: 0.8522, Recall: 0.8080, F1: 0.8238
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 5, 3, 1, 5, 0, 5, 3, 0, 2, 0, 2, 0, 5, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 5, 0, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7674, Accuracy: 0.7619, Precision: 0.7738, Recall: 0.6071, F1: 0.6361
Epoch 20/70
Train Loss: 0.1276, Accuracy: 0.9497, Precision: 0.9226, Recall: 0.9170, F1: 0.9197
Validation Loss: 0.7332, Accuracy: 0.8550, Precision: 0.8256, Recall: 0.7942, F1: 0.8055
Testing Loss: 0.6225, Accuracy: 0.8551, Precision: 0.8266, Recall: 0.7902, F1: 0.8038
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 2, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6385, Accuracy: 0.8333, Precision: 0.8714, Recall: 0.8095, F1: 0.8129
Epoch 21/70
Train Loss: 0.0998, Accuracy: 0.9580, Precision: 0.9331, Recall: 0.9292, F1: 0.9311
Validation Loss: 0.7472, Accuracy: 0.8635, Precision: 0.8309, Recall: 0.8279, F1: 0.8286
Testing Loss: 0.5869, Accuracy: 0.8708, Precision: 0.8285, Recall: 0.8081, F1: 0.8149
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 5, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 1, 1, 2, 1, 0, 1, 2, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5398, Accuracy: 0.8571, Precision: 0.7571, Recall: 0.6825, F1: 0.6909
Epoch 22/70
Train Loss: 0.0969, Accuracy: 0.9611, Precision: 0.9366, Recall: 0.9352, F1: 0.9359
Validation Loss: 0.7784, Accuracy: 0.8614, Precision: 0.8301, Recall: 0.8065, F1: 0.8158
Testing Loss: 0.6108, Accuracy: 0.8696, Precision: 0.8415, Recall: 0.7987, F1: 0.8150
LM Predictions:  [2, 2, 0, 0, 0, 5, 4, 5, 1, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 2, 3, 5, 0, 0, 5, 5, 0, 0, 0, 4, 0, 3, 0, 0, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8956, Accuracy: 0.5952, Precision: 0.7381, Recall: 0.5397, F1: 0.5481
Epoch 23/70
Train Loss: 0.0933, Accuracy: 0.9606, Precision: 0.9361, Recall: 0.9375, F1: 0.9367
Validation Loss: 0.7860, Accuracy: 0.8316, Precision: 0.8222, Recall: 0.7610, F1: 0.7649
Testing Loss: 0.6259, Accuracy: 0.8514, Precision: 0.8368, Recall: 0.7739, F1: 0.7801
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3355, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8762, F1: 0.8903
Epoch 24/70
Train Loss: 0.1059, Accuracy: 0.9578, Precision: 0.9376, Recall: 0.9318, F1: 0.9346
Validation Loss: 0.7668, Accuracy: 0.8529, Precision: 0.8151, Recall: 0.7883, F1: 0.7962
Testing Loss: 0.6749, Accuracy: 0.8671, Precision: 0.8387, Recall: 0.7948, F1: 0.8089
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4823, Accuracy: 0.9048, Precision: 0.9367, Recall: 0.8667, F1: 0.8797
Epoch 25/70
Train Loss: 0.1157, Accuracy: 0.9504, Precision: 0.9209, Recall: 0.9257, F1: 0.9231
Validation Loss: 0.7769, Accuracy: 0.8614, Precision: 0.8221, Recall: 0.8274, F1: 0.8230
Testing Loss: 0.5959, Accuracy: 0.8696, Precision: 0.8355, Recall: 0.8359, F1: 0.8350
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 5, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 2, 1, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3771, Accuracy: 0.8810, Precision: 0.7791, Recall: 0.7235, F1: 0.7439
Epoch 26/70
Train Loss: 0.0854, Accuracy: 0.9625, Precision: 0.9366, Recall: 0.9461, F1: 0.9408
Validation Loss: 0.7242, Accuracy: 0.8614, Precision: 0.8230, Recall: 0.8040, F1: 0.8093
Testing Loss: 0.6673, Accuracy: 0.8563, Precision: 0.8256, Recall: 0.7813, F1: 0.7966
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3358, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 27/70
Train Loss: 0.0762, Accuracy: 0.9613, Precision: 0.9364, Recall: 0.9345, F1: 0.9354
Validation Loss: 0.8021, Accuracy: 0.8657, Precision: 0.8343, Recall: 0.8187, F1: 0.8239
Testing Loss: 0.6783, Accuracy: 0.8732, Precision: 0.8402, Recall: 0.8191, F1: 0.8278
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1831, Accuracy: 0.9286, Precision: 0.8000, Recall: 0.7593, F1: 0.7767
Epoch 28/70
Train Loss: 0.1005, Accuracy: 0.9587, Precision: 0.9370, Recall: 0.9315, F1: 0.9342
Validation Loss: 0.7342, Accuracy: 0.8571, Precision: 0.8254, Recall: 0.7925, F1: 0.8052
Testing Loss: 0.6545, Accuracy: 0.8671, Precision: 0.8389, Recall: 0.7810, F1: 0.8009
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 5, 0, 2, 3, 1, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4638, Accuracy: 0.8810, Precision: 0.7806, Recall: 0.6984, F1: 0.7202
Epoch 29/70
Train Loss: 0.0801, Accuracy: 0.9640, Precision: 0.9402, Recall: 0.9472, F1: 0.9435
Validation Loss: 0.8195, Accuracy: 0.8657, Precision: 0.8362, Recall: 0.8152, F1: 0.8245
Testing Loss: 0.7383, Accuracy: 0.8587, Precision: 0.8316, Recall: 0.7957, F1: 0.8111
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 0, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2291, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8857, F1: 0.8836
Epoch 30/70
Train Loss: 0.0772, Accuracy: 0.9659, Precision: 0.9470, Recall: 0.9388, F1: 0.9427
Validation Loss: 0.8136, Accuracy: 0.8593, Precision: 0.8160, Recall: 0.7925, F1: 0.7999
Testing Loss: 0.7559, Accuracy: 0.8635, Precision: 0.8328, Recall: 0.7826, F1: 0.7978
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 0, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2906, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8048, F1: 0.8166
Epoch 31/70
Train Loss: 0.0713, Accuracy: 0.9640, Precision: 0.9435, Recall: 0.9352, F1: 0.9392
Validation Loss: 0.7971, Accuracy: 0.8486, Precision: 0.8187, Recall: 0.8123, F1: 0.8151
Testing Loss: 0.7925, Accuracy: 0.8551, Precision: 0.8366, Recall: 0.8104, F1: 0.8212
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1780, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9333, F1: 0.9436
Epoch 32/70
Train Loss: 0.0906, Accuracy: 0.9597, Precision: 0.9320, Recall: 0.9415, F1: 0.9365
Validation Loss: 0.8077, Accuracy: 0.8443, Precision: 0.8035, Recall: 0.7829, F1: 0.7885
Testing Loss: 0.6830, Accuracy: 0.8611, Precision: 0.8097, Recall: 0.7732, F1: 0.7814
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1532, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9286, F1: 0.9307
Epoch 33/70
Train Loss: 0.0735, Accuracy: 0.9647, Precision: 0.9438, Recall: 0.9343, F1: 0.9388
Validation Loss: 0.7806, Accuracy: 0.8635, Precision: 0.8276, Recall: 0.8343, F1: 0.8297
Testing Loss: 0.6188, Accuracy: 0.8696, Precision: 0.8433, Recall: 0.8359, F1: 0.8379
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 5, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2770, Accuracy: 0.8095, Precision: 0.7564, Recall: 0.6614, F1: 0.6892
Epoch 34/70
Train Loss: 0.0781, Accuracy: 0.9668, Precision: 0.9448, Recall: 0.9516, F1: 0.9480
Validation Loss: 0.7541, Accuracy: 0.8507, Precision: 0.8249, Recall: 0.7827, F1: 0.7985
Testing Loss: 0.6812, Accuracy: 0.8587, Precision: 0.8275, Recall: 0.7770, F1: 0.7937
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1448, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9429, F1: 0.9467
Epoch 35/70
Train Loss: 0.0831, Accuracy: 0.9621, Precision: 0.9393, Recall: 0.9353, F1: 0.9373
Validation Loss: 0.9062, Accuracy: 0.8785, Precision: 0.8461, Recall: 0.8262, F1: 0.8335
Testing Loss: 0.8192, Accuracy: 0.8659, Precision: 0.8321, Recall: 0.7980, F1: 0.8113
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1641, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9048, F1: 0.9160
Epoch 36/70
Train Loss: 0.0634, Accuracy: 0.9649, Precision: 0.9416, Recall: 0.9461, F1: 0.9437
Validation Loss: 0.8272, Accuracy: 0.8593, Precision: 0.8173, Recall: 0.8133, F1: 0.8131
Testing Loss: 0.7082, Accuracy: 0.8684, Precision: 0.8269, Recall: 0.8062, F1: 0.8137
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1496, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9524, F1: 0.9544
Epoch 37/70
Train Loss: 0.0660, Accuracy: 0.9649, Precision: 0.9488, Recall: 0.9357, F1: 0.9418
Validation Loss: 0.8923, Accuracy: 0.8593, Precision: 0.8191, Recall: 0.8060, F1: 0.8082
Testing Loss: 0.7272, Accuracy: 0.8671, Precision: 0.8268, Recall: 0.7934, F1: 0.8020
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1434, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8857, F1: 0.8896
Epoch 38/70
Train Loss: 0.0580, Accuracy: 0.9659, Precision: 0.9424, Recall: 0.9452, F1: 0.9437
Validation Loss: 0.9606, Accuracy: 0.8571, Precision: 0.8196, Recall: 0.8071, F1: 0.8099
Testing Loss: 0.8527, Accuracy: 0.8684, Precision: 0.8382, Recall: 0.8027, F1: 0.8132
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1560, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9132
Epoch 39/70
Train Loss: 0.0587, Accuracy: 0.9685, Precision: 0.9439, Recall: 0.9505, F1: 0.9471
Validation Loss: 0.9960, Accuracy: 0.8507, Precision: 0.8116, Recall: 0.7855, F1: 0.7923
Testing Loss: 0.8570, Accuracy: 0.8671, Precision: 0.8292, Recall: 0.7769, F1: 0.7906
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 2, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3791, Accuracy: 0.8333, Precision: 0.8867, Recall: 0.8000, F1: 0.8147
Epoch 40/70
Train Loss: 0.0725, Accuracy: 0.9656, Precision: 0.9531, Recall: 0.9321, F1: 0.9413
Validation Loss: 0.9473, Accuracy: 0.8571, Precision: 0.8156, Recall: 0.8042, F1: 0.8054
Testing Loss: 0.8945, Accuracy: 0.8575, Precision: 0.8144, Recall: 0.7980, F1: 0.8027
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 5, 0, 5, 0, 2, 3, 4, 0, 0, 1, 5, 0, 4, 1, 4, 0, 0, 1, 1, 1, 5, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4778, Accuracy: 0.8333, Precision: 0.7917, Recall: 0.6706, F1: 0.7078
Epoch 41/70
Train Loss: 0.0717, Accuracy: 0.9606, Precision: 0.9330, Recall: 0.9363, F1: 0.9346
Validation Loss: 0.8292, Accuracy: 0.8593, Precision: 0.8210, Recall: 0.8363, F1: 0.8276
Testing Loss: 0.7484, Accuracy: 0.8611, Precision: 0.8245, Recall: 0.8312, F1: 0.8262
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 5, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2215, Accuracy: 0.9286, Precision: 0.8030, Recall: 0.7500, F1: 0.7682
Epoch 42/70
Train Loss: 0.0638, Accuracy: 0.9677, Precision: 0.9492, Recall: 0.9397, F1: 0.9441
Validation Loss: 0.8702, Accuracy: 0.8657, Precision: 0.8284, Recall: 0.8203, F1: 0.8230
Testing Loss: 0.7852, Accuracy: 0.8696, Precision: 0.8371, Recall: 0.8213, F1: 0.8286
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1521, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 43/70
Train Loss: 0.0826, Accuracy: 0.9604, Precision: 0.9372, Recall: 0.9372, F1: 0.9372
Validation Loss: 0.8483, Accuracy: 0.8742, Precision: 0.8497, Recall: 0.8315, F1: 0.8399
Testing Loss: 0.8737, Accuracy: 0.8623, Precision: 0.8317, Recall: 0.8201, F1: 0.8243
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4006, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9095, F1: 0.9199
Epoch 44/70
Train Loss: 0.0675, Accuracy: 0.9632, Precision: 0.9412, Recall: 0.9405, F1: 0.9409
Validation Loss: 0.8426, Accuracy: 0.8614, Precision: 0.8253, Recall: 0.8488, F1: 0.8315
Testing Loss: 0.7310, Accuracy: 0.8696, Precision: 0.8348, Recall: 0.8419, F1: 0.8356
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2695, Accuracy: 0.8571, Precision: 0.7778, Recall: 0.7077, F1: 0.7323
Epoch 45/70
Train Loss: 0.0785, Accuracy: 0.9630, Precision: 0.9403, Recall: 0.9366, F1: 0.9384
Validation Loss: 0.8857, Accuracy: 0.8529, Precision: 0.8161, Recall: 0.7968, F1: 0.8046
Testing Loss: 0.7909, Accuracy: 0.8599, Precision: 0.8235, Recall: 0.7929, F1: 0.8056
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1861, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8810, F1: 0.8909
Epoch 46/70
Train Loss: 0.0615, Accuracy: 0.9680, Precision: 0.9459, Recall: 0.9515, F1: 0.9484
Validation Loss: 0.9930, Accuracy: 0.8443, Precision: 0.8004, Recall: 0.7691, F1: 0.7778
Testing Loss: 0.8711, Accuracy: 0.8611, Precision: 0.8324, Recall: 0.7733, F1: 0.7889
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1935, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 47/70
Train Loss: 0.0858, Accuracy: 0.9604, Precision: 0.9417, Recall: 0.9329, F1: 0.9369
Validation Loss: 0.9318, Accuracy: 0.8401, Precision: 0.8035, Recall: 0.7895, F1: 0.7942
Testing Loss: 0.7976, Accuracy: 0.8466, Precision: 0.8020, Recall: 0.7629, F1: 0.7775
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2144, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 48/70
Train Loss: 0.0729, Accuracy: 0.9637, Precision: 0.9452, Recall: 0.9367, F1: 0.9409
Validation Loss: 0.9020, Accuracy: 0.8550, Precision: 0.8160, Recall: 0.7955, F1: 0.8005
Testing Loss: 0.8504, Accuracy: 0.8623, Precision: 0.8184, Recall: 0.7845, F1: 0.7956
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1298, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 49/70
Train Loss: 0.0636, Accuracy: 0.9656, Precision: 0.9438, Recall: 0.9460, F1: 0.9449
Validation Loss: 0.8725, Accuracy: 0.8550, Precision: 0.8169, Recall: 0.7972, F1: 0.8031
Testing Loss: 0.8337, Accuracy: 0.8635, Precision: 0.8235, Recall: 0.7885, F1: 0.7998
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1198, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 50/70
Train Loss: 0.0558, Accuracy: 0.9668, Precision: 0.9487, Recall: 0.9390, F1: 0.9436
Validation Loss: 0.9089, Accuracy: 0.8571, Precision: 0.8188, Recall: 0.8133, F1: 0.8159
Testing Loss: 0.7835, Accuracy: 0.8647, Precision: 0.8299, Recall: 0.8181, F1: 0.8229
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1568, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8524, F1: 0.8643
Epoch 51/70
Train Loss: 0.0744, Accuracy: 0.9604, Precision: 0.9416, Recall: 0.9259, F1: 0.9327
Validation Loss: 0.8761, Accuracy: 0.8550, Precision: 0.8233, Recall: 0.7917, F1: 0.8036
Testing Loss: 0.8166, Accuracy: 0.8599, Precision: 0.8264, Recall: 0.7834, F1: 0.7997
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2205, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 52/70
Train Loss: 0.0574, Accuracy: 0.9670, Precision: 0.9476, Recall: 0.9429, F1: 0.9450
Validation Loss: 1.0039, Accuracy: 0.8465, Precision: 0.8166, Recall: 0.7820, F1: 0.7933
Testing Loss: 0.9288, Accuracy: 0.8575, Precision: 0.8336, Recall: 0.7795, F1: 0.7992
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1903, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 53/70
Train Loss: 0.0564, Accuracy: 0.9687, Precision: 0.9495, Recall: 0.9474, F1: 0.9484
Validation Loss: 0.9716, Accuracy: 0.8678, Precision: 0.8422, Recall: 0.8138, F1: 0.8258
Testing Loss: 0.8757, Accuracy: 0.8659, Precision: 0.8309, Recall: 0.7960, F1: 0.8090
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1394, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 54/70
Train Loss: 0.0598, Accuracy: 0.9663, Precision: 0.9438, Recall: 0.9524, F1: 0.9478
Validation Loss: 0.9382, Accuracy: 0.8550, Precision: 0.8100, Recall: 0.8136, F1: 0.8103
Testing Loss: 0.8299, Accuracy: 0.8720, Precision: 0.8386, Recall: 0.8332, F1: 0.8358
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2038, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 55/70
Train Loss: 0.0495, Accuracy: 0.9682, Precision: 0.9488, Recall: 0.9483, F1: 0.9486
Validation Loss: 0.9765, Accuracy: 0.8571, Precision: 0.8303, Recall: 0.8111, F1: 0.8195
Testing Loss: 0.8630, Accuracy: 0.8659, Precision: 0.8274, Recall: 0.7909, F1: 0.8049
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1826, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 56/70
Train Loss: 0.0481, Accuracy: 0.9704, Precision: 0.9554, Recall: 0.9440, F1: 0.9491
Validation Loss: 1.0008, Accuracy: 0.8507, Precision: 0.8078, Recall: 0.7861, F1: 0.7918
Testing Loss: 0.9089, Accuracy: 0.8635, Precision: 0.8284, Recall: 0.7900, F1: 0.8030
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1744, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8667, F1: 0.8764
Epoch 57/70
Train Loss: 0.0491, Accuracy: 0.9720, Precision: 0.9491, Recall: 0.9639, F1: 0.9556
Validation Loss: 1.0106, Accuracy: 0.8443, Precision: 0.8071, Recall: 0.7898, F1: 0.7943
Testing Loss: 0.8890, Accuracy: 0.8684, Precision: 0.8333, Recall: 0.7986, F1: 0.8112
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1052, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0708, Accuracy: 0.9668, Precision: 0.9526, Recall: 0.9365, F1: 0.9437
Validation Loss: 1.0291, Accuracy: 0.8529, Precision: 0.8178, Recall: 0.7956, F1: 0.8019
Testing Loss: 0.9492, Accuracy: 0.8551, Precision: 0.8173, Recall: 0.7917, F1: 0.8024
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1696, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 59/70
Train Loss: 0.0627, Accuracy: 0.9689, Precision: 0.9459, Recall: 0.9543, F1: 0.9498
Validation Loss: 0.9979, Accuracy: 0.8571, Precision: 0.8280, Recall: 0.8192, F1: 0.8210
Testing Loss: 0.8764, Accuracy: 0.8659, Precision: 0.8262, Recall: 0.8159, F1: 0.8205
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2284, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8103
Epoch 60/70
Train Loss: 0.0709, Accuracy: 0.9635, Precision: 0.9411, Recall: 0.9482, F1: 0.9444
Validation Loss: 0.8509, Accuracy: 0.8614, Precision: 0.8317, Recall: 0.7952, F1: 0.8066
Testing Loss: 0.8506, Accuracy: 0.8563, Precision: 0.8163, Recall: 0.7773, F1: 0.7876
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1444, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0642, Accuracy: 0.9659, Precision: 0.9526, Recall: 0.9341, F1: 0.9423
Validation Loss: 0.7596, Accuracy: 0.8635, Precision: 0.8266, Recall: 0.8292, F1: 0.8269
Testing Loss: 0.6704, Accuracy: 0.8611, Precision: 0.8150, Recall: 0.8019, F1: 0.8074
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 5, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1896, Accuracy: 0.9524, Precision: 0.8167, Recall: 0.7817, F1: 0.7966
Epoch 62/70
Train Loss: 0.0617, Accuracy: 0.9649, Precision: 0.9441, Recall: 0.9353, F1: 0.9394
Validation Loss: 0.8435, Accuracy: 0.8699, Precision: 0.8342, Recall: 0.8193, F1: 0.8242
Testing Loss: 0.8525, Accuracy: 0.8599, Precision: 0.8148, Recall: 0.7784, F1: 0.7891
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3020, Accuracy: 0.8571, Precision: 0.9000, Recall: 0.8714, F1: 0.8626
Epoch 63/70
Train Loss: 0.0478, Accuracy: 0.9647, Precision: 0.9428, Recall: 0.9341, F1: 0.9382
Validation Loss: 0.9158, Accuracy: 0.8699, Precision: 0.8438, Recall: 0.8273, F1: 0.8347
Testing Loss: 0.9338, Accuracy: 0.8647, Precision: 0.8309, Recall: 0.8095, F1: 0.8188
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2090, Accuracy: 0.9762, Precision: 0.9714, Recall: 0.9857, F1: 0.9772
Epoch 64/70
Train Loss: 0.0488, Accuracy: 0.9689, Precision: 0.9489, Recall: 0.9474, F1: 0.9482
Validation Loss: 0.9219, Accuracy: 0.8571, Precision: 0.8234, Recall: 0.8086, F1: 0.8116
Testing Loss: 0.8376, Accuracy: 0.8575, Precision: 0.8111, Recall: 0.7842, F1: 0.7913
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1832, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 65/70
Train Loss: 0.0493, Accuracy: 0.9696, Precision: 0.9527, Recall: 0.9486, F1: 0.9506
Validation Loss: 1.0112, Accuracy: 0.8614, Precision: 0.8297, Recall: 0.7949, F1: 0.8062
Testing Loss: 0.9775, Accuracy: 0.8647, Precision: 0.8350, Recall: 0.7861, F1: 0.8021
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1934, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 66/70
Train Loss: 0.0661, Accuracy: 0.9630, Precision: 0.9396, Recall: 0.9491, F1: 0.9439
Validation Loss: 0.9351, Accuracy: 0.8550, Precision: 0.8096, Recall: 0.8086, F1: 0.8075
Testing Loss: 0.9179, Accuracy: 0.8527, Precision: 0.8047, Recall: 0.7831, F1: 0.7896
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1404, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 67/70
Train Loss: 0.0581, Accuracy: 0.9661, Precision: 0.9407, Recall: 0.9503, F1: 0.9453
Validation Loss: 0.9167, Accuracy: 0.8529, Precision: 0.8394, Recall: 0.8015, F1: 0.8178
Testing Loss: 0.9704, Accuracy: 0.8394, Precision: 0.8216, Recall: 0.7744, F1: 0.7931
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1677, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9132
Epoch 68/70
Train Loss: 0.0657, Accuracy: 0.9687, Precision: 0.9487, Recall: 0.9447, F1: 0.9467
Validation Loss: 0.9188, Accuracy: 0.8657, Precision: 0.8364, Recall: 0.8026, F1: 0.8133
Testing Loss: 0.8860, Accuracy: 0.8466, Precision: 0.8035, Recall: 0.7540, F1: 0.7672
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2086, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 69/70
Train Loss: 0.0660, Accuracy: 0.9637, Precision: 0.9428, Recall: 0.9291, F1: 0.9351
Validation Loss: 0.9961, Accuracy: 0.8550, Precision: 0.8156, Recall: 0.7708, F1: 0.7796
Testing Loss: 0.9741, Accuracy: 0.8466, Precision: 0.8144, Recall: 0.7502, F1: 0.7626
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1297, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 70/70
Train Loss: 0.0633, Accuracy: 0.9666, Precision: 0.9504, Recall: 0.9364, F1: 0.9428
Validation Loss: 0.9368, Accuracy: 0.8678, Precision: 0.8255, Recall: 0.8131, F1: 0.8171
Testing Loss: 0.9153, Accuracy: 0.8587, Precision: 0.8169, Recall: 0.7796, F1: 0.7914
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1255, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.6189, Accuracy: 0.7972, Precision: 0.7466, Recall: 0.7180, F1: 0.7249
Validation Loss: 0.3775, Accuracy: 0.8806, Precision: 0.8527, Recall: 0.8575, F1: 0.8542
Testing Loss: 0.3205, Accuracy: 0.9070, Precision: 0.8874, Recall: 0.8902, F1: 0.8866
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 0, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5756, Accuracy: 0.0476, Precision: 0.0794, Recall: 0.0423, F1: 0.0542
Epoch 2/70
Train Loss: 0.3309, Accuracy: 0.8987, Precision: 0.8513, Recall: 0.8530, F1: 0.8518
Validation Loss: 0.3963, Accuracy: 0.8806, Precision: 0.8678, Recall: 0.8224, F1: 0.8404
Testing Loss: 0.3023, Accuracy: 0.9046, Precision: 0.8842, Recall: 0.8585, F1: 0.8699
LM Predictions:  [5, 5, 1, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 1, 2, 5, 0, 4, 0, 5, 5, 5, 0, 5, 5, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6247, Accuracy: 0.1667, Precision: 0.3718, Recall: 0.1151, F1: 0.1459
Epoch 3/70
Train Loss: 0.2598, Accuracy: 0.9172, Precision: 0.8770, Recall: 0.8790, F1: 0.8779
Validation Loss: 0.4312, Accuracy: 0.8721, Precision: 0.8706, Recall: 0.8131, F1: 0.8299
Testing Loss: 0.3494, Accuracy: 0.8913, Precision: 0.8865, Recall: 0.8118, F1: 0.8312
LM Predictions:  [5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 1, 0, 5, 0, 4, 0, 0, 0, 5, 0, 5, 5, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5701, Accuracy: 0.1667, Precision: 0.2851, Recall: 0.1217, F1: 0.1263
Epoch 4/70
Train Loss: 0.2199, Accuracy: 0.9281, Precision: 0.8885, Recall: 0.8930, F1: 0.8904
Validation Loss: 0.4813, Accuracy: 0.8721, Precision: 0.8762, Recall: 0.8020, F1: 0.8231
Testing Loss: 0.3537, Accuracy: 0.8877, Precision: 0.8911, Recall: 0.8021, F1: 0.8218
LM Predictions:  [5, 5, 1, 5, 5, 5, 5, 5, 1, 0, 0, 0, 0, 5, 0, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 0, 5, 1, 0, 0, 0, 4, 0, 0, 0, 5, 0, 5, 5, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1843, Accuracy: 0.2381, Precision: 0.3772, Recall: 0.1640, F1: 0.1753
Epoch 5/70
Train Loss: 0.1944, Accuracy: 0.9346, Precision: 0.8983, Recall: 0.8977, F1: 0.8980
Validation Loss: 0.5336, Accuracy: 0.8657, Precision: 0.8393, Recall: 0.8081, F1: 0.8208
Testing Loss: 0.3774, Accuracy: 0.8865, Precision: 0.8599, Recall: 0.8316, F1: 0.8442
LM Predictions:  [2, 5, 0, 5, 5, 5, 5, 5, 1, 0, 0, 5, 0, 1, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 1, 0, 5, 0, 4, 0, 5, 5, 5, 5, 5, 5, 1, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9554, Accuracy: 0.2381, Precision: 0.5606, Recall: 0.1733, F1: 0.2300
Epoch 6/70
Train Loss: 0.1674, Accuracy: 0.9383, Precision: 0.9001, Recall: 0.9031, F1: 0.9015
Validation Loss: 0.5255, Accuracy: 0.8699, Precision: 0.8543, Recall: 0.8020, F1: 0.8193
Testing Loss: 0.4343, Accuracy: 0.8816, Precision: 0.8591, Recall: 0.8128, F1: 0.8292
LM Predictions:  [5, 5, 0, 5, 5, 1, 4, 5, 1, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 3, 1, 0, 5, 0, 4, 0, 0, 5, 5, 0, 1, 0, 1, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0983, Accuracy: 0.3095, Precision: 0.3833, Recall: 0.2183, F1: 0.2308
Epoch 7/70
Train Loss: 0.1434, Accuracy: 0.9466, Precision: 0.9108, Recall: 0.9183, F1: 0.9142
Validation Loss: 0.5368, Accuracy: 0.8721, Precision: 0.8391, Recall: 0.8266, F1: 0.8317
Testing Loss: 0.4232, Accuracy: 0.8829, Precision: 0.8460, Recall: 0.8300, F1: 0.8367
LM Predictions:  [2, 2, 1, 5, 5, 1, 4, 5, 1, 0, 2, 0, 5, 1, 4, 0, 4, 3, 0, 2, 5, 5, 0, 5, 0, 0, 1, 1, 0, 5, 5, 4, 0, 5, 5, 5, 4, 1, 5, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0992, Accuracy: 0.5238, Precision: 0.7063, Recall: 0.4286, F1: 0.4984
Epoch 8/70
Train Loss: 0.1300, Accuracy: 0.9481, Precision: 0.9138, Recall: 0.9191, F1: 0.9161
Validation Loss: 0.5729, Accuracy: 0.8721, Precision: 0.8428, Recall: 0.8241, F1: 0.8324
Testing Loss: 0.4118, Accuracy: 0.8829, Precision: 0.8511, Recall: 0.8217, F1: 0.8333
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 3, 5, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 5, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7833, Accuracy: 0.7857, Precision: 0.7405, Recall: 0.6270, F1: 0.6621
Epoch 9/70
Train Loss: 0.1354, Accuracy: 0.9490, Precision: 0.9186, Recall: 0.9258, F1: 0.9220
Validation Loss: 0.6903, Accuracy: 0.8529, Precision: 0.8332, Recall: 0.8218, F1: 0.8240
Testing Loss: 0.4941, Accuracy: 0.8744, Precision: 0.8407, Recall: 0.8325, F1: 0.8352
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 5, 2, 5, 2, 3, 5, 0, 5, 1, 1, 0, 0, 0, 4, 0, 3, 5, 1, 1, 1, 2, 1, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7408, Accuracy: 0.7143, Precision: 0.7167, Recall: 0.5873, F1: 0.6411
Epoch 10/70
Train Loss: 0.1047, Accuracy: 0.9568, Precision: 0.9277, Recall: 0.9289, F1: 0.9282
Validation Loss: 0.6261, Accuracy: 0.8721, Precision: 0.8451, Recall: 0.8204, F1: 0.8302
Testing Loss: 0.5058, Accuracy: 0.8853, Precision: 0.8555, Recall: 0.8303, F1: 0.8416
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 5, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6669, Accuracy: 0.7619, Precision: 0.7421, Recall: 0.5992, F1: 0.6292
Epoch 11/70
Train Loss: 0.1042, Accuracy: 0.9585, Precision: 0.9296, Recall: 0.9357, F1: 0.9325
Validation Loss: 0.6305, Accuracy: 0.8742, Precision: 0.8475, Recall: 0.8242, F1: 0.8342
Testing Loss: 0.5361, Accuracy: 0.8768, Precision: 0.8421, Recall: 0.8144, F1: 0.8259
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 5, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5052, Accuracy: 0.8333, Precision: 0.7612, Recall: 0.6389, F1: 0.6395
Epoch 12/70
Train Loss: 0.0963, Accuracy: 0.9571, Precision: 0.9306, Recall: 0.9357, F1: 0.9330
Validation Loss: 0.5643, Accuracy: 0.8571, Precision: 0.8296, Recall: 0.8100, F1: 0.8187
Testing Loss: 0.4661, Accuracy: 0.8792, Precision: 0.8465, Recall: 0.8101, F1: 0.8243
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 4, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3912, Accuracy: 0.8095, Precision: 0.8533, Recall: 0.7667, F1: 0.7857
Epoch 13/70
Train Loss: 0.0819, Accuracy: 0.9616, Precision: 0.9381, Recall: 0.9401, F1: 0.9391
Validation Loss: 0.6665, Accuracy: 0.8593, Precision: 0.8288, Recall: 0.8276, F1: 0.8252
Testing Loss: 0.5511, Accuracy: 0.8756, Precision: 0.8490, Recall: 0.8344, F1: 0.8398
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3380, Accuracy: 0.9524, Precision: 0.9550, Recall: 0.9333, F1: 0.9398
Epoch 14/70
Train Loss: 0.0807, Accuracy: 0.9623, Precision: 0.9391, Recall: 0.9356, F1: 0.9373
Validation Loss: 0.6700, Accuracy: 0.8763, Precision: 0.8469, Recall: 0.8385, F1: 0.8425
Testing Loss: 0.5951, Accuracy: 0.8804, Precision: 0.8522, Recall: 0.8325, F1: 0.8412
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2526, Accuracy: 0.8571, Precision: 0.7582, Recall: 0.7024, F1: 0.7179
Epoch 15/70
Train Loss: 0.0704, Accuracy: 0.9613, Precision: 0.9327, Recall: 0.9431, F1: 0.9375
Validation Loss: 0.7396, Accuracy: 0.8721, Precision: 0.8455, Recall: 0.8155, F1: 0.8265
Testing Loss: 0.6051, Accuracy: 0.8780, Precision: 0.8492, Recall: 0.8062, F1: 0.8219
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2250, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9000, F1: 0.9017
Epoch 16/70
Train Loss: 0.0683, Accuracy: 0.9623, Precision: 0.9382, Recall: 0.9319, F1: 0.9349
Validation Loss: 0.7286, Accuracy: 0.8529, Precision: 0.8129, Recall: 0.8175, F1: 0.8138
Testing Loss: 0.5885, Accuracy: 0.8635, Precision: 0.8210, Recall: 0.8120, F1: 0.8160
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2106, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9429, F1: 0.9467
Epoch 17/70
Train Loss: 0.0653, Accuracy: 0.9663, Precision: 0.9422, Recall: 0.9467, F1: 0.9444
Validation Loss: 0.7846, Accuracy: 0.8593, Precision: 0.8325, Recall: 0.8038, F1: 0.8137
Testing Loss: 0.6352, Accuracy: 0.8744, Precision: 0.8394, Recall: 0.8045, F1: 0.8157
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2807, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8377
Epoch 18/70
Train Loss: 0.0621, Accuracy: 0.9661, Precision: 0.9415, Recall: 0.9462, F1: 0.9437
Validation Loss: 0.8592, Accuracy: 0.8614, Precision: 0.8310, Recall: 0.8001, F1: 0.8122
Testing Loss: 0.7213, Accuracy: 0.8756, Precision: 0.8510, Recall: 0.8009, F1: 0.8170
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2685, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 19/70
Train Loss: 0.0611, Accuracy: 0.9692, Precision: 0.9496, Recall: 0.9509, F1: 0.9502
Validation Loss: 0.8634, Accuracy: 0.8593, Precision: 0.8242, Recall: 0.7956, F1: 0.8050
Testing Loss: 0.7456, Accuracy: 0.8696, Precision: 0.8417, Recall: 0.7972, F1: 0.8125
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1746, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9048, F1: 0.9119
Epoch 20/70
Train Loss: 0.0893, Accuracy: 0.9580, Precision: 0.9328, Recall: 0.9333, F1: 0.9330
Validation Loss: 0.7822, Accuracy: 0.8571, Precision: 0.8143, Recall: 0.8000, F1: 0.8049
Testing Loss: 0.7112, Accuracy: 0.8684, Precision: 0.8354, Recall: 0.7966, F1: 0.8105
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3036, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7810, F1: 0.8049
Epoch 21/70
Train Loss: 0.0692, Accuracy: 0.9632, Precision: 0.9409, Recall: 0.9431, F1: 0.9419
Validation Loss: 0.8247, Accuracy: 0.8699, Precision: 0.8400, Recall: 0.8245, F1: 0.8313
Testing Loss: 0.7729, Accuracy: 0.8744, Precision: 0.8455, Recall: 0.8187, F1: 0.8303
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1273, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9381, F1: 0.9464
Epoch 22/70
Train Loss: 0.0772, Accuracy: 0.9632, Precision: 0.9448, Recall: 0.9432, F1: 0.9440
Validation Loss: 0.8759, Accuracy: 0.8678, Precision: 0.8370, Recall: 0.8364, F1: 0.8359
Testing Loss: 0.6789, Accuracy: 0.8720, Precision: 0.8292, Recall: 0.8197, F1: 0.8242
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1312, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8810, F1: 0.8909
Epoch 23/70
Train Loss: 0.0602, Accuracy: 0.9654, Precision: 0.9438, Recall: 0.9411, F1: 0.9424
Validation Loss: 0.7462, Accuracy: 0.8657, Precision: 0.8280, Recall: 0.8402, F1: 0.8333
Testing Loss: 0.6158, Accuracy: 0.8756, Precision: 0.8446, Recall: 0.8265, F1: 0.8348
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1601, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9571, F1: 0.9572
Epoch 24/70
Train Loss: 0.0629, Accuracy: 0.9668, Precision: 0.9442, Recall: 0.9461, F1: 0.9452
Validation Loss: 0.8167, Accuracy: 0.8571, Precision: 0.8265, Recall: 0.8063, F1: 0.8131
Testing Loss: 0.7134, Accuracy: 0.8732, Precision: 0.8393, Recall: 0.8047, F1: 0.8152
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 1, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2590, Accuracy: 0.8333, Precision: 0.9000, Recall: 0.8016, F1: 0.8246
Epoch 25/70
Train Loss: 0.0559, Accuracy: 0.9699, Precision: 0.9528, Recall: 0.9449, F1: 0.9486
Validation Loss: 0.8977, Accuracy: 0.8721, Precision: 0.8388, Recall: 0.8393, F1: 0.8387
Testing Loss: 0.7804, Accuracy: 0.8744, Precision: 0.8432, Recall: 0.8306, F1: 0.8362
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1043, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9857, F1: 0.9821
Epoch 26/70
Train Loss: 0.0545, Accuracy: 0.9656, Precision: 0.9416, Recall: 0.9454, F1: 0.9434
Validation Loss: 0.8584, Accuracy: 0.8742, Precision: 0.8469, Recall: 0.8336, F1: 0.8396
Testing Loss: 0.8253, Accuracy: 0.8623, Precision: 0.8331, Recall: 0.8043, F1: 0.8168
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2501, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8103
Epoch 27/70
Train Loss: 0.0521, Accuracy: 0.9675, Precision: 0.9437, Recall: 0.9464, F1: 0.9450
Validation Loss: 0.8721, Accuracy: 0.8593, Precision: 0.8223, Recall: 0.8001, F1: 0.8074
Testing Loss: 0.8453, Accuracy: 0.8671, Precision: 0.8370, Recall: 0.7976, F1: 0.8108
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0996, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0543, Accuracy: 0.9704, Precision: 0.9487, Recall: 0.9551, F1: 0.9519
Validation Loss: 0.8887, Accuracy: 0.8678, Precision: 0.8369, Recall: 0.8237, F1: 0.8286
Testing Loss: 0.8632, Accuracy: 0.8708, Precision: 0.8359, Recall: 0.8107, F1: 0.8204
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1388, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9048, F1: 0.9197
Epoch 29/70
Train Loss: 0.0587, Accuracy: 0.9699, Precision: 0.9510, Recall: 0.9499, F1: 0.9505
Validation Loss: 0.9952, Accuracy: 0.8614, Precision: 0.8260, Recall: 0.8108, F1: 0.8150
Testing Loss: 0.8106, Accuracy: 0.8720, Precision: 0.8343, Recall: 0.8013, F1: 0.8114
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 2, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1871, Accuracy: 0.9286, Precision: 0.9351, Recall: 0.9381, F1: 0.9310
Epoch 30/70
Train Loss: 0.0622, Accuracy: 0.9651, Precision: 0.9389, Recall: 0.9500, F1: 0.9442
Validation Loss: 0.8603, Accuracy: 0.8657, Precision: 0.8346, Recall: 0.8339, F1: 0.8340
Testing Loss: 0.8290, Accuracy: 0.8696, Precision: 0.8388, Recall: 0.8064, F1: 0.8205
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1527, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8810, F1: 0.8909
Epoch 31/70
Train Loss: 0.0488, Accuracy: 0.9715, Precision: 0.9495, Recall: 0.9604, F1: 0.9544
Validation Loss: 0.9621, Accuracy: 0.8635, Precision: 0.8274, Recall: 0.8100, F1: 0.8162
Testing Loss: 0.9118, Accuracy: 0.8744, Precision: 0.8437, Recall: 0.8095, F1: 0.8230
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1555, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 32/70
Train Loss: 0.0616, Accuracy: 0.9647, Precision: 0.9428, Recall: 0.9424, F1: 0.9426
Validation Loss: 0.8692, Accuracy: 0.8657, Precision: 0.8401, Recall: 0.8190, F1: 0.8282
Testing Loss: 0.8217, Accuracy: 0.8647, Precision: 0.8448, Recall: 0.8180, F1: 0.8296
LM Predictions:  [2, 2, 0, 1, 0, 1, 5, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2515, Accuracy: 0.7619, Precision: 0.7500, Recall: 0.6032, F1: 0.6276
Epoch 33/70
Train Loss: 0.0861, Accuracy: 0.9621, Precision: 0.9427, Recall: 0.9403, F1: 0.9415
Validation Loss: 0.9230, Accuracy: 0.8571, Precision: 0.8289, Recall: 0.8029, F1: 0.8136
Testing Loss: 0.8034, Accuracy: 0.8768, Precision: 0.8495, Recall: 0.8203, F1: 0.8328
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1968, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 34/70
Train Loss: 0.0545, Accuracy: 0.9637, Precision: 0.9389, Recall: 0.9420, F1: 0.9404
Validation Loss: 0.8841, Accuracy: 0.8593, Precision: 0.8236, Recall: 0.8129, F1: 0.8169
Testing Loss: 0.7568, Accuracy: 0.8671, Precision: 0.8330, Recall: 0.8021, F1: 0.8120
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1717, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8524, F1: 0.8643
Epoch 35/70
Train Loss: 0.0473, Accuracy: 0.9699, Precision: 0.9515, Recall: 0.9461, F1: 0.9487
Validation Loss: 1.0155, Accuracy: 0.8550, Precision: 0.8297, Recall: 0.7875, F1: 0.8032
Testing Loss: 0.8947, Accuracy: 0.8671, Precision: 0.8522, Recall: 0.7923, F1: 0.8123
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2171, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 36/70
Train Loss: 0.0517, Accuracy: 0.9685, Precision: 0.9495, Recall: 0.9476, F1: 0.9485
Validation Loss: 0.9798, Accuracy: 0.8465, Precision: 0.8041, Recall: 0.8012, F1: 0.7975
Testing Loss: 0.8609, Accuracy: 0.8659, Precision: 0.8312, Recall: 0.8142, F1: 0.8194
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1317, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9333, F1: 0.9400
Epoch 37/70
Train Loss: 0.0517, Accuracy: 0.9692, Precision: 0.9454, Recall: 0.9563, F1: 0.9504
Validation Loss: 0.8810, Accuracy: 0.8571, Precision: 0.8276, Recall: 0.7977, F1: 0.8084
Testing Loss: 0.8147, Accuracy: 0.8684, Precision: 0.8461, Recall: 0.7953, F1: 0.8116
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1813, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 38/70
Train Loss: 0.0515, Accuracy: 0.9651, Precision: 0.9425, Recall: 0.9371, F1: 0.9397
Validation Loss: 0.8986, Accuracy: 0.8657, Precision: 0.8428, Recall: 0.8164, F1: 0.8276
Testing Loss: 0.9021, Accuracy: 0.8611, Precision: 0.8432, Recall: 0.8041, F1: 0.8207
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1561, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 39/70
Train Loss: 0.0677, Accuracy: 0.9670, Precision: 0.9539, Recall: 0.9424, F1: 0.9478
Validation Loss: 0.9005, Accuracy: 0.8699, Precision: 0.8451, Recall: 0.8345, F1: 0.8394
Testing Loss: 0.7406, Accuracy: 0.8816, Precision: 0.8497, Recall: 0.8374, F1: 0.8430
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2144, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 40/70
Train Loss: 0.0532, Accuracy: 0.9670, Precision: 0.9477, Recall: 0.9451, F1: 0.9464
Validation Loss: 0.8301, Accuracy: 0.8763, Precision: 0.8498, Recall: 0.8413, F1: 0.8451
Testing Loss: 0.7641, Accuracy: 0.8768, Precision: 0.8493, Recall: 0.8294, F1: 0.8382
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1515, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 41/70
Train Loss: 0.0472, Accuracy: 0.9694, Precision: 0.9495, Recall: 0.9512, F1: 0.9503
Validation Loss: 0.9290, Accuracy: 0.8593, Precision: 0.8282, Recall: 0.7995, F1: 0.8098
Testing Loss: 0.8148, Accuracy: 0.8804, Precision: 0.8536, Recall: 0.8126, F1: 0.8263
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1587, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9048, F1: 0.9083
Epoch 42/70
Train Loss: 0.0455, Accuracy: 0.9699, Precision: 0.9496, Recall: 0.9470, F1: 0.9483
Validation Loss: 0.8976, Accuracy: 0.8742, Precision: 0.8395, Recall: 0.8409, F1: 0.8389
Testing Loss: 0.8177, Accuracy: 0.8744, Precision: 0.8393, Recall: 0.8205, F1: 0.8287
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1024, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9429, F1: 0.9467
Epoch 43/70
Train Loss: 0.0445, Accuracy: 0.9718, Precision: 0.9553, Recall: 0.9510, F1: 0.9531
Validation Loss: 0.9586, Accuracy: 0.8699, Precision: 0.8413, Recall: 0.8378, F1: 0.8390
Testing Loss: 0.8026, Accuracy: 0.8792, Precision: 0.8490, Recall: 0.8375, F1: 0.8428
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1578, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8429, F1: 0.8526
Epoch 44/70
Train Loss: 0.0563, Accuracy: 0.9659, Precision: 0.9434, Recall: 0.9460, F1: 0.9447
Validation Loss: 0.8601, Accuracy: 0.8657, Precision: 0.8372, Recall: 0.8038, F1: 0.8161
Testing Loss: 0.8510, Accuracy: 0.8708, Precision: 0.8562, Recall: 0.8007, F1: 0.8199
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1663, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8619, F1: 0.8758
Epoch 45/70
Train Loss: 0.0489, Accuracy: 0.9689, Precision: 0.9488, Recall: 0.9483, F1: 0.9485
Validation Loss: 0.9101, Accuracy: 0.8529, Precision: 0.7957, Recall: 0.7652, F1: 0.7686
Testing Loss: 0.7458, Accuracy: 0.8744, Precision: 0.8393, Recall: 0.7896, F1: 0.8026
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1843, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 46/70
Train Loss: 0.0576, Accuracy: 0.9654, Precision: 0.9439, Recall: 0.9404, F1: 0.9420
Validation Loss: 0.9189, Accuracy: 0.8614, Precision: 0.8216, Recall: 0.7942, F1: 0.8012
Testing Loss: 0.8838, Accuracy: 0.8684, Precision: 0.8286, Recall: 0.7958, F1: 0.8062
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2148, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 47/70
Train Loss: 0.0628, Accuracy: 0.9661, Precision: 0.9478, Recall: 0.9393, F1: 0.9434
Validation Loss: 0.8932, Accuracy: 0.8699, Precision: 0.8307, Recall: 0.8158, F1: 0.8209
Testing Loss: 0.8230, Accuracy: 0.8768, Precision: 0.8443, Recall: 0.8289, F1: 0.8359
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 3]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1921, Accuracy: 0.8810, Precision: 0.8985, Recall: 0.8714, F1: 0.8755
Epoch 48/70
Train Loss: 0.0516, Accuracy: 0.9687, Precision: 0.9455, Recall: 0.9542, F1: 0.9496
Validation Loss: 0.9159, Accuracy: 0.8699, Precision: 0.8329, Recall: 0.7998, F1: 0.8123
Testing Loss: 0.8965, Accuracy: 0.8756, Precision: 0.8499, Recall: 0.8037, F1: 0.8200
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2297, Accuracy: 0.8810, Precision: 0.9135, Recall: 0.8524, F1: 0.8580
Epoch 49/70
Train Loss: 0.0447, Accuracy: 0.9689, Precision: 0.9536, Recall: 0.9390, F1: 0.9455
Validation Loss: 0.9712, Accuracy: 0.8699, Precision: 0.8388, Recall: 0.8219, F1: 0.8292
Testing Loss: 0.8921, Accuracy: 0.8804, Precision: 0.8501, Recall: 0.8349, F1: 0.8418
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1697, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 50/70
Train Loss: 0.0438, Accuracy: 0.9694, Precision: 0.9468, Recall: 0.9543, F1: 0.9504
Validation Loss: 0.9583, Accuracy: 0.8614, Precision: 0.8287, Recall: 0.8023, F1: 0.8112
Testing Loss: 0.9003, Accuracy: 0.8575, Precision: 0.8309, Recall: 0.7872, F1: 0.8021
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2232, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 51/70
Train Loss: 0.0548, Accuracy: 0.9675, Precision: 0.9489, Recall: 0.9443, F1: 0.9466
Validation Loss: 0.9124, Accuracy: 0.8657, Precision: 0.8314, Recall: 0.8044, F1: 0.8129
Testing Loss: 0.8172, Accuracy: 0.8756, Precision: 0.8515, Recall: 0.8112, F1: 0.8255
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2031, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 52/70
Train Loss: 0.0558, Accuracy: 0.9673, Precision: 0.9440, Recall: 0.9504, F1: 0.9471
Validation Loss: 0.8800, Accuracy: 0.8571, Precision: 0.8234, Recall: 0.7857, F1: 0.7976
Testing Loss: 0.7903, Accuracy: 0.8732, Precision: 0.8444, Recall: 0.7956, F1: 0.8103
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1727, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 53/70
Train Loss: 0.0498, Accuracy: 0.9687, Precision: 0.9492, Recall: 0.9482, F1: 0.9487
Validation Loss: 0.9430, Accuracy: 0.8742, Precision: 0.8454, Recall: 0.8381, F1: 0.8415
Testing Loss: 0.8943, Accuracy: 0.8756, Precision: 0.8414, Recall: 0.8251, F1: 0.8326
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1334, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 54/70
Train Loss: 0.0437, Accuracy: 0.9713, Precision: 0.9505, Recall: 0.9528, F1: 0.9516
Validation Loss: 0.9990, Accuracy: 0.8721, Precision: 0.8333, Recall: 0.8087, F1: 0.8154
Testing Loss: 0.9609, Accuracy: 0.8756, Precision: 0.8458, Recall: 0.8096, F1: 0.8201
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1384, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 55/70
Train Loss: 0.0470, Accuracy: 0.9668, Precision: 0.9449, Recall: 0.9413, F1: 0.9430
Validation Loss: 0.9480, Accuracy: 0.8721, Precision: 0.8398, Recall: 0.8262, F1: 0.8319
Testing Loss: 0.9412, Accuracy: 0.8696, Precision: 0.8360, Recall: 0.8133, F1: 0.8227
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1908, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 56/70
Train Loss: 0.0424, Accuracy: 0.9694, Precision: 0.9466, Recall: 0.9542, F1: 0.9501
Validation Loss: 1.0172, Accuracy: 0.8699, Precision: 0.8423, Recall: 0.8097, F1: 0.8227
Testing Loss: 0.9591, Accuracy: 0.8732, Precision: 0.8463, Recall: 0.8022, F1: 0.8183
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1963, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 57/70
Train Loss: 0.0416, Accuracy: 0.9723, Precision: 0.9566, Recall: 0.9502, F1: 0.9533
Validation Loss: 1.0042, Accuracy: 0.8806, Precision: 0.8541, Recall: 0.8387, F1: 0.8454
Testing Loss: 0.9385, Accuracy: 0.8708, Precision: 0.8376, Recall: 0.8235, F1: 0.8302
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1145, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 58/70
Train Loss: 0.0423, Accuracy: 0.9701, Precision: 0.9503, Recall: 0.9485, F1: 0.9494
Validation Loss: 1.0834, Accuracy: 0.8699, Precision: 0.8517, Recall: 0.8152, F1: 0.8305
Testing Loss: 1.0127, Accuracy: 0.8720, Precision: 0.8455, Recall: 0.8164, F1: 0.8293
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1797, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917
Epoch 59/70
Train Loss: 0.0699, Accuracy: 0.9628, Precision: 0.9414, Recall: 0.9368, F1: 0.9390
Validation Loss: 0.9216, Accuracy: 0.8657, Precision: 0.8377, Recall: 0.8173, F1: 0.8245
Testing Loss: 0.8458, Accuracy: 0.8696, Precision: 0.8343, Recall: 0.8043, F1: 0.8171
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1851, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8377
Epoch 60/70
Train Loss: 0.0658, Accuracy: 0.9637, Precision: 0.9368, Recall: 0.9497, F1: 0.9426
Validation Loss: 0.8068, Accuracy: 0.8763, Precision: 0.8373, Recall: 0.8363, F1: 0.8361
Testing Loss: 0.7886, Accuracy: 0.8744, Precision: 0.8368, Recall: 0.8330, F1: 0.8345
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1255, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 61/70
Train Loss: 0.0592, Accuracy: 0.9682, Precision: 0.9500, Recall: 0.9417, F1: 0.9455
Validation Loss: 0.8538, Accuracy: 0.8529, Precision: 0.8077, Recall: 0.8250, F1: 0.8156
Testing Loss: 0.7186, Accuracy: 0.8684, Precision: 0.8256, Recall: 0.8250, F1: 0.8227
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2575, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 62/70
Train Loss: 0.0542, Accuracy: 0.9685, Precision: 0.9495, Recall: 0.9426, F1: 0.9459
Validation Loss: 1.0106, Accuracy: 0.8571, Precision: 0.8261, Recall: 0.8133, F1: 0.8190
Testing Loss: 0.7862, Accuracy: 0.8780, Precision: 0.8469, Recall: 0.8330, F1: 0.8392
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 3, 4, 0, 0, 1, 2, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3709, Accuracy: 0.8571, Precision: 0.7582, Recall: 0.7143, F1: 0.7147
Epoch 63/70
Train Loss: 0.0491, Accuracy: 0.9692, Precision: 0.9522, Recall: 0.9444, F1: 0.9481
Validation Loss: 1.0498, Accuracy: 0.8699, Precision: 0.8423, Recall: 0.8182, F1: 0.8283
Testing Loss: 0.8963, Accuracy: 0.8720, Precision: 0.8370, Recall: 0.8187, F1: 0.8267
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1708, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8143, F1: 0.8286
Epoch 64/70
Train Loss: 0.0413, Accuracy: 0.9727, Precision: 0.9550, Recall: 0.9538, F1: 0.9544
Validation Loss: 1.0062, Accuracy: 0.8678, Precision: 0.8354, Recall: 0.8066, F1: 0.8174
Testing Loss: 0.9324, Accuracy: 0.8756, Precision: 0.8514, Recall: 0.8113, F1: 0.8248
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1298, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 65/70
Train Loss: 0.0515, Accuracy: 0.9675, Precision: 0.9514, Recall: 0.9426, F1: 0.9467
Validation Loss: 1.0550, Accuracy: 0.8593, Precision: 0.8265, Recall: 0.8155, F1: 0.8200
Testing Loss: 0.8894, Accuracy: 0.8732, Precision: 0.8378, Recall: 0.8234, F1: 0.8299
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1465, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9132
Epoch 66/70
Train Loss: 0.0426, Accuracy: 0.9706, Precision: 0.9460, Recall: 0.9606, F1: 0.9529
Validation Loss: 1.0733, Accuracy: 0.8635, Precision: 0.8341, Recall: 0.8023, F1: 0.8150
Testing Loss: 0.9136, Accuracy: 0.8756, Precision: 0.8602, Recall: 0.7985, F1: 0.8180
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2343, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 67/70
Train Loss: 0.0474, Accuracy: 0.9687, Precision: 0.9514, Recall: 0.9408, F1: 0.9458
Validation Loss: 0.9957, Accuracy: 0.8593, Precision: 0.8220, Recall: 0.7983, F1: 0.8080
Testing Loss: 0.8540, Accuracy: 0.8720, Precision: 0.8415, Recall: 0.8099, F1: 0.8229
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2285, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 68/70
Train Loss: 0.0633, Accuracy: 0.9647, Precision: 0.9454, Recall: 0.9350, F1: 0.9397
Validation Loss: 0.9586, Accuracy: 0.8614, Precision: 0.8344, Recall: 0.8124, F1: 0.8222
Testing Loss: 0.8375, Accuracy: 0.8708, Precision: 0.8462, Recall: 0.8153, F1: 0.8289
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1419, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 69/70
Train Loss: 0.0599, Accuracy: 0.9699, Precision: 0.9505, Recall: 0.9445, F1: 0.9474
Validation Loss: 1.0973, Accuracy: 0.8593, Precision: 0.8267, Recall: 0.8044, F1: 0.8140
Testing Loss: 0.9084, Accuracy: 0.8684, Precision: 0.8362, Recall: 0.8111, F1: 0.8220
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1352, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 70/70
Train Loss: 0.0586, Accuracy: 0.9668, Precision: 0.9437, Recall: 0.9525, F1: 0.9479
Validation Loss: 0.9095, Accuracy: 0.8678, Precision: 0.8311, Recall: 0.8092, F1: 0.8171
Testing Loss: 0.8469, Accuracy: 0.8768, Precision: 0.8557, Recall: 0.8139, F1: 0.8297
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1832, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917

