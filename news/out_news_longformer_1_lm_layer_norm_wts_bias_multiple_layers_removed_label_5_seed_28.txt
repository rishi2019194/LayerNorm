Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.2168, Accuracy: 0.5281, Precision: 0.4257, Recall: 0.4008, F1: 0.3895
Validation Loss: 0.8296, Accuracy: 0.7377, Precision: 0.6243, Recall: 0.5847, F1: 0.5859
Testing Loss: 0.7952, Accuracy: 0.7258, Precision: 0.5897, Recall: 0.5755, F1: 0.5629
LM Predictions:  [0, 3, 0, 3, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7981, Accuracy: 0.1905, Precision: 0.1242, Recall: 0.2400, F1: 0.1310
Epoch 2/70
Train Loss: 0.6810, Accuracy: 0.7761, Precision: 0.6589, Recall: 0.6570, F1: 0.6443
Validation Loss: 0.6447, Accuracy: 0.7889, Precision: 0.7267, Recall: 0.6815, F1: 0.6892
Testing Loss: 0.5925, Accuracy: 0.8068, Precision: 0.7610, Recall: 0.6987, F1: 0.7096
LM Predictions:  [0, 3, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3573, Accuracy: 0.1190, Precision: 0.1048, Recall: 0.1500, F1: 0.0648
Epoch 3/70
Train Loss: 0.5332, Accuracy: 0.8442, Precision: 0.7853, Recall: 0.7627, F1: 0.7713
Validation Loss: 0.6983, Accuracy: 0.8145, Precision: 0.7022, Recall: 0.6916, F1: 0.6915
Testing Loss: 0.5748, Accuracy: 0.8454, Precision: 0.8930, Recall: 0.7285, F1: 0.7275
LM Predictions:  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.8948, Accuracy: 0.1667, Precision: 0.3263, Recall: 0.2400, F1: 0.1162
Epoch 4/70
Train Loss: 0.4851, Accuracy: 0.8537, Precision: 0.8002, Recall: 0.7750, F1: 0.7840
Validation Loss: 0.5342, Accuracy: 0.8529, Precision: 0.8165, Recall: 0.8085, F1: 0.8109
Testing Loss: 0.4239, Accuracy: 0.8744, Precision: 0.8414, Recall: 0.8453, F1: 0.8417
LM Predictions:  [0, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0658, Accuracy: 0.0952, Precision: 0.2083, Recall: 0.1167, F1: 0.0891
Epoch 5/70
Train Loss: 0.4164, Accuracy: 0.8717, Precision: 0.8205, Recall: 0.8108, F1: 0.8150
Validation Loss: 0.5652, Accuracy: 0.8550, Precision: 0.8158, Recall: 0.7934, F1: 0.8034
Testing Loss: 0.4456, Accuracy: 0.8708, Precision: 0.8421, Recall: 0.8098, F1: 0.8214
LM Predictions:  [0, 3, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8747, Accuracy: 0.0952, Precision: 0.1905, Recall: 0.1167, F1: 0.0688
Epoch 6/70
Train Loss: 0.3882, Accuracy: 0.8852, Precision: 0.8408, Recall: 0.8339, F1: 0.8368
Validation Loss: 0.5681, Accuracy: 0.8550, Precision: 0.8247, Recall: 0.8377, F1: 0.8253
Testing Loss: 0.4137, Accuracy: 0.8720, Precision: 0.8476, Recall: 0.8497, F1: 0.8422
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2953, Accuracy: 0.0238, Precision: 0.0833, Recall: 0.0167, F1: 0.0278
Epoch 7/70
Train Loss: 0.3690, Accuracy: 0.8874, Precision: 0.8437, Recall: 0.8421, F1: 0.8426
Validation Loss: 0.5248, Accuracy: 0.8614, Precision: 0.8373, Recall: 0.8160, F1: 0.8223
Testing Loss: 0.3846, Accuracy: 0.8877, Precision: 0.8615, Recall: 0.8325, F1: 0.8429
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 2, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2221, Accuracy: 0.0952, Precision: 0.1917, Recall: 0.1208, F1: 0.0770
Epoch 8/70
Train Loss: 0.3344, Accuracy: 0.8949, Precision: 0.8474, Recall: 0.8445, F1: 0.8457
Validation Loss: 0.5661, Accuracy: 0.8507, Precision: 0.8135, Recall: 0.7878, F1: 0.7949
Testing Loss: 0.3854, Accuracy: 0.8865, Precision: 0.8733, Recall: 0.8246, F1: 0.8385
LM Predictions:  [0, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8388, Accuracy: 0.2143, Precision: 0.4222, Recall: 0.2083, F1: 0.2174
Epoch 9/70
Train Loss: 0.3091, Accuracy: 0.9066, Precision: 0.8670, Recall: 0.8614, F1: 0.8640
Validation Loss: 0.5564, Accuracy: 0.8571, Precision: 0.8565, Recall: 0.7867, F1: 0.7897
Testing Loss: 0.4395, Accuracy: 0.8816, Precision: 0.8859, Recall: 0.7928, F1: 0.7912
LM Predictions:  [0, 3, 5, 0, 4, 0, 5, 0, 0, 5, 0, 0, 2, 0, 5, 0, 5, 4, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7343, Accuracy: 0.1905, Precision: 0.3950, Recall: 0.1875, F1: 0.1696
Epoch 10/70
Train Loss: 0.2933, Accuracy: 0.9123, Precision: 0.8765, Recall: 0.8685, F1: 0.8720
Validation Loss: 0.6526, Accuracy: 0.8422, Precision: 0.8873, Recall: 0.7572, F1: 0.7683
Testing Loss: 0.4951, Accuracy: 0.8611, Precision: 0.8429, Recall: 0.7772, F1: 0.7846
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 5, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3111, Accuracy: 0.1667, Precision: 0.5230, Recall: 0.1875, F1: 0.1369
Epoch 11/70
Train Loss: 0.2859, Accuracy: 0.9130, Precision: 0.8747, Recall: 0.8692, F1: 0.8718
Validation Loss: 0.5610, Accuracy: 0.8742, Precision: 0.8399, Recall: 0.8544, F1: 0.8466
Testing Loss: 0.4222, Accuracy: 0.8877, Precision: 0.8556, Recall: 0.8641, F1: 0.8583
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 3, 0, 5, 2, 0, 0, 0, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9135, Accuracy: 0.1667, Precision: 0.4667, Recall: 0.1708, F1: 0.1859
Epoch 12/70
Train Loss: 0.2627, Accuracy: 0.9196, Precision: 0.8801, Recall: 0.8801, F1: 0.8798
Validation Loss: 0.5727, Accuracy: 0.8763, Precision: 0.8417, Recall: 0.8650, F1: 0.8512
Testing Loss: 0.4714, Accuracy: 0.8804, Precision: 0.8490, Recall: 0.8668, F1: 0.8550
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 3, 5, 5, 2, 5, 5, 0, 5, 5, 5, 0, 4, 5, 0, 5, 5, 4, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8029, Accuracy: 0.1905, Precision: 0.4881, Recall: 0.1875, F1: 0.2278
Epoch 13/70
Train Loss: 0.2421, Accuracy: 0.9239, Precision: 0.8851, Recall: 0.8815, F1: 0.8832
Validation Loss: 0.5869, Accuracy: 0.8678, Precision: 0.8275, Recall: 0.8206, F1: 0.8228
Testing Loss: 0.4838, Accuracy: 0.8841, Precision: 0.8567, Recall: 0.8382, F1: 0.8455
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 2, 5, 3, 5, 5, 5, 0, 3, 5, 0, 2, 5, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 4, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7630, Accuracy: 0.1905, Precision: 0.4551, Recall: 0.1875, F1: 0.2000
Epoch 14/70
Train Loss: 0.2692, Accuracy: 0.9189, Precision: 0.8866, Recall: 0.8757, F1: 0.8809
Validation Loss: 0.6487, Accuracy: 0.8401, Precision: 0.8215, Recall: 0.7901, F1: 0.8008
Testing Loss: 0.4484, Accuracy: 0.8792, Precision: 0.8614, Recall: 0.8235, F1: 0.8369
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 0, 5, 2, 0, 3, 3, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 5, 3, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7132, Accuracy: 0.2381, Precision: 0.4524, Recall: 0.2250, F1: 0.2337
Epoch 15/70
Train Loss: 0.2433, Accuracy: 0.9222, Precision: 0.8829, Recall: 0.8807, F1: 0.8816
Validation Loss: 0.5767, Accuracy: 0.8550, Precision: 0.8715, Recall: 0.7857, F1: 0.8065
Testing Loss: 0.4980, Accuracy: 0.8575, Precision: 0.8501, Recall: 0.7696, F1: 0.7876
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 5, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5598, Accuracy: 0.1905, Precision: 0.5238, Recall: 0.2042, F1: 0.1633
Epoch 16/70
Train Loss: 0.2199, Accuracy: 0.9298, Precision: 0.8945, Recall: 0.8873, F1: 0.8908
Validation Loss: 0.6172, Accuracy: 0.8657, Precision: 0.8290, Recall: 0.8293, F1: 0.8264
Testing Loss: 0.4839, Accuracy: 0.8816, Precision: 0.8476, Recall: 0.8604, F1: 0.8521
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 3, 3, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6745, Accuracy: 0.2381, Precision: 0.5714, Recall: 0.2250, F1: 0.2755
Epoch 17/70
Train Loss: 0.2049, Accuracy: 0.9346, Precision: 0.8964, Recall: 0.9063, F1: 0.9005
Validation Loss: 0.6496, Accuracy: 0.8635, Precision: 0.8644, Recall: 0.7922, F1: 0.8120
Testing Loss: 0.5227, Accuracy: 0.8720, Precision: 0.8617, Recall: 0.7886, F1: 0.8035
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 5, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 5, 3, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4568, Accuracy: 0.2381, Precision: 0.4284, Recall: 0.2417, F1: 0.2053
Epoch 18/70
Train Loss: 0.2079, Accuracy: 0.9334, Precision: 0.9007, Recall: 0.9010, F1: 0.9005
Validation Loss: 0.7065, Accuracy: 0.8571, Precision: 0.8962, Recall: 0.7543, F1: 0.7550
Testing Loss: 0.5954, Accuracy: 0.8599, Precision: 0.8509, Recall: 0.7568, F1: 0.7616
LM Predictions:  [0, 5, 5, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 5, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 0, 3, 0, 0, 4, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4420, Accuracy: 0.2857, Precision: 0.4464, Recall: 0.2875, F1: 0.2377
Epoch 19/70
Train Loss: 0.2114, Accuracy: 0.9312, Precision: 0.8937, Recall: 0.8864, F1: 0.8899
Validation Loss: 0.6881, Accuracy: 0.8571, Precision: 0.8292, Recall: 0.8062, F1: 0.8147
Testing Loss: 0.5172, Accuracy: 0.8756, Precision: 0.8456, Recall: 0.8358, F1: 0.8398
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 4, 0, 3, 5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0439, Accuracy: 0.2381, Precision: 0.6333, Recall: 0.2227, F1: 0.2658
Epoch 20/70
Train Loss: 0.2128, Accuracy: 0.9376, Precision: 0.9017, Recall: 0.9016, F1: 0.9015
Validation Loss: 0.5606, Accuracy: 0.8635, Precision: 0.8246, Recall: 0.8479, F1: 0.8319
Testing Loss: 0.4247, Accuracy: 0.8792, Precision: 0.8430, Recall: 0.8570, F1: 0.8475
LM Predictions:  [5, 1, 5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 2, 5, 3, 5, 5, 4, 5, 3, 3, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 0, 5, 2, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6068, Accuracy: 0.2857, Precision: 0.6944, Recall: 0.2435, F1: 0.3402
Epoch 21/70
Train Loss: 0.1946, Accuracy: 0.9367, Precision: 0.9008, Recall: 0.9065, F1: 0.9034
Validation Loss: 0.6574, Accuracy: 0.8571, Precision: 0.8385, Recall: 0.7845, F1: 0.7979
Testing Loss: 0.5218, Accuracy: 0.8756, Precision: 0.8587, Recall: 0.7901, F1: 0.8046
LM Predictions:  [0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 5, 0, 4, 3, 3, 5, 0, 0, 0, 0, 2, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1066, Accuracy: 0.3333, Precision: 0.4734, Recall: 0.3083, F1: 0.2888
Epoch 22/70
Train Loss: 0.1901, Accuracy: 0.9426, Precision: 0.9107, Recall: 0.9060, F1: 0.9083
Validation Loss: 0.6859, Accuracy: 0.8593, Precision: 0.8256, Recall: 0.7896, F1: 0.8037
Testing Loss: 0.5518, Accuracy: 0.8696, Precision: 0.8543, Recall: 0.7929, F1: 0.8119
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 0, 4, 5, 0, 0, 2, 0, 3, 0, 5, 0, 0, 3, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 3, 3, 5, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5024, Accuracy: 0.2619, Precision: 0.4672, Recall: 0.2417, F1: 0.2484
Epoch 23/70
Train Loss: 0.1967, Accuracy: 0.9395, Precision: 0.9034, Recall: 0.9063, F1: 0.9045
Validation Loss: 0.6525, Accuracy: 0.8443, Precision: 0.8765, Recall: 0.7533, F1: 0.7587
Testing Loss: 0.5394, Accuracy: 0.8575, Precision: 0.8472, Recall: 0.7573, F1: 0.7604
LM Predictions:  [0, 5, 5, 0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 1, 0, 4, 5, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5595, Accuracy: 0.3095, Precision: 0.5309, Recall: 0.3083, F1: 0.2726
Epoch 24/70
Train Loss: 0.1879, Accuracy: 0.9433, Precision: 0.9113, Recall: 0.9043, F1: 0.9077
Validation Loss: 0.6685, Accuracy: 0.8593, Precision: 0.8382, Recall: 0.7910, F1: 0.8090
Testing Loss: 0.5135, Accuracy: 0.8696, Precision: 0.8422, Recall: 0.7907, F1: 0.8107
LM Predictions:  [0, 5, 1, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 5, 3, 0, 2, 5, 0, 3, 0, 0, 2, 0, 0, 0, 0, 5, 1, 0, 4, 5, 3, 5, 0, 0, 0, 0, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4523, Accuracy: 0.2381, Precision: 0.4833, Recall: 0.2292, F1: 0.2306
Epoch 25/70
Train Loss: 0.2356, Accuracy: 0.9232, Precision: 0.8847, Recall: 0.8881, F1: 0.8856
Validation Loss: 0.6804, Accuracy: 0.8294, Precision: 0.7817, Recall: 0.8104, F1: 0.7914
Testing Loss: 0.5998, Accuracy: 0.8514, Precision: 0.8226, Recall: 0.8312, F1: 0.8213
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 4, 0, 3, 5, 5, 2, 5, 5, 5, 5, 5, 2, 0, 4, 5, 2, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6552, Accuracy: 0.2381, Precision: 0.5000, Recall: 0.2250, F1: 0.2789
Epoch 26/70
Train Loss: 0.2169, Accuracy: 0.9329, Precision: 0.8960, Recall: 0.8992, F1: 0.8975
Validation Loss: 0.6416, Accuracy: 0.8443, Precision: 0.7873, Recall: 0.8056, F1: 0.7954
Testing Loss: 0.5551, Accuracy: 0.8708, Precision: 0.8309, Recall: 0.8529, F1: 0.8395
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 4, 4, 5, 4, 0, 3, 4, 3, 5, 5, 4, 5, 3, 3, 4, 2, 5, 0, 5, 4, 4, 2, 0, 4, 5, 3, 5, 5, 4, 5, 4, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0023, Accuracy: 0.4524, Precision: 0.4960, Recall: 0.3750, F1: 0.4082
Epoch 27/70
Train Loss: 0.1750, Accuracy: 0.9438, Precision: 0.9113, Recall: 0.9097, F1: 0.9104
Validation Loss: 0.7427, Accuracy: 0.8486, Precision: 0.8323, Recall: 0.7970, F1: 0.8070
Testing Loss: 0.5905, Accuracy: 0.8611, Precision: 0.8431, Recall: 0.8209, F1: 0.8283
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 0, 3, 0, 5, 4, 0, 3, 0, 5, 2, 5, 0, 5, 0, 5, 2, 0, 4, 5, 3, 5, 0, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5519, Accuracy: 0.2381, Precision: 0.4778, Recall: 0.2250, F1: 0.2431
Epoch 28/70
Train Loss: 0.1819, Accuracy: 0.9421, Precision: 0.9108, Recall: 0.9135, F1: 0.9119
Validation Loss: 0.6072, Accuracy: 0.8550, Precision: 0.8233, Recall: 0.7932, F1: 0.8047
Testing Loss: 0.5264, Accuracy: 0.8671, Precision: 0.8335, Recall: 0.7868, F1: 0.8007
LM Predictions:  [0, 5, 2, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 4, 3, 0, 5, 4, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 1, 0, 4, 3, 3, 5, 0, 0, 0, 0, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6259, Accuracy: 0.4048, Precision: 0.5000, Recall: 0.3792, F1: 0.3563
Epoch 29/70
Train Loss: 0.1613, Accuracy: 0.9469, Precision: 0.9162, Recall: 0.9139, F1: 0.9150
Validation Loss: 0.6888, Accuracy: 0.8571, Precision: 0.8270, Recall: 0.7980, F1: 0.8099
Testing Loss: 0.6279, Accuracy: 0.8539, Precision: 0.8131, Recall: 0.7774, F1: 0.7901
LM Predictions:  [0, 5, 1, 0, 5, 5, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 4, 3, 3, 5, 0, 0, 0, 0, 5, 1, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2560, Accuracy: 0.2857, Precision: 0.4827, Recall: 0.2708, F1: 0.2400
Epoch 30/70
Train Loss: 0.1648, Accuracy: 0.9469, Precision: 0.9163, Recall: 0.9148, F1: 0.9155
Validation Loss: 0.6998, Accuracy: 0.8550, Precision: 0.8356, Recall: 0.7814, F1: 0.8026
Testing Loss: 0.5722, Accuracy: 0.8502, Precision: 0.8283, Recall: 0.7603, F1: 0.7801
LM Predictions:  [0, 5, 1, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 4, 3, 0, 5, 4, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 4, 3, 3, 5, 0, 0, 0, 0, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3912, Accuracy: 0.3810, Precision: 0.5119, Recall: 0.3583, F1: 0.3302
Epoch 31/70
Train Loss: 0.1619, Accuracy: 0.9464, Precision: 0.9166, Recall: 0.9214, F1: 0.9188
Validation Loss: 0.6969, Accuracy: 0.8507, Precision: 0.8053, Recall: 0.7907, F1: 0.7972
Testing Loss: 0.5559, Accuracy: 0.8732, Precision: 0.8365, Recall: 0.8273, F1: 0.8317
LM Predictions:  [0, 5, 2, 0, 5, 5, 5, 4, 4, 5, 0, 0, 2, 4, 3, 5, 5, 4, 0, 3, 3, 0, 2, 5, 0, 0, 4, 4, 1, 0, 4, 3, 3, 5, 0, 4, 0, 0, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9955, Accuracy: 0.4762, Precision: 0.5083, Recall: 0.4000, F1: 0.4206
Epoch 32/70
Train Loss: 0.1453, Accuracy: 0.9493, Precision: 0.9171, Recall: 0.9253, F1: 0.9208
Validation Loss: 0.6878, Accuracy: 0.8635, Precision: 0.8262, Recall: 0.7984, F1: 0.8088
Testing Loss: 0.5762, Accuracy: 0.8635, Precision: 0.8233, Recall: 0.7865, F1: 0.8001
LM Predictions:  [0, 5, 2, 5, 5, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 5, 2, 4, 0, 3, 3, 0, 2, 0, 0, 0, 0, 0, 3, 0, 4, 3, 3, 2, 0, 0, 0, 0, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9241, Accuracy: 0.4524, Precision: 0.4923, Recall: 0.4250, F1: 0.3839
Epoch 33/70
Train Loss: 0.1404, Accuracy: 0.9519, Precision: 0.9288, Recall: 0.9220, F1: 0.9253
Validation Loss: 0.6759, Accuracy: 0.8550, Precision: 0.8084, Recall: 0.8125, F1: 0.8091
Testing Loss: 0.6116, Accuracy: 0.8563, Precision: 0.8222, Recall: 0.8106, F1: 0.8152
LM Predictions:  [0, 5, 2, 0, 5, 5, 5, 4, 4, 5, 4, 0, 2, 4, 3, 5, 5, 4, 5, 3, 3, 4, 2, 0, 0, 5, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8198, Accuracy: 0.5714, Precision: 0.5235, Recall: 0.4750, F1: 0.4801
Epoch 34/70
Train Loss: 0.1606, Accuracy: 0.9438, Precision: 0.9142, Recall: 0.9085, F1: 0.9112
Validation Loss: 0.7911, Accuracy: 0.8507, Precision: 0.8144, Recall: 0.7906, F1: 0.7998
Testing Loss: 0.6078, Accuracy: 0.8599, Precision: 0.8276, Recall: 0.8101, F1: 0.8176
LM Predictions:  [0, 2, 2, 0, 2, 5, 5, 5, 4, 5, 5, 0, 2, 5, 3, 5, 2, 4, 5, 3, 3, 5, 3, 5, 0, 5, 5, 5, 1, 0, 4, 3, 3, 2, 0, 0, 0, 5, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7885, Accuracy: 0.4048, Precision: 0.5149, Recall: 0.3542, F1: 0.3900
Epoch 35/70
Train Loss: 0.1470, Accuracy: 0.9521, Precision: 0.9251, Recall: 0.9309, F1: 0.9275
Validation Loss: 0.7324, Accuracy: 0.8401, Precision: 0.8002, Recall: 0.7669, F1: 0.7791
Testing Loss: 0.6223, Accuracy: 0.8563, Precision: 0.8325, Recall: 0.7787, F1: 0.7971
LM Predictions:  [0, 1, 1, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 1, 0, 4, 3, 3, 5, 5, 0, 5, 0, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8547, Accuracy: 0.4286, Precision: 0.5855, Recall: 0.3977, F1: 0.3879
Epoch 36/70
Train Loss: 0.1394, Accuracy: 0.9519, Precision: 0.9236, Recall: 0.9276, F1: 0.9254
Validation Loss: 0.8004, Accuracy: 0.8443, Precision: 0.8005, Recall: 0.7772, F1: 0.7860
Testing Loss: 0.6962, Accuracy: 0.8587, Precision: 0.8130, Recall: 0.7771, F1: 0.7897
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 0, 4, 5, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 1, 0, 4, 3, 3, 2, 0, 0, 0, 0, 5, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5494, Accuracy: 0.4524, Precision: 0.5889, Recall: 0.4060, F1: 0.4112
Epoch 37/70
Train Loss: 0.1342, Accuracy: 0.9512, Precision: 0.9228, Recall: 0.9238, F1: 0.9233
Validation Loss: 0.7604, Accuracy: 0.8145, Precision: 0.7825, Recall: 0.7718, F1: 0.7704
Testing Loss: 0.5808, Accuracy: 0.8490, Precision: 0.8134, Recall: 0.8062, F1: 0.8079
LM Predictions:  [0, 1, 1, 5, 5, 5, 5, 4, 4, 5, 4, 0, 2, 4, 3, 5, 5, 4, 0, 3, 3, 4, 3, 5, 0, 5, 4, 4, 1, 0, 4, 3, 3, 5, 5, 4, 5, 4, 5, 4, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0235, Accuracy: 0.5238, Precision: 0.6278, Recall: 0.4269, F1: 0.4710
Epoch 38/70
Train Loss: 0.1473, Accuracy: 0.9490, Precision: 0.9212, Recall: 0.9222, F1: 0.9217
Validation Loss: 0.9296, Accuracy: 0.8316, Precision: 0.7807, Recall: 0.7738, F1: 0.7724
Testing Loss: 0.7054, Accuracy: 0.8599, Precision: 0.8035, Recall: 0.7986, F1: 0.7985
LM Predictions:  [0, 1, 5, 3, 5, 5, 0, 4, 4, 0, 4, 0, 2, 4, 3, 5, 3, 4, 0, 3, 3, 4, 3, 5, 3, 0, 4, 4, 1, 0, 4, 3, 3, 3, 5, 4, 5, 4, 3, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5707, Accuracy: 0.5952, Precision: 0.6178, Recall: 0.5102, F1: 0.5006
Epoch 39/70
Train Loss: 0.1637, Accuracy: 0.9407, Precision: 0.9106, Recall: 0.9084, F1: 0.9094
Validation Loss: 0.8237, Accuracy: 0.8017, Precision: 0.7738, Recall: 0.7368, F1: 0.7516
Testing Loss: 0.7272, Accuracy: 0.8128, Precision: 0.7790, Recall: 0.7401, F1: 0.7553
LM Predictions:  [0, 2, 2, 0, 2, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 2, 0, 2, 5, 0, 0, 0, 0, 2, 0, 4, 2, 2, 2, 5, 0, 5, 0, 2, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7292, Accuracy: 0.4048, Precision: 0.4630, Recall: 0.3958, F1: 0.3110
Epoch 40/70
Train Loss: 0.1674, Accuracy: 0.9459, Precision: 0.9207, Recall: 0.9240, F1: 0.9219
Validation Loss: 0.7923, Accuracy: 0.8358, Precision: 0.7842, Recall: 0.7434, F1: 0.7588
Testing Loss: 0.6761, Accuracy: 0.8575, Precision: 0.8258, Recall: 0.7754, F1: 0.7883
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 0, 0, 0, 4, 0, 2, 0, 4, 3, 3, 5, 0, 0, 0, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4900, Accuracy: 0.5000, Precision: 0.7045, Recall: 0.4602, F1: 0.4443
Epoch 41/70
Train Loss: 0.1352, Accuracy: 0.9514, Precision: 0.9311, Recall: 0.9188, F1: 0.9246
Validation Loss: 0.7548, Accuracy: 0.8422, Precision: 0.7888, Recall: 0.7931, F1: 0.7878
Testing Loss: 0.6678, Accuracy: 0.8527, Precision: 0.7956, Recall: 0.8006, F1: 0.7976
LM Predictions:  [0, 2, 2, 0, 5, 5, 0, 4, 4, 5, 0, 0, 2, 4, 3, 5, 5, 4, 0, 3, 3, 4, 3, 5, 0, 5, 4, 4, 2, 0, 4, 3, 3, 5, 5, 4, 5, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2918, Accuracy: 0.6190, Precision: 0.5556, Recall: 0.5208, F1: 0.5255
Epoch 42/70
Train Loss: 0.1134, Accuracy: 0.9599, Precision: 0.9334, Recall: 0.9449, F1: 0.9385
Validation Loss: 0.8522, Accuracy: 0.8465, Precision: 0.8032, Recall: 0.7773, F1: 0.7861
Testing Loss: 0.7513, Accuracy: 0.8623, Precision: 0.8157, Recall: 0.7885, F1: 0.7955
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 4, 4, 5, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 5, 0, 0, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 5, 4, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0824, Accuracy: 0.7143, Precision: 0.6902, Recall: 0.5995, F1: 0.5851
Epoch 43/70
Train Loss: 0.1223, Accuracy: 0.9568, Precision: 0.9340, Recall: 0.9327, F1: 0.9333
Validation Loss: 0.7899, Accuracy: 0.8486, Precision: 0.8090, Recall: 0.7846, F1: 0.7949
Testing Loss: 0.6590, Accuracy: 0.8539, Precision: 0.8171, Recall: 0.7786, F1: 0.7924
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 0, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 3, 5, 0, 0, 5, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2846, Accuracy: 0.5476, Precision: 0.6867, Recall: 0.4954, F1: 0.4836
Epoch 44/70
Train Loss: 0.1179, Accuracy: 0.9559, Precision: 0.9295, Recall: 0.9326, F1: 0.9310
Validation Loss: 0.8419, Accuracy: 0.8443, Precision: 0.7803, Recall: 0.7601, F1: 0.7634
Testing Loss: 0.6812, Accuracy: 0.8635, Precision: 0.8347, Recall: 0.7773, F1: 0.7832
LM Predictions:  [0, 1, 1, 0, 5, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 0, 5, 4, 0, 3, 0, 4, 3, 0, 0, 4, 4, 4, 1, 0, 4, 3, 3, 5, 0, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4976, Accuracy: 0.6190, Precision: 0.6197, Recall: 0.5329, F1: 0.5113
Epoch 45/70
Train Loss: 0.1162, Accuracy: 0.9561, Precision: 0.9315, Recall: 0.9339, F1: 0.9326
Validation Loss: 0.8255, Accuracy: 0.8443, Precision: 0.8010, Recall: 0.7830, F1: 0.7906
Testing Loss: 0.6977, Accuracy: 0.8563, Precision: 0.8099, Recall: 0.7887, F1: 0.7957
LM Predictions:  [0, 5, 5, 0, 2, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 0, 5, 4, 0, 3, 0, 4, 3, 5, 0, 4, 4, 4, 2, 0, 4, 3, 3, 5, 0, 4, 5, 4, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3492, Accuracy: 0.5952, Precision: 0.6750, Recall: 0.5185, F1: 0.4966
Epoch 46/70
Train Loss: 0.1216, Accuracy: 0.9549, Precision: 0.9295, Recall: 0.9286, F1: 0.9290
Validation Loss: 0.8766, Accuracy: 0.8443, Precision: 0.8217, Recall: 0.7719, F1: 0.7905
Testing Loss: 0.8549, Accuracy: 0.8442, Precision: 0.8143, Recall: 0.7559, F1: 0.7739
LM Predictions:  [0, 2, 2, 0, 5, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 4, 3, 5, 0, 0, 4, 0, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3480, Accuracy: 0.5714, Precision: 0.6852, Recall: 0.5060, F1: 0.4841
Epoch 47/70
Train Loss: 0.1166, Accuracy: 0.9564, Precision: 0.9381, Recall: 0.9283, F1: 0.9329
Validation Loss: 0.8211, Accuracy: 0.8380, Precision: 0.7926, Recall: 0.7787, F1: 0.7850
Testing Loss: 0.7656, Accuracy: 0.8490, Precision: 0.8036, Recall: 0.7879, F1: 0.7933
LM Predictions:  [0, 1, 2, 0, 5, 1, 0, 4, 4, 5, 4, 0, 2, 4, 3, 0, 1, 4, 0, 3, 0, 4, 3, 5, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9554, Accuracy: 0.6667, Precision: 0.6328, Recall: 0.5597, F1: 0.5595
Epoch 48/70
Train Loss: 0.1143, Accuracy: 0.9530, Precision: 0.9270, Recall: 0.9262, F1: 0.9265
Validation Loss: 0.8260, Accuracy: 0.8422, Precision: 0.8009, Recall: 0.7795, F1: 0.7884
Testing Loss: 0.6758, Accuracy: 0.8575, Precision: 0.8063, Recall: 0.7801, F1: 0.7885
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1079, Accuracy: 0.5238, Precision: 0.6629, Recall: 0.4787, F1: 0.4493
Epoch 49/70
Train Loss: 0.1032, Accuracy: 0.9564, Precision: 0.9316, Recall: 0.9286, F1: 0.9301
Validation Loss: 0.8579, Accuracy: 0.8529, Precision: 0.8066, Recall: 0.7978, F1: 0.8020
Testing Loss: 0.7700, Accuracy: 0.8623, Precision: 0.8181, Recall: 0.8030, F1: 0.8079
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 5, 4, 0, 2, 4, 3, 0, 1, 4, 0, 3, 0, 4, 3, 5, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 0, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0755, Accuracy: 0.6667, Precision: 0.6328, Recall: 0.5597, F1: 0.5595
Epoch 50/70
Train Loss: 0.0949, Accuracy: 0.9616, Precision: 0.9409, Recall: 0.9375, F1: 0.9392
Validation Loss: 0.8468, Accuracy: 0.8571, Precision: 0.8185, Recall: 0.8009, F1: 0.8085
Testing Loss: 0.8000, Accuracy: 0.8635, Precision: 0.8196, Recall: 0.8036, F1: 0.8110
LM Predictions:  [0, 1, 2, 0, 5, 1, 0, 0, 4, 5, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 3, 2, 3, 0, 0, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9823, Accuracy: 0.5714, Precision: 0.6850, Recall: 0.4995, F1: 0.5016
Epoch 51/70
Train Loss: 0.1178, Accuracy: 0.9576, Precision: 0.9332, Recall: 0.9421, F1: 0.9374
Validation Loss: 0.8517, Accuracy: 0.8358, Precision: 0.8128, Recall: 0.7692, F1: 0.7863
Testing Loss: 0.8023, Accuracy: 0.8406, Precision: 0.8123, Recall: 0.7701, F1: 0.7871
LM Predictions:  [0, 5, 2, 0, 5, 1, 0, 0, 4, 5, 0, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 5, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4090, Accuracy: 0.4286, Precision: 0.6970, Recall: 0.3894, F1: 0.3990
Epoch 52/70
Train Loss: 0.1101, Accuracy: 0.9602, Precision: 0.9345, Recall: 0.9440, F1: 0.9390
Validation Loss: 0.9018, Accuracy: 0.8380, Precision: 0.7900, Recall: 0.7699, F1: 0.7779
Testing Loss: 0.7450, Accuracy: 0.8551, Precision: 0.8103, Recall: 0.7770, F1: 0.7850
LM Predictions:  [0, 1, 1, 0, 5, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 5, 4, 0, 3, 0, 4, 3, 0, 0, 0, 4, 4, 2, 0, 4, 3, 3, 5, 0, 4, 0, 4, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3648, Accuracy: 0.6190, Precision: 0.6632, Recall: 0.5370, F1: 0.5238
Epoch 53/70
Train Loss: 0.1311, Accuracy: 0.9533, Precision: 0.9297, Recall: 0.9195, F1: 0.9242
Validation Loss: 0.9225, Accuracy: 0.8422, Precision: 0.7973, Recall: 0.7744, F1: 0.7807
Testing Loss: 0.7431, Accuracy: 0.8514, Precision: 0.7964, Recall: 0.7803, F1: 0.7843
LM Predictions:  [0, 1, 2, 2, 1, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 3, 3, 4, 0, 3, 3, 4, 3, 1, 0, 4, 4, 4, 2, 0, 4, 3, 3, 3, 2, 4, 0, 4, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9264, Accuracy: 0.7619, Precision: 0.7767, Recall: 0.7739, F1: 0.7437
Epoch 54/70
Train Loss: 0.1080, Accuracy: 0.9590, Precision: 0.9409, Recall: 0.9292, F1: 0.9347
Validation Loss: 0.9136, Accuracy: 0.8294, Precision: 0.7726, Recall: 0.7697, F1: 0.7674
Testing Loss: 0.7843, Accuracy: 0.8563, Precision: 0.7981, Recall: 0.8020, F1: 0.7991
LM Predictions:  [0, 1, 1, 3, 1, 1, 0, 4, 4, 5, 4, 0, 2, 4, 3, 3, 1, 4, 0, 3, 3, 4, 3, 1, 3, 4, 4, 4, 2, 0, 4, 3, 3, 3, 3, 4, 5, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9791, Accuracy: 0.7381, Precision: 0.6690, Recall: 0.6093, F1: 0.6213
Epoch 55/70
Train Loss: 0.1006, Accuracy: 0.9597, Precision: 0.9393, Recall: 0.9363, F1: 0.9377
Validation Loss: 0.7769, Accuracy: 0.8529, Precision: 0.7999, Recall: 0.7834, F1: 0.7891
Testing Loss: 0.7032, Accuracy: 0.8647, Precision: 0.8162, Recall: 0.7893, F1: 0.7932
LM Predictions:  [0, 1, 2, 1, 2, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 4, 4, 4, 2, 0, 4, 3, 3, 2, 2, 4, 3, 4, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8281, Accuracy: 0.8095, Precision: 0.8273, Recall: 0.8239, F1: 0.7955
Epoch 56/70
Train Loss: 0.1203, Accuracy: 0.9547, Precision: 0.9297, Recall: 0.9311, F1: 0.9304
Validation Loss: 0.8868, Accuracy: 0.8507, Precision: 0.8171, Recall: 0.7841, F1: 0.7981
Testing Loss: 0.8160, Accuracy: 0.8599, Precision: 0.8198, Recall: 0.7811, F1: 0.7935
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 5, 0, 0, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 3, 4, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8827, Accuracy: 0.7381, Precision: 0.6782, Recall: 0.6329, F1: 0.5922
Epoch 57/70
Train Loss: 0.1035, Accuracy: 0.9623, Precision: 0.9376, Recall: 0.9427, F1: 0.9400
Validation Loss: 0.9069, Accuracy: 0.8465, Precision: 0.8122, Recall: 0.7769, F1: 0.7919
Testing Loss: 0.8050, Accuracy: 0.8551, Precision: 0.8187, Recall: 0.7734, F1: 0.7902
LM Predictions:  [0, 1, 2, 0, 2, 1, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 3, 2, 5, 0, 0, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9788, Accuracy: 0.5476, Precision: 0.6875, Recall: 0.4995, F1: 0.4750
Epoch 58/70
Train Loss: 0.1058, Accuracy: 0.9566, Precision: 0.9345, Recall: 0.9267, F1: 0.9305
Validation Loss: 0.8321, Accuracy: 0.8380, Precision: 0.7783, Recall: 0.7458, F1: 0.7477
Testing Loss: 0.7887, Accuracy: 0.8575, Precision: 0.8339, Recall: 0.7791, F1: 0.7839
LM Predictions:  [0, 3, 2, 0, 2, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 1, 4, 0, 3, 0, 4, 3, 1, 0, 0, 4, 4, 2, 0, 4, 3, 3, 1, 0, 4, 3, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0547, Accuracy: 0.6905, Precision: 0.7136, Recall: 0.7117, F1: 0.6730
Epoch 59/70
Train Loss: 0.1175, Accuracy: 0.9547, Precision: 0.9273, Recall: 0.9260, F1: 0.9265
Validation Loss: 0.9639, Accuracy: 0.8486, Precision: 0.8204, Recall: 0.8002, F1: 0.8061
Testing Loss: 0.8006, Accuracy: 0.8587, Precision: 0.8219, Recall: 0.8155, F1: 0.8178
LM Predictions:  [0, 3, 2, 0, 5, 1, 0, 4, 4, 5, 5, 0, 2, 4, 3, 0, 5, 4, 5, 0, 0, 4, 3, 5, 0, 5, 0, 4, 2, 5, 4, 3, 3, 5, 5, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2998, Accuracy: 0.5476, Precision: 0.6889, Recall: 0.4560, F1: 0.4903
Epoch 60/70
Train Loss: 0.2275, Accuracy: 0.9113, Precision: 0.9061, Recall: 0.9060, F1: 0.9059
Validation Loss: 0.9132, Accuracy: 0.8273, Precision: 0.8173, Recall: 0.7843, F1: 0.7880
Testing Loss: 0.7563, Accuracy: 0.8527, Precision: 0.8265, Recall: 0.8117, F1: 0.8139
LM Predictions:  [0, 1, 2, 0, 5, 1, 5, 4, 4, 5, 4, 0, 2, 4, 3, 0, 5, 4, 5, 0, 0, 4, 5, 5, 0, 4, 4, 4, 2, 5, 4, 3, 3, 5, 5, 4, 0, 4, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3434, Accuracy: 0.5000, Precision: 0.6204, Recall: 0.4037, F1: 0.4427
Epoch 61/70
Train Loss: 0.1237, Accuracy: 0.9568, Precision: 0.9332, Recall: 0.9428, F1: 0.9377
Validation Loss: 0.8593, Accuracy: 0.8337, Precision: 0.7972, Recall: 0.7713, F1: 0.7798
Testing Loss: 0.7608, Accuracy: 0.8502, Precision: 0.8311, Recall: 0.8081, F1: 0.8170
LM Predictions:  [0, 1, 2, 1, 5, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 3, 5, 4, 0, 3, 3, 4, 3, 5, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 1, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8369, Accuracy: 0.7619, Precision: 0.7097, Recall: 0.6449, F1: 0.6513
Epoch 62/70
Train Loss: 0.1120, Accuracy: 0.9557, Precision: 0.9297, Recall: 0.9377, F1: 0.9335
Validation Loss: 0.8489, Accuracy: 0.8358, Precision: 0.8074, Recall: 0.7760, F1: 0.7864
Testing Loss: 0.7108, Accuracy: 0.8599, Precision: 0.8356, Recall: 0.8080, F1: 0.8201
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 0, 4, 5, 0, 0, 2, 0, 3, 0, 2, 4, 5, 3, 0, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 3, 1, 1, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8576, Accuracy: 0.5714, Precision: 0.6759, Recall: 0.5009, F1: 0.5139
Epoch 63/70
Train Loss: 0.0950, Accuracy: 0.9649, Precision: 0.9454, Recall: 0.9490, F1: 0.9470
Validation Loss: 0.8558, Accuracy: 0.8443, Precision: 0.8125, Recall: 0.7733, F1: 0.7850
Testing Loss: 0.7413, Accuracy: 0.8563, Precision: 0.8210, Recall: 0.7814, F1: 0.7921
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 3, 1, 4, 0, 3, 0, 4, 3, 1, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6281, Accuracy: 0.8095, Precision: 0.8333, Recall: 0.8206, F1: 0.7988
Epoch 64/70
Train Loss: 0.0899, Accuracy: 0.9611, Precision: 0.9469, Recall: 0.9325, F1: 0.9389
Validation Loss: 0.8706, Accuracy: 0.8465, Precision: 0.7939, Recall: 0.7744, F1: 0.7817
Testing Loss: 0.8024, Accuracy: 0.8587, Precision: 0.8255, Recall: 0.7818, F1: 0.7886
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 0, 1, 4, 0, 3, 0, 4, 3, 0, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8858, Accuracy: 0.7143, Precision: 0.7769, Recall: 0.7339, F1: 0.7034
Epoch 65/70
Train Loss: 0.0909, Accuracy: 0.9602, Precision: 0.9406, Recall: 0.9425, F1: 0.9415
Validation Loss: 0.8381, Accuracy: 0.8529, Precision: 0.8235, Recall: 0.7929, F1: 0.8061
Testing Loss: 0.7306, Accuracy: 0.8527, Precision: 0.8240, Recall: 0.7792, F1: 0.7944
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 1, 4, 0, 3, 0, 4, 3, 1, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 1, 4, 5, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8024, Accuracy: 0.7619, Precision: 0.6856, Recall: 0.6486, F1: 0.6337
Epoch 66/70
Train Loss: 0.0849, Accuracy: 0.9640, Precision: 0.9447, Recall: 0.9430, F1: 0.9437
Validation Loss: 0.9286, Accuracy: 0.8422, Precision: 0.8002, Recall: 0.7844, F1: 0.7909
Testing Loss: 0.7982, Accuracy: 0.8563, Precision: 0.8283, Recall: 0.7935, F1: 0.8075
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9405, Accuracy: 0.6190, Precision: 0.8214, Recall: 0.6633, F1: 0.6423
Epoch 67/70
Train Loss: 0.0948, Accuracy: 0.9609, Precision: 0.9395, Recall: 0.9403, F1: 0.9399
Validation Loss: 0.8459, Accuracy: 0.8337, Precision: 0.8074, Recall: 0.7551, F1: 0.7737
Testing Loss: 0.7772, Accuracy: 0.8575, Precision: 0.8393, Recall: 0.7725, F1: 0.7885
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 0, 0, 4, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7952, Accuracy: 0.7619, Precision: 0.8532, Recall: 0.7789, F1: 0.7550
Epoch 68/70
Train Loss: 0.0924, Accuracy: 0.9611, Precision: 0.9468, Recall: 0.9324, F1: 0.9387
Validation Loss: 0.9481, Accuracy: 0.8422, Precision: 0.8103, Recall: 0.7851, F1: 0.7941
Testing Loss: 0.8632, Accuracy: 0.8502, Precision: 0.8067, Recall: 0.7892, F1: 0.7971
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 4, 0, 3, 4, 3, 1, 2, 4, 0, 3, 3, 4, 3, 1, 0, 4, 4, 4, 2, 0, 4, 3, 3, 1, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7903, Accuracy: 0.8333, Precision: 0.8445, Recall: 0.8428, F1: 0.8263
Epoch 69/70
Train Loss: 0.0934, Accuracy: 0.9659, Precision: 0.9425, Recall: 0.9546, F1: 0.9481
Validation Loss: 0.9253, Accuracy: 0.8380, Precision: 0.8034, Recall: 0.7605, F1: 0.7765
Testing Loss: 0.7782, Accuracy: 0.8575, Precision: 0.8189, Recall: 0.7894, F1: 0.7998
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 4, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 0, 0, 4, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9890, Accuracy: 0.7381, Precision: 0.8351, Recall: 0.7589, F1: 0.7359
Epoch 70/70
Train Loss: 0.0893, Accuracy: 0.9604, Precision: 0.9384, Recall: 0.9372, F1: 0.9377
Validation Loss: 0.9831, Accuracy: 0.8422, Precision: 0.8064, Recall: 0.7819, F1: 0.7903
Testing Loss: 0.8303, Accuracy: 0.8623, Precision: 0.8304, Recall: 0.8063, F1: 0.8170
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 4, 3, 3, 2, 3, 4, 1, 0, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9990, Accuracy: 0.6429, Precision: 0.8020, Recall: 0.6861, F1: 0.6603
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.9536, Accuracy: 0.6533, Precision: 0.7271, Recall: 0.5166, F1: 0.5248
Validation Loss: 0.5897, Accuracy: 0.8486, Precision: 0.8945, Recall: 0.7188, F1: 0.7245
Testing Loss: 0.5284, Accuracy: 0.8563, Precision: 0.8997, Recall: 0.7424, F1: 0.7541
LM Predictions:  [0, 5, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 3, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.5466, Accuracy: 0.0952, Precision: 0.0495, Recall: 0.1167, F1: 0.0500
Epoch 2/70
Train Loss: 0.4802, Accuracy: 0.8577, Precision: 0.7977, Recall: 0.7808, F1: 0.7868
Validation Loss: 0.4559, Accuracy: 0.8785, Precision: 0.8543, Recall: 0.8712, F1: 0.8586
Testing Loss: 0.3729, Accuracy: 0.8961, Precision: 0.8699, Recall: 0.8813, F1: 0.8717
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 3, 0, 5, 2, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4783, Accuracy: 0.0952, Precision: 0.2083, Recall: 0.1042, F1: 0.1124
Epoch 3/70
Train Loss: 0.3982, Accuracy: 0.8810, Precision: 0.8333, Recall: 0.8300, F1: 0.8316
Validation Loss: 0.4407, Accuracy: 0.8934, Precision: 0.8597, Recall: 0.8709, F1: 0.8646
Testing Loss: 0.3877, Accuracy: 0.8853, Precision: 0.8535, Recall: 0.8648, F1: 0.8552
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 4, 4, 5, 0, 5, 5, 5, 0, 5, 4, 0, 5, 0, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 4, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0882, Accuracy: 0.1429, Precision: 0.3056, Recall: 0.1542, F1: 0.1561
Epoch 4/70
Train Loss: 0.3339, Accuracy: 0.9032, Precision: 0.8557, Recall: 0.8579, F1: 0.8567
Validation Loss: 0.4848, Accuracy: 0.8827, Precision: 0.8660, Recall: 0.8187, F1: 0.8374
Testing Loss: 0.3622, Accuracy: 0.8973, Precision: 0.8783, Recall: 0.8466, F1: 0.8605
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 4, 5, 5, 0, 5, 0, 3, 0, 5, 2, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2223, Accuracy: 0.1429, Precision: 0.4444, Recall: 0.1542, F1: 0.1374
Epoch 5/70
Train Loss: 0.2976, Accuracy: 0.9115, Precision: 0.8746, Recall: 0.8663, F1: 0.8702
Validation Loss: 0.5019, Accuracy: 0.8763, Precision: 0.8711, Recall: 0.8106, F1: 0.8346
Testing Loss: 0.3879, Accuracy: 0.8816, Precision: 0.8613, Recall: 0.8225, F1: 0.8390
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 4, 5, 5, 0, 5, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0990, Accuracy: 0.1429, Precision: 0.5294, Recall: 0.1542, F1: 0.1431
Epoch 6/70
Train Loss: 0.2708, Accuracy: 0.9165, Precision: 0.8824, Recall: 0.8758, F1: 0.8790
Validation Loss: 0.5058, Accuracy: 0.8678, Precision: 0.8480, Recall: 0.8082, F1: 0.8236
Testing Loss: 0.3850, Accuracy: 0.8780, Precision: 0.8554, Recall: 0.8142, F1: 0.8291
LM Predictions:  [0, 3, 2, 0, 5, 5, 5, 0, 4, 5, 5, 0, 3, 0, 3, 0, 3, 4, 0, 3, 0, 0, 3, 0, 0, 0, 0, 5, 2, 0, 4, 4, 5, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1620, Accuracy: 0.2857, Precision: 0.4044, Recall: 0.2625, F1: 0.2703
Epoch 7/70
Train Loss: 0.2307, Accuracy: 0.9312, Precision: 0.9005, Recall: 0.8955, F1: 0.8979
Validation Loss: 0.4574, Accuracy: 0.8934, Precision: 0.8616, Recall: 0.8583, F1: 0.8598
Testing Loss: 0.3856, Accuracy: 0.8841, Precision: 0.8517, Recall: 0.8475, F1: 0.8490
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 4, 5, 5, 0, 3, 0, 3, 0, 5, 4, 0, 5, 0, 5, 3, 5, 0, 5, 0, 5, 2, 0, 4, 4, 5, 5, 5, 5, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5439, Accuracy: 0.2619, Precision: 0.4412, Recall: 0.2458, F1: 0.2692
Epoch 8/70
Train Loss: 0.2164, Accuracy: 0.9305, Precision: 0.8949, Recall: 0.8936, F1: 0.8941
Validation Loss: 0.4734, Accuracy: 0.8763, Precision: 0.8392, Recall: 0.8134, F1: 0.8241
Testing Loss: 0.3732, Accuracy: 0.8816, Precision: 0.8542, Recall: 0.8364, F1: 0.8446
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 0, 4, 5, 0, 0, 3, 5, 3, 5, 5, 4, 0, 3, 0, 0, 3, 5, 0, 0, 0, 0, 2, 0, 4, 3, 5, 5, 5, 0, 0, 5, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3219, Accuracy: 0.3095, Precision: 0.5000, Recall: 0.2792, F1: 0.3067
Epoch 9/70
Train Loss: 0.1984, Accuracy: 0.9350, Precision: 0.9011, Recall: 0.8999, F1: 0.9005
Validation Loss: 0.6052, Accuracy: 0.8785, Precision: 0.8484, Recall: 0.8319, F1: 0.8389
Testing Loss: 0.4454, Accuracy: 0.8792, Precision: 0.8520, Recall: 0.8287, F1: 0.8391
LM Predictions:  [0, 5, 2, 0, 5, 5, 5, 5, 4, 0, 5, 0, 5, 5, 3, 5, 3, 4, 0, 3, 0, 0, 3, 5, 0, 5, 5, 5, 2, 0, 4, 3, 0, 5, 5, 0, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1458, Accuracy: 0.3571, Precision: 0.5179, Recall: 0.3333, F1: 0.3510
Epoch 10/70
Train Loss: 0.1924, Accuracy: 0.9372, Precision: 0.9009, Recall: 0.9025, F1: 0.9016
Validation Loss: 0.6191, Accuracy: 0.8486, Precision: 0.8355, Recall: 0.7331, F1: 0.7547
Testing Loss: 0.5648, Accuracy: 0.8623, Precision: 0.8849, Recall: 0.7570, F1: 0.7813
LM Predictions:  [0, 5, 2, 0, 0, 5, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 2, 4, 0, 3, 0, 0, 5, 0, 0, 0, 0, 5, 2, 0, 4, 5, 0, 0, 0, 4, 0, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5562, Accuracy: 0.3810, Precision: 0.5333, Recall: 0.3708, F1: 0.3346
Epoch 11/70
Train Loss: 0.1655, Accuracy: 0.9457, Precision: 0.9161, Recall: 0.9123, F1: 0.9141
Validation Loss: 0.6537, Accuracy: 0.8678, Precision: 0.8359, Recall: 0.7980, F1: 0.8124
Testing Loss: 0.5515, Accuracy: 0.8684, Precision: 0.8425, Recall: 0.7918, F1: 0.8095
LM Predictions:  [0, 1, 2, 0, 0, 5, 5, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8236, Accuracy: 0.4762, Precision: 0.6970, Recall: 0.4269, F1: 0.4275
Epoch 12/70
Train Loss: 0.1648, Accuracy: 0.9464, Precision: 0.9196, Recall: 0.9145, F1: 0.9170
Validation Loss: 0.5692, Accuracy: 0.8891, Precision: 0.8648, Recall: 0.8332, F1: 0.8451
Testing Loss: 0.4563, Accuracy: 0.8768, Precision: 0.8449, Recall: 0.8085, F1: 0.8220
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 0, 0, 5, 0, 0, 2, 0, 4, 3, 3, 0, 0, 4, 0, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8331, Accuracy: 0.5476, Precision: 0.7083, Recall: 0.4935, F1: 0.4754
Epoch 13/70
Train Loss: 0.1612, Accuracy: 0.9462, Precision: 0.9170, Recall: 0.9201, F1: 0.9184
Validation Loss: 0.6260, Accuracy: 0.8849, Precision: 0.8481, Recall: 0.8291, F1: 0.8363
Testing Loss: 0.4576, Accuracy: 0.8865, Precision: 0.8580, Recall: 0.8333, F1: 0.8439
LM Predictions:  [0, 1, 2, 0, 5, 5, 4, 4, 4, 4, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 0, 0, 5, 0, 4, 2, 0, 4, 3, 3, 5, 0, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2376, Accuracy: 0.5714, Precision: 0.6653, Recall: 0.4769, F1: 0.4889
Epoch 14/70
Train Loss: 0.1501, Accuracy: 0.9474, Precision: 0.9181, Recall: 0.9248, F1: 0.9210
Validation Loss: 0.7024, Accuracy: 0.8529, Precision: 0.8114, Recall: 0.7838, F1: 0.7927
Testing Loss: 0.5587, Accuracy: 0.8659, Precision: 0.8238, Recall: 0.7900, F1: 0.8011
LM Predictions:  [0, 1, 2, 0, 5, 1, 2, 4, 4, 4, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 5, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9264, Accuracy: 0.6667, Precision: 0.6470, Recall: 0.5514, F1: 0.5556
Epoch 15/70
Train Loss: 0.1268, Accuracy: 0.9509, Precision: 0.9245, Recall: 0.9151, F1: 0.9196
Validation Loss: 0.5429, Accuracy: 0.8678, Precision: 0.8269, Recall: 0.8206, F1: 0.8230
Testing Loss: 0.4439, Accuracy: 0.8708, Precision: 0.8374, Recall: 0.8203, F1: 0.8272
LM Predictions:  [0, 1, 2, 0, 4, 5, 0, 0, 4, 0, 0, 0, 2, 5, 3, 3, 2, 4, 0, 3, 3, 0, 3, 5, 0, 5, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2808, Accuracy: 0.5476, Precision: 0.6689, Recall: 0.4935, F1: 0.4652
Epoch 16/70
Train Loss: 0.1217, Accuracy: 0.9571, Precision: 0.9304, Recall: 0.9263, F1: 0.9283
Validation Loss: 0.6774, Accuracy: 0.8721, Precision: 0.8353, Recall: 0.8160, F1: 0.8238
Testing Loss: 0.5742, Accuracy: 0.8659, Precision: 0.8285, Recall: 0.7926, F1: 0.8050
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 4, 4, 4, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 5, 0, 5, 0, 4, 2, 0, 4, 3, 3, 2, 3, 0, 0, 0, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1441, Accuracy: 0.6190, Precision: 0.6639, Recall: 0.5310, F1: 0.5065
Epoch 17/70
Train Loss: 0.1210, Accuracy: 0.9545, Precision: 0.9331, Recall: 0.9281, F1: 0.9305
Validation Loss: 0.6978, Accuracy: 0.8742, Precision: 0.8469, Recall: 0.8211, F1: 0.8325
Testing Loss: 0.5999, Accuracy: 0.8611, Precision: 0.8314, Recall: 0.8059, F1: 0.8171
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 4, 4, 0, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 5, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8004, Accuracy: 0.6905, Precision: 0.7262, Recall: 0.5972, F1: 0.5968
Epoch 18/70
Train Loss: 0.1133, Accuracy: 0.9542, Precision: 0.9260, Recall: 0.9318, F1: 0.9287
Validation Loss: 0.6977, Accuracy: 0.8550, Precision: 0.8343, Recall: 0.7668, F1: 0.7889
Testing Loss: 0.6148, Accuracy: 0.8587, Precision: 0.8304, Recall: 0.7737, F1: 0.7939
LM Predictions:  [0, 1, 2, 0, 0, 5, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 5, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0321, Accuracy: 0.5476, Precision: 0.7063, Recall: 0.4931, F1: 0.4959
Epoch 19/70
Train Loss: 0.1051, Accuracy: 0.9590, Precision: 0.9350, Recall: 0.9348, F1: 0.9348
Validation Loss: 0.7022, Accuracy: 0.8678, Precision: 0.8241, Recall: 0.8127, F1: 0.8169
Testing Loss: 0.6256, Accuracy: 0.8684, Precision: 0.8340, Recall: 0.8056, F1: 0.8181
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 4, 4, 4, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 5, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0328, Accuracy: 0.6667, Precision: 0.7013, Recall: 0.5639, F1: 0.5753
Epoch 20/70
Train Loss: 0.1018, Accuracy: 0.9592, Precision: 0.9341, Recall: 0.9370, F1: 0.9355
Validation Loss: 0.7662, Accuracy: 0.8550, Precision: 0.8007, Recall: 0.7418, F1: 0.7506
Testing Loss: 0.6281, Accuracy: 0.8611, Precision: 0.8616, Recall: 0.7630, F1: 0.7859
LM Predictions:  [0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 0, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1600, Accuracy: 0.4762, Precision: 0.7000, Recall: 0.4370, F1: 0.4201
Epoch 21/70
Train Loss: 0.1053, Accuracy: 0.9547, Precision: 0.9310, Recall: 0.9242, F1: 0.9275
Validation Loss: 0.7352, Accuracy: 0.8614, Precision: 0.8189, Recall: 0.7891, F1: 0.8012
Testing Loss: 0.6024, Accuracy: 0.8684, Precision: 0.8501, Recall: 0.8042, F1: 0.8234
LM Predictions:  [0, 1, 2, 1, 5, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 5, 2, 4, 0, 3, 3, 0, 3, 5, 1, 5, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0452, Accuracy: 0.6429, Precision: 0.7262, Recall: 0.5718, F1: 0.5765
Epoch 22/70
Train Loss: 0.1050, Accuracy: 0.9533, Precision: 0.9285, Recall: 0.9241, F1: 0.9262
Validation Loss: 0.7687, Accuracy: 0.8571, Precision: 0.8185, Recall: 0.8056, F1: 0.8088
Testing Loss: 0.7299, Accuracy: 0.8454, Precision: 0.8096, Recall: 0.8009, F1: 0.8030
LM Predictions:  [0, 1, 2, 0, 4, 1, 0, 4, 4, 4, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 5, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 3, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5007, Accuracy: 0.7857, Precision: 0.6963, Recall: 0.6491, F1: 0.6422
Epoch 23/70
Train Loss: 0.0936, Accuracy: 0.9592, Precision: 0.9348, Recall: 0.9314, F1: 0.9330
Validation Loss: 0.7415, Accuracy: 0.8827, Precision: 0.8490, Recall: 0.8125, F1: 0.8231
Testing Loss: 0.7371, Accuracy: 0.8635, Precision: 0.8297, Recall: 0.7755, F1: 0.7875
LM Predictions:  [0, 1, 2, 1, 1, 3, 0, 4, 4, 0, 3, 0, 2, 4, 3, 1, 2, 4, 3, 3, 3, 0, 3, 1, 1, 5, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3410, Accuracy: 0.9048, Precision: 0.7704, Recall: 0.7606, F1: 0.7631
Epoch 24/70
Train Loss: 0.1021, Accuracy: 0.9571, Precision: 0.9318, Recall: 0.9322, F1: 0.9320
Validation Loss: 0.8762, Accuracy: 0.8635, Precision: 0.8308, Recall: 0.7926, F1: 0.8060
Testing Loss: 0.7206, Accuracy: 0.8587, Precision: 0.8295, Recall: 0.7781, F1: 0.7926
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 3, 4, 3, 1, 0, 0, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4778, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8439, F1: 0.8169
Epoch 25/70
Train Loss: 0.0979, Accuracy: 0.9587, Precision: 0.9350, Recall: 0.9341, F1: 0.9345
Validation Loss: 0.8268, Accuracy: 0.8657, Precision: 0.8392, Recall: 0.7912, F1: 0.8104
Testing Loss: 0.7241, Accuracy: 0.8635, Precision: 0.8486, Recall: 0.7780, F1: 0.8007
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 1, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4716, Accuracy: 0.7619, Precision: 0.8514, Recall: 0.8000, F1: 0.7590
Epoch 26/70
Train Loss: 0.0925, Accuracy: 0.9623, Precision: 0.9419, Recall: 0.9361, F1: 0.9389
Validation Loss: 0.7091, Accuracy: 0.8657, Precision: 0.8206, Recall: 0.8262, F1: 0.8232
Testing Loss: 0.6691, Accuracy: 0.8647, Precision: 0.8364, Recall: 0.8194, F1: 0.8264
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 5, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5284, Accuracy: 0.7857, Precision: 0.7308, Recall: 0.6718, F1: 0.6589
Epoch 27/70
Train Loss: 0.0910, Accuracy: 0.9587, Precision: 0.9349, Recall: 0.9355, F1: 0.9352
Validation Loss: 0.7789, Accuracy: 0.8657, Precision: 0.8134, Recall: 0.8031, F1: 0.8063
Testing Loss: 0.6124, Accuracy: 0.8684, Precision: 0.8338, Recall: 0.8066, F1: 0.8185
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2428, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 28/70
Train Loss: 0.0799, Accuracy: 0.9647, Precision: 0.9454, Recall: 0.9433, F1: 0.9443
Validation Loss: 0.7490, Accuracy: 0.8806, Precision: 0.8442, Recall: 0.8160, F1: 0.8280
Testing Loss: 0.7494, Accuracy: 0.8623, Precision: 0.8345, Recall: 0.7926, F1: 0.8099
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 0, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6766, Accuracy: 0.6905, Precision: 0.8388, Recall: 0.7217, F1: 0.6909
Epoch 29/70
Train Loss: 0.0898, Accuracy: 0.9609, Precision: 0.9400, Recall: 0.9366, F1: 0.9383
Validation Loss: 0.6760, Accuracy: 0.8678, Precision: 0.8326, Recall: 0.8008, F1: 0.8137
Testing Loss: 0.6307, Accuracy: 0.8599, Precision: 0.8315, Recall: 0.7783, F1: 0.7959
LM Predictions:  [0, 1, 2, 1, 1, 5, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3851, Accuracy: 0.9048, Precision: 0.7708, Recall: 0.7611, F1: 0.7528
Epoch 30/70
Train Loss: 0.0833, Accuracy: 0.9628, Precision: 0.9428, Recall: 0.9416, F1: 0.9421
Validation Loss: 0.8452, Accuracy: 0.8678, Precision: 0.8287, Recall: 0.8010, F1: 0.8119
Testing Loss: 0.7225, Accuracy: 0.8647, Precision: 0.8359, Recall: 0.8067, F1: 0.8191
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3377, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8911, F1: 0.8657
Epoch 31/70
Train Loss: 0.0837, Accuracy: 0.9640, Precision: 0.9436, Recall: 0.9364, F1: 0.9397
Validation Loss: 0.8157, Accuracy: 0.8678, Precision: 0.8461, Recall: 0.7955, F1: 0.8158
Testing Loss: 0.8048, Accuracy: 0.8527, Precision: 0.8366, Recall: 0.7629, F1: 0.7863
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3368, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8200, F1: 0.7886
Epoch 32/70
Train Loss: 0.0906, Accuracy: 0.9597, Precision: 0.9425, Recall: 0.9311, F1: 0.9364
Validation Loss: 0.7578, Accuracy: 0.8699, Precision: 0.8294, Recall: 0.8099, F1: 0.8185
Testing Loss: 0.7152, Accuracy: 0.8575, Precision: 0.8294, Recall: 0.7882, F1: 0.8050
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3287, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8000, F1: 0.7643
Epoch 33/70
Train Loss: 0.0729, Accuracy: 0.9651, Precision: 0.9453, Recall: 0.9426, F1: 0.9439
Validation Loss: 0.7515, Accuracy: 0.8827, Precision: 0.8370, Recall: 0.8355, F1: 0.8361
Testing Loss: 0.7459, Accuracy: 0.8780, Precision: 0.8497, Recall: 0.8241, F1: 0.8348
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2235, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 34/70
Train Loss: 0.0823, Accuracy: 0.9621, Precision: 0.9378, Recall: 0.9428, F1: 0.9402
Validation Loss: 0.8329, Accuracy: 0.8486, Precision: 0.8000, Recall: 0.7996, F1: 0.7993
Testing Loss: 0.7303, Accuracy: 0.8563, Precision: 0.8182, Recall: 0.8107, F1: 0.8132
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1537, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 35/70
Train Loss: 0.0736, Accuracy: 0.9628, Precision: 0.9389, Recall: 0.9454, F1: 0.9420
Validation Loss: 0.8748, Accuracy: 0.8614, Precision: 0.8136, Recall: 0.7980, F1: 0.8038
Testing Loss: 0.8311, Accuracy: 0.8587, Precision: 0.8242, Recall: 0.7928, F1: 0.8037
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1524, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.0710, Accuracy: 0.9599, Precision: 0.9357, Recall: 0.9374, F1: 0.9366
Validation Loss: 0.8683, Accuracy: 0.8742, Precision: 0.8394, Recall: 0.8193, F1: 0.8284
Testing Loss: 0.8442, Accuracy: 0.8696, Precision: 0.8418, Recall: 0.8112, F1: 0.8243
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2086, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 37/70
Train Loss: 0.0785, Accuracy: 0.9599, Precision: 0.9355, Recall: 0.9474, F1: 0.9410
Validation Loss: 0.8985, Accuracy: 0.8571, Precision: 0.8119, Recall: 0.8016, F1: 0.8047
Testing Loss: 0.7242, Accuracy: 0.8563, Precision: 0.8161, Recall: 0.7881, F1: 0.7945
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 4, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1437, Accuracy: 0.9762, Precision: 0.9818, Recall: 0.9600, F1: 0.9683
Epoch 38/70
Train Loss: 0.0663, Accuracy: 0.9649, Precision: 0.9452, Recall: 0.9448, F1: 0.9450
Validation Loss: 0.9326, Accuracy: 0.8593, Precision: 0.8181, Recall: 0.7994, F1: 0.8069
Testing Loss: 0.8513, Accuracy: 0.8611, Precision: 0.8276, Recall: 0.7799, F1: 0.7937
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2618, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8716
Epoch 39/70
Train Loss: 0.0714, Accuracy: 0.9616, Precision: 0.9383, Recall: 0.9384, F1: 0.9383
Validation Loss: 0.8561, Accuracy: 0.8571, Precision: 0.8021, Recall: 0.7812, F1: 0.7851
Testing Loss: 0.8199, Accuracy: 0.8551, Precision: 0.8110, Recall: 0.7939, F1: 0.7994
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1907, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
Epoch 40/70
Train Loss: 0.0731, Accuracy: 0.9644, Precision: 0.9429, Recall: 0.9428, F1: 0.9429
Validation Loss: 0.9483, Accuracy: 0.8486, Precision: 0.8081, Recall: 0.7934, F1: 0.7985
Testing Loss: 0.8172, Accuracy: 0.8563, Precision: 0.8138, Recall: 0.7943, F1: 0.8022
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3126, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8889, F1: 0.8564
Epoch 41/70
Train Loss: 0.0676, Accuracy: 0.9621, Precision: 0.9365, Recall: 0.9454, F1: 0.9406
Validation Loss: 0.9452, Accuracy: 0.8699, Precision: 0.8347, Recall: 0.8014, F1: 0.8155
Testing Loss: 0.8621, Accuracy: 0.8635, Precision: 0.8367, Recall: 0.7819, F1: 0.8005
LM Predictions:  [0, 1, 2, 1, 5, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4058, Accuracy: 0.9286, Precision: 0.8056, Recall: 0.7796, F1: 0.7886
Epoch 42/70
Train Loss: 0.0693, Accuracy: 0.9649, Precision: 0.9408, Recall: 0.9488, F1: 0.9447
Validation Loss: 0.9638, Accuracy: 0.8635, Precision: 0.8372, Recall: 0.7812, F1: 0.8007
Testing Loss: 0.9296, Accuracy: 0.8563, Precision: 0.8342, Recall: 0.7629, F1: 0.7835
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3658, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7778, F1: 0.7478
Epoch 43/70
Train Loss: 0.0671, Accuracy: 0.9659, Precision: 0.9508, Recall: 0.9384, F1: 0.9442
Validation Loss: 0.8884, Accuracy: 0.8635, Precision: 0.8138, Recall: 0.8040, F1: 0.8068
Testing Loss: 0.8465, Accuracy: 0.8599, Precision: 0.8214, Recall: 0.8099, F1: 0.8151
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 5, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2508, Accuracy: 0.9762, Precision: 0.8333, Recall: 0.8125, F1: 0.8222
Epoch 44/70
Train Loss: 0.0852, Accuracy: 0.9604, Precision: 0.9388, Recall: 0.9361, F1: 0.9374
Validation Loss: 1.0065, Accuracy: 0.8550, Precision: 0.8328, Recall: 0.7897, F1: 0.8056
Testing Loss: 0.9036, Accuracy: 0.8611, Precision: 0.8449, Recall: 0.7771, F1: 0.7981
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5144, Accuracy: 0.6429, Precision: 0.8500, Recall: 0.6839, F1: 0.6731
Epoch 45/70
Train Loss: 0.0740, Accuracy: 0.9625, Precision: 0.9414, Recall: 0.9402, F1: 0.9407
Validation Loss: 0.8966, Accuracy: 0.8614, Precision: 0.8325, Recall: 0.8040, F1: 0.8156
Testing Loss: 0.8555, Accuracy: 0.8599, Precision: 0.8318, Recall: 0.7988, F1: 0.8124
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3270, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 46/70
Train Loss: 0.0616, Accuracy: 0.9668, Precision: 0.9485, Recall: 0.9456, F1: 0.9470
Validation Loss: 0.9247, Accuracy: 0.8614, Precision: 0.8133, Recall: 0.7968, F1: 0.8038
Testing Loss: 0.8498, Accuracy: 0.8623, Precision: 0.8321, Recall: 0.8031, F1: 0.8156
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2927, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7911, F1: 0.7571
Epoch 47/70
Train Loss: 0.0518, Accuracy: 0.9663, Precision: 0.9485, Recall: 0.9429, F1: 0.9456
Validation Loss: 0.9546, Accuracy: 0.8721, Precision: 0.8406, Recall: 0.8192, F1: 0.8289
Testing Loss: 0.8435, Accuracy: 0.8599, Precision: 0.8263, Recall: 0.7930, F1: 0.8068
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2966, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 48/70
Train Loss: 0.0640, Accuracy: 0.9659, Precision: 0.9445, Recall: 0.9566, F1: 0.9501
Validation Loss: 0.8995, Accuracy: 0.8763, Precision: 0.8436, Recall: 0.8240, F1: 0.8328
Testing Loss: 0.8383, Accuracy: 0.8478, Precision: 0.8196, Recall: 0.7886, F1: 0.8021
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3731, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7689, F1: 0.7326
Epoch 49/70
Train Loss: 0.0598, Accuracy: 0.9661, Precision: 0.9452, Recall: 0.9509, F1: 0.9477
Validation Loss: 0.9143, Accuracy: 0.8657, Precision: 0.8430, Recall: 0.8055, F1: 0.8208
Testing Loss: 0.9007, Accuracy: 0.8563, Precision: 0.8310, Recall: 0.7780, F1: 0.7974
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3343, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8550, F1: 0.8321
Epoch 50/70
Train Loss: 0.0574, Accuracy: 0.9651, Precision: 0.9425, Recall: 0.9459, F1: 0.9442
Validation Loss: 0.9946, Accuracy: 0.8593, Precision: 0.8178, Recall: 0.7918, F1: 0.8025
Testing Loss: 0.9319, Accuracy: 0.8514, Precision: 0.8195, Recall: 0.7753, F1: 0.7928
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3137, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8550, F1: 0.8186
Epoch 51/70
Train Loss: 0.0539, Accuracy: 0.9644, Precision: 0.9440, Recall: 0.9417, F1: 0.9428
Validation Loss: 0.9713, Accuracy: 0.8635, Precision: 0.8370, Recall: 0.7900, F1: 0.8077
Testing Loss: 0.9060, Accuracy: 0.8623, Precision: 0.8258, Recall: 0.7745, F1: 0.7917
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 1, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2917, Accuracy: 0.8333, Precision: 0.8576, Recall: 0.8461, F1: 0.8228
Epoch 52/70
Train Loss: 0.0671, Accuracy: 0.9661, Precision: 0.9459, Recall: 0.9450, F1: 0.9454
Validation Loss: 1.0193, Accuracy: 0.8443, Precision: 0.8189, Recall: 0.7715, F1: 0.7885
Testing Loss: 0.8688, Accuracy: 0.8623, Precision: 0.8442, Recall: 0.7831, F1: 0.7985
LM Predictions:  [0, 1, 0, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4535, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8300, F1: 0.7968
Epoch 53/70
Train Loss: 0.0812, Accuracy: 0.9606, Precision: 0.9424, Recall: 0.9372, F1: 0.9397
Validation Loss: 0.9527, Accuracy: 0.8465, Precision: 0.8087, Recall: 0.7753, F1: 0.7879
Testing Loss: 0.7784, Accuracy: 0.8587, Precision: 0.8235, Recall: 0.7848, F1: 0.7987
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2596, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8800, F1: 0.8393
Epoch 54/70
Train Loss: 0.0579, Accuracy: 0.9642, Precision: 0.9434, Recall: 0.9383, F1: 0.9408
Validation Loss: 0.9875, Accuracy: 0.8593, Precision: 0.8262, Recall: 0.8009, F1: 0.8112
Testing Loss: 0.8710, Accuracy: 0.8539, Precision: 0.8295, Recall: 0.7989, F1: 0.8124
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2301, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8150, F1: 0.7919
Epoch 55/70
Train Loss: 0.0681, Accuracy: 0.9661, Precision: 0.9519, Recall: 0.9420, F1: 0.9467
Validation Loss: 1.0136, Accuracy: 0.8593, Precision: 0.8136, Recall: 0.8011, F1: 0.8058
Testing Loss: 0.8668, Accuracy: 0.8635, Precision: 0.8274, Recall: 0.8020, F1: 0.8125
LM Predictions:  [0, 1, 2, 1, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2684, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9178, F1: 0.8958
Epoch 56/70
Train Loss: 0.0606, Accuracy: 0.9673, Precision: 0.9435, Recall: 0.9535, F1: 0.9482
Validation Loss: 0.9457, Accuracy: 0.8614, Precision: 0.8281, Recall: 0.8078, F1: 0.8155
Testing Loss: 0.9250, Accuracy: 0.8635, Precision: 0.8323, Recall: 0.7889, F1: 0.8025
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2413, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8261, F1: 0.8053
Epoch 57/70
Train Loss: 0.0554, Accuracy: 0.9673, Precision: 0.9448, Recall: 0.9507, F1: 0.9477
Validation Loss: 0.9532, Accuracy: 0.8635, Precision: 0.8221, Recall: 0.7969, F1: 0.8064
Testing Loss: 0.9265, Accuracy: 0.8599, Precision: 0.8243, Recall: 0.7851, F1: 0.7970
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2864, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7911, F1: 0.7571
Epoch 58/70
Train Loss: 0.0495, Accuracy: 0.9699, Precision: 0.9535, Recall: 0.9508, F1: 0.9520
Validation Loss: 1.0000, Accuracy: 0.8614, Precision: 0.8264, Recall: 0.7971, F1: 0.8092
Testing Loss: 0.9335, Accuracy: 0.8659, Precision: 0.8373, Recall: 0.8016, F1: 0.8161
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3507, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7778, F1: 0.7478
Epoch 59/70
Train Loss: 0.0767, Accuracy: 0.9613, Precision: 0.9350, Recall: 0.9488, F1: 0.9411
Validation Loss: 0.9282, Accuracy: 0.8678, Precision: 0.8295, Recall: 0.7982, F1: 0.8109
Testing Loss: 0.8720, Accuracy: 0.8647, Precision: 0.8353, Recall: 0.8059, F1: 0.8187
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 0, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3674, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8728, F1: 0.8499
Epoch 60/70
Train Loss: 0.0957, Accuracy: 0.9590, Precision: 0.9304, Recall: 0.9577, F1: 0.9419
Validation Loss: 0.8986, Accuracy: 0.8550, Precision: 0.8116, Recall: 0.7862, F1: 0.7973
Testing Loss: 0.8732, Accuracy: 0.8611, Precision: 0.8297, Recall: 0.7949, F1: 0.8096
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3004, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8778, F1: 0.8466
Epoch 61/70
Train Loss: 0.0643, Accuracy: 0.9659, Precision: 0.9455, Recall: 0.9487, F1: 0.9470
Validation Loss: 0.9243, Accuracy: 0.8699, Precision: 0.8194, Recall: 0.8114, F1: 0.8148
Testing Loss: 0.8737, Accuracy: 0.8696, Precision: 0.8379, Recall: 0.8108, F1: 0.8230
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2469, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 62/70
Train Loss: 0.0514, Accuracy: 0.9685, Precision: 0.9477, Recall: 0.9531, F1: 0.9503
Validation Loss: 0.9043, Accuracy: 0.8678, Precision: 0.8176, Recall: 0.8265, F1: 0.8216
Testing Loss: 0.8572, Accuracy: 0.8647, Precision: 0.8299, Recall: 0.8192, F1: 0.8240
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1276, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 63/70
Train Loss: 0.0615, Accuracy: 0.9656, Precision: 0.9450, Recall: 0.9476, F1: 0.9463
Validation Loss: 0.8800, Accuracy: 0.8721, Precision: 0.8271, Recall: 0.8201, F1: 0.8230
Testing Loss: 0.9199, Accuracy: 0.8587, Precision: 0.8181, Recall: 0.8019, F1: 0.8090
LM Predictions:  [0, 1, 5, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 5, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2409, Accuracy: 0.9524, Precision: 0.8333, Recall: 0.7917, F1: 0.8095
Epoch 64/70
Train Loss: 0.0619, Accuracy: 0.9651, Precision: 0.9436, Recall: 0.9491, F1: 0.9463
Validation Loss: 0.8474, Accuracy: 0.8721, Precision: 0.8390, Recall: 0.8126, F1: 0.8242
Testing Loss: 0.8378, Accuracy: 0.8647, Precision: 0.8402, Recall: 0.8029, F1: 0.8186
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3021, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7750, F1: 0.7462
Epoch 65/70
Train Loss: 0.0489, Accuracy: 0.9661, Precision: 0.9426, Recall: 0.9540, F1: 0.9479
Validation Loss: 0.9661, Accuracy: 0.8571, Precision: 0.8174, Recall: 0.7818, F1: 0.7945
Testing Loss: 0.9464, Accuracy: 0.8647, Precision: 0.8435, Recall: 0.7918, F1: 0.8102
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2105, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 66/70
Train Loss: 0.0507, Accuracy: 0.9668, Precision: 0.9460, Recall: 0.9447, F1: 0.9452
Validation Loss: 1.0341, Accuracy: 0.8678, Precision: 0.8247, Recall: 0.8047, F1: 0.8134
Testing Loss: 0.9657, Accuracy: 0.8684, Precision: 0.8428, Recall: 0.8034, F1: 0.8189
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2677, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 67/70
Train Loss: 0.0488, Accuracy: 0.9661, Precision: 0.9469, Recall: 0.9435, F1: 0.9451
Validation Loss: 1.0952, Accuracy: 0.8465, Precision: 0.8077, Recall: 0.7720, F1: 0.7840
Testing Loss: 0.9577, Accuracy: 0.8647, Precision: 0.8376, Recall: 0.8017, F1: 0.8172
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3299, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7311, F1: 0.7131
Epoch 68/70
Train Loss: 0.0470, Accuracy: 0.9704, Precision: 0.9491, Recall: 0.9571, F1: 0.9528
Validation Loss: 1.0506, Accuracy: 0.8657, Precision: 0.8325, Recall: 0.7863, F1: 0.8023
Testing Loss: 0.9899, Accuracy: 0.8623, Precision: 0.8384, Recall: 0.7765, F1: 0.7967
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 2, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2619, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8000, F1: 0.7643
Epoch 69/70
Train Loss: 0.0439, Accuracy: 0.9718, Precision: 0.9568, Recall: 0.9515, F1: 0.9541
Validation Loss: 1.0916, Accuracy: 0.8614, Precision: 0.8124, Recall: 0.7957, F1: 0.8024
Testing Loss: 0.9696, Accuracy: 0.8696, Precision: 0.8418, Recall: 0.8142, F1: 0.8265
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2979, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8550, F1: 0.8186
Epoch 70/70
Train Loss: 0.0562, Accuracy: 0.9675, Precision: 0.9455, Recall: 0.9508, F1: 0.9480
Validation Loss: 0.9361, Accuracy: 0.8635, Precision: 0.8230, Recall: 0.7873, F1: 0.8002
Testing Loss: 0.9510, Accuracy: 0.8587, Precision: 0.8204, Recall: 0.7796, F1: 0.7947
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2333, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9150, F1: 0.8942
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.5838, Accuracy: 0.8212, Precision: 0.7937, Recall: 0.7382, F1: 0.7582
Validation Loss: 0.4183, Accuracy: 0.8934, Precision: 0.8807, Recall: 0.8546, F1: 0.8662
Testing Loss: 0.3011, Accuracy: 0.9022, Precision: 0.8790, Recall: 0.8807, F1: 0.8783
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 2, 0, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0952, Accuracy: 0.0476, Precision: 0.0303, Recall: 0.0667, F1: 0.0417
Epoch 2/70
Train Loss: 0.3356, Accuracy: 0.8931, Precision: 0.8502, Recall: 0.8470, F1: 0.8485
Validation Loss: 0.4164, Accuracy: 0.8934, Precision: 0.8736, Recall: 0.8543, F1: 0.8627
Testing Loss: 0.2759, Accuracy: 0.9070, Precision: 0.8778, Recall: 0.8752, F1: 0.8760
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 2, 0, 5, 0, 5, 2, 5, 0, 0, 0, 5, 5, 5, 4, 5, 5, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9513, Accuracy: 0.1190, Precision: 0.2525, Recall: 0.1208, F1: 0.1275
Epoch 3/70
Train Loss: 0.2701, Accuracy: 0.9151, Precision: 0.8780, Recall: 0.8819, F1: 0.8796
Validation Loss: 0.4411, Accuracy: 0.8913, Precision: 0.8627, Recall: 0.8506, F1: 0.8555
Testing Loss: 0.3119, Accuracy: 0.9022, Precision: 0.8745, Recall: 0.8602, F1: 0.8662
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 5, 0, 3, 0, 5, 4, 0, 5, 0, 5, 2, 5, 0, 0, 0, 5, 5, 5, 4, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8253, Accuracy: 0.1905, Precision: 0.4500, Recall: 0.1708, F1: 0.2033
Epoch 4/70
Train Loss: 0.2124, Accuracy: 0.9303, Precision: 0.8914, Recall: 0.8961, F1: 0.8933
Validation Loss: 0.5131, Accuracy: 0.8891, Precision: 0.8647, Recall: 0.8383, F1: 0.8492
Testing Loss: 0.4082, Accuracy: 0.8925, Precision: 0.8777, Recall: 0.8336, F1: 0.8507
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 4, 5, 5, 0, 2, 0, 3, 0, 5, 4, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 4, 2, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2397, Accuracy: 0.2619, Precision: 0.6496, Recall: 0.2435, F1: 0.2750
Epoch 5/70
Train Loss: 0.1927, Accuracy: 0.9348, Precision: 0.8999, Recall: 0.9011, F1: 0.9001
Validation Loss: 0.5056, Accuracy: 0.8998, Precision: 0.8668, Recall: 0.8567, F1: 0.8606
Testing Loss: 0.3831, Accuracy: 0.8937, Precision: 0.8707, Recall: 0.8393, F1: 0.8521
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 4, 5, 5, 0, 2, 0, 3, 0, 5, 4, 0, 3, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 4, 0, 5, 2, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8870, Accuracy: 0.2857, Precision: 0.6468, Recall: 0.2602, F1: 0.2974
Epoch 6/70
Train Loss: 0.1623, Accuracy: 0.9426, Precision: 0.9082, Recall: 0.9169, F1: 0.9120
Validation Loss: 0.5781, Accuracy: 0.8763, Precision: 0.8499, Recall: 0.8074, F1: 0.8197
Testing Loss: 0.3932, Accuracy: 0.8889, Precision: 0.8906, Recall: 0.8031, F1: 0.8220
LM Predictions:  [0, 1, 5, 0, 0, 0, 5, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 0, 0, 2, 0, 0, 5, 0, 4, 3, 0, 0, 0, 0, 1, 5, 3, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5805, Accuracy: 0.4762, Precision: 0.6984, Recall: 0.4204, F1: 0.4452
Epoch 7/70
Train Loss: 0.1476, Accuracy: 0.9459, Precision: 0.9115, Recall: 0.9172, F1: 0.9140
Validation Loss: 0.5682, Accuracy: 0.8785, Precision: 0.8328, Recall: 0.8435, F1: 0.8371
Testing Loss: 0.3863, Accuracy: 0.8877, Precision: 0.8494, Recall: 0.8589, F1: 0.8538
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 4, 4, 5, 5, 0, 2, 4, 3, 0, 3, 4, 3, 3, 0, 4, 3, 1, 0, 2, 0, 5, 2, 5, 4, 3, 3, 5, 5, 4, 1, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9479, Accuracy: 0.5952, Precision: 0.7083, Recall: 0.4931, F1: 0.5553
Epoch 8/70
Train Loss: 0.1331, Accuracy: 0.9497, Precision: 0.9185, Recall: 0.9224, F1: 0.9201
Validation Loss: 0.5432, Accuracy: 0.8849, Precision: 0.8512, Recall: 0.8131, F1: 0.8277
Testing Loss: 0.4685, Accuracy: 0.8768, Precision: 0.8684, Recall: 0.7996, F1: 0.8202
LM Predictions:  [0, 1, 5, 0, 5, 5, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 5, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1631, Accuracy: 0.5714, Precision: 0.7130, Recall: 0.5120, F1: 0.5120
Epoch 9/70
Train Loss: 0.1228, Accuracy: 0.9507, Precision: 0.9194, Recall: 0.9247, F1: 0.9216
Validation Loss: 0.6176, Accuracy: 0.8678, Precision: 0.8439, Recall: 0.8078, F1: 0.8227
Testing Loss: 0.4970, Accuracy: 0.8696, Precision: 0.8513, Recall: 0.8018, F1: 0.8212
LM Predictions:  [0, 1, 0, 0, 1, 5, 0, 4, 4, 5, 5, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 0, 5, 2, 0, 4, 3, 3, 5, 1, 0, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7394, Accuracy: 0.6429, Precision: 0.7179, Recall: 0.5509, F1: 0.5860
Epoch 10/70
Train Loss: 0.1186, Accuracy: 0.9545, Precision: 0.9273, Recall: 0.9319, F1: 0.9293
Validation Loss: 0.6385, Accuracy: 0.8827, Precision: 0.8578, Recall: 0.8200, F1: 0.8348
Testing Loss: 0.4804, Accuracy: 0.8768, Precision: 0.8644, Recall: 0.8023, F1: 0.8223
LM Predictions:  [0, 1, 2, 1, 1, 5, 0, 4, 4, 5, 5, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7930, Accuracy: 0.6667, Precision: 0.7143, Recall: 0.5736, F1: 0.5952
Epoch 11/70
Train Loss: 0.1077, Accuracy: 0.9561, Precision: 0.9287, Recall: 0.9353, F1: 0.9317
Validation Loss: 0.5855, Accuracy: 0.8913, Precision: 0.8618, Recall: 0.8338, F1: 0.8458
Testing Loss: 0.4444, Accuracy: 0.8841, Precision: 0.8525, Recall: 0.8138, F1: 0.8279
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 5, 0, 0, 1, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5438, Accuracy: 0.8095, Precision: 0.7424, Recall: 0.6903, F1: 0.6882
Epoch 12/70
Train Loss: 0.0919, Accuracy: 0.9573, Precision: 0.9330, Recall: 0.9339, F1: 0.9334
Validation Loss: 0.6028, Accuracy: 0.8785, Precision: 0.8502, Recall: 0.8088, F1: 0.8251
Testing Loss: 0.5036, Accuracy: 0.8732, Precision: 0.8621, Recall: 0.7897, F1: 0.8123
LM Predictions:  [0, 1, 0, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8190, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7189, F1: 0.7043
Epoch 13/70
Train Loss: 0.0914, Accuracy: 0.9580, Precision: 0.9344, Recall: 0.9352, F1: 0.9348
Validation Loss: 0.7189, Accuracy: 0.8742, Precision: 0.8452, Recall: 0.8206, F1: 0.8301
Testing Loss: 0.5151, Accuracy: 0.8720, Precision: 0.8397, Recall: 0.8071, F1: 0.8202
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4649, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8328, F1: 0.8088
Epoch 14/70
Train Loss: 0.0895, Accuracy: 0.9606, Precision: 0.9358, Recall: 0.9430, F1: 0.9391
Validation Loss: 0.6739, Accuracy: 0.8699, Precision: 0.8214, Recall: 0.8125, F1: 0.8145
Testing Loss: 0.4920, Accuracy: 0.8780, Precision: 0.8377, Recall: 0.8086, F1: 0.8186
LM Predictions:  [0, 1, 2, 0, 5, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 3, 4, 0, 3, 0, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5662, Accuracy: 0.6667, Precision: 0.6979, Recall: 0.5806, F1: 0.5724
Epoch 15/70
Train Loss: 0.0890, Accuracy: 0.9583, Precision: 0.9358, Recall: 0.9334, F1: 0.9346
Validation Loss: 0.7342, Accuracy: 0.8593, Precision: 0.8295, Recall: 0.7811, F1: 0.7997
Testing Loss: 0.5548, Accuracy: 0.8720, Precision: 0.8340, Recall: 0.7895, F1: 0.8055
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3632, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8750, F1: 0.8511
Epoch 16/70
Train Loss: 0.0769, Accuracy: 0.9585, Precision: 0.9351, Recall: 0.9366, F1: 0.9358
Validation Loss: 0.7244, Accuracy: 0.8763, Precision: 0.8515, Recall: 0.8194, F1: 0.8327
Testing Loss: 0.5548, Accuracy: 0.8684, Precision: 0.8305, Recall: 0.7957, F1: 0.8093
LM Predictions:  [0, 1, 0, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2254, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
Epoch 17/70
Train Loss: 0.0786, Accuracy: 0.9611, Precision: 0.9377, Recall: 0.9376, F1: 0.9376
Validation Loss: 0.7752, Accuracy: 0.8721, Precision: 0.8437, Recall: 0.8176, F1: 0.8272
Testing Loss: 0.5593, Accuracy: 0.8780, Precision: 0.8507, Recall: 0.8081, F1: 0.8245
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3926, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 18/70
Train Loss: 0.0753, Accuracy: 0.9628, Precision: 0.9413, Recall: 0.9416, F1: 0.9414
Validation Loss: 0.7255, Accuracy: 0.8721, Precision: 0.8237, Recall: 0.8251, F1: 0.8236
Testing Loss: 0.5939, Accuracy: 0.8708, Precision: 0.8372, Recall: 0.8133, F1: 0.8224
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1839, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9550, F1: 0.9428
Epoch 19/70
Train Loss: 0.0756, Accuracy: 0.9621, Precision: 0.9387, Recall: 0.9411, F1: 0.9399
Validation Loss: 0.8209, Accuracy: 0.8486, Precision: 0.8248, Recall: 0.7972, F1: 0.8084
Testing Loss: 0.7367, Accuracy: 0.8599, Precision: 0.8394, Recall: 0.8008, F1: 0.8163
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2229, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9328, F1: 0.9182
Epoch 20/70
Train Loss: 0.0736, Accuracy: 0.9592, Precision: 0.9348, Recall: 0.9402, F1: 0.9374
Validation Loss: 0.7708, Accuracy: 0.8657, Precision: 0.8323, Recall: 0.8164, F1: 0.8220
Testing Loss: 0.6247, Accuracy: 0.8696, Precision: 0.8312, Recall: 0.8003, F1: 0.8122
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2389, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8928, F1: 0.8729
Epoch 21/70
Train Loss: 0.0668, Accuracy: 0.9625, Precision: 0.9397, Recall: 0.9416, F1: 0.9406
Validation Loss: 0.7953, Accuracy: 0.8635, Precision: 0.8227, Recall: 0.8141, F1: 0.8179
Testing Loss: 0.6410, Accuracy: 0.8768, Precision: 0.8378, Recall: 0.8246, F1: 0.8305
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2160, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 22/70
Train Loss: 0.0593, Accuracy: 0.9656, Precision: 0.9434, Recall: 0.9513, F1: 0.9472
Validation Loss: 0.7700, Accuracy: 0.8699, Precision: 0.8308, Recall: 0.8163, F1: 0.8225
Testing Loss: 0.6419, Accuracy: 0.8684, Precision: 0.8363, Recall: 0.8009, F1: 0.8151
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2117, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9600, F1: 0.9444
Epoch 23/70
Train Loss: 0.0598, Accuracy: 0.9656, Precision: 0.9455, Recall: 0.9454, F1: 0.9455
Validation Loss: 0.7856, Accuracy: 0.8699, Precision: 0.8181, Recall: 0.7941, F1: 0.8007
Testing Loss: 0.6756, Accuracy: 0.8720, Precision: 0.8357, Recall: 0.8003, F1: 0.8136
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2903, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8328, F1: 0.8138
Epoch 24/70
Train Loss: 0.0634, Accuracy: 0.9663, Precision: 0.9469, Recall: 0.9471, F1: 0.9470
Validation Loss: 0.8237, Accuracy: 0.8678, Precision: 0.8453, Recall: 0.8038, F1: 0.8196
Testing Loss: 0.7224, Accuracy: 0.8696, Precision: 0.8412, Recall: 0.8104, F1: 0.8240
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2657, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8757
Epoch 25/70
Train Loss: 0.0715, Accuracy: 0.9630, Precision: 0.9429, Recall: 0.9400, F1: 0.9414
Validation Loss: 0.7951, Accuracy: 0.8635, Precision: 0.8327, Recall: 0.8173, F1: 0.8236
Testing Loss: 0.7723, Accuracy: 0.8635, Precision: 0.8356, Recall: 0.8106, F1: 0.8204
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2906, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8328, F1: 0.8138
Epoch 26/70
Train Loss: 0.0562, Accuracy: 0.9644, Precision: 0.9446, Recall: 0.9440, F1: 0.9443
Validation Loss: 0.7852, Accuracy: 0.8699, Precision: 0.8406, Recall: 0.8181, F1: 0.8278
Testing Loss: 0.6894, Accuracy: 0.8756, Precision: 0.8432, Recall: 0.8234, F1: 0.8322
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3166, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8378, F1: 0.8141
Epoch 27/70
Train Loss: 0.0636, Accuracy: 0.9656, Precision: 0.9447, Recall: 0.9457, F1: 0.9451
Validation Loss: 0.8398, Accuracy: 0.8721, Precision: 0.8378, Recall: 0.8207, F1: 0.8283
Testing Loss: 0.7843, Accuracy: 0.8720, Precision: 0.8323, Recall: 0.8066, F1: 0.8172
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2135, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9350, F1: 0.9183
Epoch 28/70
Train Loss: 0.0559, Accuracy: 0.9644, Precision: 0.9460, Recall: 0.9389, F1: 0.9423
Validation Loss: 0.9122, Accuracy: 0.8721, Precision: 0.8344, Recall: 0.8121, F1: 0.8209
Testing Loss: 0.7959, Accuracy: 0.8756, Precision: 0.8453, Recall: 0.7989, F1: 0.8138
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4035, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8906, F1: 0.8728
Epoch 29/70
Train Loss: 0.0660, Accuracy: 0.9628, Precision: 0.9413, Recall: 0.9423, F1: 0.9418
Validation Loss: 0.8988, Accuracy: 0.8742, Precision: 0.8303, Recall: 0.8297, F1: 0.8298
Testing Loss: 0.7804, Accuracy: 0.8756, Precision: 0.8363, Recall: 0.8179, F1: 0.8253
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3344, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 30/70
Train Loss: 0.0727, Accuracy: 0.9616, Precision: 0.9396, Recall: 0.9428, F1: 0.9411
Validation Loss: 0.9486, Accuracy: 0.8678, Precision: 0.8237, Recall: 0.8209, F1: 0.8220
Testing Loss: 0.8047, Accuracy: 0.8708, Precision: 0.8339, Recall: 0.8177, F1: 0.8242
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2138, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8757
Epoch 31/70
Train Loss: 0.0640, Accuracy: 0.9654, Precision: 0.9466, Recall: 0.9429, F1: 0.9447
Validation Loss: 0.8521, Accuracy: 0.8571, Precision: 0.8137, Recall: 0.8194, F1: 0.8157
Testing Loss: 0.7485, Accuracy: 0.8659, Precision: 0.8267, Recall: 0.8227, F1: 0.8229
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1720, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9750, F1: 0.9685
Epoch 32/70
Train Loss: 0.0630, Accuracy: 0.9630, Precision: 0.9409, Recall: 0.9411, F1: 0.9410
Validation Loss: 0.8625, Accuracy: 0.8678, Precision: 0.8246, Recall: 0.8138, F1: 0.8187
Testing Loss: 0.7771, Accuracy: 0.8659, Precision: 0.8321, Recall: 0.8042, F1: 0.8154
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2219, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8706, F1: 0.8514
Epoch 33/70
Train Loss: 0.0520, Accuracy: 0.9685, Precision: 0.9517, Recall: 0.9485, F1: 0.9501
Validation Loss: 0.9046, Accuracy: 0.8785, Precision: 0.8408, Recall: 0.8253, F1: 0.8323
Testing Loss: 0.8006, Accuracy: 0.8792, Precision: 0.8535, Recall: 0.8275, F1: 0.8387
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1533, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0501, Accuracy: 0.9711, Precision: 0.9515, Recall: 0.9514, F1: 0.9515
Validation Loss: 0.8962, Accuracy: 0.8785, Precision: 0.8392, Recall: 0.8211, F1: 0.8291
Testing Loss: 0.8233, Accuracy: 0.8804, Precision: 0.8488, Recall: 0.8126, F1: 0.8244
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2675, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8861, F1: 0.8629
Epoch 35/70
Train Loss: 0.0466, Accuracy: 0.9685, Precision: 0.9508, Recall: 0.9466, F1: 0.9486
Validation Loss: 0.9544, Accuracy: 0.8827, Precision: 0.8564, Recall: 0.8256, F1: 0.8384
Testing Loss: 0.8333, Accuracy: 0.8792, Precision: 0.8495, Recall: 0.8088, F1: 0.8229
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2588, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8133, F1: 0.7930
Epoch 36/70
Train Loss: 0.0594, Accuracy: 0.9675, Precision: 0.9499, Recall: 0.9453, F1: 0.9475
Validation Loss: 0.9576, Accuracy: 0.8614, Precision: 0.8274, Recall: 0.8138, F1: 0.8184
Testing Loss: 0.8276, Accuracy: 0.8635, Precision: 0.8274, Recall: 0.7997, F1: 0.8108
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2799, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8461, F1: 0.8249
Epoch 37/70
Train Loss: 0.0530, Accuracy: 0.9668, Precision: 0.9461, Recall: 0.9486, F1: 0.9473
Validation Loss: 0.9286, Accuracy: 0.8763, Precision: 0.8347, Recall: 0.8272, F1: 0.8303
Testing Loss: 0.7956, Accuracy: 0.8756, Precision: 0.8396, Recall: 0.8180, F1: 0.8268
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3354, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 38/70
Train Loss: 0.0716, Accuracy: 0.9632, Precision: 0.9413, Recall: 0.9392, F1: 0.9402
Validation Loss: 0.9533, Accuracy: 0.8593, Precision: 0.8114, Recall: 0.8107, F1: 0.8090
Testing Loss: 0.7671, Accuracy: 0.8659, Precision: 0.8172, Recall: 0.8003, F1: 0.8061
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2366, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 39/70
Train Loss: 0.0642, Accuracy: 0.9635, Precision: 0.9424, Recall: 0.9469, F1: 0.9445
Validation Loss: 0.8842, Accuracy: 0.8699, Precision: 0.8312, Recall: 0.8077, F1: 0.8174
Testing Loss: 0.7662, Accuracy: 0.8732, Precision: 0.8381, Recall: 0.8062, F1: 0.8187
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2227, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8978, F1: 0.8716
Epoch 40/70
Train Loss: 0.0566, Accuracy: 0.9644, Precision: 0.9450, Recall: 0.9412, F1: 0.9431
Validation Loss: 1.0090, Accuracy: 0.8678, Precision: 0.8291, Recall: 0.8053, F1: 0.8155
Testing Loss: 0.8356, Accuracy: 0.8768, Precision: 0.8432, Recall: 0.8013, F1: 0.8156
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2142, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8778, F1: 0.8527
Epoch 41/70
Train Loss: 0.0600, Accuracy: 0.9644, Precision: 0.9422, Recall: 0.9412, F1: 0.9417
Validation Loss: 1.0321, Accuracy: 0.8593, Precision: 0.8122, Recall: 0.7888, F1: 0.7982
Testing Loss: 0.8317, Accuracy: 0.8732, Precision: 0.8436, Recall: 0.7951, F1: 0.8107
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2646, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 42/70
Train Loss: 0.0617, Accuracy: 0.9651, Precision: 0.9431, Recall: 0.9470, F1: 0.9448
Validation Loss: 0.9034, Accuracy: 0.8721, Precision: 0.8242, Recall: 0.8180, F1: 0.8204
Testing Loss: 0.8421, Accuracy: 0.8756, Precision: 0.8461, Recall: 0.8039, F1: 0.8191
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2392, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8461, F1: 0.8249
Epoch 43/70
Train Loss: 0.0572, Accuracy: 0.9668, Precision: 0.9514, Recall: 0.9417, F1: 0.9462
Validation Loss: 0.9361, Accuracy: 0.8657, Precision: 0.8328, Recall: 0.8044, F1: 0.8167
Testing Loss: 0.9280, Accuracy: 0.8623, Precision: 0.8362, Recall: 0.7878, F1: 0.8052
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3257, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 44/70
Train Loss: 0.0514, Accuracy: 0.9670, Precision: 0.9492, Recall: 0.9429, F1: 0.9460
Validation Loss: 0.9945, Accuracy: 0.8657, Precision: 0.8253, Recall: 0.8103, F1: 0.8155
Testing Loss: 0.8816, Accuracy: 0.8671, Precision: 0.8348, Recall: 0.8108, F1: 0.8216
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2252, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 45/70
Train Loss: 0.0487, Accuracy: 0.9689, Precision: 0.9470, Recall: 0.9584, F1: 0.9523
Validation Loss: 1.0438, Accuracy: 0.8678, Precision: 0.8337, Recall: 0.8015, F1: 0.8142
Testing Loss: 0.9206, Accuracy: 0.8684, Precision: 0.8364, Recall: 0.7938, F1: 0.8098
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2494, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8078
Epoch 46/70
Train Loss: 0.0508, Accuracy: 0.9687, Precision: 0.9505, Recall: 0.9473, F1: 0.9489
Validation Loss: 1.0037, Accuracy: 0.8635, Precision: 0.8246, Recall: 0.8162, F1: 0.8185
Testing Loss: 0.8543, Accuracy: 0.8684, Precision: 0.8334, Recall: 0.8145, F1: 0.8227
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2281, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8578, F1: 0.8353
Epoch 47/70
Train Loss: 0.0478, Accuracy: 0.9659, Precision: 0.9439, Recall: 0.9491, F1: 0.9464
Validation Loss: 1.0478, Accuracy: 0.8678, Precision: 0.8299, Recall: 0.8123, F1: 0.8195
Testing Loss: 0.9201, Accuracy: 0.8756, Precision: 0.8394, Recall: 0.8058, F1: 0.8182
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2472, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8550, F1: 0.8321
Epoch 48/70
Train Loss: 0.0497, Accuracy: 0.9677, Precision: 0.9458, Recall: 0.9519, F1: 0.9487
Validation Loss: 0.9953, Accuracy: 0.8678, Precision: 0.8240, Recall: 0.8076, F1: 0.8146
Testing Loss: 0.8752, Accuracy: 0.8623, Precision: 0.8317, Recall: 0.7925, F1: 0.8072
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3185, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 49/70
Train Loss: 0.0622, Accuracy: 0.9661, Precision: 0.9482, Recall: 0.9448, F1: 0.9465
Validation Loss: 0.9378, Accuracy: 0.8721, Precision: 0.8315, Recall: 0.8135, F1: 0.8213
Testing Loss: 0.8851, Accuracy: 0.8647, Precision: 0.8395, Recall: 0.7939, F1: 0.8095
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2433, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8178, F1: 0.7935
Epoch 50/70
Train Loss: 0.0614, Accuracy: 0.9651, Precision: 0.9441, Recall: 0.9452, F1: 0.9446
Validation Loss: 1.1239, Accuracy: 0.8593, Precision: 0.8186, Recall: 0.8032, F1: 0.8091
Testing Loss: 0.9275, Accuracy: 0.8659, Precision: 0.8382, Recall: 0.7991, F1: 0.8125
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2112, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8711, F1: 0.8456
Epoch 51/70
Train Loss: 0.0482, Accuracy: 0.9685, Precision: 0.9459, Recall: 0.9540, F1: 0.9498
Validation Loss: 1.0832, Accuracy: 0.8699, Precision: 0.8279, Recall: 0.8053, F1: 0.8146
Testing Loss: 0.8506, Accuracy: 0.8708, Precision: 0.8428, Recall: 0.7879, F1: 0.8055
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3034, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7483, F1: 0.7376
Epoch 52/70
Train Loss: 0.0466, Accuracy: 0.9685, Precision: 0.9523, Recall: 0.9506, F1: 0.9514
Validation Loss: 1.0992, Accuracy: 0.8699, Precision: 0.8376, Recall: 0.7973, F1: 0.8121
Testing Loss: 0.9451, Accuracy: 0.8720, Precision: 0.8528, Recall: 0.7901, F1: 0.8074
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3369, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7261, F1: 0.7165
Epoch 53/70
Train Loss: 0.0573, Accuracy: 0.9644, Precision: 0.9439, Recall: 0.9427, F1: 0.9433
Validation Loss: 1.1034, Accuracy: 0.8465, Precision: 0.8064, Recall: 0.8069, F1: 0.8042
Testing Loss: 0.8291, Accuracy: 0.8684, Precision: 0.8376, Recall: 0.8299, F1: 0.8331
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2943, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 54/70
Train Loss: 0.0526, Accuracy: 0.9637, Precision: 0.9421, Recall: 0.9384, F1: 0.9402
Validation Loss: 1.0196, Accuracy: 0.8699, Precision: 0.8281, Recall: 0.8079, F1: 0.8157
Testing Loss: 0.9273, Accuracy: 0.8756, Precision: 0.8551, Recall: 0.8042, F1: 0.8207
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1899, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9400, F1: 0.9186
Epoch 55/70
Train Loss: 0.0669, Accuracy: 0.9623, Precision: 0.9433, Recall: 0.9402, F1: 0.9416
Validation Loss: 1.0045, Accuracy: 0.8635, Precision: 0.8206, Recall: 0.8077, F1: 0.8133
Testing Loss: 0.8011, Accuracy: 0.8647, Precision: 0.8287, Recall: 0.7866, F1: 0.8031
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2954, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 56/70
Train Loss: 0.0600, Accuracy: 0.9647, Precision: 0.9538, Recall: 0.9325, F1: 0.9421
Validation Loss: 1.0189, Accuracy: 0.8571, Precision: 0.8177, Recall: 0.8031, F1: 0.8088
Testing Loss: 0.9266, Accuracy: 0.8671, Precision: 0.8393, Recall: 0.8190, F1: 0.8270
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 5, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2928, Accuracy: 0.7857, Precision: 0.7308, Recall: 0.6759, F1: 0.6561
Epoch 57/70
Train Loss: 0.0552, Accuracy: 0.9670, Precision: 0.9434, Recall: 0.9596, F1: 0.9508
Validation Loss: 1.1498, Accuracy: 0.8507, Precision: 0.8107, Recall: 0.7854, F1: 0.7935
Testing Loss: 0.9554, Accuracy: 0.8635, Precision: 0.8328, Recall: 0.7839, F1: 0.7971
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3021, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 58/70
Train Loss: 0.0488, Accuracy: 0.9708, Precision: 0.9585, Recall: 0.9504, F1: 0.9544
Validation Loss: 1.1575, Accuracy: 0.8678, Precision: 0.8400, Recall: 0.8067, F1: 0.8197
Testing Loss: 0.9598, Accuracy: 0.8563, Precision: 0.8271, Recall: 0.7878, F1: 0.8039
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1754, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0572, Accuracy: 0.9632, Precision: 0.9427, Recall: 0.9442, F1: 0.9434
Validation Loss: 1.0954, Accuracy: 0.8657, Precision: 0.8345, Recall: 0.8074, F1: 0.8185
Testing Loss: 0.9292, Accuracy: 0.8671, Precision: 0.8431, Recall: 0.8017, F1: 0.8188
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2166, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 60/70
Train Loss: 0.0512, Accuracy: 0.9663, Precision: 0.9507, Recall: 0.9397, F1: 0.9448
Validation Loss: 1.0823, Accuracy: 0.8657, Precision: 0.8358, Recall: 0.8052, F1: 0.8171
Testing Loss: 0.8380, Accuracy: 0.8744, Precision: 0.8471, Recall: 0.8111, F1: 0.8268
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2206, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 61/70
Train Loss: 0.0475, Accuracy: 0.9673, Precision: 0.9460, Recall: 0.9482, F1: 0.9471
Validation Loss: 1.0296, Accuracy: 0.8699, Precision: 0.8270, Recall: 0.8221, F1: 0.8243
Testing Loss: 0.8840, Accuracy: 0.8732, Precision: 0.8441, Recall: 0.8150, F1: 0.8269
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2801, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 62/70
Train Loss: 0.0439, Accuracy: 0.9718, Precision: 0.9579, Recall: 0.9515, F1: 0.9546
Validation Loss: 1.0583, Accuracy: 0.8678, Precision: 0.8290, Recall: 0.8140, F1: 0.8203
Testing Loss: 0.8683, Accuracy: 0.8756, Precision: 0.8461, Recall: 0.8200, F1: 0.8312
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2148, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 63/70
Train Loss: 0.0425, Accuracy: 0.9692, Precision: 0.9494, Recall: 0.9516, F1: 0.9505
Validation Loss: 1.0868, Accuracy: 0.8721, Precision: 0.8439, Recall: 0.8058, F1: 0.8209
Testing Loss: 0.9527, Accuracy: 0.8732, Precision: 0.8464, Recall: 0.8015, F1: 0.8178
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2845, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 64/70
Train Loss: 0.0477, Accuracy: 0.9692, Precision: 0.9522, Recall: 0.9468, F1: 0.9494
Validation Loss: 1.1063, Accuracy: 0.8763, Precision: 0.8450, Recall: 0.8107, F1: 0.8239
Testing Loss: 0.9067, Accuracy: 0.8756, Precision: 0.8479, Recall: 0.8040, F1: 0.8209
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2139, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 65/70
Train Loss: 0.0474, Accuracy: 0.9675, Precision: 0.9485, Recall: 0.9463, F1: 0.9474
Validation Loss: 1.0270, Accuracy: 0.8827, Precision: 0.8481, Recall: 0.8249, F1: 0.8348
Testing Loss: 0.9756, Accuracy: 0.8732, Precision: 0.8423, Recall: 0.8055, F1: 0.8186
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2694, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7461, F1: 0.7351
Epoch 66/70
Train Loss: 0.0497, Accuracy: 0.9651, Precision: 0.9414, Recall: 0.9471, F1: 0.9440
Validation Loss: 1.0598, Accuracy: 0.8699, Precision: 0.8278, Recall: 0.8121, F1: 0.8177
Testing Loss: 0.8932, Accuracy: 0.8756, Precision: 0.8462, Recall: 0.8106, F1: 0.8241
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1816, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9528, F1: 0.9416
Epoch 67/70
Train Loss: 0.0444, Accuracy: 0.9659, Precision: 0.9462, Recall: 0.9418, F1: 0.9439
Validation Loss: 1.1558, Accuracy: 0.8699, Precision: 0.8441, Recall: 0.8048, F1: 0.8196
Testing Loss: 1.0370, Accuracy: 0.8671, Precision: 0.8442, Recall: 0.7959, F1: 0.8141
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3725, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 68/70
Train Loss: 0.0756, Accuracy: 0.9604, Precision: 0.9427, Recall: 0.9343, F1: 0.9382
Validation Loss: 1.0392, Accuracy: 0.8614, Precision: 0.8246, Recall: 0.8106, F1: 0.8146
Testing Loss: 0.7383, Accuracy: 0.8659, Precision: 0.8188, Recall: 0.7955, F1: 0.8010
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2543, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8061, F1: 0.7848
Epoch 69/70
Train Loss: 0.0627, Accuracy: 0.9663, Precision: 0.9501, Recall: 0.9433, F1: 0.9466
Validation Loss: 1.0223, Accuracy: 0.8699, Precision: 0.8227, Recall: 0.8169, F1: 0.8182
Testing Loss: 0.8769, Accuracy: 0.8732, Precision: 0.8304, Recall: 0.8131, F1: 0.8185
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2042, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8911, F1: 0.8657
Epoch 70/70
Train Loss: 0.0513, Accuracy: 0.9670, Precision: 0.9448, Recall: 0.9481, F1: 0.9464
Validation Loss: 1.0922, Accuracy: 0.8657, Precision: 0.8290, Recall: 0.7933, F1: 0.8058
Testing Loss: 0.8578, Accuracy: 0.8744, Precision: 0.8416, Recall: 0.7871, F1: 0.7998
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2098, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8533, F1: 0.8276

