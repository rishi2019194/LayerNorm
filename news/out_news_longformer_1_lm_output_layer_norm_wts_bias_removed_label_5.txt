Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4245, Accuracy: 0.4266, Precision: 0.2899, Recall: 0.2926, F1: 0.2641
Validation Loss: 1.0739, Accuracy: 0.6397, Precision: 0.4311, Recall: 0.4950, F1: 0.4557
Testing Loss: 0.9487, Accuracy: 0.6643, Precision: 0.6052, Recall: 0.5056, F1: 0.4683
LM Predictions:  [0, 0, 0, 0, 0, 0, 4, 0, 2, 0, 4, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6109, Accuracy: 0.0714, Precision: 0.0171, Recall: 0.1200, F1: 0.0300
Epoch 2/70
Train Loss: 0.8304, Accuracy: 0.7228, Precision: 0.5700, Recall: 0.5909, F1: 0.5778
Validation Loss: 0.6384, Accuracy: 0.8188, Precision: 0.6903, Recall: 0.6847, F1: 0.6813
Testing Loss: 0.5664, Accuracy: 0.8200, Precision: 0.6927, Recall: 0.6906, F1: 0.6833
LM Predictions:  [0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.6060, Accuracy: 0.1429, Precision: 0.1576, Recall: 0.2050, F1: 0.1071
Epoch 3/70
Train Loss: 0.5640, Accuracy: 0.8221, Precision: 0.7504, Recall: 0.7180, F1: 0.7219
Validation Loss: 0.5135, Accuracy: 0.8678, Precision: 0.8354, Recall: 0.8002, F1: 0.7990
Testing Loss: 0.4683, Accuracy: 0.8780, Precision: 0.8713, Recall: 0.8219, F1: 0.8304
LM Predictions:  [0, 3, 5, 0, 4, 0, 5, 5, 5, 4, 5, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 5, 0, 2, 4, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0957, Accuracy: 0.0714, Precision: 0.0371, Recall: 0.0833, F1: 0.0418
Epoch 4/70
Train Loss: 0.4762, Accuracy: 0.8594, Precision: 0.8082, Recall: 0.8004, F1: 0.8036
Validation Loss: 0.5390, Accuracy: 0.8593, Precision: 0.8430, Recall: 0.8044, F1: 0.8170
Testing Loss: 0.4271, Accuracy: 0.8865, Precision: 0.8726, Recall: 0.8363, F1: 0.8501
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 3, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2262, Accuracy: 0.0476, Precision: 0.0175, Recall: 0.0667, F1: 0.0278
Epoch 5/70
Train Loss: 0.4363, Accuracy: 0.8648, Precision: 0.8164, Recall: 0.8148, F1: 0.8154
Validation Loss: 0.5387, Accuracy: 0.8742, Precision: 0.8474, Recall: 0.8366, F1: 0.8410
Testing Loss: 0.4356, Accuracy: 0.8720, Precision: 0.8472, Recall: 0.8300, F1: 0.8354
LM Predictions:  [0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 0, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.5456, Accuracy: 0.0476, Precision: 0.0185, Recall: 0.0667, F1: 0.0290
Epoch 6/70
Train Loss: 0.4094, Accuracy: 0.8710, Precision: 0.8218, Recall: 0.8156, F1: 0.8186
Validation Loss: 0.4738, Accuracy: 0.8849, Precision: 0.8659, Recall: 0.8382, F1: 0.8498
Testing Loss: 0.4083, Accuracy: 0.8804, Precision: 0.8622, Recall: 0.8226, F1: 0.8358
LM Predictions:  [0, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 4, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0239, Accuracy: 0.0714, Precision: 0.1842, Recall: 0.0875, F1: 0.0648
Epoch 7/70
Train Loss: 0.3811, Accuracy: 0.8817, Precision: 0.8331, Recall: 0.8233, F1: 0.8279
Validation Loss: 0.5037, Accuracy: 0.8721, Precision: 0.8569, Recall: 0.8087, F1: 0.8255
Testing Loss: 0.3848, Accuracy: 0.8877, Precision: 0.8756, Recall: 0.8218, F1: 0.8394
LM Predictions:  [0, 3, 5, 0, 4, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 4, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7722, Accuracy: 0.0952, Precision: 0.2685, Recall: 0.1042, F1: 0.0938
Epoch 8/70
Train Loss: 0.3637, Accuracy: 0.8883, Precision: 0.8426, Recall: 0.8338, F1: 0.8379
Validation Loss: 0.5226, Accuracy: 0.8763, Precision: 0.8525, Recall: 0.8385, F1: 0.8431
Testing Loss: 0.4057, Accuracy: 0.8937, Precision: 0.8775, Recall: 0.8671, F1: 0.8697
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 2, 0, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1007, Accuracy: 0.0714, Precision: 0.1136, Recall: 0.0875, F1: 0.0750
Epoch 9/70
Train Loss: 0.3467, Accuracy: 0.8938, Precision: 0.8534, Recall: 0.8500, F1: 0.8515
Validation Loss: 0.5389, Accuracy: 0.8721, Precision: 0.8455, Recall: 0.8469, F1: 0.8443
Testing Loss: 0.3703, Accuracy: 0.8901, Precision: 0.8663, Recall: 0.8751, F1: 0.8672
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4412, Accuracy: 0.0952, Precision: 0.3810, Recall: 0.1042, F1: 0.1229
Epoch 10/70
Train Loss: 0.3286, Accuracy: 0.8987, Precision: 0.8585, Recall: 0.8465, F1: 0.8522
Validation Loss: 0.5060, Accuracy: 0.8763, Precision: 0.8505, Recall: 0.8211, F1: 0.8333
Testing Loss: 0.3644, Accuracy: 0.8913, Precision: 0.8747, Recall: 0.8468, F1: 0.8588
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 2, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0366, Accuracy: 0.0952, Precision: 0.0868, Recall: 0.1208, F1: 0.0779
Epoch 11/70
Train Loss: 0.2998, Accuracy: 0.9063, Precision: 0.8670, Recall: 0.8638, F1: 0.8650
Validation Loss: 0.5275, Accuracy: 0.8763, Precision: 0.8417, Recall: 0.8306, F1: 0.8347
Testing Loss: 0.3906, Accuracy: 0.8961, Precision: 0.8700, Recall: 0.8483, F1: 0.8576
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 3, 5, 3, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 0, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9917, Accuracy: 0.1190, Precision: 0.1607, Recall: 0.1375, F1: 0.1098
Epoch 12/70
Train Loss: 0.2903, Accuracy: 0.9111, Precision: 0.8715, Recall: 0.8734, F1: 0.8721
Validation Loss: 0.5868, Accuracy: 0.8678, Precision: 0.8496, Recall: 0.8024, F1: 0.8213
Testing Loss: 0.4674, Accuracy: 0.8853, Precision: 0.8710, Recall: 0.8249, F1: 0.8425
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9465, Accuracy: 0.0714, Precision: 0.1029, Recall: 0.0875, F1: 0.0636
Epoch 13/70
Train Loss: 0.2785, Accuracy: 0.9118, Precision: 0.8746, Recall: 0.8679, F1: 0.8711
Validation Loss: 0.5293, Accuracy: 0.8678, Precision: 0.8497, Recall: 0.8030, F1: 0.8221
Testing Loss: 0.4121, Accuracy: 0.8671, Precision: 0.8601, Recall: 0.7939, F1: 0.8139
LM Predictions:  [0, 3, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2745, Accuracy: 0.0952, Precision: 0.1917, Recall: 0.1208, F1: 0.0770
Epoch 14/70
Train Loss: 0.2523, Accuracy: 0.9232, Precision: 0.8880, Recall: 0.8807, F1: 0.8841
Validation Loss: 0.5849, Accuracy: 0.8593, Precision: 0.8286, Recall: 0.8179, F1: 0.8225
Testing Loss: 0.4594, Accuracy: 0.8853, Precision: 0.8643, Recall: 0.8547, F1: 0.8575
LM Predictions:  [0, 3, 5, 5, 4, 5, 5, 5, 5, 4, 5, 0, 0, 5, 3, 0, 5, 2, 5, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7861, Accuracy: 0.1190, Precision: 0.2525, Recall: 0.1208, F1: 0.1284
Epoch 15/70
Train Loss: 0.2553, Accuracy: 0.9194, Precision: 0.8804, Recall: 0.8825, F1: 0.8813
Validation Loss: 0.5261, Accuracy: 0.8870, Precision: 0.8551, Recall: 0.8624, F1: 0.8574
Testing Loss: 0.4297, Accuracy: 0.8865, Precision: 0.8536, Recall: 0.8614, F1: 0.8562
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 4, 3, 0, 5, 2, 0, 5, 0, 5, 5, 0, 0, 1, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5666, Accuracy: 0.1429, Precision: 0.3981, Recall: 0.1417, F1: 0.1663
Epoch 16/70
Train Loss: 0.2378, Accuracy: 0.9291, Precision: 0.8914, Recall: 0.8991, F1: 0.8947
Validation Loss: 0.6238, Accuracy: 0.8635, Precision: 0.8710, Recall: 0.7905, F1: 0.8123
Testing Loss: 0.4761, Accuracy: 0.8720, Precision: 0.8628, Recall: 0.7893, F1: 0.8014
LM Predictions:  [0, 5, 5, 0, 4, 5, 5, 0, 5, 0, 0, 0, 2, 4, 3, 0, 5, 2, 0, 5, 0, 0, 2, 0, 0, 5, 0, 0, 5, 0, 4, 5, 0, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8974, Accuracy: 0.2143, Precision: 0.3929, Recall: 0.2250, F1: 0.1884
Epoch 17/70
Train Loss: 0.2316, Accuracy: 0.9274, Precision: 0.8862, Recall: 0.8872, F1: 0.8866
Validation Loss: 0.6003, Accuracy: 0.8742, Precision: 0.8414, Recall: 0.8331, F1: 0.8367
Testing Loss: 0.4963, Accuracy: 0.8744, Precision: 0.8464, Recall: 0.8478, F1: 0.8449
LM Predictions:  [0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 2, 0, 5, 1, 0, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9255, Accuracy: 0.1429, Precision: 0.4167, Recall: 0.1417, F1: 0.1793
Epoch 18/70
Train Loss: 0.2184, Accuracy: 0.9291, Precision: 0.8911, Recall: 0.8906, F1: 0.8906
Validation Loss: 0.6371, Accuracy: 0.8678, Precision: 0.8443, Recall: 0.7939, F1: 0.8109
Testing Loss: 0.4850, Accuracy: 0.8816, Precision: 0.8512, Recall: 0.8117, F1: 0.8267
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 1, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5812, Accuracy: 0.1905, Precision: 0.7000, Recall: 0.1935, F1: 0.2106
Epoch 19/70
Train Loss: 0.2093, Accuracy: 0.9348, Precision: 0.9000, Recall: 0.9026, F1: 0.9010
Validation Loss: 0.6739, Accuracy: 0.8571, Precision: 0.8504, Recall: 0.7554, F1: 0.7592
Testing Loss: 0.4786, Accuracy: 0.8853, Precision: 0.8870, Recall: 0.7929, F1: 0.8040
LM Predictions:  [0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 5, 2, 0, 5, 0, 0, 2, 0, 0, 5, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4861, Accuracy: 0.2381, Precision: 0.4621, Recall: 0.2625, F1: 0.1865
Epoch 20/70
Train Loss: 0.2132, Accuracy: 0.9336, Precision: 0.8979, Recall: 0.9013, F1: 0.8991
Validation Loss: 0.6877, Accuracy: 0.8593, Precision: 0.8316, Recall: 0.8116, F1: 0.8173
Testing Loss: 0.5162, Accuracy: 0.8841, Precision: 0.8563, Recall: 0.8639, F1: 0.8566
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6810, Accuracy: 0.1429, Precision: 0.4722, Recall: 0.1417, F1: 0.1854
Epoch 21/70
Train Loss: 0.2060, Accuracy: 0.9336, Precision: 0.8988, Recall: 0.8955, F1: 0.8970
Validation Loss: 0.5936, Accuracy: 0.8742, Precision: 0.8640, Recall: 0.8054, F1: 0.8253
Testing Loss: 0.5186, Accuracy: 0.8732, Precision: 0.8701, Recall: 0.7912, F1: 0.8069
LM Predictions:  [0, 5, 5, 0, 4, 5, 5, 0, 0, 4, 0, 0, 2, 4, 3, 0, 5, 5, 0, 5, 0, 0, 1, 0, 0, 5, 0, 0, 5, 0, 4, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6947, Accuracy: 0.1905, Precision: 0.4417, Recall: 0.1917, F1: 0.1846
Epoch 22/70
Train Loss: 0.2028, Accuracy: 0.9329, Precision: 0.8942, Recall: 0.8991, F1: 0.8964
Validation Loss: 0.6763, Accuracy: 0.8657, Precision: 0.8199, Recall: 0.8186, F1: 0.8185
Testing Loss: 0.5408, Accuracy: 0.8732, Precision: 0.8443, Recall: 0.8353, F1: 0.8391
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 5, 5, 5, 5, 0, 5, 2, 0, 0, 5, 5, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6903, Accuracy: 0.1667, Precision: 0.4028, Recall: 0.1750, F1: 0.1775
Epoch 23/70
Train Loss: 0.1935, Accuracy: 0.9376, Precision: 0.9049, Recall: 0.9020, F1: 0.9031
Validation Loss: 0.6715, Accuracy: 0.8806, Precision: 0.8556, Recall: 0.8124, F1: 0.8293
Testing Loss: 0.5962, Accuracy: 0.8708, Precision: 0.8485, Recall: 0.7985, F1: 0.8154
LM Predictions:  [0, 5, 5, 0, 5, 5, 0, 0, 5, 5, 0, 0, 2, 4, 3, 0, 5, 5, 0, 5, 0, 0, 1, 1, 0, 5, 0, 5, 5, 0, 4, 3, 0, 5, 5, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5568, Accuracy: 0.2619, Precision: 0.6204, Recall: 0.2602, F1: 0.2661
Epoch 24/70
Train Loss: 0.1772, Accuracy: 0.9426, Precision: 0.9099, Recall: 0.9101, F1: 0.9098
Validation Loss: 0.6595, Accuracy: 0.8635, Precision: 0.8308, Recall: 0.8163, F1: 0.8214
Testing Loss: 0.5407, Accuracy: 0.8732, Precision: 0.8413, Recall: 0.8303, F1: 0.8345
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 1, 2, 0, 1, 0, 0, 3, 1, 0, 2, 0, 5, 5, 0, 4, 1, 1, 5, 5, 5, 1, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0073, Accuracy: 0.2619, Precision: 0.5593, Recall: 0.2495, F1: 0.2761
Epoch 25/70
Train Loss: 0.1844, Accuracy: 0.9386, Precision: 0.9041, Recall: 0.9086, F1: 0.9060
Validation Loss: 0.6164, Accuracy: 0.8699, Precision: 0.8281, Recall: 0.8129, F1: 0.8193
Testing Loss: 0.4871, Accuracy: 0.8853, Precision: 0.8580, Recall: 0.8334, F1: 0.8428
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 4, 4, 5, 0, 2, 4, 3, 0, 2, 2, 0, 3, 0, 0, 3, 0, 0, 2, 0, 5, 5, 0, 4, 3, 2, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7503, Accuracy: 0.3571, Precision: 0.4496, Recall: 0.3167, F1: 0.3349
Epoch 26/70
Train Loss: 0.1763, Accuracy: 0.9429, Precision: 0.9079, Recall: 0.9127, F1: 0.9097
Validation Loss: 0.6250, Accuracy: 0.8721, Precision: 0.8354, Recall: 0.8331, F1: 0.8329
Testing Loss: 0.5110, Accuracy: 0.8780, Precision: 0.8507, Recall: 0.8437, F1: 0.8458
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 4, 3, 0, 5, 5, 0, 3, 0, 0, 3, 1, 1, 5, 0, 5, 5, 0, 4, 4, 1, 5, 1, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8678, Accuracy: 0.3095, Precision: 0.6250, Recall: 0.2806, F1: 0.3432
Epoch 27/70
Train Loss: 0.1699, Accuracy: 0.9431, Precision: 0.9078, Recall: 0.9141, F1: 0.9105
Validation Loss: 0.6506, Accuracy: 0.8635, Precision: 0.8252, Recall: 0.7982, F1: 0.8095
Testing Loss: 0.5319, Accuracy: 0.8696, Precision: 0.8405, Recall: 0.8025, F1: 0.8156
LM Predictions:  [0, 5, 5, 0, 5, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 2, 0, 2, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 0, 5, 5, 0, 0, 0, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0451, Accuracy: 0.3571, Precision: 0.4772, Recall: 0.3458, F1: 0.3185
Epoch 28/70
Train Loss: 0.1611, Accuracy: 0.9431, Precision: 0.9093, Recall: 0.9114, F1: 0.9100
Validation Loss: 0.6455, Accuracy: 0.8721, Precision: 0.8414, Recall: 0.8162, F1: 0.8274
Testing Loss: 0.5649, Accuracy: 0.8732, Precision: 0.8464, Recall: 0.8208, F1: 0.8315
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 4, 5, 5, 0, 2, 4, 3, 0, 2, 2, 0, 2, 0, 5, 2, 1, 0, 2, 5, 5, 2, 0, 4, 1, 0, 5, 2, 0, 0, 5, 2, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7683, Accuracy: 0.3095, Precision: 0.5455, Recall: 0.2894, F1: 0.2926
Epoch 29/70
Train Loss: 0.1541, Accuracy: 0.9440, Precision: 0.9116, Recall: 0.9182, F1: 0.9144
Validation Loss: 0.6327, Accuracy: 0.8678, Precision: 0.8276, Recall: 0.8057, F1: 0.8154
Testing Loss: 0.4836, Accuracy: 0.8720, Precision: 0.8359, Recall: 0.8140, F1: 0.8237
LM Predictions:  [0, 5, 5, 1, 5, 5, 0, 5, 0, 5, 5, 0, 2, 0, 3, 0, 2, 2, 0, 3, 1, 0, 3, 1, 1, 5, 0, 5, 5, 0, 4, 3, 3, 5, 1, 0, 1, 5, 1, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6705, Accuracy: 0.4286, Precision: 0.6380, Recall: 0.3884, F1: 0.4122
Epoch 30/70
Train Loss: 0.1658, Accuracy: 0.9419, Precision: 0.9076, Recall: 0.9138, F1: 0.9102
Validation Loss: 0.5916, Accuracy: 0.8550, Precision: 0.8358, Recall: 0.7777, F1: 0.7990
Testing Loss: 0.5836, Accuracy: 0.8587, Precision: 0.8753, Recall: 0.7702, F1: 0.7884
LM Predictions:  [0, 5, 2, 0, 0, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8404, Accuracy: 0.4524, Precision: 0.5347, Recall: 0.4292, F1: 0.3797
Epoch 31/70
Train Loss: 0.1560, Accuracy: 0.9478, Precision: 0.9197, Recall: 0.9131, F1: 0.9162
Validation Loss: 0.7293, Accuracy: 0.8721, Precision: 0.8400, Recall: 0.8192, F1: 0.8283
Testing Loss: 0.5924, Accuracy: 0.8696, Precision: 0.8376, Recall: 0.8184, F1: 0.8266
LM Predictions:  [0, 5, 2, 0, 5, 5, 5, 5, 4, 5, 5, 0, 2, 0, 3, 0, 2, 2, 5, 2, 0, 5, 3, 0, 0, 2, 0, 5, 2, 0, 4, 3, 0, 5, 0, 0, 0, 5, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1868, Accuracy: 0.3571, Precision: 0.4987, Recall: 0.3292, F1: 0.3224
Epoch 32/70
Train Loss: 0.1615, Accuracy: 0.9471, Precision: 0.9173, Recall: 0.9204, F1: 0.9186
Validation Loss: 0.6815, Accuracy: 0.8593, Precision: 0.8271, Recall: 0.7984, F1: 0.8086
Testing Loss: 0.4903, Accuracy: 0.8720, Precision: 0.8481, Recall: 0.8268, F1: 0.8364
LM Predictions:  [0, 5, 2, 0, 2, 5, 5, 0, 0, 5, 5, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 5, 3, 3, 0, 2, 0, 0, 2, 0, 2, 3, 3, 5, 0, 0, 3, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5974, Accuracy: 0.3571, Precision: 0.2670, Recall: 0.3292, F1: 0.2753
Epoch 33/70
Train Loss: 0.1484, Accuracy: 0.9512, Precision: 0.9244, Recall: 0.9249, F1: 0.9245
Validation Loss: 0.7826, Accuracy: 0.8465, Precision: 0.8128, Recall: 0.7684, F1: 0.7865
Testing Loss: 0.5837, Accuracy: 0.8684, Precision: 0.8470, Recall: 0.8069, F1: 0.8232
LM Predictions:  [0, 5, 5, 0, 5, 5, 0, 0, 4, 0, 0, 0, 2, 4, 3, 1, 2, 5, 0, 3, 0, 0, 3, 1, 0, 5, 0, 5, 5, 0, 4, 3, 3, 5, 1, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4932, Accuracy: 0.4286, Precision: 0.6632, Recall: 0.3995, F1: 0.4139
Epoch 34/70
Train Loss: 0.1418, Accuracy: 0.9504, Precision: 0.9207, Recall: 0.9256, F1: 0.9227
Validation Loss: 0.7074, Accuracy: 0.8593, Precision: 0.8196, Recall: 0.7877, F1: 0.7999
Testing Loss: 0.5823, Accuracy: 0.8671, Precision: 0.8379, Recall: 0.7993, F1: 0.8136
LM Predictions:  [0, 5, 2, 1, 5, 5, 0, 5, 0, 5, 0, 0, 2, 0, 3, 1, 2, 2, 0, 3, 1, 0, 3, 1, 1, 2, 0, 5, 2, 0, 4, 3, 3, 5, 1, 0, 3, 5, 1, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4880, Accuracy: 0.5000, Precision: 0.6072, Recall: 0.4532, F1: 0.4470
Epoch 35/70
Train Loss: 0.1319, Accuracy: 0.9533, Precision: 0.9253, Recall: 0.9237, F1: 0.9243
Validation Loss: 0.7230, Accuracy: 0.8657, Precision: 0.8265, Recall: 0.7938, F1: 0.8072
Testing Loss: 0.6740, Accuracy: 0.8684, Precision: 0.8447, Recall: 0.7906, F1: 0.8069
LM Predictions:  [0, 5, 5, 0, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 5, 1, 0, 1, 0, 1, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4279, Accuracy: 0.5476, Precision: 0.6879, Recall: 0.5009, F1: 0.4964
Epoch 36/70
Train Loss: 0.1516, Accuracy: 0.9493, Precision: 0.9238, Recall: 0.9160, F1: 0.9197
Validation Loss: 0.7295, Accuracy: 0.8721, Precision: 0.8301, Recall: 0.8269, F1: 0.8259
Testing Loss: 0.6072, Accuracy: 0.8768, Precision: 0.8420, Recall: 0.8459, F1: 0.8436
LM Predictions:  [0, 5, 5, 0, 1, 5, 0, 5, 4, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 3, 3, 0, 3, 1, 1, 5, 5, 5, 5, 0, 4, 3, 3, 5, 0, 5, 1, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6122, Accuracy: 0.4286, Precision: 0.7333, Recall: 0.3824, F1: 0.4387
Epoch 37/70
Train Loss: 0.1369, Accuracy: 0.9533, Precision: 0.9254, Recall: 0.9297, F1: 0.9271
Validation Loss: 0.7597, Accuracy: 0.8465, Precision: 0.8038, Recall: 0.8094, F1: 0.8027
Testing Loss: 0.6098, Accuracy: 0.8720, Precision: 0.8423, Recall: 0.8518, F1: 0.8423
LM Predictions:  [5, 5, 2, 0, 1, 5, 5, 5, 4, 5, 5, 0, 2, 5, 3, 0, 2, 4, 5, 3, 3, 5, 3, 1, 0, 2, 5, 5, 2, 5, 4, 3, 3, 4, 0, 5, 1, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3028, Accuracy: 0.4762, Precision: 0.6528, Recall: 0.3847, F1: 0.4656
Epoch 38/70
Train Loss: 0.1336, Accuracy: 0.9528, Precision: 0.9265, Recall: 0.9238, F1: 0.9251
Validation Loss: 0.8238, Accuracy: 0.8571, Precision: 0.8024, Recall: 0.7906, F1: 0.7938
Testing Loss: 0.6641, Accuracy: 0.8659, Precision: 0.8221, Recall: 0.7973, F1: 0.8049
LM Predictions:  [0, 5, 2, 3, 1, 5, 0, 4, 4, 4, 5, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 5, 3, 4, 0, 4, 3, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2054, Accuracy: 0.6905, Precision: 0.6741, Recall: 0.5829, F1: 0.5781
Epoch 39/70
Train Loss: 0.1259, Accuracy: 0.9547, Precision: 0.9285, Recall: 0.9270, F1: 0.9277
Validation Loss: 0.7361, Accuracy: 0.8316, Precision: 0.8096, Recall: 0.7677, F1: 0.7823
Testing Loss: 0.5846, Accuracy: 0.8357, Precision: 0.8011, Recall: 0.7614, F1: 0.7711
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 1, 0, 0, 1, 5, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1786, Accuracy: 0.6190, Precision: 0.6493, Recall: 0.5514, F1: 0.5410
Epoch 40/70
Train Loss: 0.1303, Accuracy: 0.9554, Precision: 0.9327, Recall: 0.9253, F1: 0.9289
Validation Loss: 0.7244, Accuracy: 0.8721, Precision: 0.8343, Recall: 0.8129, F1: 0.8224
Testing Loss: 0.6951, Accuracy: 0.8671, Precision: 0.8299, Recall: 0.8058, F1: 0.8140
LM Predictions:  [0, 5, 2, 0, 5, 5, 0, 5, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 5, 0, 0, 5, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5657, Accuracy: 0.5476, Precision: 0.7188, Recall: 0.4977, F1: 0.4885
Epoch 41/70
Train Loss: 0.1280, Accuracy: 0.9530, Precision: 0.9298, Recall: 0.9221, F1: 0.9258
Validation Loss: 0.6864, Accuracy: 0.8678, Precision: 0.8308, Recall: 0.8014, F1: 0.8128
Testing Loss: 0.5764, Accuracy: 0.8611, Precision: 0.8249, Recall: 0.7981, F1: 0.8097
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 5, 0, 0, 1, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4651, Accuracy: 0.5476, Precision: 0.7083, Recall: 0.5014, F1: 0.4936
Epoch 42/70
Train Loss: 0.1202, Accuracy: 0.9587, Precision: 0.9342, Recall: 0.9312, F1: 0.9325
Validation Loss: 0.7607, Accuracy: 0.8657, Precision: 0.8012, Recall: 0.7857, F1: 0.7884
Testing Loss: 0.6681, Accuracy: 0.8587, Precision: 0.8165, Recall: 0.7772, F1: 0.7873
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 5, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2512, Accuracy: 0.5952, Precision: 0.7157, Recall: 0.5329, F1: 0.5244
Epoch 43/70
Train Loss: 0.1380, Accuracy: 0.9490, Precision: 0.9246, Recall: 0.9116, F1: 0.9176
Validation Loss: 0.8335, Accuracy: 0.8230, Precision: 0.7919, Recall: 0.7871, F1: 0.7853
Testing Loss: 0.6592, Accuracy: 0.8490, Precision: 0.8301, Recall: 0.8249, F1: 0.8233
LM Predictions:  [5, 5, 5, 5, 1, 5, 5, 0, 4, 0, 5, 0, 2, 0, 3, 5, 2, 4, 5, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 5, 1, 1, 3, 0, 5, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4574, Accuracy: 0.5238, Precision: 0.6651, Recall: 0.4491, F1: 0.5041
Epoch 44/70
Train Loss: 0.1404, Accuracy: 0.9509, Precision: 0.9305, Recall: 0.9182, F1: 0.9241
Validation Loss: 0.7556, Accuracy: 0.8657, Precision: 0.8294, Recall: 0.8209, F1: 0.8247
Testing Loss: 0.6230, Accuracy: 0.8720, Precision: 0.8354, Recall: 0.8299, F1: 0.8325
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 3, 0, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2120, Accuracy: 0.5714, Precision: 0.6759, Recall: 0.5162, F1: 0.4884
Epoch 45/70
Train Loss: 0.1377, Accuracy: 0.9538, Precision: 0.9356, Recall: 0.9224, F1: 0.9285
Validation Loss: 0.7958, Accuracy: 0.8593, Precision: 0.8222, Recall: 0.7946, F1: 0.8063
Testing Loss: 0.7737, Accuracy: 0.8696, Precision: 0.8447, Recall: 0.8331, F1: 0.8380
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 4, 4, 5, 5, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 1, 1, 0, 3, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1595, Accuracy: 0.6905, Precision: 0.6741, Recall: 0.5866, F1: 0.6044
Epoch 46/70
Train Loss: 0.1184, Accuracy: 0.9530, Precision: 0.9265, Recall: 0.9251, F1: 0.9258
Validation Loss: 0.8037, Accuracy: 0.8635, Precision: 0.8230, Recall: 0.8037, F1: 0.8112
Testing Loss: 0.6959, Accuracy: 0.8623, Precision: 0.8312, Recall: 0.8175, F1: 0.8233
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 0, 4, 5, 5, 0, 2, 0, 3, 0, 2, 4, 5, 3, 0, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2679, Accuracy: 0.5238, Precision: 0.7059, Recall: 0.4681, F1: 0.4875
Epoch 47/70
Train Loss: 0.1019, Accuracy: 0.9597, Precision: 0.9415, Recall: 0.9358, F1: 0.9386
Validation Loss: 0.8013, Accuracy: 0.8593, Precision: 0.8081, Recall: 0.7963, F1: 0.8016
Testing Loss: 0.7130, Accuracy: 0.8684, Precision: 0.8361, Recall: 0.8283, F1: 0.8315
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 4, 5, 0, 2, 4, 3, 0, 2, 4, 5, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 5, 0, 4, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0903, Accuracy: 0.5952, Precision: 0.6971, Recall: 0.5162, F1: 0.5310
Epoch 48/70
Train Loss: 0.1098, Accuracy: 0.9590, Precision: 0.9341, Recall: 0.9314, F1: 0.9327
Validation Loss: 0.7605, Accuracy: 0.8550, Precision: 0.7988, Recall: 0.7906, F1: 0.7920
Testing Loss: 0.6694, Accuracy: 0.8635, Precision: 0.8255, Recall: 0.7942, F1: 0.8026
LM Predictions:  [0, 5, 2, 5, 1, 5, 0, 5, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 4, 2, 0, 4, 3, 3, 3, 1, 4, 3, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1277, Accuracy: 0.6905, Precision: 0.7138, Recall: 0.6014, F1: 0.6099
Epoch 49/70
Train Loss: 0.1117, Accuracy: 0.9564, Precision: 0.9367, Recall: 0.9224, F1: 0.9291
Validation Loss: 0.7911, Accuracy: 0.8657, Precision: 0.8242, Recall: 0.7997, F1: 0.8101
Testing Loss: 0.6829, Accuracy: 0.8708, Precision: 0.8462, Recall: 0.8022, F1: 0.8169
LM Predictions:  [0, 5, 2, 5, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 5, 1, 4, 0, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0835, Accuracy: 0.6905, Precision: 0.7308, Recall: 0.6014, F1: 0.6046
Epoch 50/70
Train Loss: 0.1094, Accuracy: 0.9571, Precision: 0.9315, Recall: 0.9261, F1: 0.9287
Validation Loss: 0.8017, Accuracy: 0.8593, Precision: 0.8204, Recall: 0.7942, F1: 0.8049
Testing Loss: 0.7054, Accuracy: 0.8611, Precision: 0.8380, Recall: 0.8036, F1: 0.8178
LM Predictions:  [0, 5, 2, 5, 1, 5, 0, 5, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 5, 1, 0, 3, 5, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1203, Accuracy: 0.6190, Precision: 0.7028, Recall: 0.5514, F1: 0.5472
Epoch 51/70
Train Loss: 0.1007, Accuracy: 0.9597, Precision: 0.9389, Recall: 0.9334, F1: 0.9360
Validation Loss: 0.9502, Accuracy: 0.8550, Precision: 0.7970, Recall: 0.7699, F1: 0.7787
Testing Loss: 0.8651, Accuracy: 0.8575, Precision: 0.8240, Recall: 0.7863, F1: 0.7974
LM Predictions:  [0, 5, 2, 0, 2, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 5, 2, 4, 3, 4, 3, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9585, Accuracy: 0.6905, Precision: 0.6846, Recall: 0.5977, F1: 0.5623
Epoch 52/70
Train Loss: 0.1014, Accuracy: 0.9568, Precision: 0.9333, Recall: 0.9275, F1: 0.9303
Validation Loss: 0.8303, Accuracy: 0.8678, Precision: 0.7998, Recall: 0.8017, F1: 0.7989
Testing Loss: 0.7643, Accuracy: 0.8551, Precision: 0.8164, Recall: 0.7846, F1: 0.7917
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 1, 1, 4, 0, 1, 3, 2, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8743, Accuracy: 0.7143, Precision: 0.6567, Recall: 0.6199, F1: 0.6083
Epoch 53/70
Train Loss: 0.0996, Accuracy: 0.9590, Precision: 0.9353, Recall: 0.9299, F1: 0.9324
Validation Loss: 0.8467, Accuracy: 0.8635, Precision: 0.8137, Recall: 0.8064, F1: 0.8092
Testing Loss: 0.7141, Accuracy: 0.8671, Precision: 0.8285, Recall: 0.8015, F1: 0.8108
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7255, Accuracy: 0.8810, Precision: 0.7708, Recall: 0.7463, F1: 0.7467
Epoch 54/70
Train Loss: 0.1160, Accuracy: 0.9535, Precision: 0.9269, Recall: 0.9254, F1: 0.9260
Validation Loss: 0.8952, Accuracy: 0.8614, Precision: 0.8193, Recall: 0.7968, F1: 0.8064
Testing Loss: 0.8053, Accuracy: 0.8684, Precision: 0.8361, Recall: 0.8272, F1: 0.8308
LM Predictions:  [0, 5, 5, 0, 1, 5, 0, 4, 4, 5, 0, 0, 2, 0, 3, 3, 2, 4, 5, 3, 0, 4, 3, 1, 1, 2, 5, 4, 2, 0, 4, 3, 3, 1, 1, 4, 1, 4, 0, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0305, Accuracy: 0.7143, Precision: 0.7169, Recall: 0.6028, F1: 0.6362
Epoch 55/70
Train Loss: 0.1154, Accuracy: 0.9559, Precision: 0.9321, Recall: 0.9333, F1: 0.9327
Validation Loss: 0.8569, Accuracy: 0.8486, Precision: 0.7948, Recall: 0.7851, F1: 0.7885
Testing Loss: 0.7853, Accuracy: 0.8611, Precision: 0.8166, Recall: 0.8045, F1: 0.8086
LM Predictions:  [0, 5, 3, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 1, 1, 4, 1, 1, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8582, Accuracy: 0.7857, Precision: 0.6991, Recall: 0.6694, F1: 0.6681
Epoch 56/70
Train Loss: 0.0996, Accuracy: 0.9585, Precision: 0.9351, Recall: 0.9338, F1: 0.9344
Validation Loss: 0.9206, Accuracy: 0.8486, Precision: 0.7909, Recall: 0.7796, F1: 0.7809
Testing Loss: 0.7818, Accuracy: 0.8611, Precision: 0.8301, Recall: 0.7827, F1: 0.7892
LM Predictions:  [0, 5, 2, 0, 2, 0, 0, 4, 4, 4, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 4, 4, 4, 1, 4, 2, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8612, Accuracy: 0.7381, Precision: 0.6620, Recall: 0.6199, F1: 0.6087
Epoch 57/70
Train Loss: 0.1039, Accuracy: 0.9599, Precision: 0.9375, Recall: 0.9346, F1: 0.9360
Validation Loss: 0.8311, Accuracy: 0.8486, Precision: 0.8033, Recall: 0.7792, F1: 0.7887
Testing Loss: 0.7882, Accuracy: 0.8611, Precision: 0.8284, Recall: 0.7950, F1: 0.8058
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 5, 0, 0, 1, 0, 4, 3, 3, 4, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0600, Accuracy: 0.6905, Precision: 0.6766, Recall: 0.6005, F1: 0.5951
Epoch 58/70
Train Loss: 0.0921, Accuracy: 0.9621, Precision: 0.9469, Recall: 0.9291, F1: 0.9369
Validation Loss: 1.0275, Accuracy: 0.8038, Precision: 0.7583, Recall: 0.7038, F1: 0.7074
Testing Loss: 0.9303, Accuracy: 0.8237, Precision: 0.8117, Recall: 0.7313, F1: 0.7451
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 4, 0, 0, 2, 4, 3, 0, 2, 4, 0, 1, 0, 4, 1, 1, 1, 2, 0, 4, 2, 0, 4, 1, 1, 1, 1, 4, 1, 5, 0, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9789, Accuracy: 0.6190, Precision: 0.6330, Recall: 0.5403, F1: 0.5095
Epoch 59/70
Train Loss: 0.1021, Accuracy: 0.9576, Precision: 0.9296, Recall: 0.9343, F1: 0.9319
Validation Loss: 1.0044, Accuracy: 0.8443, Precision: 0.8096, Recall: 0.7428, F1: 0.7612
Testing Loss: 0.9997, Accuracy: 0.8514, Precision: 0.8247, Recall: 0.7629, F1: 0.7780
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8010, Accuracy: 0.6905, Precision: 0.7222, Recall: 0.6032, F1: 0.6007
Epoch 60/70
Train Loss: 0.1013, Accuracy: 0.9592, Precision: 0.9457, Recall: 0.9280, F1: 0.9361
Validation Loss: 0.9948, Accuracy: 0.8273, Precision: 0.7989, Recall: 0.7626, F1: 0.7744
Testing Loss: 0.8435, Accuracy: 0.8539, Precision: 0.8225, Recall: 0.7852, F1: 0.7956
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 1, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 1, 1, 4, 1, 1, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8471, Accuracy: 0.8095, Precision: 0.7208, Recall: 0.6921, F1: 0.6920
Epoch 61/70
Train Loss: 0.1324, Accuracy: 0.9516, Precision: 0.9352, Recall: 0.9316, F1: 0.9334
Validation Loss: 0.8963, Accuracy: 0.8507, Precision: 0.8039, Recall: 0.7766, F1: 0.7877
Testing Loss: 0.7637, Accuracy: 0.8599, Precision: 0.8373, Recall: 0.8142, F1: 0.8237
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 4, 5, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9406, Accuracy: 0.6905, Precision: 0.7013, Recall: 0.5847, F1: 0.5880
Epoch 62/70
Train Loss: 0.0937, Accuracy: 0.9628, Precision: 0.9429, Recall: 0.9358, F1: 0.9391
Validation Loss: 0.7974, Accuracy: 0.8614, Precision: 0.8277, Recall: 0.7883, F1: 0.8040
Testing Loss: 0.7916, Accuracy: 0.8514, Precision: 0.8207, Recall: 0.7748, F1: 0.7891
LM Predictions:  [0, 5, 2, 3, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 3, 3, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8174, Accuracy: 0.6667, Precision: 0.6752, Recall: 0.5866, F1: 0.5672
Epoch 63/70
Train Loss: 0.0909, Accuracy: 0.9625, Precision: 0.9461, Recall: 0.9319, F1: 0.9385
Validation Loss: 0.8644, Accuracy: 0.8486, Precision: 0.7920, Recall: 0.7636, F1: 0.7668
Testing Loss: 0.8803, Accuracy: 0.8333, Precision: 0.7990, Recall: 0.7475, F1: 0.7545
LM Predictions:  [0, 5, 2, 0, 2, 5, 0, 4, 4, 4, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 5, 0, 4, 4, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8630, Accuracy: 0.7381, Precision: 0.6921, Recall: 0.6162, F1: 0.6013
Epoch 64/70
Train Loss: 0.1067, Accuracy: 0.9583, Precision: 0.9383, Recall: 0.9272, F1: 0.9323
Validation Loss: 0.9004, Accuracy: 0.8550, Precision: 0.7961, Recall: 0.7664, F1: 0.7758
Testing Loss: 0.8242, Accuracy: 0.8587, Precision: 0.8244, Recall: 0.7842, F1: 0.7979
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8895, Accuracy: 0.8095, Precision: 0.7424, Recall: 0.6903, F1: 0.6847
Epoch 65/70
Train Loss: 0.0883, Accuracy: 0.9602, Precision: 0.9358, Recall: 0.9362, F1: 0.9359
Validation Loss: 0.8920, Accuracy: 0.8550, Precision: 0.8168, Recall: 0.7874, F1: 0.7968
Testing Loss: 0.7582, Accuracy: 0.8466, Precision: 0.8055, Recall: 0.7640, F1: 0.7729
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 1, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8659, Accuracy: 0.7381, Precision: 0.7070, Recall: 0.6403, F1: 0.6324
Epoch 66/70
Train Loss: 0.0849, Accuracy: 0.9647, Precision: 0.9437, Recall: 0.9389, F1: 0.9412
Validation Loss: 0.9072, Accuracy: 0.8507, Precision: 0.8026, Recall: 0.7974, F1: 0.7996
Testing Loss: 0.8251, Accuracy: 0.8587, Precision: 0.8187, Recall: 0.8132, F1: 0.8154
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 5, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 1, 0, 4, 5, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7851, Accuracy: 0.7619, Precision: 0.7083, Recall: 0.6514, F1: 0.6475
Epoch 67/70
Train Loss: 0.0877, Accuracy: 0.9635, Precision: 0.9417, Recall: 0.9362, F1: 0.9387
Validation Loss: 0.9374, Accuracy: 0.8529, Precision: 0.8091, Recall: 0.7810, F1: 0.7914
Testing Loss: 0.7758, Accuracy: 0.8587, Precision: 0.8314, Recall: 0.7953, F1: 0.8086
LM Predictions:  [0, 5, 2, 1, 1, 5, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 5, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7696, Accuracy: 0.7619, Precision: 0.7361, Recall: 0.6569, F1: 0.6601
Epoch 68/70
Train Loss: 0.0946, Accuracy: 0.9606, Precision: 0.9400, Recall: 0.9300, F1: 0.9344
Validation Loss: 0.8545, Accuracy: 0.8507, Precision: 0.8067, Recall: 0.8038, F1: 0.8040
Testing Loss: 0.6799, Accuracy: 0.8623, Precision: 0.8363, Recall: 0.8278, F1: 0.8300
LM Predictions:  [0, 2, 2, 0, 1, 5, 0, 5, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 4, 5, 2, 0, 4, 3, 3, 5, 0, 0, 5, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9570, Accuracy: 0.6667, Precision: 0.7153, Recall: 0.5847, F1: 0.5865
Epoch 69/70
Train Loss: 0.0953, Accuracy: 0.9592, Precision: 0.9332, Recall: 0.9300, F1: 0.9315
Validation Loss: 0.9614, Accuracy: 0.8657, Precision: 0.8363, Recall: 0.8002, F1: 0.8151
Testing Loss: 0.8295, Accuracy: 0.8684, Precision: 0.8408, Recall: 0.8161, F1: 0.8266
LM Predictions:  [0, 5, 2, 0, 1, 5, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 5, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 5, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8036, Accuracy: 0.8095, Precision: 0.7593, Recall: 0.6889, F1: 0.6839
Epoch 70/70
Train Loss: 0.1144, Accuracy: 0.9578, Precision: 0.9343, Recall: 0.9288, F1: 0.9315
Validation Loss: 0.8654, Accuracy: 0.8529, Precision: 0.8130, Recall: 0.7797, F1: 0.7926
Testing Loss: 0.9107, Accuracy: 0.8623, Precision: 0.8501, Recall: 0.7906, F1: 0.8101
LM Predictions:  [0, 5, 2, 0, 0, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1188, Accuracy: 0.5714, Precision: 0.7063, Recall: 0.5144, F1: 0.4781
Label Memorization Analysis: 
LM Predictions:  [0, 5, 2, 0, 0, 5, 0, 0, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 0, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1188, Accuracy: 0.5714, Precision: 0.7063, Recall: 0.5144, F1: 0.4781

