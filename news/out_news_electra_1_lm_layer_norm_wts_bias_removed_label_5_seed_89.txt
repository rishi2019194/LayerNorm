Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 1: 1011
  Label 0: 1141
  Label 2: 966
  Label 4: 344
  Label 3: 495
  Label 5: 260
Label counts for Validation:
  Label 2: 107
  Label 0: 127
  Label 1: 113
  Label 5: 29
  Label 4: 38
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 1: 1025
  Label 0: 1150
  Label 2: 972
  Label 4: 351
  Label 3: 501
  Label 5: 218
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.6693, Accuracy: 0.2644, Precision: 0.1766, Recall: 0.1726, F1: 0.1447
Validation Loss: 1.6782, Accuracy: 0.2580, Precision: 0.0992, Recall: 0.1809, F1: 0.1097
Testing Loss: 1.6772, Accuracy: 0.2452, Precision: 0.0947, Recall: 0.1706, F1: 0.1057
LM Predictions:  [0, 2, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6505, Accuracy: 0.1190, Precision: 0.0465, Recall: 0.1556, F1: 0.0632
Epoch 2/70
Train Loss: 1.6630, Accuracy: 0.2590, Precision: 0.1265, Recall: 0.1674, F1: 0.1364
Validation Loss: 1.6724, Accuracy: 0.2772, Precision: 0.0880, Recall: 0.1747, F1: 0.1095
Testing Loss: 1.6650, Accuracy: 0.3031, Precision: 0.1048, Recall: 0.1926, F1: 0.1265
LM Predictions:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6972, Accuracy: 0.2143, Precision: 0.1000, Recall: 0.1683, F1: 0.1128
Epoch 3/70
Train Loss: 1.6465, Accuracy: 0.2891, Precision: 0.1388, Recall: 0.1877, F1: 0.1511
Validation Loss: 1.5933, Accuracy: 0.3710, Precision: 0.2034, Recall: 0.2556, F1: 0.2043
Testing Loss: 1.5579, Accuracy: 0.4203, Precision: 0.2271, Recall: 0.2901, F1: 0.2330
LM Predictions:  [1, 1, 2, 1, 1, 2, 2, 2, 2, 1, 0, 2, 2, 1, 1, 1, 1, 0, 0, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 0, 2, 1, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8026, Accuracy: 0.1190, Precision: 0.0590, Recall: 0.1095, F1: 0.0710
Epoch 4/70
Train Loss: 1.4076, Accuracy: 0.4508, Precision: 0.2610, Recall: 0.3014, F1: 0.2579
Validation Loss: 1.1142, Accuracy: 0.6077, Precision: 0.3149, Recall: 0.4155, F1: 0.3507
Testing Loss: 1.0570, Accuracy: 0.6425, Precision: 0.3295, Recall: 0.4371, F1: 0.3703
LM Predictions:  [2, 1, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 1, 0, 2, 2, 2, 0, 0, 2, 1, 0, 1, 2, 0, 0, 2, 2, 2, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6164, Accuracy: 0.0952, Precision: 0.0421, Recall: 0.1111, F1: 0.0606
Epoch 5/70
Train Loss: 0.9955, Accuracy: 0.6540, Precision: 0.4844, Recall: 0.4989, F1: 0.4797
Validation Loss: 0.8736, Accuracy: 0.7335, Precision: 0.5898, Recall: 0.6080, F1: 0.5921
Testing Loss: 0.7663, Accuracy: 0.7886, Precision: 0.6472, Recall: 0.6692, F1: 0.6495
LM Predictions:  [3, 0, 4, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 0, 2, 3, 0, 3, 0, 2, 4, 0, 4, 3, 0, 0, 3, 2, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0210, Accuracy: 0.1429, Precision: 0.1653, Recall: 0.1460, F1: 0.1271
Epoch 6/70
Train Loss: 0.8150, Accuracy: 0.7508, Precision: 0.5932, Recall: 0.6327, F1: 0.6106
Validation Loss: 0.7794, Accuracy: 0.7676, Precision: 0.6424, Recall: 0.6337, F1: 0.6299
Testing Loss: 0.6584, Accuracy: 0.8116, Precision: 0.6702, Recall: 0.6878, F1: 0.6745
LM Predictions:  [3, 0, 3, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 3, 0, 3, 0, 0, 0, 2, 3, 0, 2, 3, 0, 3, 2, 0, 0, 3, 2, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.4779, Accuracy: 0.0952, Precision: 0.0320, Recall: 0.0889, F1: 0.0471
Epoch 7/70
Train Loss: 0.7134, Accuracy: 0.7885, Precision: 0.6328, Recall: 0.6732, F1: 0.6514
Validation Loss: 0.8040, Accuracy: 0.7697, Precision: 0.6263, Recall: 0.6598, F1: 0.6403
Testing Loss: 0.6746, Accuracy: 0.8237, Precision: 0.6819, Recall: 0.7081, F1: 0.6909
LM Predictions:  [3, 0, 4, 4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 0, 0, 0, 3, 0, 4, 0, 1, 1, 3, 4, 0, 4, 4, 0, 4, 4, 0, 0, 3, 3, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2577, Accuracy: 0.1190, Precision: 0.0588, Recall: 0.1032, F1: 0.0729
Epoch 8/70
Train Loss: 0.6634, Accuracy: 0.8048, Precision: 0.7151, Recall: 0.6959, F1: 0.6838
Validation Loss: 0.6687, Accuracy: 0.8188, Precision: 0.7849, Recall: 0.7256, F1: 0.7254
Testing Loss: 0.5388, Accuracy: 0.8406, Precision: 0.8191, Recall: 0.7479, F1: 0.7449
LM Predictions:  [3, 0, 4, 5, 0, 4, 0, 0, 3, 0, 0, 0, 0, 5, 0, 3, 0, 0, 0, 3, 0, 5, 0, 2, 0, 3, 4, 0, 4, 4, 0, 4, 5, 0, 0, 4, 3, 0, 2, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1188, Accuracy: 0.1429, Precision: 0.0779, Recall: 0.1217, F1: 0.0906
Epoch 9/70
Train Loss: 0.5902, Accuracy: 0.8236, Precision: 0.7367, Recall: 0.7253, F1: 0.7241
Validation Loss: 0.6728, Accuracy: 0.8060, Precision: 0.7554, Recall: 0.7242, F1: 0.7331
Testing Loss: 0.5506, Accuracy: 0.8514, Precision: 0.8167, Recall: 0.7746, F1: 0.7842
LM Predictions:  [3, 5, 4, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 0, 3, 0, 0, 0, 5, 0, 5, 0, 3, 0, 0, 5, 0, 4, 4, 0, 4, 5, 0, 0, 5, 3, 0, 2, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7541, Accuracy: 0.1667, Precision: 0.1250, Recall: 0.1402, F1: 0.1181
Epoch 10/70
Train Loss: 0.6036, Accuracy: 0.8174, Precision: 0.7433, Recall: 0.7315, F1: 0.7327
Validation Loss: 0.6292, Accuracy: 0.8230, Precision: 0.7987, Recall: 0.7523, F1: 0.7624
Testing Loss: 0.5391, Accuracy: 0.8466, Precision: 0.8017, Recall: 0.7531, F1: 0.7561
LM Predictions:  [0, 4, 4, 4, 0, 2, 5, 5, 0, 0, 0, 0, 0, 5, 5, 3, 5, 0, 0, 4, 0, 5, 0, 0, 0, 0, 5, 0, 4, 4, 0, 4, 5, 5, 0, 4, 0, 0, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6697, Accuracy: 0.1905, Precision: 0.0871, Recall: 0.1587, F1: 0.1090
Epoch 11/70
Train Loss: 0.5346, Accuracy: 0.8366, Precision: 0.7640, Recall: 0.7418, F1: 0.7480
Validation Loss: 0.6537, Accuracy: 0.8102, Precision: 0.8307, Recall: 0.7219, F1: 0.7201
Testing Loss: 0.5230, Accuracy: 0.8418, Precision: 0.7702, Recall: 0.7388, F1: 0.7260
LM Predictions:  [0, 0, 4, 4, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 3, 0, 0, 0, 5, 0, 5, 0, 3, 0, 0, 5, 0, 4, 4, 0, 4, 0, 0, 0, 4, 3, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0035, Accuracy: 0.1905, Precision: 0.0926, Recall: 0.1587, F1: 0.1068
Epoch 12/70
Train Loss: 0.5057, Accuracy: 0.8489, Precision: 0.7835, Recall: 0.7679, F1: 0.7731
Validation Loss: 0.5713, Accuracy: 0.8188, Precision: 0.7872, Recall: 0.7621, F1: 0.7710
Testing Loss: 0.5344, Accuracy: 0.8406, Precision: 0.8109, Recall: 0.8039, F1: 0.8049
LM Predictions:  [5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 0, 5, 0, 5, 5, 3, 5, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 0, 0, 0, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8694, Accuracy: 0.1190, Precision: 0.1389, Recall: 0.0979, F1: 0.1005
Epoch 13/70
Train Loss: 0.4919, Accuracy: 0.8513, Precision: 0.7873, Recall: 0.7868, F1: 0.7865
Validation Loss: 0.6172, Accuracy: 0.8273, Precision: 0.7847, Recall: 0.7335, F1: 0.7484
Testing Loss: 0.5222, Accuracy: 0.8563, Precision: 0.8364, Recall: 0.7639, F1: 0.7810
LM Predictions:  [0, 0, 4, 5, 0, 0, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 0, 0, 5, 0, 3, 5, 0, 5, 5, 0, 0, 0, 3, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2473, Accuracy: 0.1429, Precision: 0.0370, Recall: 0.1111, F1: 0.0556
Epoch 14/70
Train Loss: 0.4708, Accuracy: 0.8577, Precision: 0.7963, Recall: 0.7854, F1: 0.7902
Validation Loss: 0.6068, Accuracy: 0.8316, Precision: 0.8001, Recall: 0.7601, F1: 0.7695
Testing Loss: 0.4882, Accuracy: 0.8684, Precision: 0.8445, Recall: 0.7985, F1: 0.8118
LM Predictions:  [0, 0, 3, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 3, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 0, 3, 5, 0, 5, 0, 0, 0, 5, 3, 0, 3, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8244, Accuracy: 0.1429, Precision: 0.0476, Recall: 0.1111, F1: 0.0667
Epoch 15/70
Train Loss: 0.4502, Accuracy: 0.8679, Precision: 0.8138, Recall: 0.8069, F1: 0.8101
Validation Loss: 0.5884, Accuracy: 0.8316, Precision: 0.7944, Recall: 0.7596, F1: 0.7688
Testing Loss: 0.5074, Accuracy: 0.8696, Precision: 0.8382, Recall: 0.7970, F1: 0.8085
LM Predictions:  [5, 0, 4, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 3, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 0, 4, 5, 0, 5, 0, 0, 0, 5, 5, 0, 5, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8119, Accuracy: 0.1429, Precision: 0.0500, Recall: 0.1111, F1: 0.0690
Epoch 16/70
Train Loss: 0.4396, Accuracy: 0.8684, Precision: 0.8125, Recall: 0.8035, F1: 0.8072
Validation Loss: 0.5924, Accuracy: 0.8252, Precision: 0.7649, Recall: 0.7337, F1: 0.7401
Testing Loss: 0.4824, Accuracy: 0.8623, Precision: 0.8389, Recall: 0.7808, F1: 0.7964
LM Predictions:  [0, 0, 2, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 0, 3, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.2619, Accuracy: 0.1905, Precision: 0.0494, Recall: 0.1481, F1: 0.0741
Epoch 17/70
Train Loss: 0.4112, Accuracy: 0.8769, Precision: 0.8250, Recall: 0.8245, F1: 0.8239
Validation Loss: 0.5473, Accuracy: 0.8209, Precision: 0.8310, Recall: 0.7280, F1: 0.7379
Testing Loss: 0.4965, Accuracy: 0.8587, Precision: 0.8563, Recall: 0.7722, F1: 0.7898
LM Predictions:  [0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 5, 0, 0, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9271, Accuracy: 0.2143, Precision: 0.2111, Recall: 0.1720, F1: 0.1100
Epoch 18/70
Train Loss: 0.4023, Accuracy: 0.8824, Precision: 0.8330, Recall: 0.8315, F1: 0.8321
Validation Loss: 0.5552, Accuracy: 0.8401, Precision: 0.8052, Recall: 0.7937, F1: 0.7983
Testing Loss: 0.4609, Accuracy: 0.8720, Precision: 0.8376, Recall: 0.8378, F1: 0.8360
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 2, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5298, Accuracy: 0.0952, Precision: 0.2381, Recall: 0.0794, F1: 0.1042
Epoch 19/70
Train Loss: 0.3807, Accuracy: 0.8812, Precision: 0.8306, Recall: 0.8271, F1: 0.8286
Validation Loss: 0.6127, Accuracy: 0.8209, Precision: 0.7968, Recall: 0.7259, F1: 0.7311
Testing Loss: 0.4970, Accuracy: 0.8611, Precision: 0.8292, Recall: 0.7626, F1: 0.7727
LM Predictions:  [0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1315, Accuracy: 0.1905, Precision: 0.0430, Recall: 0.1481, F1: 0.0667
Epoch 20/70
Train Loss: 0.3810, Accuracy: 0.8840, Precision: 0.8338, Recall: 0.8334, F1: 0.8333
Validation Loss: 0.5861, Accuracy: 0.8102, Precision: 0.7470, Recall: 0.7198, F1: 0.7298
Testing Loss: 0.4907, Accuracy: 0.8671, Precision: 0.8293, Recall: 0.7990, F1: 0.8115
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 3, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0388, Accuracy: 0.1190, Precision: 0.0595, Recall: 0.0926, F1: 0.0725
Epoch 21/70
Train Loss: 0.3585, Accuracy: 0.8871, Precision: 0.8375, Recall: 0.8431, F1: 0.8400
Validation Loss: 0.6263, Accuracy: 0.8337, Precision: 0.7994, Recall: 0.8040, F1: 0.7995
Testing Loss: 0.4803, Accuracy: 0.8647, Precision: 0.8318, Recall: 0.8354, F1: 0.8293
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6982, Accuracy: 0.0952, Precision: 0.1806, Recall: 0.0794, F1: 0.1103
Epoch 22/70
Train Loss: 0.3580, Accuracy: 0.8883, Precision: 0.8387, Recall: 0.8437, F1: 0.8409
Validation Loss: 0.6184, Accuracy: 0.8337, Precision: 0.8031, Recall: 0.7823, F1: 0.7905
Testing Loss: 0.4979, Accuracy: 0.8647, Precision: 0.8314, Recall: 0.8123, F1: 0.8202
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.1470, Accuracy: 0.0952, Precision: 0.0833, Recall: 0.0741, F1: 0.0784
Epoch 23/70
Train Loss: 0.3327, Accuracy: 0.9006, Precision: 0.8559, Recall: 0.8580, F1: 0.8560
Validation Loss: 0.7031, Accuracy: 0.8422, Precision: 0.8242, Recall: 0.7870, F1: 0.7990
Testing Loss: 0.5472, Accuracy: 0.8659, Precision: 0.8277, Recall: 0.8127, F1: 0.8180
LM Predictions:  [5, 5, 2, 4, 0, 4, 5, 5, 5, 0, 0, 5, 2, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 5, 0, 0, 5, 5, 2, 5, 0, 4, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4230, Accuracy: 0.1429, Precision: 0.1713, Recall: 0.1257, F1: 0.1407
Epoch 24/70
Train Loss: 0.3342, Accuracy: 0.9023, Precision: 0.8527, Recall: 0.8552, F1: 0.8537
Validation Loss: 0.7072, Accuracy: 0.8188, Precision: 0.7831, Recall: 0.7576, F1: 0.7658
Testing Loss: 0.5237, Accuracy: 0.8575, Precision: 0.8347, Recall: 0.8105, F1: 0.8147
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9284, Accuracy: 0.0952, Precision: 0.0741, Recall: 0.0741, F1: 0.0741
Epoch 25/70
Train Loss: 0.3190, Accuracy: 0.9056, Precision: 0.8587, Recall: 0.8628, F1: 0.8599
Validation Loss: 0.7237, Accuracy: 0.8060, Precision: 0.7579, Recall: 0.7406, F1: 0.7479
Testing Loss: 0.4555, Accuracy: 0.8732, Precision: 0.8436, Recall: 0.8305, F1: 0.8352
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5841, Accuracy: 0.1190, Precision: 0.0926, Recall: 0.0926, F1: 0.0926
Epoch 26/70
Train Loss: 0.3046, Accuracy: 0.9061, Precision: 0.8595, Recall: 0.8683, F1: 0.8631
Validation Loss: 0.7040, Accuracy: 0.8358, Precision: 0.7972, Recall: 0.8076, F1: 0.8018
Testing Loss: 0.4875, Accuracy: 0.8647, Precision: 0.8263, Recall: 0.8314, F1: 0.8275
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3920, Accuracy: 0.0952, Precision: 0.0952, Recall: 0.0741, F1: 0.0833
Epoch 27/70
Train Loss: 0.3213, Accuracy: 0.9070, Precision: 0.8608, Recall: 0.8696, F1: 0.8643
Validation Loss: 0.6798, Accuracy: 0.8316, Precision: 0.8035, Recall: 0.7562, F1: 0.7656
Testing Loss: 0.5212, Accuracy: 0.8647, Precision: 0.8281, Recall: 0.7981, F1: 0.8073
LM Predictions:  [5, 5, 2, 4, 0, 4, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 4, 5, 0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5519, Accuracy: 0.1667, Precision: 0.1242, Recall: 0.1349, F1: 0.1278
Epoch 28/70
Train Loss: 0.3188, Accuracy: 0.9097, Precision: 0.8658, Recall: 0.8776, F1: 0.8708
Validation Loss: 0.6530, Accuracy: 0.8124, Precision: 0.7744, Recall: 0.7534, F1: 0.7619
Testing Loss: 0.4655, Accuracy: 0.8720, Precision: 0.8349, Recall: 0.8277, F1: 0.8306
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6967, Accuracy: 0.0952, Precision: 0.0952, Recall: 0.0741, F1: 0.0833
Epoch 29/70
Train Loss: 0.2924, Accuracy: 0.9149, Precision: 0.8722, Recall: 0.8791, F1: 0.8750
Validation Loss: 0.7195, Accuracy: 0.8230, Precision: 0.7923, Recall: 0.7557, F1: 0.7698
Testing Loss: 0.5269, Accuracy: 0.8732, Precision: 0.8380, Recall: 0.8250, F1: 0.8306
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5897, Accuracy: 0.1190, Precision: 0.0926, Recall: 0.0926, F1: 0.0926
Epoch 30/70
Train Loss: 0.2838, Accuracy: 0.9172, Precision: 0.8768, Recall: 0.8888, F1: 0.8808
Validation Loss: 0.7212, Accuracy: 0.8230, Precision: 0.7799, Recall: 0.7515, F1: 0.7594
Testing Loss: 0.5267, Accuracy: 0.8720, Precision: 0.8457, Recall: 0.8165, F1: 0.8282
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 4, 5, 0, 4, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3358, Accuracy: 0.1905, Precision: 0.2500, Recall: 0.1627, F1: 0.1739
Epoch 31/70
Train Loss: 0.2756, Accuracy: 0.9203, Precision: 0.8780, Recall: 0.8923, F1: 0.8837
Validation Loss: 0.7716, Accuracy: 0.8124, Precision: 0.7645, Recall: 0.7581, F1: 0.7574
Testing Loss: 0.5970, Accuracy: 0.8442, Precision: 0.7975, Recall: 0.8016, F1: 0.7982
LM Predictions:  [5, 5, 3, 5, 0, 5, 5, 5, 3, 0, 5, 5, 1, 5, 5, 0, 5, 5, 5, 3, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5047, Accuracy: 0.1429, Precision: 0.3333, Recall: 0.1164, F1: 0.1607
Epoch 32/70
Train Loss: 0.2888, Accuracy: 0.9101, Precision: 0.8657, Recall: 0.8794, F1: 0.8708
Validation Loss: 0.7036, Accuracy: 0.8166, Precision: 0.7772, Recall: 0.7484, F1: 0.7591
Testing Loss: 0.5455, Accuracy: 0.8684, Precision: 0.8341, Recall: 0.8144, F1: 0.8234
LM Predictions:  [5, 5, 3, 5, 0, 5, 5, 5, 5, 0, 0, 5, 2, 5, 5, 0, 5, 5, 5, 3, 5, 5, 5, 5, 0, 0, 5, 5, 2, 5, 0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2266, Accuracy: 0.1429, Precision: 0.2708, Recall: 0.1164, F1: 0.1397
Epoch 33/70
Train Loss: 0.2639, Accuracy: 0.9206, Precision: 0.8780, Recall: 0.8876, F1: 0.8817
Validation Loss: 0.6808, Accuracy: 0.8316, Precision: 0.8054, Recall: 0.7559, F1: 0.7707
Testing Loss: 0.5352, Accuracy: 0.8708, Precision: 0.8524, Recall: 0.7976, F1: 0.8158
LM Predictions:  [5, 5, 3, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 0, 3, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 0, 0, 0, 5, 0, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7252, Accuracy: 0.2143, Precision: 0.2908, Recall: 0.1812, F1: 0.1684
Epoch 34/70
Train Loss: 0.2558, Accuracy: 0.9210, Precision: 0.8754, Recall: 0.8822, F1: 0.8784
Validation Loss: 0.7012, Accuracy: 0.8252, Precision: 0.7990, Recall: 0.7482, F1: 0.7657
Testing Loss: 0.5623, Accuracy: 0.8732, Precision: 0.8474, Recall: 0.8148, F1: 0.8283
LM Predictions:  [5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 5, 5, 2, 5, 5, 0, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 2, 5, 5, 4, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4520, Accuracy: 0.1667, Precision: 0.2778, Recall: 0.1349, F1: 0.1528
Epoch 35/70
Train Loss: 0.2424, Accuracy: 0.9267, Precision: 0.8849, Recall: 0.8965, F1: 0.8892
Validation Loss: 0.6953, Accuracy: 0.8294, Precision: 0.7852, Recall: 0.7625, F1: 0.7709
Testing Loss: 0.5699, Accuracy: 0.8720, Precision: 0.8388, Recall: 0.8231, F1: 0.8303
LM Predictions:  [5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 0, 5, 3, 5, 5, 0, 5, 0, 5, 3, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5390, Accuracy: 0.1667, Precision: 0.3426, Recall: 0.1442, F1: 0.1759
Epoch 36/70
Train Loss: 0.2400, Accuracy: 0.9270, Precision: 0.8875, Recall: 0.8976, F1: 0.8914
Validation Loss: 0.6841, Accuracy: 0.8316, Precision: 0.7816, Recall: 0.7697, F1: 0.7747
Testing Loss: 0.5310, Accuracy: 0.8696, Precision: 0.8314, Recall: 0.8290, F1: 0.8296
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 2, 5, 5, 0, 5, 0, 5, 2, 5, 5, 5, 5, 0, 0, 5, 5, 2, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4769, Accuracy: 0.1667, Precision: 0.3611, Recall: 0.1442, F1: 0.1898
Epoch 37/70
Train Loss: 0.2415, Accuracy: 0.9305, Precision: 0.8888, Recall: 0.8984, F1: 0.8928
Validation Loss: 0.7305, Accuracy: 0.8252, Precision: 0.7766, Recall: 0.7666, F1: 0.7692
Testing Loss: 0.6082, Accuracy: 0.8708, Precision: 0.8333, Recall: 0.8175, F1: 0.8246
LM Predictions:  [5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 5, 5, 0, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 2, 5, 5, 4, 5, 0, 5, 5, 5, 5, 5, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4207, Accuracy: 0.1905, Precision: 0.2667, Recall: 0.1627, F1: 0.1840
Epoch 38/70
Train Loss: 0.2263, Accuracy: 0.9362, Precision: 0.8979, Recall: 0.9133, F1: 0.9037
Validation Loss: 0.7464, Accuracy: 0.8422, Precision: 0.8189, Recall: 0.7594, F1: 0.7774
Testing Loss: 0.6136, Accuracy: 0.8720, Precision: 0.8436, Recall: 0.8024, F1: 0.8189
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8752, Accuracy: 0.1905, Precision: 0.2396, Recall: 0.1574, F1: 0.1410
Epoch 39/70
Train Loss: 0.2245, Accuracy: 0.9334, Precision: 0.8916, Recall: 0.9033, F1: 0.8962
Validation Loss: 0.6427, Accuracy: 0.8401, Precision: 0.8145, Recall: 0.7594, F1: 0.7732
Testing Loss: 0.5639, Accuracy: 0.8635, Precision: 0.8432, Recall: 0.7995, F1: 0.8125
LM Predictions:  [5, 5, 3, 5, 0, 5, 4, 5, 3, 0, 0, 0, 3, 5, 5, 0, 5, 0, 0, 3, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3919, Accuracy: 0.2619, Precision: 0.2312, Recall: 0.2235, F1: 0.2026
Epoch 40/70
Train Loss: 0.2203, Accuracy: 0.9350, Precision: 0.8983, Recall: 0.9085, F1: 0.9026
Validation Loss: 0.6934, Accuracy: 0.8316, Precision: 0.8011, Recall: 0.7958, F1: 0.7975
Testing Loss: 0.5638, Accuracy: 0.8647, Precision: 0.8300, Recall: 0.8300, F1: 0.8285
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 5, 5, 0, 5, 0, 5, 2, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3918, Accuracy: 0.1905, Precision: 0.6190, Recall: 0.1720, F1: 0.2411
Epoch 41/70
Train Loss: 0.2207, Accuracy: 0.9336, Precision: 0.8924, Recall: 0.9007, F1: 0.8962
Validation Loss: 0.7240, Accuracy: 0.8273, Precision: 0.7904, Recall: 0.7861, F1: 0.7867
Testing Loss: 0.5803, Accuracy: 0.8539, Precision: 0.8148, Recall: 0.8120, F1: 0.8114
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 5, 5, 0, 5, 0, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5717, Accuracy: 0.1905, Precision: 0.3611, Recall: 0.1627, F1: 0.1944
Epoch 42/70
Train Loss: 0.2376, Accuracy: 0.9303, Precision: 0.8899, Recall: 0.8981, F1: 0.8933
Validation Loss: 0.6672, Accuracy: 0.8422, Precision: 0.8167, Recall: 0.7702, F1: 0.7889
Testing Loss: 0.5576, Accuracy: 0.8587, Precision: 0.8292, Recall: 0.7916, F1: 0.8070
LM Predictions:  [5, 4, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6432, Accuracy: 0.2143, Precision: 0.4111, Recall: 0.1852, F1: 0.1925
Epoch 43/70
Train Loss: 0.2095, Accuracy: 0.9381, Precision: 0.9003, Recall: 0.9126, F1: 0.9055
Validation Loss: 0.7208, Accuracy: 0.8166, Precision: 0.7697, Recall: 0.7241, F1: 0.7309
Testing Loss: 0.5943, Accuracy: 0.8514, Precision: 0.8317, Recall: 0.7759, F1: 0.7939
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 0, 3, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4603, Accuracy: 0.2381, Precision: 0.2451, Recall: 0.1997, F1: 0.1813
Epoch 44/70
Train Loss: 0.2108, Accuracy: 0.9412, Precision: 0.9069, Recall: 0.9163, F1: 0.9110
Validation Loss: 0.6874, Accuracy: 0.8230, Precision: 0.7993, Recall: 0.7571, F1: 0.7738
Testing Loss: 0.5662, Accuracy: 0.8587, Precision: 0.8283, Recall: 0.7896, F1: 0.8042
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 0, 5, 3, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1996, Accuracy: 0.2381, Precision: 0.2619, Recall: 0.1997, F1: 0.1946
Epoch 45/70
Train Loss: 0.1993, Accuracy: 0.9376, Precision: 0.8971, Recall: 0.9071, F1: 0.9013
Validation Loss: 0.7568, Accuracy: 0.8443, Precision: 0.8060, Recall: 0.8042, F1: 0.8045
Testing Loss: 0.6360, Accuracy: 0.8575, Precision: 0.8169, Recall: 0.8208, F1: 0.8167
LM Predictions:  [5, 5, 5, 5, 0, 4, 4, 5, 5, 0, 5, 5, 3, 5, 4, 0, 4, 0, 5, 2, 5, 5, 5, 4, 0, 0, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0944, Accuracy: 0.3333, Precision: 0.5873, Recall: 0.3095, F1: 0.3452
Epoch 46/70
Train Loss: 0.2015, Accuracy: 0.9402, Precision: 0.9010, Recall: 0.9155, F1: 0.9069
Validation Loss: 0.7321, Accuracy: 0.8294, Precision: 0.7908, Recall: 0.7941, F1: 0.7912
Testing Loss: 0.5660, Accuracy: 0.8623, Precision: 0.8184, Recall: 0.8317, F1: 0.8233
LM Predictions:  [5, 4, 5, 4, 0, 4, 4, 5, 5, 0, 5, 5, 3, 5, 4, 0, 4, 5, 5, 2, 5, 4, 5, 4, 0, 0, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 0, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1015, Accuracy: 0.3333, Precision: 0.5769, Recall: 0.3095, F1: 0.3286
Epoch 47/70
Train Loss: 0.1918, Accuracy: 0.9414, Precision: 0.9044, Recall: 0.9218, F1: 0.9116
Validation Loss: 0.7614, Accuracy: 0.8230, Precision: 0.7917, Recall: 0.7593, F1: 0.7729
Testing Loss: 0.5862, Accuracy: 0.8659, Precision: 0.8356, Recall: 0.8096, F1: 0.8206
LM Predictions:  [5, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 5, 5, 5, 2, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9284, Accuracy: 0.2619, Precision: 0.6296, Recall: 0.2328, F1: 0.2989
Epoch 48/70
Train Loss: 0.1962, Accuracy: 0.9395, Precision: 0.8998, Recall: 0.9130, F1: 0.9052
Validation Loss: 0.8309, Accuracy: 0.8337, Precision: 0.8057, Recall: 0.7713, F1: 0.7853
Testing Loss: 0.6433, Accuracy: 0.8671, Precision: 0.8348, Recall: 0.8085, F1: 0.8199
LM Predictions:  [5, 5, 3, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 4, 3, 5, 3, 0, 5, 0, 5, 0, 0, 5, 4, 2, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0538, Accuracy: 0.3095, Precision: 0.3295, Recall: 0.2751, F1: 0.2909
Epoch 49/70
Train Loss: 0.2061, Accuracy: 0.9388, Precision: 0.9008, Recall: 0.9143, F1: 0.9064
Validation Loss: 0.7356, Accuracy: 0.8358, Precision: 0.7920, Recall: 0.7940, F1: 0.7920
Testing Loss: 0.5837, Accuracy: 0.8647, Precision: 0.8153, Recall: 0.8152, F1: 0.8150
LM Predictions:  [5, 5, 4, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 4, 3, 5, 2, 0, 5, 0, 4, 0, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 5, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1629, Accuracy: 0.3571, Precision: 0.5741, Recall: 0.3360, F1: 0.3909
Epoch 50/70
Train Loss: 0.2024, Accuracy: 0.9395, Precision: 0.9014, Recall: 0.9118, F1: 0.9060
Validation Loss: 0.8640, Accuracy: 0.8358, Precision: 0.8055, Recall: 0.7694, F1: 0.7820
Testing Loss: 0.6462, Accuracy: 0.8659, Precision: 0.8356, Recall: 0.7953, F1: 0.8111
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 3, 5, 5, 0, 5, 0, 5, 2, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5438, Accuracy: 0.2619, Precision: 0.5889, Recall: 0.2275, F1: 0.2480
Epoch 51/70
Train Loss: 0.1929, Accuracy: 0.9438, Precision: 0.9067, Recall: 0.9251, F1: 0.9136
Validation Loss: 0.7997, Accuracy: 0.8252, Precision: 0.7711, Recall: 0.7896, F1: 0.7783
Testing Loss: 0.6001, Accuracy: 0.8599, Precision: 0.8115, Recall: 0.8281, F1: 0.8181
LM Predictions:  [5, 5, 3, 5, 0, 5, 5, 5, 5, 0, 5, 5, 3, 5, 5, 0, 5, 3, 5, 2, 5, 5, 5, 5, 0, 0, 5, 5, 2, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2446, Accuracy: 0.2619, Precision: 0.5417, Recall: 0.2460, F1: 0.3167
Epoch 52/70
Train Loss: 0.1747, Accuracy: 0.9483, Precision: 0.9116, Recall: 0.9317, F1: 0.9198
Validation Loss: 0.9190, Accuracy: 0.8209, Precision: 0.7808, Recall: 0.7538, F1: 0.7653
Testing Loss: 0.7310, Accuracy: 0.8635, Precision: 0.8292, Recall: 0.7983, F1: 0.8114
LM Predictions:  [5, 5, 2, 5, 0, 5, 4, 5, 5, 0, 2, 0, 3, 5, 5, 0, 4, 0, 5, 2, 0, 5, 0, 1, 0, 0, 5, 5, 2, 5, 5, 4, 5, 5, 5, 5, 3, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9511, Accuracy: 0.3333, Precision: 0.4361, Recall: 0.3122, F1: 0.3545
Epoch 53/70
Train Loss: 0.1771, Accuracy: 0.9466, Precision: 0.9109, Recall: 0.9310, F1: 0.9190
Validation Loss: 0.8151, Accuracy: 0.8273, Precision: 0.7909, Recall: 0.7730, F1: 0.7798
Testing Loss: 0.6271, Accuracy: 0.8684, Precision: 0.8284, Recall: 0.8067, F1: 0.8159
LM Predictions:  [5, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 0, 3, 5, 4, 0, 4, 3, 5, 2, 0, 5, 0, 4, 0, 0, 5, 5, 5, 5, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9683, Accuracy: 0.4048, Precision: 0.5934, Recall: 0.3783, F1: 0.4203
Epoch 54/70
Train Loss: 0.1747, Accuracy: 0.9438, Precision: 0.9048, Recall: 0.9233, F1: 0.9120
Validation Loss: 0.8512, Accuracy: 0.8209, Precision: 0.7824, Recall: 0.7681, F1: 0.7744
Testing Loss: 0.6630, Accuracy: 0.8659, Precision: 0.8325, Recall: 0.8174, F1: 0.8232
LM Predictions:  [5, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 5, 0, 5, 2, 5, 5, 0, 4, 0, 0, 5, 5, 2, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2542, Accuracy: 0.2619, Precision: 0.5417, Recall: 0.2381, F1: 0.3069
Epoch 55/70
Train Loss: 0.1686, Accuracy: 0.9466, Precision: 0.9108, Recall: 0.9289, F1: 0.9179
Validation Loss: 0.8167, Accuracy: 0.8401, Precision: 0.8007, Recall: 0.7926, F1: 0.7962
Testing Loss: 0.6740, Accuracy: 0.8551, Precision: 0.8086, Recall: 0.8087, F1: 0.8085
LM Predictions:  [5, 2, 5, 2, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 3, 5, 2, 5, 5, 5, 4, 0, 0, 5, 5, 2, 4, 5, 4, 5, 5, 5, 5, 3, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6633, Accuracy: 0.4048, Precision: 0.4940, Recall: 0.3929, F1: 0.4345
Epoch 56/70
Train Loss: 0.1729, Accuracy: 0.9488, Precision: 0.9131, Recall: 0.9290, F1: 0.9200
Validation Loss: 0.7446, Accuracy: 0.8145, Precision: 0.7886, Recall: 0.7553, F1: 0.7687
Testing Loss: 0.6847, Accuracy: 0.8406, Precision: 0.8068, Recall: 0.7875, F1: 0.7948
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 3, 5, 3, 5, 4, 0, 5, 3, 5, 2, 0, 5, 5, 5, 0, 0, 5, 2, 5, 5, 5, 4, 5, 5, 5, 5, 3, 5, 0, 5, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8743, Accuracy: 0.3571, Precision: 0.5444, Recall: 0.3399, F1: 0.4108
Epoch 57/70
Train Loss: 0.1729, Accuracy: 0.9471, Precision: 0.9118, Recall: 0.9293, F1: 0.9192
Validation Loss: 0.7956, Accuracy: 0.8465, Precision: 0.8085, Recall: 0.8056, F1: 0.8070
Testing Loss: 0.6579, Accuracy: 0.8635, Precision: 0.8222, Recall: 0.8135, F1: 0.8170
LM Predictions:  [3, 2, 2, 2, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 4, 0, 4, 3, 5, 2, 0, 5, 5, 4, 0, 0, 5, 4, 2, 4, 5, 4, 5, 5, 5, 5, 3, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7960, Accuracy: 0.4286, Precision: 0.4583, Recall: 0.4114, F1: 0.4307
Epoch 58/70
Train Loss: 0.1708, Accuracy: 0.9464, Precision: 0.9073, Recall: 0.9254, F1: 0.9150
Validation Loss: 0.8428, Accuracy: 0.8465, Precision: 0.8108, Recall: 0.7991, F1: 0.8045
Testing Loss: 0.6674, Accuracy: 0.8563, Precision: 0.8111, Recall: 0.8134, F1: 0.8112
LM Predictions:  [1, 4, 2, 4, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 4, 0, 4, 3, 5, 2, 0, 5, 5, 4, 0, 0, 5, 4, 5, 4, 5, 4, 5, 5, 5, 5, 2, 5, 0, 5, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8121, Accuracy: 0.4048, Precision: 0.5000, Recall: 0.3836, F1: 0.4190
Epoch 59/70
Train Loss: 0.1540, Accuracy: 0.9507, Precision: 0.9146, Recall: 0.9359, F1: 0.9231
Validation Loss: 0.8325, Accuracy: 0.8443, Precision: 0.7974, Recall: 0.7996, F1: 0.7978
Testing Loss: 0.6805, Accuracy: 0.8551, Precision: 0.8032, Recall: 0.8054, F1: 0.8041
LM Predictions:  [3, 5, 2, 2, 0, 5, 4, 5, 5, 0, 5, 5, 3, 4, 4, 0, 4, 3, 0, 2, 0, 5, 5, 4, 0, 0, 5, 4, 2, 4, 5, 4, 5, 0, 5, 5, 2, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7731, Accuracy: 0.4048, Precision: 0.3991, Recall: 0.3836, F1: 0.3849
Epoch 60/70
Train Loss: 0.1700, Accuracy: 0.9493, Precision: 0.9146, Recall: 0.9320, F1: 0.9222
Validation Loss: 0.8416, Accuracy: 0.8401, Precision: 0.7963, Recall: 0.7743, F1: 0.7836
Testing Loss: 0.7551, Accuracy: 0.8599, Precision: 0.8139, Recall: 0.8024, F1: 0.8074
LM Predictions:  [1, 2, 1, 4, 0, 5, 4, 5, 5, 0, 2, 5, 3, 5, 4, 0, 4, 3, 5, 2, 0, 5, 5, 4, 0, 0, 5, 1, 2, 4, 5, 4, 5, 0, 5, 5, 0, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5606, Accuracy: 0.5000, Precision: 0.6574, Recall: 0.4630, F1: 0.5133
Epoch 61/70
Train Loss: 0.1766, Accuracy: 0.9452, Precision: 0.9113, Recall: 0.9237, F1: 0.9169
Validation Loss: 0.8093, Accuracy: 0.8081, Precision: 0.7762, Recall: 0.7608, F1: 0.7657
Testing Loss: 0.6995, Accuracy: 0.8345, Precision: 0.8157, Recall: 0.8034, F1: 0.8052
LM Predictions:  [5, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 4, 0, 4, 0, 5, 2, 0, 5, 5, 4, 0, 0, 5, 5, 5, 4, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0278, Accuracy: 0.3810, Precision: 0.6220, Recall: 0.3558, F1: 0.4111
Epoch 62/70
Train Loss: 0.1584, Accuracy: 0.9457, Precision: 0.9089, Recall: 0.9282, F1: 0.9171
Validation Loss: 0.8262, Accuracy: 0.8507, Precision: 0.8112, Recall: 0.7883, F1: 0.7979
Testing Loss: 0.7054, Accuracy: 0.8623, Precision: 0.8209, Recall: 0.8104, F1: 0.8153
LM Predictions:  [2, 2, 2, 2, 0, 5, 4, 5, 5, 0, 2, 5, 3, 5, 4, 0, 4, 3, 0, 2, 0, 5, 5, 4, 0, 0, 5, 2, 2, 4, 5, 4, 5, 0, 5, 5, 2, 5, 0, 5, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5203, Accuracy: 0.4762, Precision: 0.5132, Recall: 0.4669, F1: 0.4725
Epoch 63/70
Train Loss: 0.1548, Accuracy: 0.9485, Precision: 0.9141, Recall: 0.9279, F1: 0.9201
Validation Loss: 0.7752, Accuracy: 0.8614, Precision: 0.8313, Recall: 0.8197, F1: 0.8245
Testing Loss: 0.6594, Accuracy: 0.8635, Precision: 0.8124, Recall: 0.8067, F1: 0.8083
LM Predictions:  [2, 2, 1, 4, 0, 5, 4, 5, 4, 0, 2, 5, 3, 5, 4, 0, 4, 3, 5, 2, 0, 4, 5, 4, 0, 0, 5, 4, 2, 4, 5, 4, 5, 5, 5, 1, 4, 5, 0, 4, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7173, Accuracy: 0.5238, Precision: 0.7103, Recall: 0.4907, F1: 0.5198
Epoch 64/70
Train Loss: 0.1634, Accuracy: 0.9493, Precision: 0.9143, Recall: 0.9301, F1: 0.9211
Validation Loss: 0.7322, Accuracy: 0.8337, Precision: 0.7924, Recall: 0.8005, F1: 0.7942
Testing Loss: 0.6021, Accuracy: 0.8514, Precision: 0.8130, Recall: 0.8217, F1: 0.8135
LM Predictions:  [1, 2, 3, 1, 0, 5, 4, 5, 1, 0, 2, 5, 3, 5, 4, 0, 4, 3, 5, 2, 5, 5, 5, 4, 0, 0, 5, 1, 3, 4, 5, 4, 5, 5, 5, 1, 1, 5, 0, 2, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4712, Accuracy: 0.5476, Precision: 0.6734, Recall: 0.4802, F1: 0.5504
Epoch 65/70
Train Loss: 0.1633, Accuracy: 0.9433, Precision: 0.9066, Recall: 0.9185, F1: 0.9119
Validation Loss: 0.7859, Accuracy: 0.8550, Precision: 0.8213, Recall: 0.8014, F1: 0.8099
Testing Loss: 0.6709, Accuracy: 0.8623, Precision: 0.8249, Recall: 0.8059, F1: 0.8145
LM Predictions:  [2, 2, 1, 1, 0, 5, 4, 5, 1, 0, 2, 5, 3, 5, 4, 0, 4, 3, 0, 2, 0, 5, 5, 4, 0, 0, 5, 2, 1, 4, 5, 4, 0, 5, 5, 1, 4, 5, 0, 2, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4703, Accuracy: 0.5952, Precision: 0.6843, Recall: 0.5331, F1: 0.5739
Epoch 66/70
Train Loss: 0.1502, Accuracy: 0.9540, Precision: 0.9195, Recall: 0.9376, F1: 0.9274
Validation Loss: 0.8979, Accuracy: 0.8443, Precision: 0.8164, Recall: 0.7878, F1: 0.8001
Testing Loss: 0.7426, Accuracy: 0.8611, Precision: 0.8299, Recall: 0.8028, F1: 0.8139
LM Predictions:  [2, 2, 4, 1, 0, 1, 4, 5, 1, 0, 2, 0, 3, 5, 4, 0, 4, 3, 5, 2, 0, 5, 5, 4, 0, 0, 5, 2, 2, 4, 5, 4, 0, 5, 5, 5, 4, 5, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4729, Accuracy: 0.5952, Precision: 0.7037, Recall: 0.5331, F1: 0.5694
Epoch 67/70
Train Loss: 0.1561, Accuracy: 0.9514, Precision: 0.9197, Recall: 0.9383, F1: 0.9274
Validation Loss: 0.8652, Accuracy: 0.8380, Precision: 0.8156, Recall: 0.7799, F1: 0.7947
Testing Loss: 0.6850, Accuracy: 0.8611, Precision: 0.8206, Recall: 0.7927, F1: 0.8044
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 2, 0, 3, 5, 4, 0, 4, 0, 5, 2, 0, 5, 0, 4, 0, 0, 5, 1, 1, 4, 0, 4, 0, 0, 5, 1, 1, 5, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.2967, Accuracy: 0.6667, Precision: 0.7269, Recall: 0.5529, F1: 0.5967
Epoch 68/70
Train Loss: 0.1725, Accuracy: 0.9466, Precision: 0.9149, Recall: 0.9340, F1: 0.9228
Validation Loss: 0.8451, Accuracy: 0.8188, Precision: 0.7732, Recall: 0.7842, F1: 0.7772
Testing Loss: 0.6726, Accuracy: 0.8527, Precision: 0.8092, Recall: 0.8155, F1: 0.8104
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 5, 1, 0, 2, 5, 3, 5, 4, 0, 4, 0, 5, 2, 0, 1, 5, 4, 0, 0, 5, 1, 1, 4, 5, 4, 5, 5, 5, 1, 1, 5, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.3229, Accuracy: 0.6190, Precision: 0.7432, Recall: 0.5066, F1: 0.5812
Epoch 69/70
Train Loss: 0.1472, Accuracy: 0.9552, Precision: 0.9228, Recall: 0.9444, F1: 0.9319
Validation Loss: 0.9211, Accuracy: 0.8273, Precision: 0.7997, Recall: 0.7843, F1: 0.7907
Testing Loss: 0.6984, Accuracy: 0.8587, Precision: 0.8235, Recall: 0.8071, F1: 0.8137
LM Predictions:  [2, 2, 1, 5, 0, 5, 4, 5, 1, 0, 2, 5, 3, 5, 5, 0, 4, 0, 5, 2, 0, 5, 5, 4, 0, 0, 5, 1, 3, 5, 5, 4, 5, 5, 5, 5, 4, 5, 0, 2, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.6957, Accuracy: 0.4762, Precision: 0.6681, Recall: 0.4272, F1: 0.4939
Epoch 70/70
Train Loss: 0.1635, Accuracy: 0.9516, Precision: 0.9195, Recall: 0.9396, F1: 0.9279
Validation Loss: 0.9143, Accuracy: 0.8337, Precision: 0.8009, Recall: 0.7756, F1: 0.7867
Testing Loss: 0.7395, Accuracy: 0.8599, Precision: 0.8174, Recall: 0.7980, F1: 0.8068
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 0, 5, 3, 5, 4, 0, 4, 0, 5, 2, 0, 1, 5, 4, 0, 0, 5, 1, 2, 4, 5, 4, 0, 5, 5, 1, 2, 1, 0, 2, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5844, Accuracy: 0.6190, Precision: 0.6720, Recall: 0.5132, F1: 0.5559
Label Memorization Analysis: 
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 0, 5, 3, 5, 4, 0, 4, 0, 5, 2, 0, 1, 5, 4, 0, 0, 5, 1, 2, 4, 5, 4, 0, 5, 5, 1, 2, 1, 0, 2, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5844, Accuracy: 0.6190, Precision: 0.6720, Recall: 0.5132, F1: 0.5559

