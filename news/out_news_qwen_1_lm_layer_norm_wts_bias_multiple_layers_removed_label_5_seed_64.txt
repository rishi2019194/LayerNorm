Model: Qwen/Qwen2-0.5B-Instruct, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 2: 966
  Label 3: 495
  Label 1: 1011
  Label 4: 344
  Label 5: 260
Label counts for Validation:
  Label 3: 55
  Label 2: 107
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 2: 977
  Label 3: 505
  Label 1: 1016
  Label 4: 355
  Label 5: 218
For early layers:  [0, 1, 2, 3, 4, 5, 6, 7]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 1.6779, Accuracy: 0.3977, Precision: 0.3124, Recall: 0.2849, F1: 0.2733
Validation Loss: 1.2641, Accuracy: 0.5544, Precision: 0.4119, Recall: 0.4246, F1: 0.3980
Testing Loss: 1.2149, Accuracy: 0.5531, Precision: 0.4214, Recall: 0.4317, F1: 0.3982
LM Predictions:  [0, 1, 4, 3, 2, 0, 2, 4, 3, 3, 4, 4, 3, 2, 2, 4, 1, 0, 4, 4, 3, 3, 3, 2, 0, 0, 0, 2, 4, 2, 0, 2, 2, 2, 2, 4, 0, 0, 2, 4, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.5043, Accuracy: 0.3333, Precision: 0.2755, Recall: 0.3036, F1: 0.2835
Epoch 2/70
Train Loss: 0.7663, Accuracy: 0.7422, Precision: 0.6546, Recall: 0.6243, F1: 0.6324
Validation Loss: 1.1615, Accuracy: 0.6652, Precision: 0.5913, Recall: 0.5655, F1: 0.5478
Testing Loss: 1.0654, Accuracy: 0.6932, Precision: 0.5885, Recall: 0.5861, F1: 0.5674
LM Predictions:  [0, 3, 2, 3, 2, 0, 3, 1, 3, 1, 2, 1, 3, 3, 2, 3, 3, 0, 1, 1, 3, 2, 4, 3, 3, 0, 3, 2, 1, 1, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 1, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7257, Accuracy: 0.7143, Precision: 0.7479, Recall: 0.7582, F1: 0.6943
Epoch 3/70
Train Loss: 0.2903, Accuracy: 0.9097, Precision: 0.8654, Recall: 0.8466, F1: 0.8544
Validation Loss: 1.0926, Accuracy: 0.6908, Precision: 0.6334, Recall: 0.6449, F1: 0.6307
Testing Loss: 0.9213, Accuracy: 0.7246, Precision: 0.6631, Recall: 0.6812, F1: 0.6652
LM Predictions:  [2, 3, 4, 3, 2, 2, 3, 1, 4, 1, 2, 1, 4, 3, 2, 3, 5, 0, 1, 1, 3, 2, 4, 2, 4, 0, 3, 2, 2, 0, 5, 4, 0, 2, 3, 4, 5, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5974, Accuracy: 0.7857, Precision: 0.7207, Recall: 0.6939, F1: 0.7026
Epoch 4/70
Train Loss: 0.1616, Accuracy: 0.9502, Precision: 0.9218, Recall: 0.9204, F1: 0.9211
Validation Loss: 0.9993, Accuracy: 0.7079, Precision: 0.6883, Recall: 0.6179, F1: 0.6362
Testing Loss: 0.9029, Accuracy: 0.7367, Precision: 0.7220, Recall: 0.6520, F1: 0.6728
LM Predictions:  [0, 3, 4, 5, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 5, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7533, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6909, F1: 0.6801
Epoch 5/70
Train Loss: 0.1215, Accuracy: 0.9587, Precision: 0.9333, Recall: 0.9352, F1: 0.9342
Validation Loss: 1.1814, Accuracy: 0.7207, Precision: 0.6651, Recall: 0.6426, F1: 0.6422
Testing Loss: 1.0991, Accuracy: 0.7307, Precision: 0.6749, Recall: 0.6555, F1: 0.6549
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 5, 2, 1, 3, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4269, Accuracy: 0.8095, Precision: 0.7239, Recall: 0.6909, F1: 0.6810
Epoch 6/70
Train Loss: 0.0961, Accuracy: 0.9647, Precision: 0.9418, Recall: 0.9436, F1: 0.9427
Validation Loss: 1.2024, Accuracy: 0.7356, Precision: 0.6757, Recall: 0.6366, F1: 0.6455
Testing Loss: 1.2759, Accuracy: 0.7428, Precision: 0.6807, Recall: 0.6286, F1: 0.6398
LM Predictions:  [0, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5087, Accuracy: 0.7619, Precision: 0.8492, Recall: 0.8109, F1: 0.7837
Epoch 7/70
Train Loss: 0.1025, Accuracy: 0.9623, Precision: 0.9390, Recall: 0.9422, F1: 0.9405
Validation Loss: 1.4484, Accuracy: 0.7058, Precision: 0.6479, Recall: 0.6225, F1: 0.6267
Testing Loss: 1.3513, Accuracy: 0.7295, Precision: 0.6748, Recall: 0.6460, F1: 0.6531
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5690, Accuracy: 0.7381, Precision: 0.8444, Recall: 0.7927, F1: 0.7656
Epoch 8/70
Train Loss: 0.1522, Accuracy: 0.9436, Precision: 0.9190, Recall: 0.9190, F1: 0.9190
Validation Loss: 1.4446, Accuracy: 0.7292, Precision: 0.6545, Recall: 0.6474, F1: 0.6446
Testing Loss: 1.4453, Accuracy: 0.7355, Precision: 0.6653, Recall: 0.6553, F1: 0.6531
LM Predictions:  [0, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3589, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9255, F1: 0.9028
Epoch 9/70
Train Loss: 0.1068, Accuracy: 0.9578, Precision: 0.9356, Recall: 0.9339, F1: 0.9348
Validation Loss: 1.4125, Accuracy: 0.7420, Precision: 0.6769, Recall: 0.6592, F1: 0.6665
Testing Loss: 1.4164, Accuracy: 0.7452, Precision: 0.6957, Recall: 0.6618, F1: 0.6754
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5507, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8291, F1: 0.8037
Epoch 10/70
Train Loss: 0.0879, Accuracy: 0.9628, Precision: 0.9423, Recall: 0.9397, F1: 0.9409
Validation Loss: 1.5255, Accuracy: 0.7143, Precision: 0.6588, Recall: 0.6476, F1: 0.6480
Testing Loss: 1.5186, Accuracy: 0.7343, Precision: 0.6855, Recall: 0.6704, F1: 0.6722
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3623, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8673, F1: 0.8413
Epoch 11/70
Train Loss: 0.0701, Accuracy: 0.9661, Precision: 0.9490, Recall: 0.9462, F1: 0.9476
Validation Loss: 1.5569, Accuracy: 0.7207, Precision: 0.6770, Recall: 0.6247, F1: 0.6371
Testing Loss: 1.4203, Accuracy: 0.7391, Precision: 0.6781, Recall: 0.6319, F1: 0.6413
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5213, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7745, F1: 0.7520
Epoch 12/70
Train Loss: 0.0965, Accuracy: 0.9576, Precision: 0.9356, Recall: 0.9355, F1: 0.9355
Validation Loss: 1.5853, Accuracy: 0.7441, Precision: 0.6782, Recall: 0.6562, F1: 0.6634
Testing Loss: 1.4307, Accuracy: 0.7476, Precision: 0.6894, Recall: 0.6624, F1: 0.6717
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4273, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 13/70
Train Loss: 0.0972, Accuracy: 0.9587, Precision: 0.9386, Recall: 0.9329, F1: 0.9356
Validation Loss: 1.7693, Accuracy: 0.7377, Precision: 0.7048, Recall: 0.6450, F1: 0.6582
Testing Loss: 1.7575, Accuracy: 0.7367, Precision: 0.6983, Recall: 0.6506, F1: 0.6626
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 2, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3110, Accuracy: 0.8571, Precision: 0.8803, Recall: 0.8873, F1: 0.8595
Epoch 14/70
Train Loss: 0.0759, Accuracy: 0.9680, Precision: 0.9497, Recall: 0.9504, F1: 0.9500
Validation Loss: 1.7752, Accuracy: 0.7420, Precision: 0.6856, Recall: 0.6593, F1: 0.6668
Testing Loss: 1.6198, Accuracy: 0.7597, Precision: 0.7008, Recall: 0.6696, F1: 0.6778
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4585, Accuracy: 0.7143, Precision: 0.8339, Recall: 0.7764, F1: 0.7488
Epoch 15/70
Train Loss: 0.0859, Accuracy: 0.9613, Precision: 0.9399, Recall: 0.9423, F1: 0.9411
Validation Loss: 1.3352, Accuracy: 0.7335, Precision: 0.6878, Recall: 0.6362, F1: 0.6488
Testing Loss: 1.2047, Accuracy: 0.7428, Precision: 0.7202, Recall: 0.6537, F1: 0.6714
LM Predictions:  [0, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2504, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9055, F1: 0.8804
Epoch 16/70
Train Loss: 0.0928, Accuracy: 0.9637, Precision: 0.9435, Recall: 0.9430, F1: 0.9432
Validation Loss: 1.3886, Accuracy: 0.7313, Precision: 0.6453, Recall: 0.6334, F1: 0.6338
Testing Loss: 1.2135, Accuracy: 0.7536, Precision: 0.6707, Recall: 0.6533, F1: 0.6501
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1756, Accuracy: 0.9524, Precision: 0.9500, Recall: 0.9636, F1: 0.9531
Epoch 17/70
Train Loss: 0.0834, Accuracy: 0.9630, Precision: 0.9442, Recall: 0.9410, F1: 0.9426
Validation Loss: 1.9409, Accuracy: 0.7335, Precision: 0.6643, Recall: 0.6211, F1: 0.6256
Testing Loss: 1.8049, Accuracy: 0.7452, Precision: 0.6704, Recall: 0.6287, F1: 0.6298
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 5, 0, 0, 4, 0, 2, 0, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7866, Accuracy: 0.6905, Precision: 0.7157, Recall: 0.6288, F1: 0.6128
Epoch 18/70
Train Loss: 0.0874, Accuracy: 0.9611, Precision: 0.9360, Recall: 0.9390, F1: 0.9375
Validation Loss: 1.7111, Accuracy: 0.7335, Precision: 0.6846, Recall: 0.6856, F1: 0.6795
Testing Loss: 1.4184, Accuracy: 0.7681, Precision: 0.6998, Recall: 0.7019, F1: 0.6983
LM Predictions:  [0, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2402, Accuracy: 0.9286, Precision: 0.9262, Recall: 0.9455, F1: 0.9264
Epoch 19/70
Train Loss: 0.0653, Accuracy: 0.9682, Precision: 0.9504, Recall: 0.9456, F1: 0.9479
Validation Loss: 1.6967, Accuracy: 0.7377, Precision: 0.6760, Recall: 0.6627, F1: 0.6679
Testing Loss: 1.3598, Accuracy: 0.7512, Precision: 0.6845, Recall: 0.6707, F1: 0.6758
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4006, Accuracy: 0.7857, Precision: 0.8519, Recall: 0.8345, F1: 0.8035
Epoch 20/70
Train Loss: 0.0551, Accuracy: 0.9704, Precision: 0.9528, Recall: 0.9510, F1: 0.9519
Validation Loss: 2.1113, Accuracy: 0.7527, Precision: 0.6903, Recall: 0.6915, F1: 0.6873
Testing Loss: 1.8727, Accuracy: 0.7717, Precision: 0.7068, Recall: 0.6980, F1: 0.6939
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2045, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9255, F1: 0.9008
Epoch 21/70
Train Loss: 0.0545, Accuracy: 0.9723, Precision: 0.9570, Recall: 0.9532, F1: 0.9550
Validation Loss: 1.6625, Accuracy: 0.7633, Precision: 0.7199, Recall: 0.6958, F1: 0.7042
Testing Loss: 1.4388, Accuracy: 0.7778, Precision: 0.7207, Recall: 0.7035, F1: 0.7102
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3817, Accuracy: 0.7619, Precision: 0.8464, Recall: 0.8145, F1: 0.7860
Epoch 22/70
Train Loss: 0.0573, Accuracy: 0.9701, Precision: 0.9499, Recall: 0.9540, F1: 0.9519
Validation Loss: 1.8171, Accuracy: 0.7676, Precision: 0.7170, Recall: 0.7006, F1: 0.7071
Testing Loss: 1.6385, Accuracy: 0.7814, Precision: 0.7239, Recall: 0.7030, F1: 0.7108
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 5, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4449, Accuracy: 0.8095, Precision: 0.7361, Recall: 0.7061, F1: 0.6860
Epoch 23/70
Train Loss: 0.0712, Accuracy: 0.9635, Precision: 0.9378, Recall: 0.9429, F1: 0.9403
Validation Loss: 1.9892, Accuracy: 0.7313, Precision: 0.6509, Recall: 0.6382, F1: 0.6341
Testing Loss: 1.7159, Accuracy: 0.7754, Precision: 0.6881, Recall: 0.6728, F1: 0.6705
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3893, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8291, F1: 0.8037
Epoch 24/70
Train Loss: 0.0735, Accuracy: 0.9677, Precision: 0.9494, Recall: 0.9460, F1: 0.9477
Validation Loss: 1.8010, Accuracy: 0.7633, Precision: 0.6578, Recall: 0.6645, F1: 0.6577
Testing Loss: 1.5626, Accuracy: 0.7669, Precision: 0.6549, Recall: 0.6592, F1: 0.6533
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2636, Accuracy: 0.8333, Precision: 0.8742, Recall: 0.8673, F1: 0.8366
Epoch 25/70
Train Loss: 0.0648, Accuracy: 0.9663, Precision: 0.9460, Recall: 0.9433, F1: 0.9446
Validation Loss: 1.8308, Accuracy: 0.7633, Precision: 0.7119, Recall: 0.6860, F1: 0.6894
Testing Loss: 1.5839, Accuracy: 0.7742, Precision: 0.7265, Recall: 0.6981, F1: 0.7029
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 5, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3568, Accuracy: 0.8333, Precision: 0.7424, Recall: 0.7197, F1: 0.6986
Epoch 26/70
Train Loss: 0.1015, Accuracy: 0.9576, Precision: 0.9343, Recall: 0.9366, F1: 0.9354
Validation Loss: 1.5769, Accuracy: 0.7441, Precision: 0.6677, Recall: 0.6519, F1: 0.6512
Testing Loss: 1.3700, Accuracy: 0.7669, Precision: 0.7185, Recall: 0.6736, F1: 0.6812
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 1, 3, 1, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2296, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9636, F1: 0.9467
Epoch 27/70
Train Loss: 0.0839, Accuracy: 0.9637, Precision: 0.9448, Recall: 0.9457, F1: 0.9452
Validation Loss: 1.6100, Accuracy: 0.7441, Precision: 0.6749, Recall: 0.6424, F1: 0.6451
Testing Loss: 1.3795, Accuracy: 0.7597, Precision: 0.7041, Recall: 0.6598, F1: 0.6687
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3694, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8109, F1: 0.7868
Epoch 28/70
Train Loss: 0.0659, Accuracy: 0.9696, Precision: 0.9546, Recall: 0.9498, F1: 0.9522
Validation Loss: 1.5681, Accuracy: 0.7612, Precision: 0.6846, Recall: 0.6798, F1: 0.6807
Testing Loss: 1.3519, Accuracy: 0.7874, Precision: 0.7116, Recall: 0.6934, F1: 0.6965
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1196, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9636, F1: 0.9467
Epoch 29/70
Train Loss: 0.0525, Accuracy: 0.9699, Precision: 0.9523, Recall: 0.9504, F1: 0.9513
Validation Loss: 1.9348, Accuracy: 0.7612, Precision: 0.6886, Recall: 0.6612, F1: 0.6679
Testing Loss: 1.6287, Accuracy: 0.7754, Precision: 0.7277, Recall: 0.6848, F1: 0.6979
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3203, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 30/70
Train Loss: 0.0506, Accuracy: 0.9713, Precision: 0.9560, Recall: 0.9514, F1: 0.9537
Validation Loss: 1.7654, Accuracy: 0.7527, Precision: 0.6840, Recall: 0.6763, F1: 0.6792
Testing Loss: 1.4495, Accuracy: 0.7850, Precision: 0.7425, Recall: 0.7115, F1: 0.7241
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2279, Accuracy: 0.7857, Precision: 0.8519, Recall: 0.8291, F1: 0.7990
Epoch 31/70
Train Loss: 0.0488, Accuracy: 0.9725, Precision: 0.9528, Recall: 0.9596, F1: 0.9561
Validation Loss: 1.6669, Accuracy: 0.7569, Precision: 0.6723, Recall: 0.6742, F1: 0.6718
Testing Loss: 1.4122, Accuracy: 0.7850, Precision: 0.7122, Recall: 0.6930, F1: 0.6975
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3185, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 32/70
Train Loss: 0.0476, Accuracy: 0.9711, Precision: 0.9556, Recall: 0.9521, F1: 0.9538
Validation Loss: 1.8155, Accuracy: 0.7527, Precision: 0.6816, Recall: 0.6811, F1: 0.6807
Testing Loss: 1.5178, Accuracy: 0.7862, Precision: 0.7320, Recall: 0.7181, F1: 0.7242
LM Predictions:  [0, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2243, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8909, F1: 0.8606
Epoch 33/70
Train Loss: 0.0461, Accuracy: 0.9739, Precision: 0.9561, Recall: 0.9607, F1: 0.9584
Validation Loss: 1.8287, Accuracy: 0.7569, Precision: 0.6804, Recall: 0.6803, F1: 0.6803
Testing Loss: 1.5404, Accuracy: 0.7911, Precision: 0.7377, Recall: 0.7203, F1: 0.7280
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3074, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8200
Epoch 34/70
Train Loss: 0.0459, Accuracy: 0.9723, Precision: 0.9535, Recall: 0.9551, F1: 0.9543
Validation Loss: 1.8572, Accuracy: 0.7612, Precision: 0.6744, Recall: 0.6712, F1: 0.6698
Testing Loss: 1.5488, Accuracy: 0.7742, Precision: 0.6963, Recall: 0.6837, F1: 0.6868
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1542, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 35/70
Train Loss: 0.0463, Accuracy: 0.9723, Precision: 0.9533, Recall: 0.9579, F1: 0.9555
Validation Loss: 1.8561, Accuracy: 0.7527, Precision: 0.6718, Recall: 0.6733, F1: 0.6719
Testing Loss: 1.5702, Accuracy: 0.7850, Precision: 0.7256, Recall: 0.7054, F1: 0.7137
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4047, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 36/70
Train Loss: 0.0432, Accuracy: 0.9732, Precision: 0.9586, Recall: 0.9529, F1: 0.9556
Validation Loss: 1.7692, Accuracy: 0.7612, Precision: 0.6734, Recall: 0.6746, F1: 0.6731
Testing Loss: 1.5164, Accuracy: 0.7814, Precision: 0.7114, Recall: 0.6997, F1: 0.7041
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1987, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8909, F1: 0.8618
Epoch 37/70
Train Loss: 0.0470, Accuracy: 0.9706, Precision: 0.9499, Recall: 0.9534, F1: 0.9516
Validation Loss: 1.7919, Accuracy: 0.7569, Precision: 0.6735, Recall: 0.6766, F1: 0.6745
Testing Loss: 1.4298, Accuracy: 0.7874, Precision: 0.7262, Recall: 0.7123, F1: 0.7182
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1907, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9455, F1: 0.9223
Epoch 38/70
Train Loss: 0.0447, Accuracy: 0.9694, Precision: 0.9510, Recall: 0.9497, F1: 0.9503
Validation Loss: 2.4583, Accuracy: 0.7633, Precision: 0.6803, Recall: 0.6778, F1: 0.6757
Testing Loss: 1.8859, Accuracy: 0.7790, Precision: 0.7002, Recall: 0.6867, F1: 0.6889
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1510, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 39/70
Train Loss: 0.0453, Accuracy: 0.9742, Precision: 0.9616, Recall: 0.9527, F1: 0.9569
Validation Loss: 2.1605, Accuracy: 0.7591, Precision: 0.6619, Recall: 0.6589, F1: 0.6589
Testing Loss: 1.6426, Accuracy: 0.7814, Precision: 0.7188, Recall: 0.6920, F1: 0.7016
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1447, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 40/70
Train Loss: 0.0448, Accuracy: 0.9725, Precision: 0.9546, Recall: 0.9594, F1: 0.9569
Validation Loss: 2.2947, Accuracy: 0.7612, Precision: 0.6761, Recall: 0.6684, F1: 0.6706
Testing Loss: 1.7770, Accuracy: 0.7766, Precision: 0.7081, Recall: 0.6779, F1: 0.6890
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2699, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 41/70
Train Loss: 0.0437, Accuracy: 0.9756, Precision: 0.9617, Recall: 0.9582, F1: 0.9599
Validation Loss: 2.1637, Accuracy: 0.7612, Precision: 0.6915, Recall: 0.6814, F1: 0.6840
Testing Loss: 1.6307, Accuracy: 0.7862, Precision: 0.7303, Recall: 0.7083, F1: 0.7169
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2539, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 42/70
Train Loss: 0.0458, Accuracy: 0.9694, Precision: 0.9520, Recall: 0.9501, F1: 0.9510
Validation Loss: 2.9244, Accuracy: 0.7633, Precision: 0.6977, Recall: 0.6827, F1: 0.6878
Testing Loss: 2.2665, Accuracy: 0.7705, Precision: 0.7215, Recall: 0.6918, F1: 0.7031
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1629, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 43/70
Train Loss: 0.0464, Accuracy: 0.9708, Precision: 0.9559, Recall: 0.9536, F1: 0.9547
Validation Loss: 4.3798, Accuracy: 0.7527, Precision: 0.6471, Recall: 0.6586, F1: 0.6490
Testing Loss: 3.8573, Accuracy: 0.7669, Precision: 0.6950, Recall: 0.6834, F1: 0.6817
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2061, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 44/70
Train Loss: 0.1881, Accuracy: 0.9450, Precision: 0.9149, Recall: 0.9183, F1: 0.9165
Validation Loss: 1.1948, Accuracy: 0.7484, Precision: 0.6851, Recall: 0.6881, F1: 0.6784
Testing Loss: 1.1144, Accuracy: 0.7585, Precision: 0.7112, Recall: 0.6996, F1: 0.6959
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 2, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2856, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.8800, F1: 0.8619
Epoch 45/70
Train Loss: 0.0730, Accuracy: 0.9685, Precision: 0.9516, Recall: 0.9481, F1: 0.9498
Validation Loss: 1.2293, Accuracy: 0.7484, Precision: 0.6742, Recall: 0.6594, F1: 0.6620
Testing Loss: 1.1082, Accuracy: 0.7657, Precision: 0.7190, Recall: 0.6797, F1: 0.6912
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3831, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 46/70
Train Loss: 0.0562, Accuracy: 0.9720, Precision: 0.9553, Recall: 0.9541, F1: 0.9547
Validation Loss: 1.3032, Accuracy: 0.7783, Precision: 0.7228, Recall: 0.7141, F1: 0.7172
Testing Loss: 1.1573, Accuracy: 0.7862, Precision: 0.7401, Recall: 0.7308, F1: 0.7343
LM Predictions:  [0, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2373, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8836, F1: 0.8546
Epoch 47/70
Train Loss: 0.0444, Accuracy: 0.9734, Precision: 0.9536, Recall: 0.9629, F1: 0.9580
Validation Loss: 1.4046, Accuracy: 0.7697, Precision: 0.6866, Recall: 0.6788, F1: 0.6776
Testing Loss: 1.2377, Accuracy: 0.7754, Precision: 0.7174, Recall: 0.6803, F1: 0.6901
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2203, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 48/70
Train Loss: 0.0416, Accuracy: 0.9744, Precision: 0.9617, Recall: 0.9566, F1: 0.9591
Validation Loss: 1.5429, Accuracy: 0.7740, Precision: 0.6820, Recall: 0.6861, F1: 0.6801
Testing Loss: 1.3330, Accuracy: 0.7778, Precision: 0.7153, Recall: 0.6939, F1: 0.6992
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1878, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 49/70
Train Loss: 0.0432, Accuracy: 0.9725, Precision: 0.9525, Recall: 0.9625, F1: 0.9572
Validation Loss: 1.6665, Accuracy: 0.7761, Precision: 0.7015, Recall: 0.6871, F1: 0.6887
Testing Loss: 1.4520, Accuracy: 0.7826, Precision: 0.7317, Recall: 0.6960, F1: 0.7080
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2957, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 50/70
Train Loss: 0.0414, Accuracy: 0.9730, Precision: 0.9535, Recall: 0.9600, F1: 0.9566
Validation Loss: 1.6768, Accuracy: 0.7676, Precision: 0.6835, Recall: 0.6790, F1: 0.6777
Testing Loss: 1.5030, Accuracy: 0.7850, Precision: 0.7348, Recall: 0.7032, F1: 0.7148
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2693, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 51/70
Train Loss: 0.0420, Accuracy: 0.9730, Precision: 0.9560, Recall: 0.9530, F1: 0.9544
Validation Loss: 1.7123, Accuracy: 0.7761, Precision: 0.7182, Recall: 0.7086, F1: 0.7107
Testing Loss: 1.4773, Accuracy: 0.7862, Precision: 0.7355, Recall: 0.7201, F1: 0.7263
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.0782, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 52/70
Train Loss: 0.0427, Accuracy: 0.9715, Precision: 0.9547, Recall: 0.9516, F1: 0.9530
Validation Loss: 1.7319, Accuracy: 0.7761, Precision: 0.7069, Recall: 0.6993, F1: 0.7001
Testing Loss: 1.5070, Accuracy: 0.7874, Precision: 0.7395, Recall: 0.7196, F1: 0.7284
LM Predictions:  [3, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3414, Accuracy: 0.7619, Precision: 0.8464, Recall: 0.8127, F1: 0.7848
Epoch 53/70
Train Loss: 0.0418, Accuracy: 0.9768, Precision: 0.9611, Recall: 0.9640, F1: 0.9625
Validation Loss: 1.8182, Accuracy: 0.7761, Precision: 0.7147, Recall: 0.7044, F1: 0.7067
Testing Loss: 1.5613, Accuracy: 0.7802, Precision: 0.7264, Recall: 0.7047, F1: 0.7122
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1105, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 54/70
Train Loss: 0.0427, Accuracy: 0.9727, Precision: 0.9581, Recall: 0.9545, F1: 0.9563
Validation Loss: 1.9031, Accuracy: 0.7740, Precision: 0.7052, Recall: 0.7007, F1: 0.6997
Testing Loss: 1.5865, Accuracy: 0.7814, Precision: 0.7302, Recall: 0.7150, F1: 0.7208
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1357, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 55/70
Train Loss: 0.0430, Accuracy: 0.9708, Precision: 0.9533, Recall: 0.9509, F1: 0.9520
Validation Loss: 1.8943, Accuracy: 0.7825, Precision: 0.7432, Recall: 0.7331, F1: 0.7366
Testing Loss: 1.6628, Accuracy: 0.7657, Precision: 0.7272, Recall: 0.7113, F1: 0.7147
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3221, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 56/70
Train Loss: 0.0449, Accuracy: 0.9654, Precision: 0.9405, Recall: 0.9465, F1: 0.9433
Validation Loss: 1.9790, Accuracy: 0.7719, Precision: 0.6978, Recall: 0.6880, F1: 0.6839
Testing Loss: 1.7899, Accuracy: 0.7572, Precision: 0.7000, Recall: 0.6683, F1: 0.6766
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3218, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 57/70
Train Loss: 0.0471, Accuracy: 0.9699, Precision: 0.9518, Recall: 0.9505, F1: 0.9511
Validation Loss: 2.0262, Accuracy: 0.7633, Precision: 0.7026, Recall: 0.6817, F1: 0.6903
Testing Loss: 1.8410, Accuracy: 0.7452, Precision: 0.6984, Recall: 0.6663, F1: 0.6790
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4624, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 58/70
Train Loss: 0.0900, Accuracy: 0.9606, Precision: 0.9441, Recall: 0.9366, F1: 0.9401
Validation Loss: 1.3876, Accuracy: 0.7783, Precision: 0.7256, Recall: 0.6911, F1: 0.6987
Testing Loss: 1.3623, Accuracy: 0.7645, Precision: 0.7039, Recall: 0.6605, F1: 0.6748
LM Predictions:  [0, 3, 4, 0, 2, 0, 2, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 5, 2, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5088, Accuracy: 0.7143, Precision: 0.6892, Recall: 0.6455, F1: 0.6257
Epoch 59/70
Train Loss: 0.0953, Accuracy: 0.9616, Precision: 0.9476, Recall: 0.9369, F1: 0.9420
Validation Loss: 1.4941, Accuracy: 0.7697, Precision: 0.7054, Recall: 0.6757, F1: 0.6801
Testing Loss: 1.5090, Accuracy: 0.7850, Precision: 0.7343, Recall: 0.6833, F1: 0.6980
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2686, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 60/70
Train Loss: 0.0908, Accuracy: 0.9635, Precision: 0.9440, Recall: 0.9388, F1: 0.9414
Validation Loss: 1.1697, Accuracy: 0.7761, Precision: 0.7153, Recall: 0.6922, F1: 0.6946
Testing Loss: 1.0946, Accuracy: 0.7717, Precision: 0.7088, Recall: 0.6765, F1: 0.6845
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3292, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8527, F1: 0.8246
Epoch 61/70
Train Loss: 0.0621, Accuracy: 0.9682, Precision: 0.9520, Recall: 0.9443, F1: 0.9480
Validation Loss: 1.2408, Accuracy: 0.7783, Precision: 0.7130, Recall: 0.7213, F1: 0.7139
Testing Loss: 1.2263, Accuracy: 0.7838, Precision: 0.7298, Recall: 0.7213, F1: 0.7240
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1186, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 62/70
Train Loss: 0.0472, Accuracy: 0.9692, Precision: 0.9495, Recall: 0.9505, F1: 0.9500
Validation Loss: 1.3470, Accuracy: 0.7825, Precision: 0.7134, Recall: 0.6904, F1: 0.6940
Testing Loss: 1.3020, Accuracy: 0.7729, Precision: 0.7091, Recall: 0.6749, F1: 0.6849
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2118, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 63/70
Train Loss: 0.0413, Accuracy: 0.9725, Precision: 0.9538, Recall: 0.9552, F1: 0.9545
Validation Loss: 1.4917, Accuracy: 0.7846, Precision: 0.7304, Recall: 0.7152, F1: 0.7187
Testing Loss: 1.4392, Accuracy: 0.7874, Precision: 0.7426, Recall: 0.7217, F1: 0.7309
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1469, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 64/70
Train Loss: 0.0413, Accuracy: 0.9687, Precision: 0.9477, Recall: 0.9480, F1: 0.9479
Validation Loss: 1.5814, Accuracy: 0.7868, Precision: 0.7270, Recall: 0.7019, F1: 0.7079
Testing Loss: 1.5049, Accuracy: 0.7778, Precision: 0.7290, Recall: 0.6910, F1: 0.7045
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2318, Accuracy: 0.8095, Precision: 0.8611, Recall: 0.8473, F1: 0.8181
Epoch 65/70
Train Loss: 0.0404, Accuracy: 0.9723, Precision: 0.9519, Recall: 0.9570, F1: 0.9544
Validation Loss: 1.6407, Accuracy: 0.7825, Precision: 0.7213, Recall: 0.6933, F1: 0.6989
Testing Loss: 1.5623, Accuracy: 0.7838, Precision: 0.7329, Recall: 0.6939, F1: 0.7077
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2137, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 66/70
Train Loss: 0.0405, Accuracy: 0.9725, Precision: 0.9570, Recall: 0.9579, F1: 0.9574
Validation Loss: 1.6750, Accuracy: 0.7825, Precision: 0.6995, Recall: 0.6859, F1: 0.6861
Testing Loss: 1.5915, Accuracy: 0.7802, Precision: 0.7267, Recall: 0.6879, F1: 0.7014
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2113, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 67/70
Train Loss: 0.0425, Accuracy: 0.9711, Precision: 0.9557, Recall: 0.9505, F1: 0.9530
Validation Loss: 1.6576, Accuracy: 0.7825, Precision: 0.7177, Recall: 0.6946, F1: 0.6996
Testing Loss: 1.5958, Accuracy: 0.7778, Precision: 0.7293, Recall: 0.6894, F1: 0.7043
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2509, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 68/70
Train Loss: 0.0405, Accuracy: 0.9699, Precision: 0.9512, Recall: 0.9529, F1: 0.9520
Validation Loss: 1.6903, Accuracy: 0.7868, Precision: 0.7353, Recall: 0.7048, F1: 0.7132
Testing Loss: 1.6157, Accuracy: 0.7826, Precision: 0.7383, Recall: 0.6967, F1: 0.7123
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2233, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 69/70
Train Loss: 0.0416, Accuracy: 0.9708, Precision: 0.9536, Recall: 0.9516, F1: 0.9526
Validation Loss: 1.7076, Accuracy: 0.7783, Precision: 0.7157, Recall: 0.6912, F1: 0.6959
Testing Loss: 1.6338, Accuracy: 0.7790, Precision: 0.7251, Recall: 0.6894, F1: 0.7023
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2940, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 70/70
Train Loss: 0.0413, Accuracy: 0.9720, Precision: 0.9571, Recall: 0.9486, F1: 0.9525
Validation Loss: 1.7526, Accuracy: 0.7846, Precision: 0.7160, Recall: 0.6901, F1: 0.6949
Testing Loss: 1.6949, Accuracy: 0.7754, Precision: 0.7272, Recall: 0.6817, F1: 0.6979
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2949, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
For middle layers:  [8, 9, 10, 11, 12, 13, 14, 15]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.6805, Accuracy: 0.8257, Precision: 0.7581, Recall: 0.7473, F1: 0.7521
Validation Loss: 0.4009, Accuracy: 0.8529, Precision: 0.8066, Recall: 0.8018, F1: 0.7993
Testing Loss: 0.3455, Accuracy: 0.9010, Precision: 0.8674, Recall: 0.8524, F1: 0.8567
LM Predictions:  [5, 3, 4, 0, 2, 2, 0, 3, 4, 5, 2, 5, 5, 5, 2, 5, 2, 0, 5, 1, 3, 0, 5, 5, 0, 5, 0, 5, 4, 5, 0, 4, 5, 2, 5, 4, 5, 0, 2, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.5974, Accuracy: 0.4048, Precision: 0.6444, Recall: 0.3152, F1: 0.3851
Epoch 2/70
Train Loss: 0.2296, Accuracy: 0.9251, Precision: 0.8917, Recall: 0.8835, F1: 0.8875
Validation Loss: 0.6823, Accuracy: 0.8678, Precision: 0.8770, Recall: 0.7820, F1: 0.7906
Testing Loss: 0.5985, Accuracy: 0.8599, Precision: 0.8619, Recall: 0.7586, F1: 0.7673
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 2, 0, 0, 3, 2, 0, 2, 0, 1, 1, 3, 0, 0, 4, 3, 0, 0, 2, 0, 0, 0, 4, 0, 2, 0, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.4344, Accuracy: 0.5952, Precision: 0.8455, Recall: 0.6382, F1: 0.6380
Epoch 3/70
Train Loss: 0.1509, Accuracy: 0.9516, Precision: 0.9242, Recall: 0.9213, F1: 0.9227
Validation Loss: 0.5808, Accuracy: 0.8678, Precision: 0.8206, Recall: 0.8055, F1: 0.8096
Testing Loss: 0.4867, Accuracy: 0.8780, Precision: 0.8497, Recall: 0.8237, F1: 0.8351
LM Predictions:  [5, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 5, 0, 3, 2, 4, 0, 5, 4, 0, 2, 3, 4, 5, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5161, Accuracy: 0.8095, Precision: 0.7593, Recall: 0.7076, F1: 0.7133
Epoch 4/70
Train Loss: 0.0953, Accuracy: 0.9644, Precision: 0.9449, Recall: 0.9424, F1: 0.9436
Validation Loss: 0.7321, Accuracy: 0.8678, Precision: 0.8150, Recall: 0.7940, F1: 0.7965
Testing Loss: 0.6447, Accuracy: 0.8708, Precision: 0.8296, Recall: 0.7789, F1: 0.7898
LM Predictions:  [0, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4148, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8200
Epoch 5/70
Train Loss: 0.0887, Accuracy: 0.9621, Precision: 0.9401, Recall: 0.9348, F1: 0.9374
Validation Loss: 0.7086, Accuracy: 0.8678, Precision: 0.8136, Recall: 0.8100, F1: 0.8091
Testing Loss: 0.6464, Accuracy: 0.8768, Precision: 0.8363, Recall: 0.8132, F1: 0.8217
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4330, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8891, F1: 0.8557
Epoch 6/70
Train Loss: 0.0936, Accuracy: 0.9640, Precision: 0.9442, Recall: 0.9399, F1: 0.9420
Validation Loss: 0.7672, Accuracy: 0.8486, Precision: 0.8004, Recall: 0.7962, F1: 0.7939
Testing Loss: 0.6851, Accuracy: 0.8478, Precision: 0.7928, Recall: 0.7778, F1: 0.7760
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 5, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7703, Accuracy: 0.7143, Precision: 0.7188, Recall: 0.6455, F1: 0.6290
Epoch 7/70
Train Loss: 0.2009, Accuracy: 0.9355, Precision: 0.9170, Recall: 0.9076, F1: 0.9122
Validation Loss: 0.7591, Accuracy: 0.8507, Precision: 0.7991, Recall: 0.7740, F1: 0.7801
Testing Loss: 0.7036, Accuracy: 0.8587, Precision: 0.8201, Recall: 0.7748, F1: 0.7865
LM Predictions:  [0, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 0, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4385, Accuracy: 0.7619, Precision: 0.8514, Recall: 0.8091, F1: 0.7784
Epoch 8/70
Train Loss: 0.0884, Accuracy: 0.9623, Precision: 0.9433, Recall: 0.9355, F1: 0.9393
Validation Loss: 0.8642, Accuracy: 0.8529, Precision: 0.8077, Recall: 0.7988, F1: 0.8010
Testing Loss: 0.7895, Accuracy: 0.8587, Precision: 0.8271, Recall: 0.8008, F1: 0.8124
LM Predictions:  [2, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 2, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4710, Accuracy: 0.8095, Precision: 0.8571, Recall: 0.8473, F1: 0.8149
Epoch 9/70
Train Loss: 0.0659, Accuracy: 0.9692, Precision: 0.9507, Recall: 0.9485, F1: 0.9496
Validation Loss: 1.1054, Accuracy: 0.8486, Precision: 0.8012, Recall: 0.7841, F1: 0.7862
Testing Loss: 0.9628, Accuracy: 0.8671, Precision: 0.8293, Recall: 0.7910, F1: 0.8045
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 4, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3727, Accuracy: 0.8095, Precision: 0.8477, Recall: 0.8291, F1: 0.8097
Epoch 10/70
Train Loss: 0.0851, Accuracy: 0.9616, Precision: 0.9397, Recall: 0.9396, F1: 0.9396
Validation Loss: 1.0267, Accuracy: 0.8593, Precision: 0.8092, Recall: 0.7862, F1: 0.7917
Testing Loss: 0.8155, Accuracy: 0.8611, Precision: 0.8256, Recall: 0.7816, F1: 0.7964
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4279, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 11/70
Train Loss: 0.0929, Accuracy: 0.9632, Precision: 0.9455, Recall: 0.9422, F1: 0.9438
Validation Loss: 0.9140, Accuracy: 0.8486, Precision: 0.7887, Recall: 0.7751, F1: 0.7741
Testing Loss: 0.8191, Accuracy: 0.8551, Precision: 0.8026, Recall: 0.7746, F1: 0.7808
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1891, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 12/70
Train Loss: 0.0923, Accuracy: 0.9594, Precision: 0.9378, Recall: 0.9372, F1: 0.9375
Validation Loss: 0.9417, Accuracy: 0.8486, Precision: 0.7903, Recall: 0.7963, F1: 0.7923
Testing Loss: 1.0664, Accuracy: 0.8502, Precision: 0.8113, Recall: 0.7932, F1: 0.8004
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2356, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9418, F1: 0.9221
Epoch 13/70
Train Loss: 0.0858, Accuracy: 0.9642, Precision: 0.9452, Recall: 0.9429, F1: 0.9440
Validation Loss: 1.0701, Accuracy: 0.8401, Precision: 0.7869, Recall: 0.7539, F1: 0.7636
Testing Loss: 1.0453, Accuracy: 0.8551, Precision: 0.8169, Recall: 0.7706, F1: 0.7867
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2425, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9455, F1: 0.9223
Epoch 14/70
Train Loss: 0.0772, Accuracy: 0.9625, Precision: 0.9437, Recall: 0.9417, F1: 0.9427
Validation Loss: 0.9464, Accuracy: 0.8358, Precision: 0.7411, Recall: 0.7613, F1: 0.7428
Testing Loss: 0.8411, Accuracy: 0.8502, Precision: 0.7847, Recall: 0.7636, F1: 0.7620
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3435, Accuracy: 0.7619, Precision: 0.8514, Recall: 0.8109, F1: 0.7822
Epoch 15/70
Train Loss: 0.0618, Accuracy: 0.9696, Precision: 0.9531, Recall: 0.9534, F1: 0.9532
Validation Loss: 1.1758, Accuracy: 0.8614, Precision: 0.8004, Recall: 0.8058, F1: 0.8012
Testing Loss: 1.0251, Accuracy: 0.8575, Precision: 0.8123, Recall: 0.7929, F1: 0.7998
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2118, Accuracy: 0.9286, Precision: 0.9262, Recall: 0.9436, F1: 0.9274
Epoch 16/70
Train Loss: 0.0537, Accuracy: 0.9699, Precision: 0.9532, Recall: 0.9489, F1: 0.9510
Validation Loss: 1.3268, Accuracy: 0.8678, Precision: 0.8109, Recall: 0.8130, F1: 0.8098
Testing Loss: 1.1984, Accuracy: 0.8575, Precision: 0.8112, Recall: 0.7979, F1: 0.8032
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1619, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9455, F1: 0.9223
Epoch 17/70
Train Loss: 0.0487, Accuracy: 0.9718, Precision: 0.9563, Recall: 0.9541, F1: 0.9552
Validation Loss: 1.3649, Accuracy: 0.8614, Precision: 0.8092, Recall: 0.8057, F1: 0.8061
Testing Loss: 1.2248, Accuracy: 0.8563, Precision: 0.8233, Recall: 0.7962, F1: 0.8081
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1903, Accuracy: 0.9524, Precision: 0.9500, Recall: 0.9636, F1: 0.9531
Epoch 18/70
Train Loss: 0.0502, Accuracy: 0.9704, Precision: 0.9525, Recall: 0.9493, F1: 0.9509
Validation Loss: 1.3207, Accuracy: 0.8614, Precision: 0.8007, Recall: 0.7895, F1: 0.7893
Testing Loss: 1.1496, Accuracy: 0.8551, Precision: 0.8026, Recall: 0.7745, F1: 0.7832
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2649, Accuracy: 0.8333, Precision: 0.8742, Recall: 0.8655, F1: 0.8347
Epoch 19/70
Train Loss: 0.0485, Accuracy: 0.9739, Precision: 0.9590, Recall: 0.9548, F1: 0.9569
Validation Loss: 1.4677, Accuracy: 0.8593, Precision: 0.7991, Recall: 0.7835, F1: 0.7856
Testing Loss: 1.3447, Accuracy: 0.8527, Precision: 0.8044, Recall: 0.7685, F1: 0.7781
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3451, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8291, F1: 0.8037
Epoch 20/70
Train Loss: 0.0505, Accuracy: 0.9718, Precision: 0.9537, Recall: 0.9522, F1: 0.9529
Validation Loss: 1.3304, Accuracy: 0.8593, Precision: 0.8048, Recall: 0.7863, F1: 0.7904
Testing Loss: 1.2790, Accuracy: 0.8466, Precision: 0.8001, Recall: 0.7626, F1: 0.7756
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5347, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 21/70
Train Loss: 0.0485, Accuracy: 0.9715, Precision: 0.9556, Recall: 0.9551, F1: 0.9554
Validation Loss: 1.2118, Accuracy: 0.8657, Precision: 0.8035, Recall: 0.7953, F1: 0.7938
Testing Loss: 1.1224, Accuracy: 0.8551, Precision: 0.8000, Recall: 0.7754, F1: 0.7816
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2272, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 22/70
Train Loss: 0.0484, Accuracy: 0.9713, Precision: 0.9541, Recall: 0.9525, F1: 0.9533
Validation Loss: 1.4967, Accuracy: 0.8614, Precision: 0.8010, Recall: 0.8087, F1: 0.8020
Testing Loss: 1.3916, Accuracy: 0.8563, Precision: 0.8130, Recall: 0.7999, F1: 0.8052
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2845, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 23/70
Train Loss: 0.0477, Accuracy: 0.9730, Precision: 0.9558, Recall: 0.9574, F1: 0.9566
Validation Loss: 1.4511, Accuracy: 0.8635, Precision: 0.8098, Recall: 0.8114, F1: 0.8099
Testing Loss: 1.3132, Accuracy: 0.8599, Precision: 0.8170, Recall: 0.8094, F1: 0.8128
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2490, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9018, F1: 0.8738
Epoch 24/70
Train Loss: 0.0468, Accuracy: 0.9725, Precision: 0.9549, Recall: 0.9570, F1: 0.9559
Validation Loss: 1.5826, Accuracy: 0.8593, Precision: 0.8036, Recall: 0.8122, F1: 0.8070
Testing Loss: 1.4475, Accuracy: 0.8551, Precision: 0.8170, Recall: 0.8091, F1: 0.8125
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1708, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 25/70
Train Loss: 0.0476, Accuracy: 0.9725, Precision: 0.9576, Recall: 0.9536, F1: 0.9555
Validation Loss: 1.6289, Accuracy: 0.8635, Precision: 0.8073, Recall: 0.7923, F1: 0.7945
Testing Loss: 1.4828, Accuracy: 0.8587, Precision: 0.8222, Recall: 0.7877, F1: 0.8012
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4096, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 26/70
Train Loss: 0.0456, Accuracy: 0.9746, Precision: 0.9600, Recall: 0.9574, F1: 0.9587
Validation Loss: 1.5458, Accuracy: 0.8678, Precision: 0.8168, Recall: 0.8158, F1: 0.8155
Testing Loss: 1.3490, Accuracy: 0.8539, Precision: 0.8203, Recall: 0.8030, F1: 0.8105
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3374, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 27/70
Train Loss: 0.0722, Accuracy: 0.9701, Precision: 0.9525, Recall: 0.9523, F1: 0.9524
Validation Loss: 1.5183, Accuracy: 0.8188, Precision: 0.7917, Recall: 0.7926, F1: 0.7837
Testing Loss: 1.5666, Accuracy: 0.8418, Precision: 0.8155, Recall: 0.8103, F1: 0.8047
LM Predictions:  [4, 5, 5, 3, 2, 0, 4, 1, 5, 5, 2, 1, 5, 3, 5, 5, 0, 0, 1, 1, 3, 2, 5, 4, 0, 5, 3, 2, 4, 0, 2, 5, 0, 2, 3, 4, 3, 0, 2, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.2379, Accuracy: 0.6190, Precision: 0.7407, Recall: 0.5485, F1: 0.6022
Epoch 28/70
Train Loss: 0.2031, Accuracy: 0.9353, Precision: 0.9097, Recall: 0.9016, F1: 0.9055
Validation Loss: 0.7025, Accuracy: 0.8529, Precision: 0.7958, Recall: 0.7887, F1: 0.7880
Testing Loss: 0.5536, Accuracy: 0.8575, Precision: 0.8150, Recall: 0.7932, F1: 0.8020
LM Predictions:  [0, 0, 4, 0, 2, 2, 0, 2, 4, 5, 2, 1, 4, 3, 2, 3, 0, 0, 5, 1, 3, 2, 5, 5, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 2]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8037, Accuracy: 0.5952, Precision: 0.6806, Recall: 0.5121, F1: 0.5084
Epoch 29/70
Train Loss: 0.0976, Accuracy: 0.9644, Precision: 0.9444, Recall: 0.9471, F1: 0.9457
Validation Loss: 1.1032, Accuracy: 0.8380, Precision: 0.7797, Recall: 0.7638, F1: 0.7673
Testing Loss: 0.8648, Accuracy: 0.8563, Precision: 0.8162, Recall: 0.7762, F1: 0.7905
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2139, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 30/70
Train Loss: 0.0575, Accuracy: 0.9694, Precision: 0.9540, Recall: 0.9502, F1: 0.9520
Validation Loss: 1.1200, Accuracy: 0.8316, Precision: 0.7756, Recall: 0.7765, F1: 0.7747
Testing Loss: 0.8466, Accuracy: 0.8623, Precision: 0.8231, Recall: 0.8094, F1: 0.8157
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2839, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 31/70
Train Loss: 0.0467, Accuracy: 0.9699, Precision: 0.9494, Recall: 0.9529, F1: 0.9511
Validation Loss: 1.2800, Accuracy: 0.8358, Precision: 0.7590, Recall: 0.7523, F1: 0.7523
Testing Loss: 1.0401, Accuracy: 0.8514, Precision: 0.8128, Recall: 0.7671, F1: 0.7815
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3305, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 32/70
Train Loss: 0.0437, Accuracy: 0.9744, Precision: 0.9593, Recall: 0.9571, F1: 0.9582
Validation Loss: 1.3579, Accuracy: 0.8443, Precision: 0.7873, Recall: 0.7908, F1: 0.7885
Testing Loss: 1.0614, Accuracy: 0.8635, Precision: 0.8268, Recall: 0.8090, F1: 0.8172
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2081, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8655, F1: 0.8361
Epoch 33/70
Train Loss: 0.0455, Accuracy: 0.9696, Precision: 0.9498, Recall: 0.9537, F1: 0.9517
Validation Loss: 1.2827, Accuracy: 0.8443, Precision: 0.7835, Recall: 0.7717, F1: 0.7750
Testing Loss: 1.0282, Accuracy: 0.8611, Precision: 0.8296, Recall: 0.7847, F1: 0.8009
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2514, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 34/70
Train Loss: 0.0463, Accuracy: 0.9706, Precision: 0.9532, Recall: 0.9511, F1: 0.9521
Validation Loss: 1.4187, Accuracy: 0.8401, Precision: 0.7888, Recall: 0.7870, F1: 0.7868
Testing Loss: 1.0981, Accuracy: 0.8563, Precision: 0.8257, Recall: 0.8027, F1: 0.8124
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2060, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 35/70
Train Loss: 0.0443, Accuracy: 0.9730, Precision: 0.9558, Recall: 0.9558, F1: 0.9558
Validation Loss: 1.4122, Accuracy: 0.8401, Precision: 0.7904, Recall: 0.7870, F1: 0.7877
Testing Loss: 1.1193, Accuracy: 0.8587, Precision: 0.8287, Recall: 0.8051, F1: 0.8153
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2705, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 36/70
Train Loss: 0.0445, Accuracy: 0.9715, Precision: 0.9512, Recall: 0.9582, F1: 0.9545
Validation Loss: 1.4497, Accuracy: 0.8422, Precision: 0.7668, Recall: 0.7569, F1: 0.7576
Testing Loss: 1.1599, Accuracy: 0.8563, Precision: 0.8225, Recall: 0.7723, F1: 0.7897
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3356, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 37/70
Train Loss: 0.0453, Accuracy: 0.9715, Precision: 0.9539, Recall: 0.9498, F1: 0.9518
Validation Loss: 1.4220, Accuracy: 0.8443, Precision: 0.7911, Recall: 0.7805, F1: 0.7840
Testing Loss: 1.1151, Accuracy: 0.8635, Precision: 0.8336, Recall: 0.7921, F1: 0.8091
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2869, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 38/70
Train Loss: 0.0450, Accuracy: 0.9694, Precision: 0.9491, Recall: 0.9536, F1: 0.9513
Validation Loss: 1.5126, Accuracy: 0.8380, Precision: 0.7642, Recall: 0.7516, F1: 0.7527
Testing Loss: 1.1871, Accuracy: 0.8563, Precision: 0.8137, Recall: 0.7652, F1: 0.7801
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2255, Accuracy: 0.8571, Precision: 0.8800, Recall: 0.8909, F1: 0.8603
Epoch 39/70
Train Loss: 0.0434, Accuracy: 0.9737, Precision: 0.9579, Recall: 0.9568, F1: 0.9574
Validation Loss: 1.5219, Accuracy: 0.8443, Precision: 0.7908, Recall: 0.7898, F1: 0.7895
Testing Loss: 1.1649, Accuracy: 0.8599, Precision: 0.8259, Recall: 0.8009, F1: 0.8122
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2511, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8655, F1: 0.8361
Epoch 40/70
Train Loss: 0.0465, Accuracy: 0.9696, Precision: 0.9494, Recall: 0.9549, F1: 0.9521
Validation Loss: 1.5876, Accuracy: 0.8380, Precision: 0.7644, Recall: 0.7516, F1: 0.7527
Testing Loss: 1.2363, Accuracy: 0.8575, Precision: 0.8177, Recall: 0.7675, F1: 0.7835
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2979, Accuracy: 0.8095, Precision: 0.8583, Recall: 0.8545, F1: 0.8206
Epoch 41/70
Train Loss: 0.0435, Accuracy: 0.9730, Precision: 0.9567, Recall: 0.9527, F1: 0.9547
Validation Loss: 1.6189, Accuracy: 0.8401, Precision: 0.7800, Recall: 0.7798, F1: 0.7788
Testing Loss: 1.2106, Accuracy: 0.8671, Precision: 0.8303, Recall: 0.8136, F1: 0.8213
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1620, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 42/70
Train Loss: 0.0441, Accuracy: 0.9711, Precision: 0.9524, Recall: 0.9496, F1: 0.9510
Validation Loss: 1.6518, Accuracy: 0.8380, Precision: 0.7742, Recall: 0.7771, F1: 0.7748
Testing Loss: 1.2742, Accuracy: 0.8671, Precision: 0.8308, Recall: 0.8167, F1: 0.8232
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2468, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 43/70
Train Loss: 0.1220, Accuracy: 0.9521, Precision: 0.9315, Recall: 0.9324, F1: 0.9320
Validation Loss: 0.9474, Accuracy: 0.7953, Precision: 0.7632, Recall: 0.7122, F1: 0.7282
Testing Loss: 0.7840, Accuracy: 0.8357, Precision: 0.8185, Recall: 0.7632, F1: 0.7848
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 1, 5, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 5, 4, 3, 0, 0, 2, 4, 0, 0, 5, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7742, Accuracy: 0.8333, Precision: 0.7375, Recall: 0.7076, F1: 0.7098
Epoch 44/70
Train Loss: 0.1043, Accuracy: 0.9564, Precision: 0.9360, Recall: 0.9320, F1: 0.9339
Validation Loss: 1.0998, Accuracy: 0.8230, Precision: 0.7549, Recall: 0.7333, F1: 0.7374
Testing Loss: 1.0342, Accuracy: 0.8357, Precision: 0.8057, Recall: 0.7442, F1: 0.7626
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3335, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 45/70
Train Loss: 0.0557, Accuracy: 0.9685, Precision: 0.9483, Recall: 0.9507, F1: 0.9495
Validation Loss: 1.1117, Accuracy: 0.8401, Precision: 0.7638, Recall: 0.7520, F1: 0.7526
Testing Loss: 1.1657, Accuracy: 0.8418, Precision: 0.8293, Recall: 0.7508, F1: 0.7751
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2805, Accuracy: 0.8095, Precision: 0.8583, Recall: 0.8545, F1: 0.8206
Epoch 46/70
Train Loss: 0.0485, Accuracy: 0.9713, Precision: 0.9528, Recall: 0.9500, F1: 0.9514
Validation Loss: 1.1865, Accuracy: 0.8529, Precision: 0.8047, Recall: 0.8067, F1: 0.8052
Testing Loss: 1.0438, Accuracy: 0.8575, Precision: 0.8267, Recall: 0.7934, F1: 0.8079
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2077, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 47/70
Train Loss: 0.0445, Accuracy: 0.9711, Precision: 0.9503, Recall: 0.9584, F1: 0.9541
Validation Loss: 1.2360, Accuracy: 0.8486, Precision: 0.7859, Recall: 0.7787, F1: 0.7787
Testing Loss: 1.1356, Accuracy: 0.8466, Precision: 0.8139, Recall: 0.7619, F1: 0.7799
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4878, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 48/70
Train Loss: 0.0436, Accuracy: 0.9704, Precision: 0.9532, Recall: 0.9491, F1: 0.9511
Validation Loss: 1.4692, Accuracy: 0.8507, Precision: 0.8032, Recall: 0.7933, F1: 0.7974
Testing Loss: 1.2731, Accuracy: 0.8502, Precision: 0.8216, Recall: 0.7766, F1: 0.7943
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1468, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 49/70
Train Loss: 0.0426, Accuracy: 0.9727, Precision: 0.9533, Recall: 0.9581, F1: 0.9556
Validation Loss: 1.4835, Accuracy: 0.8358, Precision: 0.7612, Recall: 0.7551, F1: 0.7541
Testing Loss: 1.3065, Accuracy: 0.8466, Precision: 0.8155, Recall: 0.7622, F1: 0.7784
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4040, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 50/70
Train Loss: 0.0431, Accuracy: 0.9715, Precision: 0.9571, Recall: 0.9539, F1: 0.9554
Validation Loss: 1.5240, Accuracy: 0.8507, Precision: 0.7919, Recall: 0.7756, F1: 0.7796
Testing Loss: 1.3247, Accuracy: 0.8502, Precision: 0.8222, Recall: 0.7658, F1: 0.7838
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2194, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 51/70
Train Loss: 0.0418, Accuracy: 0.9723, Precision: 0.9553, Recall: 0.9539, F1: 0.9546
Validation Loss: 1.5213, Accuracy: 0.8465, Precision: 0.7909, Recall: 0.7892, F1: 0.7894
Testing Loss: 1.3034, Accuracy: 0.8527, Precision: 0.8226, Recall: 0.7884, F1: 0.8028
LM Predictions:  [4, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1650, Accuracy: 0.9524, Precision: 0.9500, Recall: 0.9636, F1: 0.9531
Epoch 52/70
Train Loss: 0.0423, Accuracy: 0.9718, Precision: 0.9526, Recall: 0.9561, F1: 0.9543
Validation Loss: 1.5498, Accuracy: 0.8443, Precision: 0.7770, Recall: 0.7610, F1: 0.7650
Testing Loss: 1.3608, Accuracy: 0.8454, Precision: 0.8217, Recall: 0.7557, F1: 0.7772
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2572, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 53/70
Train Loss: 0.0412, Accuracy: 0.9749, Precision: 0.9581, Recall: 0.9611, F1: 0.9595
Validation Loss: 1.5828, Accuracy: 0.8443, Precision: 0.7828, Recall: 0.7864, F1: 0.7834
Testing Loss: 1.3318, Accuracy: 0.8539, Precision: 0.8219, Recall: 0.7891, F1: 0.8020
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1248, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 54/70
Train Loss: 0.0427, Accuracy: 0.9725, Precision: 0.9540, Recall: 0.9544, F1: 0.9542
Validation Loss: 1.5727, Accuracy: 0.8507, Precision: 0.7959, Recall: 0.7945, F1: 0.7938
Testing Loss: 1.3427, Accuracy: 0.8514, Precision: 0.8142, Recall: 0.7774, F1: 0.7901
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2545, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 55/70
Train Loss: 0.0450, Accuracy: 0.9718, Precision: 0.9546, Recall: 0.9503, F1: 0.9524
Validation Loss: 1.9694, Accuracy: 0.8401, Precision: 0.7889, Recall: 0.7881, F1: 0.7877
Testing Loss: 1.5040, Accuracy: 0.8527, Precision: 0.8250, Recall: 0.7918, F1: 0.8060
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1598, Accuracy: 0.9524, Precision: 0.9500, Recall: 0.9618, F1: 0.9531
Epoch 56/70
Train Loss: 0.0439, Accuracy: 0.9746, Precision: 0.9567, Recall: 0.9625, F1: 0.9595
Validation Loss: 1.4827, Accuracy: 0.8358, Precision: 0.7715, Recall: 0.7617, F1: 0.7626
Testing Loss: 1.2625, Accuracy: 0.8430, Precision: 0.8176, Recall: 0.7456, F1: 0.7686
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4198, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 57/70
Train Loss: 0.0875, Accuracy: 0.9668, Precision: 0.9519, Recall: 0.9358, F1: 0.9430
Validation Loss: 1.2315, Accuracy: 0.8337, Precision: 0.7835, Recall: 0.7559, F1: 0.7669
Testing Loss: 1.0705, Accuracy: 0.8478, Precision: 0.8224, Recall: 0.7469, F1: 0.7714
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5458, Accuracy: 0.6905, Precision: 0.8303, Recall: 0.7564, F1: 0.7298
Epoch 58/70
Train Loss: 0.0887, Accuracy: 0.9635, Precision: 0.9462, Recall: 0.9393, F1: 0.9426
Validation Loss: 0.7816, Accuracy: 0.8401, Precision: 0.7642, Recall: 0.7718, F1: 0.7638
Testing Loss: 0.7364, Accuracy: 0.8599, Precision: 0.8337, Recall: 0.7847, F1: 0.7960
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3896, Accuracy: 0.7619, Precision: 0.8429, Recall: 0.8164, F1: 0.7836
Epoch 59/70
Train Loss: 0.0560, Accuracy: 0.9685, Precision: 0.9512, Recall: 0.9465, F1: 0.9488
Validation Loss: 1.1983, Accuracy: 0.8316, Precision: 0.7704, Recall: 0.7704, F1: 0.7700
Testing Loss: 1.1113, Accuracy: 0.8575, Precision: 0.8210, Recall: 0.7940, F1: 0.8049
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3516, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 60/70
Train Loss: 0.0479, Accuracy: 0.9713, Precision: 0.9536, Recall: 0.9575, F1: 0.9555
Validation Loss: 1.2887, Accuracy: 0.8507, Precision: 0.7872, Recall: 0.7862, F1: 0.7851
Testing Loss: 1.1710, Accuracy: 0.8551, Precision: 0.8185, Recall: 0.7786, F1: 0.7911
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3089, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 61/70
Train Loss: 0.0434, Accuracy: 0.9727, Precision: 0.9578, Recall: 0.9515, F1: 0.9546
Validation Loss: 1.3426, Accuracy: 0.8465, Precision: 0.7869, Recall: 0.7938, F1: 0.7901
Testing Loss: 1.1584, Accuracy: 0.8575, Precision: 0.8278, Recall: 0.8055, F1: 0.8151
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1894, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 62/70
Train Loss: 0.0414, Accuracy: 0.9720, Precision: 0.9536, Recall: 0.9530, F1: 0.9533
Validation Loss: 1.3571, Accuracy: 0.8465, Precision: 0.7949, Recall: 0.7968, F1: 0.7950
Testing Loss: 1.1880, Accuracy: 0.8551, Precision: 0.8266, Recall: 0.8030, F1: 0.8126
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2285, Accuracy: 0.8095, Precision: 0.8583, Recall: 0.8545, F1: 0.8206
Epoch 63/70
Train Loss: 0.0419, Accuracy: 0.9725, Precision: 0.9556, Recall: 0.9551, F1: 0.9554
Validation Loss: 1.3855, Accuracy: 0.8422, Precision: 0.7885, Recall: 0.7995, F1: 0.7927
Testing Loss: 1.2014, Accuracy: 0.8563, Precision: 0.8270, Recall: 0.8064, F1: 0.8142
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1954, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.9018
Epoch 64/70
Train Loss: 0.0413, Accuracy: 0.9734, Precision: 0.9585, Recall: 0.9574, F1: 0.9579
Validation Loss: 1.4397, Accuracy: 0.8507, Precision: 0.7975, Recall: 0.8025, F1: 0.7996
Testing Loss: 1.2615, Accuracy: 0.8587, Precision: 0.8265, Recall: 0.8057, F1: 0.8142
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1412, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 65/70
Train Loss: 0.0431, Accuracy: 0.9706, Precision: 0.9502, Recall: 0.9559, F1: 0.9529
Validation Loss: 1.3958, Accuracy: 0.8486, Precision: 0.7893, Recall: 0.7846, F1: 0.7862
Testing Loss: 1.2312, Accuracy: 0.8539, Precision: 0.8206, Recall: 0.7786, F1: 0.7943
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2850, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 66/70
Train Loss: 0.0411, Accuracy: 0.9723, Precision: 0.9518, Recall: 0.9603, F1: 0.9559
Validation Loss: 1.4690, Accuracy: 0.8465, Precision: 0.7776, Recall: 0.7761, F1: 0.7735
Testing Loss: 1.2798, Accuracy: 0.8587, Precision: 0.8242, Recall: 0.7814, F1: 0.7956
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1900, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 67/70
Train Loss: 0.0408, Accuracy: 0.9715, Precision: 0.9567, Recall: 0.9494, F1: 0.9529
Validation Loss: 1.5132, Accuracy: 0.8465, Precision: 0.7808, Recall: 0.7818, F1: 0.7780
Testing Loss: 1.2652, Accuracy: 0.8611, Precision: 0.8231, Recall: 0.7888, F1: 0.8005
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1729, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 68/70
Train Loss: 0.0418, Accuracy: 0.9718, Precision: 0.9548, Recall: 0.9526, F1: 0.9537
Validation Loss: 1.5100, Accuracy: 0.8486, Precision: 0.7904, Recall: 0.7996, F1: 0.7947
Testing Loss: 1.3130, Accuracy: 0.8611, Precision: 0.8312, Recall: 0.8045, F1: 0.8157
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1654, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 69/70
Train Loss: 0.0402, Accuracy: 0.9737, Precision: 0.9595, Recall: 0.9539, F1: 0.9566
Validation Loss: 1.5471, Accuracy: 0.8486, Precision: 0.7994, Recall: 0.7995, F1: 0.7984
Testing Loss: 1.3440, Accuracy: 0.8611, Precision: 0.8344, Recall: 0.8114, F1: 0.8211
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1578, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 70/70
Train Loss: 0.0407, Accuracy: 0.9711, Precision: 0.9508, Recall: 0.9529, F1: 0.9518
Validation Loss: 1.4952, Accuracy: 0.8507, Precision: 0.7864, Recall: 0.7820, F1: 0.7818
Testing Loss: 1.3325, Accuracy: 0.8527, Precision: 0.8229, Recall: 0.7704, F1: 0.7891
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3368, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
For later layers:  [16, 17, 18, 19, 20, 21, 22, 23]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.7483, Accuracy: 0.8240, Precision: 0.7616, Recall: 0.7500, F1: 0.7552
Validation Loss: 0.4039, Accuracy: 0.8785, Precision: 0.8674, Recall: 0.7981, F1: 0.7921
Testing Loss: 0.3577, Accuracy: 0.8792, Precision: 0.8536, Recall: 0.7881, F1: 0.7910
LM Predictions:  [0, 3, 4, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 2, 4, 0, 0, 0, 1, 3, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 4, 0, 4, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.9689, Accuracy: 0.3333, Precision: 0.7583, Recall: 0.3891, F1: 0.3291
Epoch 2/70
Train Loss: 0.2297, Accuracy: 0.9260, Precision: 0.8905, Recall: 0.8821, F1: 0.8862
Validation Loss: 0.6039, Accuracy: 0.8337, Precision: 0.7918, Recall: 0.8146, F1: 0.7966
Testing Loss: 0.5087, Accuracy: 0.8611, Precision: 0.8219, Recall: 0.8148, F1: 0.8160
LM Predictions:  [4, 3, 4, 5, 2, 2, 0, 1, 4, 1, 2, 1, 1, 3, 2, 3, 2, 0, 1, 1, 3, 5, 4, 4, 3, 5, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 5, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6749, Accuracy: 0.8333, Precision: 0.7571, Recall: 0.7061, F1: 0.7249
Epoch 3/70
Train Loss: 0.1485, Accuracy: 0.9547, Precision: 0.9246, Recall: 0.9239, F1: 0.9242
Validation Loss: 0.5530, Accuracy: 0.8635, Precision: 0.8261, Recall: 0.8262, F1: 0.8251
Testing Loss: 0.5572, Accuracy: 0.8563, Precision: 0.8367, Recall: 0.8108, F1: 0.8206
LM Predictions:  [5, 3, 4, 3, 5, 0, 3, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 5, 4, 4, 3, 5, 0, 2, 5, 0, 5, 5, 0, 2, 3, 4, 5, 0, 2, 5, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8151, Accuracy: 0.6905, Precision: 0.7315, Recall: 0.5970, F1: 0.6452
Epoch 4/70
Train Loss: 0.1313, Accuracy: 0.9568, Precision: 0.9287, Recall: 0.9303, F1: 0.9295
Validation Loss: 0.6528, Accuracy: 0.8657, Precision: 0.8312, Recall: 0.7966, F1: 0.8077
Testing Loss: 0.7602, Accuracy: 0.8623, Precision: 0.8298, Recall: 0.7764, F1: 0.7913
LM Predictions:  [0, 3, 4, 0, 1, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 0, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7422, Accuracy: 0.7619, Precision: 0.8381, Recall: 0.8127, F1: 0.7758
Epoch 5/70
Train Loss: 0.0989, Accuracy: 0.9623, Precision: 0.9384, Recall: 0.9404, F1: 0.9393
Validation Loss: 0.9325, Accuracy: 0.8529, Precision: 0.7869, Recall: 0.7703, F1: 0.7709
Testing Loss: 0.9261, Accuracy: 0.8659, Precision: 0.8354, Recall: 0.7735, F1: 0.7919
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1890, Accuracy: 0.9524, Precision: 0.9485, Recall: 0.9636, F1: 0.9541
Epoch 6/70
Train Loss: 0.1430, Accuracy: 0.9512, Precision: 0.9273, Recall: 0.9267, F1: 0.9270
Validation Loss: 0.9614, Accuracy: 0.8486, Precision: 0.8262, Recall: 0.8026, F1: 0.8113
Testing Loss: 0.7364, Accuracy: 0.8551, Precision: 0.8275, Recall: 0.7904, F1: 0.8043
LM Predictions:  [0, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4278, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8891, F1: 0.8629
Epoch 7/70
Train Loss: 0.0907, Accuracy: 0.9635, Precision: 0.9424, Recall: 0.9408, F1: 0.9416
Validation Loss: 1.0726, Accuracy: 0.8358, Precision: 0.7889, Recall: 0.7922, F1: 0.7895
Testing Loss: 0.7771, Accuracy: 0.8659, Precision: 0.8237, Recall: 0.8175, F1: 0.8198
LM Predictions:  [4, 3, 4, 5, 2, 4, 5, 1, 4, 1, 2, 1, 4, 3, 2, 3, 4, 0, 1, 1, 3, 2, 4, 4, 4, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 5, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6318, Accuracy: 0.7857, Precision: 0.7324, Recall: 0.6909, F1: 0.6884
Epoch 8/70
Train Loss: 0.1210, Accuracy: 0.9573, Precision: 0.9354, Recall: 0.9350, F1: 0.9352
Validation Loss: 0.8328, Accuracy: 0.8507, Precision: 0.8060, Recall: 0.7802, F1: 0.7861
Testing Loss: 0.7601, Accuracy: 0.8539, Precision: 0.8243, Recall: 0.7713, F1: 0.7857
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7798, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7945, F1: 0.7711
Epoch 9/70
Train Loss: 0.0863, Accuracy: 0.9628, Precision: 0.9437, Recall: 0.9380, F1: 0.9408
Validation Loss: 0.8800, Accuracy: 0.8401, Precision: 0.7966, Recall: 0.7674, F1: 0.7744
Testing Loss: 0.8434, Accuracy: 0.8454, Precision: 0.8176, Recall: 0.7536, F1: 0.7701
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8017, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7745, F1: 0.7520
Epoch 10/70
Train Loss: 0.0727, Accuracy: 0.9673, Precision: 0.9466, Recall: 0.9484, F1: 0.9474
Validation Loss: 1.0310, Accuracy: 0.8529, Precision: 0.7937, Recall: 0.7818, F1: 0.7808
Testing Loss: 0.9799, Accuracy: 0.8599, Precision: 0.8238, Recall: 0.7763, F1: 0.7883
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2026, Accuracy: 0.9286, Precision: 0.9247, Recall: 0.9436, F1: 0.9284
Epoch 11/70
Train Loss: 0.1036, Accuracy: 0.9623, Precision: 0.9421, Recall: 0.9405, F1: 0.9413
Validation Loss: 1.1163, Accuracy: 0.8401, Precision: 0.7941, Recall: 0.7685, F1: 0.7755
Testing Loss: 0.9128, Accuracy: 0.8454, Precision: 0.8177, Recall: 0.7599, F1: 0.7769
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8447, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 12/70
Train Loss: 0.0811, Accuracy: 0.9644, Precision: 0.9458, Recall: 0.9442, F1: 0.9450
Validation Loss: 1.0853, Accuracy: 0.8337, Precision: 0.7840, Recall: 0.7619, F1: 0.7648
Testing Loss: 0.8376, Accuracy: 0.8466, Precision: 0.8130, Recall: 0.7570, F1: 0.7716
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3014, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9073, F1: 0.8792
Epoch 13/70
Train Loss: 0.0639, Accuracy: 0.9685, Precision: 0.9516, Recall: 0.9467, F1: 0.9491
Validation Loss: 1.2120, Accuracy: 0.8422, Precision: 0.7957, Recall: 0.8049, F1: 0.7982
Testing Loss: 0.9493, Accuracy: 0.8611, Precision: 0.8196, Recall: 0.8203, F1: 0.8191
LM Predictions:  [4, 3, 4, 5, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 5, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1920, Accuracy: 0.9286, Precision: 0.8194, Recall: 0.7848, F1: 0.7996
Epoch 14/70
Train Loss: 0.0696, Accuracy: 0.9670, Precision: 0.9459, Recall: 0.9453, F1: 0.9456
Validation Loss: 0.9201, Accuracy: 0.8529, Precision: 0.8112, Recall: 0.7956, F1: 0.8003
Testing Loss: 0.7384, Accuracy: 0.8732, Precision: 0.8410, Recall: 0.8166, F1: 0.8275
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2719, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 15/70
Train Loss: 0.0685, Accuracy: 0.9682, Precision: 0.9518, Recall: 0.9472, F1: 0.9494
Validation Loss: 1.0324, Accuracy: 0.8550, Precision: 0.8078, Recall: 0.7967, F1: 0.8004
Testing Loss: 0.8892, Accuracy: 0.8611, Precision: 0.8325, Recall: 0.7954, F1: 0.8108
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5236, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8127, F1: 0.7887
Epoch 16/70
Train Loss: 0.0607, Accuracy: 0.9675, Precision: 0.9489, Recall: 0.9438, F1: 0.9463
Validation Loss: 1.1058, Accuracy: 0.8422, Precision: 0.7909, Recall: 0.7979, F1: 0.7922
Testing Loss: 0.8840, Accuracy: 0.8623, Precision: 0.8240, Recall: 0.8247, F1: 0.8239
LM Predictions:  [4, 3, 4, 3, 2, 3, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1746, Accuracy: 0.9762, Precision: 0.9818, Recall: 0.9818, F1: 0.9810
Epoch 17/70
Train Loss: 0.0615, Accuracy: 0.9720, Precision: 0.9549, Recall: 0.9556, F1: 0.9553
Validation Loss: 1.0949, Accuracy: 0.8593, Precision: 0.8126, Recall: 0.7986, F1: 0.8007
Testing Loss: 0.9788, Accuracy: 0.8551, Precision: 0.8144, Recall: 0.7795, F1: 0.7899
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3660, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8527, F1: 0.8246
Epoch 18/70
Train Loss: 0.0583, Accuracy: 0.9715, Precision: 0.9551, Recall: 0.9550, F1: 0.9550
Validation Loss: 0.9857, Accuracy: 0.8571, Precision: 0.8095, Recall: 0.8137, F1: 0.8102
Testing Loss: 0.8125, Accuracy: 0.8599, Precision: 0.8270, Recall: 0.8170, F1: 0.8210
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2375, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8655, F1: 0.8361
Epoch 19/70
Train Loss: 0.0791, Accuracy: 0.9644, Precision: 0.9451, Recall: 0.9471, F1: 0.9461
Validation Loss: 0.9496, Accuracy: 0.8443, Precision: 0.7927, Recall: 0.7690, F1: 0.7757
Testing Loss: 0.7853, Accuracy: 0.8647, Precision: 0.8261, Recall: 0.7822, F1: 0.7958
LM Predictions:  [4, 0, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2661, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8818, F1: 0.8484
Epoch 20/70
Train Loss: 0.0802, Accuracy: 0.9670, Precision: 0.9500, Recall: 0.9484, F1: 0.9492
Validation Loss: 1.2831, Accuracy: 0.8316, Precision: 0.7640, Recall: 0.7405, F1: 0.7460
Testing Loss: 1.1039, Accuracy: 0.8563, Precision: 0.8175, Recall: 0.7778, F1: 0.7933
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3967, Accuracy: 0.8333, Precision: 0.8742, Recall: 0.8636, F1: 0.8296
Epoch 21/70
Train Loss: 0.0983, Accuracy: 0.9602, Precision: 0.9358, Recall: 0.9365, F1: 0.9361
Validation Loss: 0.9904, Accuracy: 0.8358, Precision: 0.7789, Recall: 0.7670, F1: 0.7703
Testing Loss: 1.0007, Accuracy: 0.8406, Precision: 0.8070, Recall: 0.7664, F1: 0.7826
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4124, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 22/70
Train Loss: 0.0738, Accuracy: 0.9663, Precision: 0.9487, Recall: 0.9461, F1: 0.9474
Validation Loss: 1.1448, Accuracy: 0.8294, Precision: 0.7756, Recall: 0.7768, F1: 0.7741
Testing Loss: 1.0215, Accuracy: 0.8575, Precision: 0.8220, Recall: 0.8000, F1: 0.8095
LM Predictions:  [0, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3012, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9091, F1: 0.8794
Epoch 23/70
Train Loss: 0.0687, Accuracy: 0.9701, Precision: 0.9506, Recall: 0.9549, F1: 0.9527
Validation Loss: 0.8870, Accuracy: 0.8358, Precision: 0.7885, Recall: 0.7811, F1: 0.7833
Testing Loss: 0.7770, Accuracy: 0.8563, Precision: 0.8209, Recall: 0.7951, F1: 0.8066
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1568, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9455, F1: 0.9223
Epoch 24/70
Train Loss: 0.0550, Accuracy: 0.9711, Precision: 0.9516, Recall: 0.9535, F1: 0.9525
Validation Loss: 1.5512, Accuracy: 0.8294, Precision: 0.7768, Recall: 0.7448, F1: 0.7515
Testing Loss: 1.2806, Accuracy: 0.8539, Precision: 0.8173, Recall: 0.7711, F1: 0.7854
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4440, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 25/70
Train Loss: 0.0527, Accuracy: 0.9732, Precision: 0.9568, Recall: 0.9525, F1: 0.9546
Validation Loss: 1.2636, Accuracy: 0.8316, Precision: 0.7674, Recall: 0.7540, F1: 0.7564
Testing Loss: 1.0796, Accuracy: 0.8514, Precision: 0.8191, Recall: 0.7711, F1: 0.7863
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4991, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 26/70
Train Loss: 0.0515, Accuracy: 0.9720, Precision: 0.9549, Recall: 0.9536, F1: 0.9543
Validation Loss: 1.3453, Accuracy: 0.8273, Precision: 0.7770, Recall: 0.7753, F1: 0.7749
Testing Loss: 1.0588, Accuracy: 0.8575, Precision: 0.8177, Recall: 0.8035, F1: 0.8100
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1393, Accuracy: 0.9524, Precision: 0.9500, Recall: 0.9618, F1: 0.9531
Epoch 27/70
Train Loss: 0.0522, Accuracy: 0.9687, Precision: 0.9488, Recall: 0.9483, F1: 0.9486
Validation Loss: 1.4054, Accuracy: 0.8316, Precision: 0.7935, Recall: 0.7748, F1: 0.7821
Testing Loss: 1.1255, Accuracy: 0.8623, Precision: 0.8311, Recall: 0.7976, F1: 0.8121
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3038, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 28/70
Train Loss: 0.0490, Accuracy: 0.9720, Precision: 0.9543, Recall: 0.9593, F1: 0.9567
Validation Loss: 1.4266, Accuracy: 0.8380, Precision: 0.7798, Recall: 0.7955, F1: 0.7862
Testing Loss: 1.1886, Accuracy: 0.8575, Precision: 0.8136, Recall: 0.8035, F1: 0.8081
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1892, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 29/70
Train Loss: 0.0471, Accuracy: 0.9739, Precision: 0.9585, Recall: 0.9597, F1: 0.9591
Validation Loss: 1.3219, Accuracy: 0.8337, Precision: 0.7821, Recall: 0.7797, F1: 0.7789
Testing Loss: 1.0309, Accuracy: 0.8623, Precision: 0.8197, Recall: 0.8071, F1: 0.8130
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1198, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 30/70
Train Loss: 0.0490, Accuracy: 0.9699, Precision: 0.9520, Recall: 0.9509, F1: 0.9514
Validation Loss: 1.3509, Accuracy: 0.8337, Precision: 0.7778, Recall: 0.7764, F1: 0.7747
Testing Loss: 1.0752, Accuracy: 0.8623, Precision: 0.8206, Recall: 0.7900, F1: 0.8011
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1734, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9600, F1: 0.9444
Epoch 31/70
Train Loss: 0.0478, Accuracy: 0.9763, Precision: 0.9622, Recall: 0.9595, F1: 0.9608
Validation Loss: 1.4776, Accuracy: 0.8273, Precision: 0.7802, Recall: 0.7514, F1: 0.7609
Testing Loss: 1.2308, Accuracy: 0.8551, Precision: 0.8176, Recall: 0.7750, F1: 0.7900
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4004, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 32/70
Train Loss: 0.0480, Accuracy: 0.9699, Precision: 0.9507, Recall: 0.9540, F1: 0.9523
Validation Loss: 1.5531, Accuracy: 0.8337, Precision: 0.7941, Recall: 0.7760, F1: 0.7827
Testing Loss: 1.2843, Accuracy: 0.8587, Precision: 0.8266, Recall: 0.8051, F1: 0.8148
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4764, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 33/70
Train Loss: 0.0502, Accuracy: 0.9734, Precision: 0.9544, Recall: 0.9590, F1: 0.9566
Validation Loss: 1.4271, Accuracy: 0.8358, Precision: 0.7740, Recall: 0.7673, F1: 0.7673
Testing Loss: 1.1281, Accuracy: 0.8587, Precision: 0.8179, Recall: 0.7833, F1: 0.7958
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2101, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8873, F1: 0.8583
Epoch 34/70
Train Loss: 0.0483, Accuracy: 0.9725, Precision: 0.9563, Recall: 0.9553, F1: 0.9558
Validation Loss: 1.4159, Accuracy: 0.8358, Precision: 0.7932, Recall: 0.7729, F1: 0.7808
Testing Loss: 1.1674, Accuracy: 0.8563, Precision: 0.8301, Recall: 0.7982, F1: 0.8122
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4104, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 35/70
Train Loss: 0.0449, Accuracy: 0.9737, Precision: 0.9543, Recall: 0.9609, F1: 0.9574
Validation Loss: 1.7653, Accuracy: 0.8380, Precision: 0.7702, Recall: 0.7562, F1: 0.7568
Testing Loss: 1.3628, Accuracy: 0.8563, Precision: 0.8118, Recall: 0.7743, F1: 0.7858
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2583, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 36/70
Train Loss: 0.0476, Accuracy: 0.9713, Precision: 0.9529, Recall: 0.9533, F1: 0.9531
Validation Loss: 1.6776, Accuracy: 0.8401, Precision: 0.7847, Recall: 0.7937, F1: 0.7878
Testing Loss: 1.3594, Accuracy: 0.8623, Precision: 0.8258, Recall: 0.8152, F1: 0.8202
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.0945, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 37/70
Train Loss: 0.0459, Accuracy: 0.9742, Precision: 0.9588, Recall: 0.9600, F1: 0.9594
Validation Loss: 1.6408, Accuracy: 0.8358, Precision: 0.7646, Recall: 0.7645, F1: 0.7596
Testing Loss: 1.3686, Accuracy: 0.8551, Precision: 0.8105, Recall: 0.7685, F1: 0.7812
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2367, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8655, F1: 0.8361
Epoch 38/70
Train Loss: 0.0478, Accuracy: 0.9730, Precision: 0.9562, Recall: 0.9573, F1: 0.9568
Validation Loss: 1.6827, Accuracy: 0.8358, Precision: 0.7642, Recall: 0.7641, F1: 0.7593
Testing Loss: 1.3953, Accuracy: 0.8527, Precision: 0.8072, Recall: 0.7739, F1: 0.7835
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3857, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 39/70
Train Loss: 0.0511, Accuracy: 0.9718, Precision: 0.9540, Recall: 0.9524, F1: 0.9532
Validation Loss: 2.3777, Accuracy: 0.8060, Precision: 0.7560, Recall: 0.7273, F1: 0.7335
Testing Loss: 2.0262, Accuracy: 0.8164, Precision: 0.7835, Recall: 0.7309, F1: 0.7424
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 0, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 0, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8570, Accuracy: 0.6905, Precision: 0.8556, Recall: 0.7127, F1: 0.7109
Epoch 40/70
Train Loss: 0.1624, Accuracy: 0.9419, Precision: 0.9211, Recall: 0.9230, F1: 0.9219
Validation Loss: 0.8960, Accuracy: 0.8273, Precision: 0.7642, Recall: 0.7512, F1: 0.7502
Testing Loss: 0.7637, Accuracy: 0.8490, Precision: 0.8014, Recall: 0.7601, F1: 0.7689
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2093, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9055, F1: 0.8795
Epoch 41/70
Train Loss: 0.0605, Accuracy: 0.9670, Precision: 0.9489, Recall: 0.9464, F1: 0.9476
Validation Loss: 1.2319, Accuracy: 0.8294, Precision: 0.7819, Recall: 0.7829, F1: 0.7783
Testing Loss: 0.9505, Accuracy: 0.8527, Precision: 0.8019, Recall: 0.7916, F1: 0.7933
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1798, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 42/70
Train Loss: 0.0565, Accuracy: 0.9715, Precision: 0.9534, Recall: 0.9513, F1: 0.9524
Validation Loss: 0.9383, Accuracy: 0.8358, Precision: 0.7527, Recall: 0.7518, F1: 0.7474
Testing Loss: 0.8190, Accuracy: 0.8611, Precision: 0.8205, Recall: 0.7801, F1: 0.7925
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3683, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 43/70
Train Loss: 0.0470, Accuracy: 0.9689, Precision: 0.9496, Recall: 0.9498, F1: 0.9497
Validation Loss: 1.1335, Accuracy: 0.8358, Precision: 0.7733, Recall: 0.7651, F1: 0.7654
Testing Loss: 0.9842, Accuracy: 0.8635, Precision: 0.8242, Recall: 0.7893, F1: 0.8018
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2311, Accuracy: 0.8095, Precision: 0.8583, Recall: 0.8545, F1: 0.8206
Epoch 44/70
Train Loss: 0.0451, Accuracy: 0.9692, Precision: 0.9507, Recall: 0.9496, F1: 0.9501
Validation Loss: 1.1431, Accuracy: 0.8358, Precision: 0.7754, Recall: 0.7695, F1: 0.7684
Testing Loss: 0.9687, Accuracy: 0.8647, Precision: 0.8228, Recall: 0.8045, F1: 0.8120
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2088, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 45/70
Train Loss: 0.0430, Accuracy: 0.9742, Precision: 0.9560, Recall: 0.9613, F1: 0.9586
Validation Loss: 1.1657, Accuracy: 0.8401, Precision: 0.7840, Recall: 0.7735, F1: 0.7763
Testing Loss: 1.0456, Accuracy: 0.8623, Precision: 0.8280, Recall: 0.8057, F1: 0.8157
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4080, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 46/70
Train Loss: 0.0445, Accuracy: 0.9706, Precision: 0.9523, Recall: 0.9514, F1: 0.9518
Validation Loss: 1.2353, Accuracy: 0.8380, Precision: 0.7868, Recall: 0.7869, F1: 0.7858
Testing Loss: 1.0674, Accuracy: 0.8659, Precision: 0.8286, Recall: 0.8160, F1: 0.8219
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2324, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 47/70
Train Loss: 0.0427, Accuracy: 0.9704, Precision: 0.9490, Recall: 0.9514, F1: 0.9502
Validation Loss: 1.2483, Accuracy: 0.8337, Precision: 0.7763, Recall: 0.7634, F1: 0.7667
Testing Loss: 1.1118, Accuracy: 0.8647, Precision: 0.8265, Recall: 0.7976, F1: 0.8094
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3421, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 48/70
Train Loss: 0.0422, Accuracy: 0.9727, Precision: 0.9545, Recall: 0.9548, F1: 0.9546
Validation Loss: 1.3811, Accuracy: 0.8337, Precision: 0.7784, Recall: 0.7608, F1: 0.7651
Testing Loss: 1.1448, Accuracy: 0.8623, Precision: 0.8182, Recall: 0.7887, F1: 0.8001
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2833, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 49/70
Train Loss: 0.0436, Accuracy: 0.9694, Precision: 0.9493, Recall: 0.9537, F1: 0.9514
Validation Loss: 1.3263, Accuracy: 0.8294, Precision: 0.7797, Recall: 0.7785, F1: 0.7778
Testing Loss: 1.1259, Accuracy: 0.8671, Precision: 0.8309, Recall: 0.8118, F1: 0.8205
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1704, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 50/70
Train Loss: 0.0462, Accuracy: 0.9687, Precision: 0.9516, Recall: 0.9433, F1: 0.9472
Validation Loss: 1.2477, Accuracy: 0.8380, Precision: 0.7809, Recall: 0.7678, F1: 0.7703
Testing Loss: 1.0952, Accuracy: 0.8635, Precision: 0.8266, Recall: 0.7940, F1: 0.8066
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2718, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8127, F1: 0.7887
Epoch 51/70
Train Loss: 0.0434, Accuracy: 0.9706, Precision: 0.9517, Recall: 0.9540, F1: 0.9528
Validation Loss: 1.3233, Accuracy: 0.8358, Precision: 0.7820, Recall: 0.7627, F1: 0.7672
Testing Loss: 1.0716, Accuracy: 0.8623, Precision: 0.8178, Recall: 0.7848, F1: 0.7962
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2206, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 52/70
Train Loss: 0.0420, Accuracy: 0.9753, Precision: 0.9586, Recall: 0.9598, F1: 0.9592
Validation Loss: 1.3223, Accuracy: 0.8443, Precision: 0.7872, Recall: 0.7850, F1: 0.7849
Testing Loss: 1.1529, Accuracy: 0.8635, Precision: 0.8240, Recall: 0.8067, F1: 0.8145
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2479, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 53/70
Train Loss: 0.0423, Accuracy: 0.9720, Precision: 0.9517, Recall: 0.9532, F1: 0.9524
Validation Loss: 1.5327, Accuracy: 0.8294, Precision: 0.7782, Recall: 0.7537, F1: 0.7608
Testing Loss: 1.2822, Accuracy: 0.8635, Precision: 0.8263, Recall: 0.7834, F1: 0.7988
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2242, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 54/70
Train Loss: 0.0464, Accuracy: 0.9701, Precision: 0.9536, Recall: 0.9495, F1: 0.9515
Validation Loss: 1.4250, Accuracy: 0.8401, Precision: 0.7872, Recall: 0.7695, F1: 0.7743
Testing Loss: 1.2160, Accuracy: 0.8623, Precision: 0.8235, Recall: 0.7842, F1: 0.7981
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2052, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 55/70
Train Loss: 0.0431, Accuracy: 0.9739, Precision: 0.9603, Recall: 0.9551, F1: 0.9576
Validation Loss: 1.3725, Accuracy: 0.8380, Precision: 0.7694, Recall: 0.7541, F1: 0.7580
Testing Loss: 1.2500, Accuracy: 0.8611, Precision: 0.8326, Recall: 0.7880, F1: 0.8046
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4319, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 56/70
Train Loss: 0.0448, Accuracy: 0.9713, Precision: 0.9529, Recall: 0.9494, F1: 0.9511
Validation Loss: 1.3917, Accuracy: 0.8358, Precision: 0.7834, Recall: 0.7825, F1: 0.7820
Testing Loss: 1.2000, Accuracy: 0.8671, Precision: 0.8350, Recall: 0.8148, F1: 0.8239
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2768, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 57/70
Train Loss: 0.0438, Accuracy: 0.9706, Precision: 0.9540, Recall: 0.9493, F1: 0.9516
Validation Loss: 1.7633, Accuracy: 0.8358, Precision: 0.7861, Recall: 0.7826, F1: 0.7829
Testing Loss: 1.4783, Accuracy: 0.8611, Precision: 0.8172, Recall: 0.7972, F1: 0.8059
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1136, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.1067, Accuracy: 0.9576, Precision: 0.9372, Recall: 0.9287, F1: 0.9328
Validation Loss: 1.0879, Accuracy: 0.8316, Precision: 0.7719, Recall: 0.7781, F1: 0.7742
Testing Loss: 0.8087, Accuracy: 0.8418, Precision: 0.7936, Recall: 0.7859, F1: 0.7895
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 0, 1, 3, 3, 5, 3, 2, 0, 1, 0, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6071, Accuracy: 0.8333, Precision: 0.7233, Recall: 0.7091, F1: 0.6987
Epoch 59/70
Train Loss: 0.1046, Accuracy: 0.9573, Precision: 0.9358, Recall: 0.9370, F1: 0.9363
Validation Loss: 0.9951, Accuracy: 0.8166, Precision: 0.7635, Recall: 0.7634, F1: 0.7588
Testing Loss: 0.7953, Accuracy: 0.8357, Precision: 0.7806, Recall: 0.7763, F1: 0.7761
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 3, 4, 0, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3695, Accuracy: 0.7143, Precision: 0.8339, Recall: 0.7309, F1: 0.7220
Epoch 60/70
Train Loss: 0.0624, Accuracy: 0.9642, Precision: 0.9431, Recall: 0.9440, F1: 0.9436
Validation Loss: 1.0164, Accuracy: 0.8443, Precision: 0.7877, Recall: 0.7845, F1: 0.7842
Testing Loss: 0.8222, Accuracy: 0.8551, Precision: 0.8117, Recall: 0.7887, F1: 0.7984
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1596, Accuracy: 0.9524, Precision: 0.9692, Recall: 0.9418, F1: 0.9516
Epoch 61/70
Train Loss: 0.0487, Accuracy: 0.9680, Precision: 0.9494, Recall: 0.9426, F1: 0.9458
Validation Loss: 1.1964, Accuracy: 0.8230, Precision: 0.7673, Recall: 0.7852, F1: 0.7737
Testing Loss: 0.9636, Accuracy: 0.8514, Precision: 0.8054, Recall: 0.8097, F1: 0.8067
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1270, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 62/70
Train Loss: 0.0451, Accuracy: 0.9692, Precision: 0.9496, Recall: 0.9503, F1: 0.9499
Validation Loss: 1.1979, Accuracy: 0.8337, Precision: 0.7809, Recall: 0.7764, F1: 0.7774
Testing Loss: 0.9837, Accuracy: 0.8611, Precision: 0.8255, Recall: 0.7979, F1: 0.8095
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2086, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8836, F1: 0.8546
Epoch 63/70
Train Loss: 0.0416, Accuracy: 0.9730, Precision: 0.9582, Recall: 0.9518, F1: 0.9549
Validation Loss: 1.2641, Accuracy: 0.8465, Precision: 0.7911, Recall: 0.7906, F1: 0.7898
Testing Loss: 1.0704, Accuracy: 0.8599, Precision: 0.8229, Recall: 0.7983, F1: 0.8089
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4198, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 64/70
Train Loss: 0.0418, Accuracy: 0.9694, Precision: 0.9497, Recall: 0.9508, F1: 0.9502
Validation Loss: 1.3270, Accuracy: 0.8380, Precision: 0.7759, Recall: 0.7691, F1: 0.7690
Testing Loss: 1.0857, Accuracy: 0.8551, Precision: 0.8124, Recall: 0.7812, F1: 0.7925
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1794, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 65/70
Train Loss: 0.0411, Accuracy: 0.9751, Precision: 0.9601, Recall: 0.9608, F1: 0.9604
Validation Loss: 1.2865, Accuracy: 0.8380, Precision: 0.7826, Recall: 0.7866, F1: 0.7840
Testing Loss: 1.0666, Accuracy: 0.8539, Precision: 0.8167, Recall: 0.8039, F1: 0.8096
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2667, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 66/70
Train Loss: 0.0425, Accuracy: 0.9715, Precision: 0.9519, Recall: 0.9553, F1: 0.9535
Validation Loss: 1.3599, Accuracy: 0.8358, Precision: 0.7809, Recall: 0.7796, F1: 0.7790
Testing Loss: 1.0858, Accuracy: 0.8551, Precision: 0.8156, Recall: 0.8032, F1: 0.8090
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1532, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 67/70
Train Loss: 0.0417, Accuracy: 0.9725, Precision: 0.9531, Recall: 0.9612, F1: 0.9570
Validation Loss: 1.3204, Accuracy: 0.8380, Precision: 0.7748, Recall: 0.7691, F1: 0.7692
Testing Loss: 1.0800, Accuracy: 0.8539, Precision: 0.8178, Recall: 0.7878, F1: 0.8003
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2643, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 68/70
Train Loss: 0.0423, Accuracy: 0.9732, Precision: 0.9559, Recall: 0.9579, F1: 0.9568
Validation Loss: 1.3317, Accuracy: 0.8401, Precision: 0.7757, Recall: 0.7658, F1: 0.7663
Testing Loss: 1.1057, Accuracy: 0.8502, Precision: 0.8034, Recall: 0.7689, F1: 0.7806
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2878, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 69/70
Train Loss: 0.0428, Accuracy: 0.9706, Precision: 0.9516, Recall: 0.9523, F1: 0.9519
Validation Loss: 1.4336, Accuracy: 0.8401, Precision: 0.7817, Recall: 0.7709, F1: 0.7715
Testing Loss: 1.1350, Accuracy: 0.8563, Precision: 0.8090, Recall: 0.7799, F1: 0.7901
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1199, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0411, Accuracy: 0.9725, Precision: 0.9561, Recall: 0.9545, F1: 0.9553
Validation Loss: 1.4256, Accuracy: 0.8380, Precision: 0.7750, Recall: 0.7708, F1: 0.7697
Testing Loss: 1.1342, Accuracy: 0.8539, Precision: 0.8066, Recall: 0.7813, F1: 0.7904
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2017, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416

