Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.9622, Accuracy: 0.6848, Precision: 0.5885, Recall: 0.5496, F1: 0.5539
Validation Loss: 0.7691, Accuracy: 0.7846, Precision: 0.6916, Recall: 0.6696, F1: 0.6669
Testing Loss: 0.7161, Accuracy: 0.7923, Precision: 0.7096, Recall: 0.6770, F1: 0.6762
LM Predictions:  [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4858, Accuracy: 0.1190, Precision: 0.0250, Recall: 0.2000, F1: 0.0444
Epoch 2/70
Train Loss: 0.5880, Accuracy: 0.8259, Precision: 0.7561, Recall: 0.7335, F1: 0.7347
Validation Loss: 0.5851, Accuracy: 0.8486, Precision: 0.8577, Recall: 0.7698, F1: 0.7923
Testing Loss: 0.5372, Accuracy: 0.8514, Precision: 0.8569, Recall: 0.7603, F1: 0.7753
LM Predictions:  [0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 5, 2, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4382, Accuracy: 0.0714, Precision: 0.0167, Recall: 0.1000, F1: 0.0286
Epoch 3/70
Train Loss: 0.4903, Accuracy: 0.8525, Precision: 0.7979, Recall: 0.7765, F1: 0.7850
Validation Loss: 0.5006, Accuracy: 0.8529, Precision: 0.8074, Recall: 0.7907, F1: 0.7964
Testing Loss: 0.4306, Accuracy: 0.8780, Precision: 0.8567, Recall: 0.8137, F1: 0.8238
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9843, Accuracy: 0.0714, Precision: 0.0200, Recall: 0.1000, F1: 0.0333
Epoch 4/70
Train Loss: 0.4270, Accuracy: 0.8746, Precision: 0.8309, Recall: 0.8168, F1: 0.8232
Validation Loss: 0.5691, Accuracy: 0.8571, Precision: 0.8779, Recall: 0.7782, F1: 0.8013
Testing Loss: 0.4822, Accuracy: 0.8539, Precision: 0.8499, Recall: 0.7799, F1: 0.7971
LM Predictions:  [0, 3, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 3, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4041, Accuracy: 0.0952, Precision: 0.1026, Recall: 0.1167, F1: 0.0600
Epoch 5/70
Train Loss: 0.4065, Accuracy: 0.8798, Precision: 0.8359, Recall: 0.8270, F1: 0.8310
Validation Loss: 0.5514, Accuracy: 0.8422, Precision: 0.8161, Recall: 0.8294, F1: 0.8156
Testing Loss: 0.4176, Accuracy: 0.8780, Precision: 0.8470, Recall: 0.8636, F1: 0.8492
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 3, 5, 2, 2, 5, 5, 0, 5, 2, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5199, Accuracy: 0.0952, Precision: 0.2667, Recall: 0.0792, F1: 0.1072
Epoch 6/70
Train Loss: 0.3744, Accuracy: 0.8881, Precision: 0.8466, Recall: 0.8336, F1: 0.8396
Validation Loss: 0.5293, Accuracy: 0.8635, Precision: 0.8309, Recall: 0.8447, F1: 0.8356
Testing Loss: 0.4605, Accuracy: 0.8768, Precision: 0.8532, Recall: 0.8491, F1: 0.8480
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 3, 5, 5, 2, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0178, Accuracy: 0.0952, Precision: 0.2167, Recall: 0.1167, F1: 0.0970
Epoch 7/70
Train Loss: 0.3487, Accuracy: 0.8992, Precision: 0.8588, Recall: 0.8539, F1: 0.8559
Validation Loss: 0.5412, Accuracy: 0.8614, Precision: 0.8498, Recall: 0.7976, F1: 0.8185
Testing Loss: 0.4785, Accuracy: 0.8647, Precision: 0.8516, Recall: 0.7800, F1: 0.8019
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 2, 0, 3, 0, 2, 5, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9723, Accuracy: 0.1667, Precision: 0.3180, Recall: 0.1792, F1: 0.1553
Epoch 8/70
Train Loss: 0.3081, Accuracy: 0.9108, Precision: 0.8737, Recall: 0.8751, F1: 0.8739
Validation Loss: 0.5450, Accuracy: 0.8785, Precision: 0.8502, Recall: 0.8605, F1: 0.8535
Testing Loss: 0.4408, Accuracy: 0.8804, Precision: 0.8603, Recall: 0.8553, F1: 0.8562
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 0, 5, 0, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4562, Accuracy: 0.1190, Precision: 0.3718, Recall: 0.1375, F1: 0.1229
Epoch 9/70
Train Loss: 0.3037, Accuracy: 0.9108, Precision: 0.8728, Recall: 0.8716, F1: 0.8719
Validation Loss: 0.5935, Accuracy: 0.8571, Precision: 0.8740, Recall: 0.7847, F1: 0.8087
Testing Loss: 0.5178, Accuracy: 0.8551, Precision: 0.8795, Recall: 0.7634, F1: 0.7869
LM Predictions:  [0, 5, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4428, Accuracy: 0.0714, Precision: 0.0179, Recall: 0.1000, F1: 0.0303
Epoch 10/70
Train Loss: 0.2909, Accuracy: 0.9184, Precision: 0.8817, Recall: 0.8816, F1: 0.8813
Validation Loss: 0.5777, Accuracy: 0.8593, Precision: 0.8349, Recall: 0.8063, F1: 0.8133
Testing Loss: 0.4283, Accuracy: 0.8792, Precision: 0.8634, Recall: 0.8124, F1: 0.8204
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8671, Accuracy: 0.1429, Precision: 0.5227, Recall: 0.1542, F1: 0.1347
Epoch 11/70
Train Loss: 0.2803, Accuracy: 0.9187, Precision: 0.8814, Recall: 0.8807, F1: 0.8806
Validation Loss: 0.5619, Accuracy: 0.8614, Precision: 0.8581, Recall: 0.8004, F1: 0.8220
Testing Loss: 0.4471, Accuracy: 0.8780, Precision: 0.8705, Recall: 0.7968, F1: 0.8167
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8526, Accuracy: 0.1190, Precision: 0.3526, Recall: 0.1375, F1: 0.0996
Epoch 12/70
Train Loss: 0.2525, Accuracy: 0.9270, Precision: 0.8921, Recall: 0.8851, F1: 0.8884
Validation Loss: 0.6162, Accuracy: 0.8593, Precision: 0.8163, Recall: 0.8364, F1: 0.8245
Testing Loss: 0.4969, Accuracy: 0.8708, Precision: 0.8413, Recall: 0.8560, F1: 0.8456
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 2, 0, 2, 3, 5, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6472, Accuracy: 0.1905, Precision: 0.4714, Recall: 0.1792, F1: 0.2192
Epoch 13/70
Train Loss: 0.2417, Accuracy: 0.9258, Precision: 0.8894, Recall: 0.8945, F1: 0.8914
Validation Loss: 0.5857, Accuracy: 0.8507, Precision: 0.8276, Recall: 0.7779, F1: 0.7953
Testing Loss: 0.5098, Accuracy: 0.8659, Precision: 0.8649, Recall: 0.7786, F1: 0.7957
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 4, 0, 0, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8058, Accuracy: 0.1667, Precision: 0.3256, Recall: 0.1708, F1: 0.1470
Epoch 14/70
Train Loss: 0.2443, Accuracy: 0.9281, Precision: 0.8934, Recall: 0.8849, F1: 0.8890
Validation Loss: 0.5726, Accuracy: 0.8529, Precision: 0.8161, Recall: 0.8315, F1: 0.8204
Testing Loss: 0.4536, Accuracy: 0.8732, Precision: 0.8360, Recall: 0.8558, F1: 0.8433
LM Predictions:  [5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 5, 0, 5, 5, 5, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0609, Accuracy: 0.1667, Precision: 0.4417, Recall: 0.1625, F1: 0.2081
Epoch 15/70
Train Loss: 0.2445, Accuracy: 0.9274, Precision: 0.8921, Recall: 0.8934, F1: 0.8921
Validation Loss: 0.6142, Accuracy: 0.8529, Precision: 0.8213, Recall: 0.7705, F1: 0.7888
Testing Loss: 0.4831, Accuracy: 0.8635, Precision: 0.8445, Recall: 0.7793, F1: 0.7966
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 5, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4915, Accuracy: 0.1667, Precision: 0.4644, Recall: 0.1750, F1: 0.1545
Epoch 16/70
Train Loss: 0.2662, Accuracy: 0.9227, Precision: 0.8849, Recall: 0.8910, F1: 0.8873
Validation Loss: 0.6347, Accuracy: 0.8401, Precision: 0.7966, Recall: 0.8011, F1: 0.7974
Testing Loss: 0.4948, Accuracy: 0.8696, Precision: 0.8405, Recall: 0.8136, F1: 0.8227
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 2, 5, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 5, 5, 5, 0, 0, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5136, Accuracy: 0.2143, Precision: 0.4651, Recall: 0.2292, F1: 0.1888
Epoch 17/70
Train Loss: 0.2302, Accuracy: 0.9300, Precision: 0.9004, Recall: 0.8965, F1: 0.8982
Validation Loss: 0.6081, Accuracy: 0.8529, Precision: 0.8125, Recall: 0.7730, F1: 0.7826
Testing Loss: 0.4975, Accuracy: 0.8804, Precision: 0.8708, Recall: 0.7974, F1: 0.8102
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 2, 0, 0, 2, 0, 3, 0, 2, 2, 0, 2, 5, 0, 2, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 5, 0, 4, 0, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5902, Accuracy: 0.2143, Precision: 0.4186, Recall: 0.2125, F1: 0.1854
Epoch 18/70
Train Loss: 0.2521, Accuracy: 0.9255, Precision: 0.8931, Recall: 0.8930, F1: 0.8924
Validation Loss: 0.6969, Accuracy: 0.8486, Precision: 0.8227, Recall: 0.7688, F1: 0.7881
Testing Loss: 0.5465, Accuracy: 0.8708, Precision: 0.8618, Recall: 0.7842, F1: 0.8053
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 5, 0, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 5, 0, 4, 0, 5, 0, 0, 0, 0, 0, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8277, Accuracy: 0.1190, Precision: 0.3542, Recall: 0.1375, F1: 0.1018
Epoch 19/70
Train Loss: 0.2241, Accuracy: 0.9291, Precision: 0.8948, Recall: 0.8937, F1: 0.8940
Validation Loss: 0.6288, Accuracy: 0.8763, Precision: 0.8530, Recall: 0.8275, F1: 0.8377
Testing Loss: 0.4981, Accuracy: 0.8708, Precision: 0.8335, Recall: 0.8173, F1: 0.8243
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 0, 5, 5, 0, 0, 2, 0, 3, 0, 3, 5, 0, 3, 5, 5, 3, 0, 0, 5, 5, 5, 5, 0, 4, 0, 3, 0, 0, 5, 5, 5, 0, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8188, Accuracy: 0.2143, Precision: 0.4778, Recall: 0.2042, F1: 0.2007
Epoch 20/70
Train Loss: 0.2155, Accuracy: 0.9362, Precision: 0.9005, Recall: 0.9067, F1: 0.9029
Validation Loss: 0.6576, Accuracy: 0.8614, Precision: 0.8275, Recall: 0.8342, F1: 0.8279
Testing Loss: 0.5245, Accuracy: 0.8829, Precision: 0.8511, Recall: 0.8614, F1: 0.8551
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 4, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7607, Accuracy: 0.2143, Precision: 0.5625, Recall: 0.2083, F1: 0.2547
Epoch 21/70
Train Loss: 0.2032, Accuracy: 0.9402, Precision: 0.9042, Recall: 0.9166, F1: 0.9087
Validation Loss: 0.6696, Accuracy: 0.8593, Precision: 0.8275, Recall: 0.8221, F1: 0.8212
Testing Loss: 0.5456, Accuracy: 0.8756, Precision: 0.8427, Recall: 0.8539, F1: 0.8449
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 0, 4, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5345, Accuracy: 0.2143, Precision: 0.5500, Recall: 0.2083, F1: 0.2444
Epoch 22/70
Train Loss: 0.1919, Accuracy: 0.9443, Precision: 0.9103, Recall: 0.9142, F1: 0.9119
Validation Loss: 0.5777, Accuracy: 0.8635, Precision: 0.8295, Recall: 0.8274, F1: 0.8269
Testing Loss: 0.5759, Accuracy: 0.8514, Precision: 0.8027, Recall: 0.7964, F1: 0.7993
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 5, 3, 0, 0, 0, 0, 3, 5, 5, 3, 0, 0, 5, 5, 0, 5, 0, 4, 0, 3, 0, 0, 4, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4687, Accuracy: 0.2619, Precision: 0.5294, Recall: 0.2417, F1: 0.2629
Epoch 23/70
Train Loss: 0.1979, Accuracy: 0.9395, Precision: 0.9036, Recall: 0.9093, F1: 0.9059
Validation Loss: 0.6299, Accuracy: 0.8593, Precision: 0.8346, Recall: 0.8102, F1: 0.8189
Testing Loss: 0.6091, Accuracy: 0.8466, Precision: 0.8227, Recall: 0.7913, F1: 0.8036
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 2, 0, 2, 0, 5, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 3, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4146, Accuracy: 0.2381, Precision: 0.4551, Recall: 0.2292, F1: 0.2381
Epoch 24/70
Train Loss: 0.1909, Accuracy: 0.9421, Precision: 0.9078, Recall: 0.9172, F1: 0.9115
Validation Loss: 0.7034, Accuracy: 0.8443, Precision: 0.7984, Recall: 0.7854, F1: 0.7894
Testing Loss: 0.6194, Accuracy: 0.8635, Precision: 0.8298, Recall: 0.8002, F1: 0.8113
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 5, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 5, 3, 5, 5, 4, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5126, Accuracy: 0.2619, Precision: 0.5357, Recall: 0.2458, F1: 0.2760
Epoch 25/70
Train Loss: 0.1926, Accuracy: 0.9400, Precision: 0.9059, Recall: 0.9093, F1: 0.9070
Validation Loss: 0.6946, Accuracy: 0.8507, Precision: 0.8087, Recall: 0.7716, F1: 0.7866
Testing Loss: 0.5972, Accuracy: 0.8611, Precision: 0.8298, Recall: 0.7803, F1: 0.7969
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 0, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 4, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5469, Accuracy: 0.2619, Precision: 0.5200, Recall: 0.2417, F1: 0.2508
Epoch 26/70
Train Loss: 0.1845, Accuracy: 0.9445, Precision: 0.9116, Recall: 0.9113, F1: 0.9111
Validation Loss: 0.6484, Accuracy: 0.8614, Precision: 0.8261, Recall: 0.7950, F1: 0.8075
Testing Loss: 0.5753, Accuracy: 0.8575, Precision: 0.8228, Recall: 0.7818, F1: 0.7958
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5215, Accuracy: 0.2143, Precision: 0.5185, Recall: 0.2125, F1: 0.2080
Epoch 27/70
Train Loss: 0.1989, Accuracy: 0.9421, Precision: 0.9133, Recall: 0.9158, F1: 0.9142
Validation Loss: 0.6755, Accuracy: 0.8635, Precision: 0.8295, Recall: 0.8114, F1: 0.8192
Testing Loss: 0.6185, Accuracy: 0.8623, Precision: 0.8297, Recall: 0.7958, F1: 0.8105
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 2, 0, 0, 2, 5, 3, 0, 2, 2, 0, 2, 0, 5, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 3, 5, 0, 0, 0, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5882, Accuracy: 0.2143, Precision: 0.4292, Recall: 0.2125, F1: 0.1984
Epoch 28/70
Train Loss: 0.1974, Accuracy: 0.9398, Precision: 0.9073, Recall: 0.9144, F1: 0.9101
Validation Loss: 0.6388, Accuracy: 0.8657, Precision: 0.8339, Recall: 0.8222, F1: 0.8277
Testing Loss: 0.5603, Accuracy: 0.8684, Precision: 0.8339, Recall: 0.8283, F1: 0.8310
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 0, 5, 3, 3, 0, 3, 5, 0, 5, 0, 5, 5, 0, 4, 5, 3, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1123, Accuracy: 0.3095, Precision: 0.5385, Recall: 0.2792, F1: 0.3131
Epoch 29/70
Train Loss: 0.1713, Accuracy: 0.9459, Precision: 0.9142, Recall: 0.9250, F1: 0.9182
Validation Loss: 0.7585, Accuracy: 0.8678, Precision: 0.8318, Recall: 0.8048, F1: 0.8150
Testing Loss: 0.6396, Accuracy: 0.8623, Precision: 0.8220, Recall: 0.7873, F1: 0.7982
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 0, 0, 3, 3, 0, 5, 0, 5, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 4, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3530, Accuracy: 0.2857, Precision: 0.5227, Recall: 0.2625, F1: 0.2787
Epoch 30/70
Train Loss: 0.1699, Accuracy: 0.9497, Precision: 0.9222, Recall: 0.9266, F1: 0.9239
Validation Loss: 0.7050, Accuracy: 0.8614, Precision: 0.8398, Recall: 0.8101, F1: 0.8203
Testing Loss: 0.6443, Accuracy: 0.8659, Precision: 0.8228, Recall: 0.7974, F1: 0.8058
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 5, 3, 0, 2, 3, 0, 3, 3, 0, 5, 0, 0, 5, 0, 0, 5, 0, 4, 5, 3, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5351, Accuracy: 0.2619, Precision: 0.4905, Recall: 0.2458, F1: 0.2486
Epoch 31/70
Train Loss: 0.1944, Accuracy: 0.9438, Precision: 0.9121, Recall: 0.9149, F1: 0.9130
Validation Loss: 0.7216, Accuracy: 0.8529, Precision: 0.8058, Recall: 0.8145, F1: 0.8087
Testing Loss: 0.5857, Accuracy: 0.8780, Precision: 0.8449, Recall: 0.8449, F1: 0.8446
LM Predictions:  [0, 5, 5, 5, 5, 1, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 2, 3, 5, 3, 3, 0, 3, 5, 5, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4337, Accuracy: 0.3333, Precision: 0.7014, Recall: 0.2977, F1: 0.3609
Epoch 32/70
Train Loss: 0.1822, Accuracy: 0.9438, Precision: 0.9116, Recall: 0.9281, F1: 0.9177
Validation Loss: 0.7639, Accuracy: 0.8571, Precision: 0.8196, Recall: 0.7984, F1: 0.8071
Testing Loss: 0.7004, Accuracy: 0.8539, Precision: 0.8271, Recall: 0.7882, F1: 0.8036
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 2, 0, 3, 3, 0, 2, 5, 5, 5, 0, 0, 5, 0, 4, 0, 3, 5, 5, 0, 5, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2687, Accuracy: 0.2619, Precision: 0.4690, Recall: 0.2458, F1: 0.2551
Epoch 33/70
Train Loss: 0.1732, Accuracy: 0.9478, Precision: 0.9171, Recall: 0.9246, F1: 0.9199
Validation Loss: 0.7273, Accuracy: 0.8657, Precision: 0.8378, Recall: 0.8034, F1: 0.8141
Testing Loss: 0.6750, Accuracy: 0.8611, Precision: 0.8247, Recall: 0.7818, F1: 0.7907
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 2, 0, 3, 3, 0, 2, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 4, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1893, Accuracy: 0.2619, Precision: 0.4394, Recall: 0.2417, F1: 0.2434
Epoch 34/70
Train Loss: 0.1690, Accuracy: 0.9440, Precision: 0.9150, Recall: 0.9123, F1: 0.9133
Validation Loss: 0.7494, Accuracy: 0.8486, Precision: 0.8127, Recall: 0.7929, F1: 0.8017
Testing Loss: 0.7027, Accuracy: 0.8611, Precision: 0.8371, Recall: 0.7940, F1: 0.8104
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 2, 4, 3, 0, 1, 1, 0, 1, 0, 0, 1, 0, 5, 5, 0, 5, 5, 0, 4, 0, 3, 5, 0, 4, 0, 5, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3484, Accuracy: 0.2619, Precision: 0.5646, Recall: 0.2435, F1: 0.2706
Epoch 35/70
Train Loss: 0.1572, Accuracy: 0.9504, Precision: 0.9196, Recall: 0.9211, F1: 0.9199
Validation Loss: 1.0127, Accuracy: 0.8252, Precision: 0.7774, Recall: 0.7513, F1: 0.7612
Testing Loss: 0.7914, Accuracy: 0.8551, Precision: 0.8324, Recall: 0.8077, F1: 0.8172
LM Predictions:  [0, 5, 5, 5, 2, 5, 5, 5, 5, 2, 5, 0, 2, 4, 3, 5, 2, 2, 5, 2, 3, 0, 2, 5, 5, 5, 0, 5, 5, 0, 4, 4, 3, 5, 5, 4, 5, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9939, Accuracy: 0.2857, Precision: 0.4306, Recall: 0.2625, F1: 0.2981
Epoch 36/70
Train Loss: 0.1935, Accuracy: 0.9424, Precision: 0.9085, Recall: 0.9081, F1: 0.9081
Validation Loss: 0.7821, Accuracy: 0.8486, Precision: 0.8032, Recall: 0.7610, F1: 0.7740
Testing Loss: 0.6742, Accuracy: 0.8539, Precision: 0.8245, Recall: 0.7607, F1: 0.7788
LM Predictions:  [0, 1, 5, 0, 2, 0, 5, 0, 0, 2, 0, 0, 2, 0, 3, 0, 2, 2, 0, 3, 0, 0, 2, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3580, Accuracy: 0.2619, Precision: 0.5833, Recall: 0.2477, F1: 0.2375
Epoch 37/70
Train Loss: 0.1828, Accuracy: 0.9455, Precision: 0.9149, Recall: 0.9166, F1: 0.9147
Validation Loss: 0.7686, Accuracy: 0.8550, Precision: 0.8285, Recall: 0.7798, F1: 0.7993
Testing Loss: 0.7245, Accuracy: 0.8527, Precision: 0.8328, Recall: 0.7714, F1: 0.7923
LM Predictions:  [0, 5, 5, 0, 2, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 3, 0, 4, 0, 0, 5, 0, 0, 5, 0, 4, 5, 3, 5, 0, 0, 0, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2057, Accuracy: 0.2619, Precision: 0.3738, Recall: 0.2458, F1: 0.2384
Epoch 38/70
Train Loss: 0.1639, Accuracy: 0.9507, Precision: 0.9213, Recall: 0.9201, F1: 0.9204
Validation Loss: 0.8047, Accuracy: 0.8529, Precision: 0.8014, Recall: 0.8171, F1: 0.8084
Testing Loss: 0.6878, Accuracy: 0.8659, Precision: 0.8237, Recall: 0.8222, F1: 0.8226
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 2, 4, 3, 5, 2, 5, 0, 3, 3, 4, 3, 5, 5, 5, 4, 4, 5, 0, 4, 3, 3, 4, 0, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8234, Accuracy: 0.4524, Precision: 0.5506, Recall: 0.3792, F1: 0.4289
Epoch 39/70
Train Loss: 0.2120, Accuracy: 0.9386, Precision: 0.9055, Recall: 0.9207, F1: 0.9122
Validation Loss: 0.6855, Accuracy: 0.8486, Precision: 0.8249, Recall: 0.7623, F1: 0.7847
Testing Loss: 0.5923, Accuracy: 0.8539, Precision: 0.8462, Recall: 0.7588, F1: 0.7802
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 1, 0, 1, 0, 0, 5, 0, 0, 5, 0, 4, 1, 1, 0, 0, 4, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0472, Accuracy: 0.2619, Precision: 0.5505, Recall: 0.2477, F1: 0.2613
Epoch 40/70
Train Loss: 0.1684, Accuracy: 0.9478, Precision: 0.9200, Recall: 0.9199, F1: 0.9198
Validation Loss: 0.7362, Accuracy: 0.8529, Precision: 0.8213, Recall: 0.8134, F1: 0.8156
Testing Loss: 0.6833, Accuracy: 0.8587, Precision: 0.8258, Recall: 0.7945, F1: 0.8082
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 1, 0, 0, 2, 0, 3, 5, 2, 1, 0, 3, 0, 5, 3, 5, 5, 5, 5, 5, 5, 0, 4, 3, 3, 5, 0, 4, 5, 5, 0, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3838, Accuracy: 0.3095, Precision: 0.5024, Recall: 0.2792, F1: 0.3112
Epoch 41/70
Train Loss: 0.1609, Accuracy: 0.9528, Precision: 0.9273, Recall: 0.9332, F1: 0.9298
Validation Loss: 0.7524, Accuracy: 0.8465, Precision: 0.8213, Recall: 0.8121, F1: 0.8106
Testing Loss: 0.6340, Accuracy: 0.8587, Precision: 0.8314, Recall: 0.8234, F1: 0.8223
LM Predictions:  [0, 1, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 0, 1, 5, 5, 0, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7220, Accuracy: 0.2143, Precision: 0.6667, Recall: 0.2144, F1: 0.2727
Epoch 42/70
Train Loss: 0.1601, Accuracy: 0.9497, Precision: 0.9175, Recall: 0.9370, F1: 0.9247
Validation Loss: 0.7853, Accuracy: 0.8635, Precision: 0.8388, Recall: 0.8125, F1: 0.8242
Testing Loss: 0.6953, Accuracy: 0.8575, Precision: 0.8307, Recall: 0.7922, F1: 0.8067
LM Predictions:  [0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 2, 4, 3, 0, 2, 0, 0, 4, 0, 0, 4, 5, 0, 5, 0, 0, 5, 0, 4, 0, 3, 5, 0, 4, 0, 5, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2324, Accuracy: 0.2857, Precision: 0.4684, Recall: 0.2792, F1: 0.2687
Epoch 43/70
Train Loss: 0.1642, Accuracy: 0.9493, Precision: 0.9163, Recall: 0.9309, F1: 0.9225
Validation Loss: 0.8091, Accuracy: 0.8486, Precision: 0.8220, Recall: 0.7566, F1: 0.7776
Testing Loss: 0.6930, Accuracy: 0.8466, Precision: 0.8409, Recall: 0.7515, F1: 0.7732
LM Predictions:  [0, 0, 5, 0, 0, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 0, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0292, Accuracy: 0.2619, Precision: 0.5179, Recall: 0.2458, F1: 0.2468
Epoch 44/70
Train Loss: 0.1666, Accuracy: 0.9474, Precision: 0.9217, Recall: 0.9236, F1: 0.9222
Validation Loss: 0.7937, Accuracy: 0.8443, Precision: 0.8178, Recall: 0.7725, F1: 0.7882
Testing Loss: 0.6550, Accuracy: 0.8611, Precision: 0.8316, Recall: 0.7767, F1: 0.7935
LM Predictions:  [0, 1, 5, 0, 1, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 1, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 0, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9662, Accuracy: 0.3095, Precision: 0.6042, Recall: 0.2829, F1: 0.3022
Epoch 45/70
Train Loss: 0.1584, Accuracy: 0.9533, Precision: 0.9265, Recall: 0.9318, F1: 0.9285
Validation Loss: 0.7530, Accuracy: 0.8507, Precision: 0.8074, Recall: 0.8020, F1: 0.8030
Testing Loss: 0.7071, Accuracy: 0.8551, Precision: 0.8210, Recall: 0.8136, F1: 0.8151
LM Predictions:  [0, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 0, 2, 4, 0, 5, 2, 1, 5, 4, 0, 4, 3, 0, 5, 5, 0, 5, 5, 0, 4, 0, 3, 5, 5, 4, 5, 5, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3748, Accuracy: 0.3333, Precision: 0.6333, Recall: 0.2995, F1: 0.3623
Epoch 46/70
Train Loss: 0.1909, Accuracy: 0.9450, Precision: 0.9094, Recall: 0.9365, F1: 0.9204
Validation Loss: 0.6621, Accuracy: 0.8529, Precision: 0.8161, Recall: 0.8134, F1: 0.8124
Testing Loss: 0.5826, Accuracy: 0.8551, Precision: 0.8159, Recall: 0.8154, F1: 0.8149
LM Predictions:  [0, 5, 5, 5, 1, 5, 5, 5, 5, 5, 5, 0, 2, 0, 3, 5, 2, 1, 5, 3, 0, 0, 3, 0, 0, 5, 5, 5, 5, 0, 4, 3, 3, 5, 5, 0, 5, 5, 5, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3744, Accuracy: 0.3095, Precision: 0.5972, Recall: 0.2810, F1: 0.3265
Epoch 47/70
Train Loss: 0.1556, Accuracy: 0.9502, Precision: 0.9206, Recall: 0.9270, F1: 0.9236
Validation Loss: 0.8176, Accuracy: 0.8571, Precision: 0.8320, Recall: 0.7838, F1: 0.7984
Testing Loss: 0.7410, Accuracy: 0.8659, Precision: 0.8410, Recall: 0.7776, F1: 0.7916
LM Predictions:  [0, 5, 5, 0, 1, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1967, Accuracy: 0.2857, Precision: 0.5625, Recall: 0.2644, F1: 0.2737
Epoch 48/70
Train Loss: 0.1472, Accuracy: 0.9523, Precision: 0.9237, Recall: 0.9280, F1: 0.9254
Validation Loss: 0.7789, Accuracy: 0.8678, Precision: 0.8350, Recall: 0.7943, F1: 0.8073
Testing Loss: 0.7199, Accuracy: 0.8684, Precision: 0.8324, Recall: 0.7837, F1: 0.7964
LM Predictions:  [0, 1, 5, 0, 1, 0, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5641, Accuracy: 0.3095, Precision: 0.5903, Recall: 0.2829, F1: 0.2989
Epoch 49/70
Train Loss: 0.2617, Accuracy: 0.9182, Precision: 0.8878, Recall: 0.8812, F1: 0.8841
Validation Loss: 0.6524, Accuracy: 0.8550, Precision: 0.8214, Recall: 0.7980, F1: 0.8075
Testing Loss: 0.6567, Accuracy: 0.8563, Precision: 0.8272, Recall: 0.8140, F1: 0.8187
LM Predictions:  [0, 0, 5, 0, 2, 5, 5, 5, 0, 5, 5, 0, 2, 0, 3, 0, 2, 1, 0, 3, 5, 0, 3, 5, 0, 5, 0, 0, 5, 0, 4, 0, 3, 5, 0, 0, 5, 5, 5, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5975, Accuracy: 0.2619, Precision: 0.4646, Recall: 0.2458, F1: 0.2501
Epoch 50/70
Train Loss: 0.1824, Accuracy: 0.9512, Precision: 0.9243, Recall: 0.9307, F1: 0.9270
Validation Loss: 0.6501, Accuracy: 0.8550, Precision: 0.8380, Recall: 0.7934, F1: 0.8067
Testing Loss: 0.7009, Accuracy: 0.8297, Precision: 0.7969, Recall: 0.7562, F1: 0.7653
LM Predictions:  [0, 0, 5, 0, 1, 5, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 0, 0, 0, 5, 0, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1987, Accuracy: 0.3095, Precision: 0.6100, Recall: 0.2935, F1: 0.2828
Epoch 51/70
Train Loss: 0.1700, Accuracy: 0.9452, Precision: 0.9218, Recall: 0.9194, F1: 0.9204
Validation Loss: 0.8444, Accuracy: 0.8443, Precision: 0.8210, Recall: 0.7610, F1: 0.7797
Testing Loss: 0.7716, Accuracy: 0.8514, Precision: 0.8200, Recall: 0.7563, F1: 0.7704
LM Predictions:  [0, 1, 5, 0, 2, 0, 5, 0, 0, 4, 0, 0, 2, 0, 2, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 2, 3, 0, 0, 4, 0, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4029, Accuracy: 0.2857, Precision: 0.4553, Recall: 0.2644, F1: 0.2622
Epoch 52/70
Train Loss: 0.2021, Accuracy: 0.9400, Precision: 0.9189, Recall: 0.9002, F1: 0.9089
Validation Loss: 0.8034, Accuracy: 0.8380, Precision: 0.7786, Recall: 0.7498, F1: 0.7565
Testing Loss: 0.7252, Accuracy: 0.8551, Precision: 0.8098, Recall: 0.7823, F1: 0.7913
LM Predictions:  [0, 1, 5, 0, 1, 5, 5, 0, 0, 1, 0, 0, 2, 4, 3, 0, 2, 1, 0, 1, 5, 4, 1, 5, 0, 5, 4, 4, 5, 0, 4, 1, 3, 5, 0, 4, 0, 4, 5, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9780, Accuracy: 0.4286, Precision: 0.6042, Recall: 0.3681, F1: 0.4014
Epoch 53/70
Train Loss: 0.1698, Accuracy: 0.9471, Precision: 0.9191, Recall: 0.9164, F1: 0.9177
Validation Loss: 0.7453, Accuracy: 0.8657, Precision: 0.8490, Recall: 0.7947, F1: 0.8160
Testing Loss: 0.7106, Accuracy: 0.8490, Precision: 0.8173, Recall: 0.7664, F1: 0.7853
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 5, 0, 0, 3, 0, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4431, Accuracy: 0.2857, Precision: 0.5727, Recall: 0.2644, F1: 0.2774
Epoch 54/70
Train Loss: 0.1503, Accuracy: 0.9535, Precision: 0.9281, Recall: 0.9280, F1: 0.9279
Validation Loss: 0.7057, Accuracy: 0.8465, Precision: 0.8152, Recall: 0.8020, F1: 0.8054
Testing Loss: 0.6996, Accuracy: 0.8394, Precision: 0.7998, Recall: 0.7844, F1: 0.7899
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 0, 3, 5, 0, 0, 3, 0, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4760, Accuracy: 0.3095, Precision: 0.6016, Recall: 0.2829, F1: 0.3041
Epoch 55/70
Train Loss: 0.1361, Accuracy: 0.9547, Precision: 0.9286, Recall: 0.9248, F1: 0.9266
Validation Loss: 0.6925, Accuracy: 0.8678, Precision: 0.8278, Recall: 0.8155, F1: 0.8207
Testing Loss: 0.7020, Accuracy: 0.8647, Precision: 0.8257, Recall: 0.8139, F1: 0.8194
LM Predictions:  [0, 1, 5, 0, 1, 5, 5, 5, 4, 5, 5, 0, 2, 4, 3, 0, 2, 1, 0, 3, 5, 4, 3, 5, 0, 5, 4, 4, 5, 0, 4, 0, 3, 5, 0, 4, 3, 4, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0482, Accuracy: 0.5000, Precision: 0.6417, Recall: 0.4181, F1: 0.4715
Epoch 56/70
Train Loss: 0.1395, Accuracy: 0.9561, Precision: 0.9303, Recall: 0.9402, F1: 0.9342
Validation Loss: 0.7440, Accuracy: 0.8550, Precision: 0.8092, Recall: 0.8165, F1: 0.8112
Testing Loss: 0.7456, Accuracy: 0.8708, Precision: 0.8310, Recall: 0.8373, F1: 0.8330
LM Predictions:  [0, 1, 5, 0, 1, 5, 5, 5, 4, 5, 5, 0, 2, 4, 3, 0, 2, 1, 0, 3, 5, 4, 3, 5, 5, 5, 4, 4, 5, 0, 4, 3, 3, 5, 0, 4, 3, 4, 5, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0079, Accuracy: 0.5238, Precision: 0.6687, Recall: 0.4347, F1: 0.5035
Epoch 57/70
Train Loss: 0.1471, Accuracy: 0.9545, Precision: 0.9222, Recall: 0.9344, F1: 0.9277
Validation Loss: 0.7528, Accuracy: 0.8550, Precision: 0.8100, Recall: 0.8125, F1: 0.8082
Testing Loss: 0.7255, Accuracy: 0.8659, Precision: 0.8250, Recall: 0.8262, F1: 0.8255
LM Predictions:  [0, 1, 5, 0, 2, 5, 5, 5, 4, 5, 5, 0, 2, 4, 3, 0, 2, 1, 0, 3, 5, 4, 3, 5, 5, 5, 4, 4, 5, 0, 4, 3, 3, 5, 0, 4, 3, 4, 5, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1043, Accuracy: 0.5000, Precision: 0.6131, Recall: 0.4162, F1: 0.4745
Epoch 58/70
Train Loss: 0.1401, Accuracy: 0.9559, Precision: 0.9247, Recall: 0.9398, F1: 0.9317
Validation Loss: 0.7240, Accuracy: 0.8742, Precision: 0.8421, Recall: 0.8004, F1: 0.8140
Testing Loss: 0.7459, Accuracy: 0.8575, Precision: 0.8242, Recall: 0.7736, F1: 0.7857
LM Predictions:  [0, 1, 5, 0, 2, 0, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 3, 0, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3278, Accuracy: 0.3571, Precision: 0.6250, Recall: 0.3329, F1: 0.3244
Epoch 59/70
Train Loss: 0.1391, Accuracy: 0.9587, Precision: 0.9341, Recall: 0.9399, F1: 0.9367
Validation Loss: 0.7222, Accuracy: 0.8635, Precision: 0.8316, Recall: 0.8119, F1: 0.8199
Testing Loss: 0.6640, Accuracy: 0.8587, Precision: 0.8204, Recall: 0.8029, F1: 0.8109
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 4, 0, 0, 0, 2, 4, 3, 0, 2, 0, 0, 3, 0, 0, 3, 5, 5, 5, 0, 4, 5, 0, 4, 3, 3, 5, 0, 4, 0, 4, 5, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9184, Accuracy: 0.4762, Precision: 0.7111, Recall: 0.4162, F1: 0.4543
Epoch 60/70
Train Loss: 0.1475, Accuracy: 0.9528, Precision: 0.9247, Recall: 0.9332, F1: 0.9283
Validation Loss: 0.7143, Accuracy: 0.8678, Precision: 0.8390, Recall: 0.8109, F1: 0.8226
Testing Loss: 0.7086, Accuracy: 0.8502, Precision: 0.8245, Recall: 0.7772, F1: 0.7935
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 5, 0, 4, 0, 0, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9496, Accuracy: 0.3810, Precision: 0.6984, Recall: 0.3495, F1: 0.3695
Epoch 61/70
Train Loss: 0.1433, Accuracy: 0.9542, Precision: 0.9300, Recall: 0.9211, F1: 0.9253
Validation Loss: 0.7456, Accuracy: 0.8721, Precision: 0.8366, Recall: 0.8192, F1: 0.8263
Testing Loss: 0.7584, Accuracy: 0.8539, Precision: 0.8239, Recall: 0.8027, F1: 0.8119
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 0, 5, 0, 0, 2, 0, 3, 0, 2, 5, 0, 3, 0, 0, 3, 0, 5, 5, 0, 0, 5, 0, 4, 3, 3, 5, 0, 4, 3, 0, 5, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0724, Accuracy: 0.3571, Precision: 0.6683, Recall: 0.3162, F1: 0.3567
Epoch 62/70
Train Loss: 0.1677, Accuracy: 0.9478, Precision: 0.9168, Recall: 0.9249, F1: 0.9202
Validation Loss: 0.7633, Accuracy: 0.8614, Precision: 0.8171, Recall: 0.8161, F1: 0.8162
Testing Loss: 0.7163, Accuracy: 0.8599, Precision: 0.8172, Recall: 0.8067, F1: 0.8110
LM Predictions:  [0, 4, 5, 0, 4, 5, 5, 5, 4, 5, 5, 0, 2, 0, 3, 0, 2, 0, 0, 3, 0, 5, 3, 0, 5, 5, 4, 4, 5, 0, 4, 3, 3, 5, 0, 4, 3, 4, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3086, Accuracy: 0.4048, Precision: 0.4621, Recall: 0.3458, F1: 0.3628
Epoch 63/70
Train Loss: 0.1515, Accuracy: 0.9578, Precision: 0.9260, Recall: 0.9479, F1: 0.9354
Validation Loss: 0.7813, Accuracy: 0.8507, Precision: 0.7971, Recall: 0.8021, F1: 0.7991
Testing Loss: 0.7482, Accuracy: 0.8623, Precision: 0.8258, Recall: 0.8173, F1: 0.8207
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 0, 4, 5, 5, 0, 2, 4, 3, 0, 2, 5, 0, 3, 0, 0, 3, 5, 5, 5, 0, 0, 5, 0, 4, 3, 3, 5, 0, 0, 0, 0, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3166, Accuracy: 0.3571, Precision: 0.7000, Recall: 0.3144, F1: 0.3623
Epoch 64/70
Train Loss: 0.1414, Accuracy: 0.9564, Precision: 0.9260, Recall: 0.9382, F1: 0.9313
Validation Loss: 0.8777, Accuracy: 0.8529, Precision: 0.8284, Recall: 0.7720, F1: 0.7911
Testing Loss: 0.7895, Accuracy: 0.8575, Precision: 0.8349, Recall: 0.7734, F1: 0.7917
LM Predictions:  [0, 1, 5, 0, 1, 0, 5, 0, 4, 1, 0, 0, 2, 4, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0, 0, 5, 4, 4, 5, 0, 4, 3, 3, 0, 0, 4, 3, 4, 0, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1762, Accuracy: 0.5000, Precision: 0.6035, Recall: 0.4181, F1: 0.4514
Epoch 65/70
Train Loss: 0.1706, Accuracy: 0.9493, Precision: 0.9270, Recall: 0.9126, F1: 0.9193
Validation Loss: 0.7368, Accuracy: 0.8614, Precision: 0.8108, Recall: 0.8041, F1: 0.8071
Testing Loss: 0.7437, Accuracy: 0.8575, Precision: 0.8235, Recall: 0.8071, F1: 0.8141
LM Predictions:  [0, 1, 5, 5, 3, 5, 5, 0, 0, 3, 0, 0, 2, 0, 0, 5, 2, 0, 0, 3, 5, 0, 3, 5, 0, 5, 0, 0, 5, 0, 4, 3, 3, 5, 0, 0, 0, 0, 5, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0785, Accuracy: 0.2857, Precision: 0.6230, Recall: 0.2644, F1: 0.2765
Epoch 66/70
Train Loss: 0.1541, Accuracy: 0.9533, Precision: 0.9248, Recall: 0.9341, F1: 0.9288
Validation Loss: 0.7679, Accuracy: 0.8657, Precision: 0.8363, Recall: 0.8011, F1: 0.8146
Testing Loss: 0.7590, Accuracy: 0.8575, Precision: 0.8273, Recall: 0.7744, F1: 0.7915
LM Predictions:  [0, 1, 5, 0, 3, 0, 5, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 3, 0, 0, 3, 0, 0, 5, 0, 0, 5, 0, 4, 3, 3, 0, 0, 0, 3, 0, 0, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2881, Accuracy: 0.3333, Precision: 0.6319, Recall: 0.3144, F1: 0.2931
Epoch 67/70
Train Loss: 0.1999, Accuracy: 0.9412, Precision: 0.9128, Recall: 0.9004, F1: 0.9057
Validation Loss: 0.7911, Accuracy: 0.8465, Precision: 0.8067, Recall: 0.7308, F1: 0.7289
Testing Loss: 0.7376, Accuracy: 0.8647, Precision: 0.8590, Recall: 0.7656, F1: 0.7723
LM Predictions:  [0, 1, 5, 0, 0, 0, 5, 0, 4, 2, 0, 0, 2, 4, 3, 0, 2, 0, 0, 3, 0, 4, 3, 0, 0, 5, 4, 4, 5, 0, 4, 3, 3, 0, 0, 4, 3, 4, 0, 5, 0, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7133, Accuracy: 0.4762, Precision: 0.6250, Recall: 0.3977, F1: 0.4125
Epoch 68/70
Train Loss: 0.1745, Accuracy: 0.9476, Precision: 0.9125, Recall: 0.9246, F1: 0.9171
Validation Loss: 0.9246, Accuracy: 0.8443, Precision: 0.7989, Recall: 0.7891, F1: 0.7873
Testing Loss: 0.8228, Accuracy: 0.8345, Precision: 0.7893, Recall: 0.7484, F1: 0.7562
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 0, 4, 5, 0, 0, 2, 0, 5, 5, 2, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 0, 5, 0, 4, 4, 3, 5, 0, 0, 4, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4093, Accuracy: 0.2381, Precision: 0.6096, Recall: 0.2310, F1: 0.2438
Epoch 69/70
Train Loss: 0.1893, Accuracy: 0.9426, Precision: 0.9094, Recall: 0.9130, F1: 0.9103
Validation Loss: 0.6240, Accuracy: 0.8657, Precision: 0.8168, Recall: 0.8305, F1: 0.8217
Testing Loss: 0.6231, Accuracy: 0.8587, Precision: 0.8178, Recall: 0.8251, F1: 0.8207
LM Predictions:  [0, 1, 5, 5, 5, 5, 5, 5, 4, 5, 0, 0, 2, 4, 5, 5, 2, 3, 5, 3, 0, 4, 3, 5, 5, 5, 4, 4, 5, 0, 4, 3, 3, 5, 5, 4, 3, 4, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9376, Accuracy: 0.4524, Precision: 0.7111, Recall: 0.3810, F1: 0.4557
Epoch 70/70
Train Loss: 0.2024, Accuracy: 0.9386, Precision: 0.9092, Recall: 0.9042, F1: 0.9066
Validation Loss: 0.6790, Accuracy: 0.8422, Precision: 0.8040, Recall: 0.7902, F1: 0.7942
Testing Loss: 0.5958, Accuracy: 0.8454, Precision: 0.8029, Recall: 0.7899, F1: 0.7957
LM Predictions:  [0, 3, 3, 5, 3, 5, 3, 5, 0, 3, 5, 0, 2, 5, 5, 5, 2, 3, 5, 3, 5, 5, 2, 5, 0, 3, 4, 0, 3, 0, 4, 3, 3, 5, 5, 4, 3, 0, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8737, Accuracy: 0.2857, Precision: 0.3926, Recall: 0.2625, F1: 0.2807
Label Memorization Analysis: 
LM Predictions:  [0, 3, 3, 5, 3, 5, 3, 5, 0, 3, 5, 0, 2, 5, 5, 5, 2, 3, 5, 3, 5, 5, 2, 5, 0, 3, 4, 0, 3, 0, 4, 3, 3, 5, 5, 4, 3, 0, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8737, Accuracy: 0.2857, Precision: 0.3926, Recall: 0.2625, F1: 0.2807

