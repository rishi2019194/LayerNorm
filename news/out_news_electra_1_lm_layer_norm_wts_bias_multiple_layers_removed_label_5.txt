Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
For early layers:  [0, 1, 2, 3]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.6550, Accuracy: 0.2879, Precision: 0.1392, Recall: 0.1849, F1: 0.1444
Validation Loss: 1.5974, Accuracy: 0.3795, Precision: 0.1366, Recall: 0.2456, F1: 0.1715
Testing Loss: 1.5920, Accuracy: 0.3853, Precision: 0.1434, Recall: 0.2483, F1: 0.1738
LM Predictions:  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8981, Accuracy: 0.0952, Precision: 0.0216, Recall: 0.1600, F1: 0.0381
Epoch 2/70
Train Loss: 1.4259, Accuracy: 0.4553, Precision: 0.2277, Recall: 0.2985, F1: 0.2452
Validation Loss: 1.2285, Accuracy: 0.5586, Precision: 0.2784, Recall: 0.3765, F1: 0.3195
Testing Loss: 1.1613, Accuracy: 0.5845, Precision: 0.2940, Recall: 0.3922, F1: 0.3326
LM Predictions:  [0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3378, Accuracy: 0.1905, Precision: 0.1067, Recall: 0.2600, F1: 0.1346
Epoch 3/70
Train Loss: 1.0523, Accuracy: 0.6407, Precision: 0.4306, Recall: 0.4847, F1: 0.4519
Validation Loss: 0.8895, Accuracy: 0.7207, Precision: 0.5158, Recall: 0.5513, F1: 0.5221
Testing Loss: 0.8245, Accuracy: 0.7258, Precision: 0.5142, Recall: 0.5547, F1: 0.5235
LM Predictions:  [0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6340, Accuracy: 0.1667, Precision: 0.0961, Recall: 0.2500, F1: 0.1084
Epoch 4/70
Train Loss: 0.8419, Accuracy: 0.7228, Precision: 0.5426, Recall: 0.5740, F1: 0.5305
Validation Loss: 0.7542, Accuracy: 0.7463, Precision: 0.5871, Recall: 0.6114, F1: 0.5939
Testing Loss: 0.7017, Accuracy: 0.7705, Precision: 0.6165, Recall: 0.6470, F1: 0.6202
LM Predictions:  [0, 3, 0, 0, 4, 0, 3, 0, 0, 3, 3, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7827, Accuracy: 0.1429, Precision: 0.1207, Recall: 0.1800, F1: 0.1083
Epoch 5/70
Train Loss: 0.6908, Accuracy: 0.7892, Precision: 0.6399, Recall: 0.6715, F1: 0.6531
Validation Loss: 0.6910, Accuracy: 0.8081, Precision: 0.7823, Recall: 0.7176, F1: 0.7335
Testing Loss: 0.5700, Accuracy: 0.8309, Precision: 0.7649, Recall: 0.7251, F1: 0.7193
LM Predictions:  [0, 5, 0, 0, 5, 0, 0, 0, 0, 3, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 3, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2456, Accuracy: 0.1190, Precision: 0.0625, Recall: 0.1542, F1: 0.0638
Epoch 6/70
Train Loss: 0.6009, Accuracy: 0.8141, Precision: 0.7314, Recall: 0.7025, F1: 0.6983
Validation Loss: 0.6635, Accuracy: 0.8081, Precision: 0.7644, Recall: 0.7043, F1: 0.7102
Testing Loss: 0.5322, Accuracy: 0.8297, Precision: 0.7636, Recall: 0.7282, F1: 0.7233
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1091, Accuracy: 0.1429, Precision: 0.1886, Recall: 0.1833, F1: 0.0691
Epoch 7/70
Train Loss: 0.5299, Accuracy: 0.8449, Precision: 0.7855, Recall: 0.7566, F1: 0.7656
Validation Loss: 0.6140, Accuracy: 0.8316, Precision: 0.7904, Recall: 0.7408, F1: 0.7547
Testing Loss: 0.4729, Accuracy: 0.8599, Precision: 0.8111, Recall: 0.7680, F1: 0.7737
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0335, Accuracy: 0.0952, Precision: 0.0208, Recall: 0.1333, F1: 0.0360
Epoch 8/70
Train Loss: 0.4700, Accuracy: 0.8603, Precision: 0.8040, Recall: 0.7873, F1: 0.7949
Validation Loss: 0.6715, Accuracy: 0.8209, Precision: 0.8077, Recall: 0.7395, F1: 0.7652
Testing Loss: 0.4978, Accuracy: 0.8575, Precision: 0.8477, Recall: 0.7743, F1: 0.7959
LM Predictions:  [0, 5, 0, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 5, 0, 0, 5, 5, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2224, Accuracy: 0.0714, Precision: 0.0192, Recall: 0.1000, F1: 0.0323
Epoch 9/70
Train Loss: 0.4353, Accuracy: 0.8705, Precision: 0.8178, Recall: 0.8090, F1: 0.8131
Validation Loss: 0.5658, Accuracy: 0.8358, Precision: 0.7751, Recall: 0.7476, F1: 0.7511
Testing Loss: 0.4797, Accuracy: 0.8551, Precision: 0.7811, Recall: 0.7656, F1: 0.7607
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 3, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.4895, Accuracy: 0.1190, Precision: 0.1048, Recall: 0.1500, F1: 0.0648
Epoch 10/70
Train Loss: 0.4130, Accuracy: 0.8831, Precision: 0.8369, Recall: 0.8337, F1: 0.8351
Validation Loss: 0.6053, Accuracy: 0.8443, Precision: 0.8341, Recall: 0.7711, F1: 0.7953
Testing Loss: 0.4845, Accuracy: 0.8720, Precision: 0.8543, Recall: 0.8030, F1: 0.8212
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 3, 0, 5, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2057, Accuracy: 0.1190, Precision: 0.1080, Recall: 0.1500, F1: 0.0694
Epoch 11/70
Train Loss: 0.3989, Accuracy: 0.8812, Precision: 0.8295, Recall: 0.8276, F1: 0.8283
Validation Loss: 0.5725, Accuracy: 0.8550, Precision: 0.8680, Recall: 0.7653, F1: 0.7944
Testing Loss: 0.4988, Accuracy: 0.8696, Precision: 0.8522, Recall: 0.7803, F1: 0.7969
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1022, Accuracy: 0.1667, Precision: 0.2747, Recall: 0.1917, F1: 0.1275
Epoch 12/70
Train Loss: 0.3814, Accuracy: 0.8897, Precision: 0.8446, Recall: 0.8454, F1: 0.8447
Validation Loss: 0.5410, Accuracy: 0.8507, Precision: 0.8168, Recall: 0.8115, F1: 0.8101
Testing Loss: 0.4247, Accuracy: 0.8780, Precision: 0.8377, Recall: 0.8337, F1: 0.8346
LM Predictions:  [0, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 0, 3, 0, 3, 0, 5, 2, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9243, Accuracy: 0.1429, Precision: 0.3090, Recall: 0.1542, F1: 0.1339
Epoch 13/70
Train Loss: 0.3670, Accuracy: 0.8904, Precision: 0.8480, Recall: 0.8547, F1: 0.8509
Validation Loss: 0.5668, Accuracy: 0.8593, Precision: 0.8221, Recall: 0.8117, F1: 0.8152
Testing Loss: 0.4765, Accuracy: 0.8732, Precision: 0.8328, Recall: 0.8195, F1: 0.8216
LM Predictions:  [0, 5, 5, 5, 4, 0, 5, 4, 5, 4, 5, 0, 2, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 0, 5, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 5, 0, 5, 4, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0435, Accuracy: 0.1429, Precision: 0.2667, Recall: 0.1542, F1: 0.1315
Epoch 14/70
Train Loss: 0.3290, Accuracy: 0.9044, Precision: 0.8613, Recall: 0.8687, F1: 0.8643
Validation Loss: 0.5809, Accuracy: 0.8443, Precision: 0.8558, Recall: 0.7741, F1: 0.8033
Testing Loss: 0.4925, Accuracy: 0.8647, Precision: 0.8489, Recall: 0.7870, F1: 0.8063
LM Predictions:  [0, 0, 0, 0, 5, 0, 5, 5, 0, 5, 5, 0, 3, 0, 0, 0, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9806, Accuracy: 0.1190, Precision: 0.3512, Recall: 0.1375, F1: 0.0976
Epoch 15/70
Train Loss: 0.3212, Accuracy: 0.9099, Precision: 0.8631, Recall: 0.8775, F1: 0.8689
Validation Loss: 0.5108, Accuracy: 0.8571, Precision: 0.8280, Recall: 0.7961, F1: 0.8092
Testing Loss: 0.4487, Accuracy: 0.8865, Precision: 0.8669, Recall: 0.8266, F1: 0.8421
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 0, 2, 0, 5, 0, 5, 5, 0, 5, 5, 5, 2, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8469, Accuracy: 0.1429, Precision: 0.3090, Recall: 0.1583, F1: 0.1385
Epoch 16/70
Train Loss: 0.3039, Accuracy: 0.9099, Precision: 0.8676, Recall: 0.8750, F1: 0.8704
Validation Loss: 0.6230, Accuracy: 0.8678, Precision: 0.8232, Recall: 0.8179, F1: 0.8203
Testing Loss: 0.5036, Accuracy: 0.8780, Precision: 0.8483, Recall: 0.8304, F1: 0.8377
LM Predictions:  [0, 3, 0, 5, 4, 0, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 2, 5, 0, 5, 0, 5, 2, 0, 0, 0, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9470, Accuracy: 0.1905, Precision: 0.2951, Recall: 0.1958, F1: 0.1844
Epoch 17/70
Train Loss: 0.2960, Accuracy: 0.9161, Precision: 0.8759, Recall: 0.8895, F1: 0.8808
Validation Loss: 0.5336, Accuracy: 0.8635, Precision: 0.8293, Recall: 0.8356, F1: 0.8311
Testing Loss: 0.4421, Accuracy: 0.8853, Precision: 0.8519, Recall: 0.8524, F1: 0.8514
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7870, Accuracy: 0.1429, Precision: 0.3718, Recall: 0.1583, F1: 0.1525
Epoch 18/70
Train Loss: 0.2889, Accuracy: 0.9172, Precision: 0.8761, Recall: 0.8899, F1: 0.8815
Validation Loss: 0.6433, Accuracy: 0.8443, Precision: 0.8142, Recall: 0.8246, F1: 0.8153
Testing Loss: 0.5073, Accuracy: 0.8720, Precision: 0.8371, Recall: 0.8490, F1: 0.8413
LM Predictions:  [0, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1394, Accuracy: 0.1667, Precision: 0.3889, Recall: 0.1750, F1: 0.1937
Epoch 19/70
Train Loss: 0.2644, Accuracy: 0.9244, Precision: 0.8869, Recall: 0.8983, F1: 0.8912
Validation Loss: 0.5649, Accuracy: 0.8571, Precision: 0.8248, Recall: 0.8158, F1: 0.8150
Testing Loss: 0.4951, Accuracy: 0.8732, Precision: 0.8407, Recall: 0.8224, F1: 0.8304
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1608, Accuracy: 0.1429, Precision: 0.5385, Recall: 0.1542, F1: 0.1532
Epoch 20/70
Train Loss: 0.2603, Accuracy: 0.9239, Precision: 0.8826, Recall: 0.8936, F1: 0.8870
Validation Loss: 0.5810, Accuracy: 0.8614, Precision: 0.8207, Recall: 0.8095, F1: 0.8146
Testing Loss: 0.5637, Accuracy: 0.8684, Precision: 0.8383, Recall: 0.8005, F1: 0.8143
LM Predictions:  [0, 5, 1, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0787, Accuracy: 0.1667, Precision: 0.5294, Recall: 0.1750, F1: 0.1727
Epoch 21/70
Train Loss: 0.3372, Accuracy: 0.9023, Precision: 0.8644, Recall: 0.8770, F1: 0.8691
Validation Loss: 0.6230, Accuracy: 0.8571, Precision: 0.8215, Recall: 0.8361, F1: 0.8257
Testing Loss: 0.4913, Accuracy: 0.8720, Precision: 0.8335, Recall: 0.8378, F1: 0.8352
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7344, Accuracy: 0.1667, Precision: 0.4551, Recall: 0.1750, F1: 0.1803
Epoch 22/70
Train Loss: 0.2635, Accuracy: 0.9286, Precision: 0.8926, Recall: 0.9067, F1: 0.8984
Validation Loss: 0.6414, Accuracy: 0.8486, Precision: 0.8234, Recall: 0.7780, F1: 0.7953
Testing Loss: 0.5541, Accuracy: 0.8708, Precision: 0.8538, Recall: 0.7926, F1: 0.8064
LM Predictions:  [0, 3, 0, 0, 2, 0, 5, 5, 0, 5, 5, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9367, Accuracy: 0.1667, Precision: 0.3829, Recall: 0.1750, F1: 0.1544
Epoch 23/70
Train Loss: 0.2343, Accuracy: 0.9329, Precision: 0.8938, Recall: 0.9072, F1: 0.8992
Validation Loss: 0.6619, Accuracy: 0.8614, Precision: 0.8231, Recall: 0.8126, F1: 0.8167
Testing Loss: 0.5316, Accuracy: 0.8659, Precision: 0.8298, Recall: 0.8263, F1: 0.8266
LM Predictions:  [0, 5, 5, 0, 2, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0371, Accuracy: 0.1667, Precision: 0.3996, Recall: 0.1750, F1: 0.1742
Epoch 24/70
Train Loss: 0.2228, Accuracy: 0.9362, Precision: 0.8984, Recall: 0.9078, F1: 0.9023
Validation Loss: 0.6774, Accuracy: 0.8422, Precision: 0.8095, Recall: 0.8060, F1: 0.8057
Testing Loss: 0.5448, Accuracy: 0.8696, Precision: 0.8345, Recall: 0.8207, F1: 0.8267
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9499, Accuracy: 0.1667, Precision: 0.3690, Recall: 0.1750, F1: 0.1749
Epoch 25/70
Train Loss: 0.2202, Accuracy: 0.9369, Precision: 0.8996, Recall: 0.9173, F1: 0.9063
Validation Loss: 0.7661, Accuracy: 0.8443, Precision: 0.8007, Recall: 0.7889, F1: 0.7910
Testing Loss: 0.5861, Accuracy: 0.8684, Precision: 0.8288, Recall: 0.8124, F1: 0.8190
LM Predictions:  [0, 3, 5, 0, 3, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0445, Accuracy: 0.1667, Precision: 0.3690, Recall: 0.1750, F1: 0.1692
Epoch 26/70
Train Loss: 0.2132, Accuracy: 0.9405, Precision: 0.9035, Recall: 0.9182, F1: 0.9092
Validation Loss: 0.6924, Accuracy: 0.8507, Precision: 0.8140, Recall: 0.8026, F1: 0.8069
Testing Loss: 0.5318, Accuracy: 0.8720, Precision: 0.8310, Recall: 0.8154, F1: 0.8213
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 4, 4, 0, 5, 5, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5259, Accuracy: 0.1667, Precision: 0.3646, Recall: 0.1750, F1: 0.1698
Epoch 27/70
Train Loss: 0.2100, Accuracy: 0.9348, Precision: 0.8961, Recall: 0.9078, F1: 0.9004
Validation Loss: 0.6473, Accuracy: 0.8465, Precision: 0.8095, Recall: 0.8006, F1: 0.8029
Testing Loss: 0.5543, Accuracy: 0.8696, Precision: 0.8384, Recall: 0.8392, F1: 0.8375
LM Predictions:  [0, 3, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 5, 5, 5, 5, 2, 0, 0, 5, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7709, Accuracy: 0.1667, Precision: 0.4236, Recall: 0.1750, F1: 0.1956
Epoch 28/70
Train Loss: 0.2216, Accuracy: 0.9355, Precision: 0.8950, Recall: 0.9168, F1: 0.9029
Validation Loss: 0.6619, Accuracy: 0.8571, Precision: 0.8126, Recall: 0.8064, F1: 0.8084
Testing Loss: 0.5978, Accuracy: 0.8744, Precision: 0.8496, Recall: 0.8135, F1: 0.8267
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 3, 0, 0, 0, 0, 5, 2, 0, 4, 5, 0, 5, 0, 5, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3385, Accuracy: 0.2143, Precision: 0.4461, Recall: 0.2125, F1: 0.2143
Epoch 29/70
Train Loss: 0.2067, Accuracy: 0.9419, Precision: 0.9055, Recall: 0.9217, F1: 0.9119
Validation Loss: 0.7394, Accuracy: 0.8465, Precision: 0.7975, Recall: 0.7942, F1: 0.7942
Testing Loss: 0.6158, Accuracy: 0.8696, Precision: 0.8390, Recall: 0.8113, F1: 0.8221
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6747, Accuracy: 0.1667, Precision: 0.3924, Recall: 0.1750, F1: 0.1663
Epoch 30/70
Train Loss: 0.2232, Accuracy: 0.9343, Precision: 0.8952, Recall: 0.9053, F1: 0.8995
Validation Loss: 0.6783, Accuracy: 0.8507, Precision: 0.8202, Recall: 0.7982, F1: 0.8072
Testing Loss: 0.5969, Accuracy: 0.8647, Precision: 0.8328, Recall: 0.8083, F1: 0.8186
LM Predictions:  [0, 1, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7328, Accuracy: 0.1905, Precision: 0.7121, Recall: 0.1935, F1: 0.2231
Epoch 31/70
Train Loss: 0.2011, Accuracy: 0.9417, Precision: 0.9069, Recall: 0.9132, F1: 0.9088
Validation Loss: 0.6606, Accuracy: 0.8486, Precision: 0.8138, Recall: 0.8222, F1: 0.8128
Testing Loss: 0.5602, Accuracy: 0.8659, Precision: 0.8357, Recall: 0.8450, F1: 0.8355
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7714, Accuracy: 0.1667, Precision: 0.5714, Recall: 0.1750, F1: 0.2106
Epoch 32/70
Train Loss: 0.1985, Accuracy: 0.9410, Precision: 0.9054, Recall: 0.9194, F1: 0.9102
Validation Loss: 0.6311, Accuracy: 0.8614, Precision: 0.8240, Recall: 0.8271, F1: 0.8239
Testing Loss: 0.5644, Accuracy: 0.8659, Precision: 0.8386, Recall: 0.8265, F1: 0.8307
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5313, Accuracy: 0.1667, Precision: 0.5069, Recall: 0.1750, F1: 0.1981
Epoch 33/70
Train Loss: 0.1949, Accuracy: 0.9410, Precision: 0.9056, Recall: 0.9142, F1: 0.9084
Validation Loss: 0.6476, Accuracy: 0.8507, Precision: 0.7967, Recall: 0.7927, F1: 0.7936
Testing Loss: 0.6712, Accuracy: 0.8539, Precision: 0.8146, Recall: 0.8105, F1: 0.8104
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 0, 5, 0, 5, 0, 2, 4, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 4, 4, 0, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3723, Accuracy: 0.2619, Precision: 0.6726, Recall: 0.2602, F1: 0.2719
Epoch 34/70
Train Loss: 0.2115, Accuracy: 0.9348, Precision: 0.8989, Recall: 0.9141, F1: 0.9034
Validation Loss: 0.7043, Accuracy: 0.8507, Precision: 0.8199, Recall: 0.7947, F1: 0.8024
Testing Loss: 0.6471, Accuracy: 0.8599, Precision: 0.8236, Recall: 0.7958, F1: 0.8074
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 0, 0, 0, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8820, Accuracy: 0.1905, Precision: 0.6961, Recall: 0.1935, F1: 0.2061
Epoch 35/70
Train Loss: 0.1965, Accuracy: 0.9407, Precision: 0.9040, Recall: 0.9197, F1: 0.9096
Validation Loss: 0.7900, Accuracy: 0.8401, Precision: 0.8118, Recall: 0.7829, F1: 0.7930
Testing Loss: 0.6675, Accuracy: 0.8599, Precision: 0.8217, Recall: 0.7990, F1: 0.8084
LM Predictions:  [0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 0, 5, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8273, Accuracy: 0.1667, Precision: 0.5333, Recall: 0.1750, F1: 0.1773
Epoch 36/70
Train Loss: 0.1936, Accuracy: 0.9431, Precision: 0.9061, Recall: 0.9111, F1: 0.9082
Validation Loss: 0.6732, Accuracy: 0.8635, Precision: 0.8376, Recall: 0.7970, F1: 0.8101
Testing Loss: 0.6230, Accuracy: 0.8575, Precision: 0.8152, Recall: 0.7775, F1: 0.7858
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5333, Accuracy: 0.2381, Precision: 0.6987, Recall: 0.2602, F1: 0.2144
Epoch 37/70
Train Loss: 0.1853, Accuracy: 0.9433, Precision: 0.9130, Recall: 0.9131, F1: 0.9126
Validation Loss: 0.7982, Accuracy: 0.8465, Precision: 0.8077, Recall: 0.7952, F1: 0.7979
Testing Loss: 0.7195, Accuracy: 0.8611, Precision: 0.8289, Recall: 0.8250, F1: 0.8256
LM Predictions:  [0, 0, 5, 0, 5, 0, 5, 5, 5, 4, 5, 0, 2, 4, 3, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4061, Accuracy: 0.1905, Precision: 0.4861, Recall: 0.1917, F1: 0.2071
Epoch 38/70
Train Loss: 0.1893, Accuracy: 0.9450, Precision: 0.9113, Recall: 0.9287, F1: 0.9185
Validation Loss: 0.7143, Accuracy: 0.8529, Precision: 0.8301, Recall: 0.7871, F1: 0.8026
Testing Loss: 0.5940, Accuracy: 0.8647, Precision: 0.8304, Recall: 0.7853, F1: 0.7968
LM Predictions:  [0, 5, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2678, Accuracy: 0.1905, Precision: 0.4701, Recall: 0.2083, F1: 0.1642
Epoch 39/70
Train Loss: 0.1923, Accuracy: 0.9424, Precision: 0.9081, Recall: 0.9154, F1: 0.9109
Validation Loss: 0.8133, Accuracy: 0.8529, Precision: 0.8288, Recall: 0.7826, F1: 0.8001
Testing Loss: 0.6424, Accuracy: 0.8684, Precision: 0.8346, Recall: 0.7896, F1: 0.8027
LM Predictions:  [0, 3, 5, 0, 5, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4630, Accuracy: 0.1905, Precision: 0.4444, Recall: 0.2083, F1: 0.1707
Epoch 40/70
Train Loss: 0.1802, Accuracy: 0.9440, Precision: 0.9078, Recall: 0.9123, F1: 0.9091
Validation Loss: 0.7588, Accuracy: 0.8465, Precision: 0.8090, Recall: 0.7995, F1: 0.8005
Testing Loss: 0.6132, Accuracy: 0.8623, Precision: 0.8216, Recall: 0.8110, F1: 0.8150
LM Predictions:  [0, 1, 5, 0, 2, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 0, 5, 5, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1223, Accuracy: 0.1905, Precision: 0.6333, Recall: 0.1935, F1: 0.2162
Epoch 41/70
Train Loss: 0.1690, Accuracy: 0.9474, Precision: 0.9128, Recall: 0.9183, F1: 0.9149
Validation Loss: 0.7668, Accuracy: 0.8486, Precision: 0.8115, Recall: 0.7883, F1: 0.7977
Testing Loss: 0.7501, Accuracy: 0.8551, Precision: 0.8215, Recall: 0.7979, F1: 0.8075
LM Predictions:  [0, 1, 5, 0, 2, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 2, 0, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 0, 5, 0, 5, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.8492, Accuracy: 0.1905, Precision: 0.6010, Recall: 0.1935, F1: 0.2041
Epoch 42/70
Train Loss: 0.1556, Accuracy: 0.9495, Precision: 0.9166, Recall: 0.9265, F1: 0.9202
Validation Loss: 0.7327, Accuracy: 0.8635, Precision: 0.8364, Recall: 0.7998, F1: 0.8145
Testing Loss: 0.6812, Accuracy: 0.8575, Precision: 0.8320, Recall: 0.7707, F1: 0.7904
LM Predictions:  [0, 1, 5, 0, 2, 0, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6221, Accuracy: 0.2143, Precision: 0.6368, Recall: 0.2269, F1: 0.1976
Epoch 43/70
Train Loss: 0.1648, Accuracy: 0.9493, Precision: 0.9198, Recall: 0.9172, F1: 0.9183
Validation Loss: 0.6931, Accuracy: 0.8486, Precision: 0.8151, Recall: 0.7948, F1: 0.7987
Testing Loss: 0.6489, Accuracy: 0.8587, Precision: 0.8250, Recall: 0.8100, F1: 0.8130
LM Predictions:  [0, 1, 5, 5, 3, 0, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4558, Accuracy: 0.1905, Precision: 0.6667, Recall: 0.1935, F1: 0.2490
Epoch 44/70
Train Loss: 0.1599, Accuracy: 0.9490, Precision: 0.9132, Recall: 0.9324, F1: 0.9198
Validation Loss: 0.8282, Accuracy: 0.8678, Precision: 0.8490, Recall: 0.8105, F1: 0.8260
Testing Loss: 0.7172, Accuracy: 0.8599, Precision: 0.8234, Recall: 0.7807, F1: 0.7949
LM Predictions:  [0, 1, 5, 0, 3, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 5, 3, 5, 0, 3, 0, 5, 3, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 5, 0, 5, 0, 5, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1347, Accuracy: 0.2619, Precision: 0.6151, Recall: 0.2602, F1: 0.2441
Epoch 45/70
Train Loss: 0.1539, Accuracy: 0.9495, Precision: 0.9189, Recall: 0.9202, F1: 0.9193
Validation Loss: 0.8133, Accuracy: 0.8230, Precision: 0.7931, Recall: 0.7613, F1: 0.7740
Testing Loss: 0.6748, Accuracy: 0.8539, Precision: 0.8223, Recall: 0.7886, F1: 0.8020
LM Predictions:  [0, 3, 5, 0, 3, 0, 0, 5, 5, 5, 0, 0, 2, 0, 3, 5, 3, 5, 0, 3, 0, 5, 3, 0, 0, 0, 5, 5, 3, 0, 4, 0, 0, 5, 0, 4, 0, 5, 0, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0479, Accuracy: 0.2619, Precision: 0.4329, Recall: 0.2583, F1: 0.2357
Epoch 46/70
Train Loss: 0.1522, Accuracy: 0.9516, Precision: 0.9215, Recall: 0.9334, F1: 0.9265
Validation Loss: 0.7688, Accuracy: 0.8529, Precision: 0.8055, Recall: 0.8106, F1: 0.8068
Testing Loss: 0.6278, Accuracy: 0.8635, Precision: 0.8232, Recall: 0.8323, F1: 0.8264
LM Predictions:  [0, 1, 5, 5, 3, 0, 5, 5, 5, 5, 0, 0, 2, 4, 3, 5, 1, 5, 5, 2, 5, 5, 3, 5, 5, 5, 5, 5, 1, 0, 4, 5, 5, 5, 5, 4, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7377, Accuracy: 0.2619, Precision: 0.5159, Recall: 0.2435, F1: 0.2999
Epoch 47/70
Train Loss: 0.1602, Accuracy: 0.9497, Precision: 0.9136, Recall: 0.9304, F1: 0.9206
Validation Loss: 0.7539, Accuracy: 0.8422, Precision: 0.7958, Recall: 0.7977, F1: 0.7936
Testing Loss: 0.6250, Accuracy: 0.8647, Precision: 0.8283, Recall: 0.8218, F1: 0.8239
LM Predictions:  [0, 5, 5, 5, 3, 0, 5, 5, 5, 5, 0, 0, 2, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3692, Accuracy: 0.1667, Precision: 0.4667, Recall: 0.1750, F1: 0.1914
Epoch 48/70
Train Loss: 0.1616, Accuracy: 0.9490, Precision: 0.9199, Recall: 0.9226, F1: 0.9208
Validation Loss: 0.7500, Accuracy: 0.8529, Precision: 0.8218, Recall: 0.8006, F1: 0.8064
Testing Loss: 0.6374, Accuracy: 0.8623, Precision: 0.8298, Recall: 0.8041, F1: 0.8147
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4018, Accuracy: 0.1905, Precision: 0.7000, Recall: 0.1935, F1: 0.2106
Epoch 49/70
Train Loss: 0.1659, Accuracy: 0.9476, Precision: 0.9150, Recall: 0.9339, F1: 0.9211
Validation Loss: 0.7883, Accuracy: 0.8465, Precision: 0.8018, Recall: 0.7907, F1: 0.7935
Testing Loss: 0.6534, Accuracy: 0.8575, Precision: 0.8126, Recall: 0.8047, F1: 0.8082
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 0, 5, 1, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1711, Accuracy: 0.2381, Precision: 0.7083, Recall: 0.2454, F1: 0.2514
Epoch 50/70
Train Loss: 0.1544, Accuracy: 0.9502, Precision: 0.9216, Recall: 0.9215, F1: 0.9213
Validation Loss: 0.7241, Accuracy: 0.8465, Precision: 0.8017, Recall: 0.7981, F1: 0.7936
Testing Loss: 0.7742, Accuracy: 0.8442, Precision: 0.7925, Recall: 0.7620, F1: 0.7673
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 0, 4, 4, 0, 0, 2, 4, 3, 0, 5, 4, 0, 5, 0, 0, 2, 0, 0, 5, 0, 5, 5, 0, 4, 0, 4, 5, 0, 4, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2001, Accuracy: 0.3095, Precision: 0.6005, Recall: 0.2935, F1: 0.2803
Epoch 51/70
Train Loss: 0.1637, Accuracy: 0.9495, Precision: 0.9260, Recall: 0.9153, F1: 0.9204
Validation Loss: 0.8494, Accuracy: 0.8422, Precision: 0.7993, Recall: 0.7970, F1: 0.7959
Testing Loss: 0.7240, Accuracy: 0.8684, Precision: 0.8300, Recall: 0.8199, F1: 0.8244
LM Predictions:  [0, 1, 5, 5, 5, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 5, 5, 0, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 0, 5, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4259, Accuracy: 0.2143, Precision: 0.7083, Recall: 0.2269, F1: 0.2241
Epoch 52/70
Train Loss: 0.1491, Accuracy: 0.9481, Precision: 0.9155, Recall: 0.9237, F1: 0.9187
Validation Loss: 0.8324, Accuracy: 0.8358, Precision: 0.8137, Recall: 0.7701, F1: 0.7861
Testing Loss: 0.7119, Accuracy: 0.8647, Precision: 0.8262, Recall: 0.7954, F1: 0.8085
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 1, 5, 5, 5, 0, 0, 3, 0, 0, 0, 5, 5, 1, 0, 4, 0, 0, 5, 0, 0, 1, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0939, Accuracy: 0.2619, Precision: 0.5926, Recall: 0.2620, F1: 0.2549
Epoch 53/70
Train Loss: 0.1778, Accuracy: 0.9426, Precision: 0.9161, Recall: 0.9156, F1: 0.9155
Validation Loss: 0.8850, Accuracy: 0.8380, Precision: 0.8047, Recall: 0.7795, F1: 0.7874
Testing Loss: 0.7253, Accuracy: 0.8599, Precision: 0.8239, Recall: 0.8158, F1: 0.8191
LM Predictions:  [0, 1, 3, 5, 5, 0, 0, 5, 5, 5, 0, 0, 2, 0, 3, 0, 3, 0, 5, 5, 0, 5, 2, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 5, 5, 0, 5, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1452, Accuracy: 0.2381, Precision: 0.5556, Recall: 0.2477, F1: 0.2361
Epoch 54/70
Train Loss: 0.1550, Accuracy: 0.9504, Precision: 0.9227, Recall: 0.9288, F1: 0.9253
Validation Loss: 0.8653, Accuracy: 0.8465, Precision: 0.8125, Recall: 0.7969, F1: 0.8023
Testing Loss: 0.6974, Accuracy: 0.8599, Precision: 0.8174, Recall: 0.7972, F1: 0.8062
LM Predictions:  [0, 1, 2, 0, 2, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 2, 0, 5, 2, 0, 5, 2, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 5, 0, 0, 5, 5, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7697, Accuracy: 0.3095, Precision: 0.6351, Recall: 0.3102, F1: 0.2606
Epoch 55/70
Train Loss: 0.1451, Accuracy: 0.9542, Precision: 0.9246, Recall: 0.9399, F1: 0.9310
Validation Loss: 0.8584, Accuracy: 0.8486, Precision: 0.8139, Recall: 0.7941, F1: 0.8011
Testing Loss: 0.7041, Accuracy: 0.8635, Precision: 0.8229, Recall: 0.7863, F1: 0.7984
LM Predictions:  [0, 1, 3, 0, 3, 0, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 3, 0, 0, 3, 0, 5, 3, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 0, 0, 0, 5, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8090, Accuracy: 0.2857, Precision: 0.5303, Recall: 0.2810, F1: 0.2634
Epoch 56/70
Train Loss: 0.1361, Accuracy: 0.9516, Precision: 0.9259, Recall: 0.9244, F1: 0.9248
Validation Loss: 0.9086, Accuracy: 0.8443, Precision: 0.8098, Recall: 0.7875, F1: 0.7963
Testing Loss: 0.7841, Accuracy: 0.8599, Precision: 0.8280, Recall: 0.7951, F1: 0.8085
LM Predictions:  [0, 1, 2, 0, 2, 0, 0, 5, 4, 5, 0, 0, 2, 4, 3, 0, 3, 4, 0, 3, 0, 5, 3, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 5, 0, 4, 0, 5, 0, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3518, Accuracy: 0.4048, Precision: 0.5475, Recall: 0.3685, F1: 0.3760
Epoch 57/70
Train Loss: 0.1311, Accuracy: 0.9516, Precision: 0.9261, Recall: 0.9206, F1: 0.9232
Validation Loss: 0.8732, Accuracy: 0.8486, Precision: 0.8232, Recall: 0.7976, F1: 0.8062
Testing Loss: 0.7747, Accuracy: 0.8587, Precision: 0.8283, Recall: 0.8021, F1: 0.8137
LM Predictions:  [0, 1, 3, 0, 2, 0, 0, 5, 0, 5, 0, 0, 2, 0, 3, 0, 3, 0, 0, 3, 0, 5, 3, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 5, 0, 5, 5, 5, 5, 2, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1415, Accuracy: 0.3095, Precision: 0.5870, Recall: 0.3019, F1: 0.2867
Epoch 58/70
Train Loss: 0.1483, Accuracy: 0.9502, Precision: 0.9237, Recall: 0.9221, F1: 0.9228
Validation Loss: 0.7670, Accuracy: 0.8401, Precision: 0.8003, Recall: 0.8062, F1: 0.8002
Testing Loss: 0.6672, Accuracy: 0.8684, Precision: 0.8363, Recall: 0.8272, F1: 0.8308
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 4, 5, 0, 0, 2, 4, 3, 0, 2, 4, 0, 2, 5, 5, 3, 0, 0, 5, 5, 5, 2, 0, 4, 0, 0, 5, 0, 4, 0, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8589, Accuracy: 0.4286, Precision: 0.6845, Recall: 0.3935, F1: 0.3968
Epoch 59/70
Train Loss: 0.1332, Accuracy: 0.9576, Precision: 0.9338, Recall: 0.9377, F1: 0.9355
Validation Loss: 0.8722, Accuracy: 0.8358, Precision: 0.7950, Recall: 0.7894, F1: 0.7908
Testing Loss: 0.7605, Accuracy: 0.8599, Precision: 0.8158, Recall: 0.7924, F1: 0.8008
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 5, 4, 5, 0, 0, 2, 4, 3, 0, 2, 4, 0, 2, 5, 5, 3, 0, 0, 0, 5, 5, 2, 0, 4, 0, 0, 5, 0, 4, 1, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6144, Accuracy: 0.4524, Precision: 0.6845, Recall: 0.4120, F1: 0.4241
Epoch 60/70
Train Loss: 0.1443, Accuracy: 0.9542, Precision: 0.9264, Recall: 0.9355, F1: 0.9303
Validation Loss: 0.9408, Accuracy: 0.8316, Precision: 0.8164, Recall: 0.7486, F1: 0.7704
Testing Loss: 0.8587, Accuracy: 0.8478, Precision: 0.8284, Recall: 0.7688, F1: 0.7903
LM Predictions:  [0, 5, 2, 0, 2, 0, 0, 0, 4, 5, 0, 0, 2, 4, 3, 0, 2, 2, 0, 2, 0, 5, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8750, Accuracy: 0.3333, Precision: 0.4665, Recall: 0.3208, F1: 0.2843
Epoch 61/70
Train Loss: 0.1483, Accuracy: 0.9514, Precision: 0.9228, Recall: 0.9241, F1: 0.9230
Validation Loss: 0.8652, Accuracy: 0.8465, Precision: 0.8096, Recall: 0.7835, F1: 0.7943
Testing Loss: 0.7789, Accuracy: 0.8466, Precision: 0.8051, Recall: 0.7621, F1: 0.7787
LM Predictions:  [0, 2, 2, 0, 2, 5, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5950, Accuracy: 0.3095, Precision: 0.4568, Recall: 0.3208, F1: 0.2360
Epoch 62/70
Train Loss: 0.1521, Accuracy: 0.9530, Precision: 0.9272, Recall: 0.9338, F1: 0.9297
Validation Loss: 0.7607, Accuracy: 0.8465, Precision: 0.8066, Recall: 0.7942, F1: 0.7990
Testing Loss: 0.6647, Accuracy: 0.8539, Precision: 0.8238, Recall: 0.7860, F1: 0.8019
LM Predictions:  [0, 1, 5, 0, 5, 0, 0, 0, 0, 5, 0, 0, 2, 0, 3, 0, 5, 0, 0, 5, 0, 0, 3, 0, 0, 0, 5, 0, 2, 0, 4, 0, 0, 5, 0, 0, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6767, Accuracy: 0.2619, Precision: 0.6389, Recall: 0.2644, F1: 0.2518
Epoch 63/70
Train Loss: 0.1447, Accuracy: 0.9545, Precision: 0.9326, Recall: 0.9343, F1: 0.9330
Validation Loss: 0.8250, Accuracy: 0.8401, Precision: 0.8159, Recall: 0.7873, F1: 0.7991
Testing Loss: 0.8022, Accuracy: 0.8539, Precision: 0.8232, Recall: 0.7751, F1: 0.7927
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 4, 5, 0, 0, 2, 4, 3, 0, 2, 0, 0, 5, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 0, 3, 5, 0, 2, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4905, Accuracy: 0.3810, Precision: 0.6389, Recall: 0.3602, F1: 0.3504
Epoch 64/70
Train Loss: 0.1358, Accuracy: 0.9530, Precision: 0.9287, Recall: 0.9235, F1: 0.9260
Validation Loss: 0.8628, Accuracy: 0.8358, Precision: 0.8066, Recall: 0.7820, F1: 0.7893
Testing Loss: 0.7958, Accuracy: 0.8551, Precision: 0.8265, Recall: 0.7820, F1: 0.8003
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 4, 5, 0, 0, 2, 4, 3, 0, 1, 4, 0, 5, 0, 0, 3, 0, 0, 5, 0, 0, 2, 0, 4, 0, 0, 5, 0, 0, 3, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7196, Accuracy: 0.3571, Precision: 0.5595, Recall: 0.3352, F1: 0.3392
Epoch 65/70
Train Loss: 0.1469, Accuracy: 0.9535, Precision: 0.9301, Recall: 0.9248, F1: 0.9274
Validation Loss: 0.8001, Accuracy: 0.8380, Precision: 0.8035, Recall: 0.7829, F1: 0.7905
Testing Loss: 0.7322, Accuracy: 0.8527, Precision: 0.8101, Recall: 0.7712, F1: 0.7855
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 1, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 5, 0, 0, 3, 0, 0, 5, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.9009, Accuracy: 0.3333, Precision: 0.5043, Recall: 0.3352, F1: 0.2861
Epoch 66/70
Train Loss: 0.1316, Accuracy: 0.9528, Precision: 0.9294, Recall: 0.9272, F1: 0.9282
Validation Loss: 0.9191, Accuracy: 0.8380, Precision: 0.8056, Recall: 0.7840, F1: 0.7907
Testing Loss: 0.8641, Accuracy: 0.8502, Precision: 0.8048, Recall: 0.7702, F1: 0.7822
LM Predictions:  [0, 1, 2, 3, 2, 0, 0, 0, 4, 5, 0, 0, 2, 4, 3, 3, 2, 0, 0, 2, 3, 0, 3, 3, 3, 5, 0, 0, 2, 0, 4, 3, 3, 3, 3, 0, 3, 5, 3, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2277, Accuracy: 0.5000, Precision: 0.5855, Recall: 0.4435, F1: 0.4034
Epoch 67/70
Train Loss: 0.1361, Accuracy: 0.9561, Precision: 0.9353, Recall: 0.9316, F1: 0.9334
Validation Loss: 0.9017, Accuracy: 0.8486, Precision: 0.8094, Recall: 0.7921, F1: 0.7982
Testing Loss: 0.8187, Accuracy: 0.8539, Precision: 0.8170, Recall: 0.7854, F1: 0.7969
LM Predictions:  [0, 1, 1, 0, 5, 0, 0, 4, 4, 5, 0, 0, 2, 4, 3, 0, 1, 4, 0, 5, 0, 0, 3, 0, 0, 0, 0, 5, 2, 0, 4, 0, 0, 0, 0, 0, 3, 5, 0, 1, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6652, Accuracy: 0.3571, Precision: 0.4901, Recall: 0.3310, F1: 0.3266
Epoch 68/70
Train Loss: 0.1222, Accuracy: 0.9576, Precision: 0.9299, Recall: 0.9433, F1: 0.9357
Validation Loss: 0.9251, Accuracy: 0.8401, Precision: 0.8061, Recall: 0.7830, F1: 0.7923
Testing Loss: 0.8371, Accuracy: 0.8563, Precision: 0.8221, Recall: 0.7801, F1: 0.7968
LM Predictions:  [0, 0, 3, 0, 2, 0, 0, 0, 4, 5, 0, 0, 2, 0, 3, 0, 2, 0, 0, 2, 0, 0, 3, 0, 0, 0, 0, 5, 2, 0, 4, 0, 0, 0, 0, 0, 0, 5, 0, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5230, Accuracy: 0.3095, Precision: 0.4076, Recall: 0.3042, F1: 0.2540
Epoch 69/70
Train Loss: 0.1259, Accuracy: 0.9552, Precision: 0.9338, Recall: 0.9284, F1: 0.9310
Validation Loss: 1.0054, Accuracy: 0.8443, Precision: 0.8160, Recall: 0.7989, F1: 0.8048
Testing Loss: 0.8356, Accuracy: 0.8599, Precision: 0.8196, Recall: 0.7897, F1: 0.8021
LM Predictions:  [0, 1, 2, 0, 5, 0, 0, 4, 4, 5, 0, 0, 2, 4, 3, 0, 2, 4, 0, 5, 0, 4, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 0, 0, 4, 1, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4308, Accuracy: 0.4762, Precision: 0.7000, Recall: 0.4245, F1: 0.4350
Epoch 70/70
Train Loss: 0.1229, Accuracy: 0.9545, Precision: 0.9357, Recall: 0.9233, F1: 0.9291
Validation Loss: 1.0155, Accuracy: 0.8358, Precision: 0.7953, Recall: 0.7751, F1: 0.7828
Testing Loss: 0.8112, Accuracy: 0.8671, Precision: 0.8347, Recall: 0.8045, F1: 0.8165
LM Predictions:  [0, 1, 2, 0, 2, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 0, 0, 0, 4, 0, 2, 0, 4, 0, 0, 5, 0, 4, 1, 0, 0, 2, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2537, Accuracy: 0.5714, Precision: 0.6429, Recall: 0.5120, F1: 0.4802
For middle layers:  [4, 5, 6, 7]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.2129, Accuracy: 0.5511, Precision: 0.4534, Recall: 0.3904, F1: 0.3799
Validation Loss: 0.7830, Accuracy: 0.7249, Precision: 0.4938, Recall: 0.5477, F1: 0.5065
Testing Loss: 0.7166, Accuracy: 0.7488, Precision: 0.5082, Recall: 0.5716, F1: 0.5258
LM Predictions:  [0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3876, Accuracy: 0.1429, Precision: 0.0687, Recall: 0.2000, F1: 0.0842
Epoch 2/70
Train Loss: 0.6116, Accuracy: 0.8276, Precision: 0.7754, Recall: 0.7019, F1: 0.6908
Validation Loss: 0.5419, Accuracy: 0.8422, Precision: 0.7207, Recall: 0.7341, F1: 0.7224
Testing Loss: 0.4964, Accuracy: 0.8490, Precision: 0.7253, Recall: 0.7417, F1: 0.7292
LM Predictions:  [0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.3323, Accuracy: 0.1429, Precision: 0.1250, Recall: 0.2200, F1: 0.0778
Epoch 3/70
Train Loss: 0.4531, Accuracy: 0.8734, Precision: 0.8186, Recall: 0.7942, F1: 0.8012
Validation Loss: 0.4906, Accuracy: 0.8891, Precision: 0.8592, Recall: 0.8600, F1: 0.8584
Testing Loss: 0.4215, Accuracy: 0.8841, Precision: 0.8547, Recall: 0.8381, F1: 0.8449
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 4, 5, 5, 5, 5, 0, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9634, Accuracy: 0.0952, Precision: 0.2024, Recall: 0.1167, F1: 0.0829
Epoch 4/70
Train Loss: 0.4038, Accuracy: 0.8755, Precision: 0.8218, Recall: 0.8235, F1: 0.8220
Validation Loss: 0.5005, Accuracy: 0.8827, Precision: 0.8649, Recall: 0.8292, F1: 0.8442
Testing Loss: 0.4471, Accuracy: 0.8756, Precision: 0.8450, Recall: 0.8139, F1: 0.8265
LM Predictions:  [0, 5, 0, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 5, 0, 5, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1267, Accuracy: 0.0952, Precision: 0.0303, Recall: 0.1333, F1: 0.0494
Epoch 5/70
Train Loss: 0.3425, Accuracy: 0.8980, Precision: 0.8511, Recall: 0.8481, F1: 0.8492
Validation Loss: 0.4877, Accuracy: 0.8891, Precision: 0.8479, Recall: 0.8654, F1: 0.8554
Testing Loss: 0.4003, Accuracy: 0.8841, Precision: 0.8458, Recall: 0.8610, F1: 0.8517
LM Predictions:  [0, 5, 0, 5, 4, 5, 5, 5, 5, 5, 5, 0, 3, 5, 5, 0, 5, 4, 0, 5, 0, 5, 5, 0, 0, 0, 0, 5, 5, 5, 4, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7700, Accuracy: 0.0952, Precision: 0.1368, Recall: 0.1000, F1: 0.0883
Epoch 6/70
Train Loss: 0.3079, Accuracy: 0.9125, Precision: 0.8685, Recall: 0.8860, F1: 0.8754
Validation Loss: 0.4983, Accuracy: 0.8891, Precision: 0.8619, Recall: 0.8573, F1: 0.8593
Testing Loss: 0.4261, Accuracy: 0.8804, Precision: 0.8430, Recall: 0.8388, F1: 0.8400
LM Predictions:  [0, 5, 0, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 4, 0, 5, 3, 5, 2, 0, 0, 0, 0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5378, Accuracy: 0.1429, Precision: 0.3944, Recall: 0.1375, F1: 0.1594
Epoch 7/70
Train Loss: 0.2853, Accuracy: 0.9168, Precision: 0.8744, Recall: 0.8834, F1: 0.8777
Validation Loss: 0.5738, Accuracy: 0.8721, Precision: 0.8683, Recall: 0.7989, F1: 0.8175
Testing Loss: 0.4459, Accuracy: 0.8792, Precision: 0.8485, Recall: 0.7940, F1: 0.8081
LM Predictions:  [0, 2, 3, 0, 5, 5, 5, 5, 5, 5, 0, 0, 3, 0, 3, 0, 3, 4, 0, 5, 3, 0, 3, 0, 0, 0, 0, 5, 3, 0, 3, 0, 0, 0, 5, 0, 0, 5, 2, 3, 3, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7147, Accuracy: 0.1667, Precision: 0.2399, Recall: 0.1667, F1: 0.1214
Epoch 8/70
Train Loss: 0.2605, Accuracy: 0.9258, Precision: 0.8853, Recall: 0.8941, F1: 0.8887
Validation Loss: 0.5065, Accuracy: 0.8763, Precision: 0.8607, Recall: 0.8163, F1: 0.8315
Testing Loss: 0.4196, Accuracy: 0.8792, Precision: 0.8418, Recall: 0.8099, F1: 0.8215
LM Predictions:  [0, 5, 3, 0, 5, 5, 5, 5, 5, 5, 0, 0, 3, 0, 0, 0, 3, 4, 0, 5, 3, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 5, 0, 0, 5, 3, 5, 5, 3]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.6479, Accuracy: 0.1667, Precision: 0.2485, Recall: 0.1667, F1: 0.1389
Epoch 9/70
Train Loss: 0.2461, Accuracy: 0.9277, Precision: 0.8889, Recall: 0.8919, F1: 0.8901
Validation Loss: 0.5282, Accuracy: 0.8849, Precision: 0.8576, Recall: 0.8499, F1: 0.8526
Testing Loss: 0.4715, Accuracy: 0.8696, Precision: 0.8351, Recall: 0.8233, F1: 0.8287
LM Predictions:  [0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 3, 0, 5, 4, 0, 5, 3, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4400, Accuracy: 0.1667, Precision: 0.3750, Recall: 0.1667, F1: 0.1699
Epoch 10/70
Train Loss: 0.2357, Accuracy: 0.9286, Precision: 0.8888, Recall: 0.8961, F1: 0.8917
Validation Loss: 0.5913, Accuracy: 0.8827, Precision: 0.8738, Recall: 0.8202, F1: 0.8398
Testing Loss: 0.5186, Accuracy: 0.8611, Precision: 0.8218, Recall: 0.7819, F1: 0.7967
LM Predictions:  [0, 4, 3, 0, 5, 5, 5, 5, 5, 5, 0, 0, 2, 5, 0, 0, 5, 4, 0, 5, 0, 0, 2, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 0, 0, 3, 5, 0, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9837, Accuracy: 0.1429, Precision: 0.2208, Recall: 0.1542, F1: 0.1263
Epoch 11/70
Train Loss: 0.2662, Accuracy: 0.9196, Precision: 0.8815, Recall: 0.8965, F1: 0.8871
Validation Loss: 0.5889, Accuracy: 0.8699, Precision: 0.8332, Recall: 0.8431, F1: 0.8345
Testing Loss: 0.5157, Accuracy: 0.8647, Precision: 0.8290, Recall: 0.8488, F1: 0.8325
LM Predictions:  [5, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 5, 5, 4, 5, 5, 3, 5, 2, 5, 5, 5, 5, 5, 5, 5, 4, 5, 3, 5, 5, 5, 0, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5953, Accuracy: 0.1429, Precision: 0.3333, Recall: 0.1167, F1: 0.1699
Epoch 12/70
Train Loss: 0.2168, Accuracy: 0.9357, Precision: 0.8986, Recall: 0.9134, F1: 0.9036
Validation Loss: 0.5653, Accuracy: 0.8721, Precision: 0.8352, Recall: 0.8270, F1: 0.8303
Testing Loss: 0.5309, Accuracy: 0.8732, Precision: 0.8431, Recall: 0.8318, F1: 0.8368
LM Predictions:  [0, 5, 0, 0, 4, 5, 5, 5, 5, 5, 0, 0, 2, 5, 3, 5, 5, 4, 0, 5, 3, 5, 2, 5, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 5, 5, 0, 5, 5, 5, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3745, Accuracy: 0.2381, Precision: 0.4167, Recall: 0.2250, F1: 0.2566
Epoch 13/70
Train Loss: 0.2125, Accuracy: 0.9376, Precision: 0.9002, Recall: 0.9146, F1: 0.9054
Validation Loss: 0.5609, Accuracy: 0.8827, Precision: 0.8563, Recall: 0.8483, F1: 0.8511
Testing Loss: 0.5323, Accuracy: 0.8684, Precision: 0.8396, Recall: 0.8285, F1: 0.8310
LM Predictions:  [0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 5, 4, 0, 5, 2, 5, 1, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 5, 5, 1, 5, 5, 5, 5, 5]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4364, Accuracy: 0.1905, Precision: 0.5278, Recall: 0.1894, F1: 0.2361
Epoch 14/70
Train Loss: 0.2022, Accuracy: 0.9400, Precision: 0.9038, Recall: 0.9193, F1: 0.9096
Validation Loss: 0.5594, Accuracy: 0.8614, Precision: 0.8249, Recall: 0.8366, F1: 0.8262
Testing Loss: 0.5061, Accuracy: 0.8780, Precision: 0.8486, Recall: 0.8583, F1: 0.8489
LM Predictions:  [5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 2, 5, 2, 4, 5, 5, 0, 5, 2, 5, 5, 5, 5, 5, 1, 5, 4, 5, 3, 5, 5, 5, 2, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.4748, Accuracy: 0.1667, Precision: 0.4444, Recall: 0.1458, F1: 0.2006
Epoch 15/70
Train Loss: 0.1934, Accuracy: 0.9391, Precision: 0.9008, Recall: 0.9105, F1: 0.9049
Validation Loss: 0.5978, Accuracy: 0.8763, Precision: 0.8486, Recall: 0.8300, F1: 0.8376
Testing Loss: 0.5328, Accuracy: 0.8671, Precision: 0.8332, Recall: 0.8113, F1: 0.8209
LM Predictions:  [0, 1, 1, 5, 1, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 0, 1, 3, 5, 2, 5, 0, 5, 0, 5, 1, 0, 4, 0, 3, 5, 0, 5, 2, 5, 1, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.0299, Accuracy: 0.3095, Precision: 0.5435, Recall: 0.2829, F1: 0.3280
Epoch 16/70
Train Loss: 0.1794, Accuracy: 0.9433, Precision: 0.9085, Recall: 0.9194, F1: 0.9130
Validation Loss: 0.6523, Accuracy: 0.8763, Precision: 0.8418, Recall: 0.8357, F1: 0.8378
Testing Loss: 0.5761, Accuracy: 0.8635, Precision: 0.8263, Recall: 0.8168, F1: 0.8207
LM Predictions:  [0, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 5, 1, 3, 5, 2, 5, 5, 5, 0, 5, 1, 0, 4, 0, 3, 5, 0, 5, 0, 5, 2, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2120, Accuracy: 0.2857, Precision: 0.5048, Recall: 0.2644, F1: 0.3129
Epoch 17/70
Train Loss: 0.1834, Accuracy: 0.9440, Precision: 0.9093, Recall: 0.9276, F1: 0.9162
Validation Loss: 0.6816, Accuracy: 0.8742, Precision: 0.8547, Recall: 0.8069, F1: 0.8259
Testing Loss: 0.6240, Accuracy: 0.8696, Precision: 0.8377, Recall: 0.7878, F1: 0.8077
LM Predictions:  [0, 1, 2, 0, 5, 5, 0, 5, 0, 5, 5, 0, 2, 5, 3, 0, 2, 4, 0, 5, 0, 0, 5, 5, 0, 0, 0, 5, 2, 0, 4, 0, 3, 5, 0, 0, 0, 5, 2, 1, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.3379, Accuracy: 0.3333, Precision: 0.5948, Recall: 0.3227, F1: 0.3211
Epoch 18/70
Train Loss: 0.1753, Accuracy: 0.9462, Precision: 0.9139, Recall: 0.9219, F1: 0.9172
Validation Loss: 0.6466, Accuracy: 0.8742, Precision: 0.8482, Recall: 0.8184, F1: 0.8317
Testing Loss: 0.5978, Accuracy: 0.8623, Precision: 0.8315, Recall: 0.7988, F1: 0.8120
LM Predictions:  [0, 4, 2, 0, 4, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 2, 4, 0, 2, 0, 5, 2, 0, 0, 3, 0, 5, 2, 0, 4, 0, 3, 0, 0, 5, 0, 5, 2, 2, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1121, Accuracy: 0.3095, Precision: 0.3403, Recall: 0.3042, F1: 0.2666
Epoch 19/70
Train Loss: 0.1705, Accuracy: 0.9476, Precision: 0.9147, Recall: 0.9334, F1: 0.9219
Validation Loss: 0.6197, Accuracy: 0.8699, Precision: 0.8471, Recall: 0.8081, F1: 0.8189
Testing Loss: 0.6189, Accuracy: 0.8563, Precision: 0.8192, Recall: 0.7843, F1: 0.7917
LM Predictions:  [0, 1, 4, 5, 4, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 5, 2, 4, 0, 2, 5, 0, 3, 5, 0, 3, 0, 0, 2, 0, 4, 3, 3, 5, 0, 0, 0, 5, 2, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2487, Accuracy: 0.4286, Precision: 0.5487, Recall: 0.4079, F1: 0.3807
Epoch 20/70
Train Loss: 0.1621, Accuracy: 0.9514, Precision: 0.9223, Recall: 0.9276, F1: 0.9247
Validation Loss: 0.6661, Accuracy: 0.8763, Precision: 0.8272, Recall: 0.8501, F1: 0.8372
Testing Loss: 0.6129, Accuracy: 0.8696, Precision: 0.8264, Recall: 0.8427, F1: 0.8321
LM Predictions:  [0, 2, 2, 0, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 5, 2, 0, 5, 3, 2, 5, 3, 5, 5, 2, 0, 4, 3, 3, 5, 0, 5, 0, 5, 2, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.1292, Accuracy: 0.3571, Precision: 0.3881, Recall: 0.3250, F1: 0.3310
Epoch 21/70
Train Loss: 0.1599, Accuracy: 0.9490, Precision: 0.9145, Recall: 0.9343, F1: 0.9223
Validation Loss: 0.7174, Accuracy: 0.8849, Precision: 0.8480, Recall: 0.8309, F1: 0.8384
Testing Loss: 0.7200, Accuracy: 0.8587, Precision: 0.8110, Recall: 0.7861, F1: 0.7938
LM Predictions:  [0, 1, 1, 0, 1, 5, 0, 0, 0, 5, 0, 0, 2, 5, 3, 5, 2, 4, 0, 3, 0, 0, 3, 5, 0, 5, 0, 5, 2, 0, 4, 3, 3, 0, 0, 5, 5, 5, 1, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8431, Accuracy: 0.4286, Precision: 0.5722, Recall: 0.3912, F1: 0.4085
Epoch 22/70
Train Loss: 0.1584, Accuracy: 0.9519, Precision: 0.9219, Recall: 0.9348, F1: 0.9274
Validation Loss: 0.6845, Accuracy: 0.8785, Precision: 0.8465, Recall: 0.7958, F1: 0.8111
Testing Loss: 0.6564, Accuracy: 0.8575, Precision: 0.8062, Recall: 0.7636, F1: 0.7743
LM Predictions:  [0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 2, 0, 0, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 2, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5855, Accuracy: 0.4762, Precision: 0.6473, Recall: 0.4454, F1: 0.4044
Epoch 23/70
Train Loss: 0.1510, Accuracy: 0.9509, Precision: 0.9165, Recall: 0.9362, F1: 0.9240
Validation Loss: 0.6787, Accuracy: 0.8870, Precision: 0.8480, Recall: 0.8372, F1: 0.8415
Testing Loss: 0.6731, Accuracy: 0.8611, Precision: 0.8111, Recall: 0.8067, F1: 0.8086
LM Predictions:  [0, 5, 1, 3, 1, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 3, 4, 0, 3, 0, 5, 3, 3, 0, 2, 5, 5, 3, 0, 4, 3, 3, 0, 0, 5, 3, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.5242, Accuracy: 0.4286, Precision: 0.6094, Recall: 0.3870, F1: 0.4127
Epoch 24/70
Train Loss: 0.1636, Accuracy: 0.9478, Precision: 0.9173, Recall: 0.9215, F1: 0.9189
Validation Loss: 0.6734, Accuracy: 0.8763, Precision: 0.8495, Recall: 0.8339, F1: 0.8408
Testing Loss: 0.6797, Accuracy: 0.8527, Precision: 0.8087, Recall: 0.7986, F1: 0.8027
LM Predictions:  [0, 4, 2, 0, 2, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 5, 3, 0, 5, 3, 0, 0, 3, 5, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 0, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8174, Accuracy: 0.4048, Precision: 0.4206, Recall: 0.3750, F1: 0.3636
Epoch 25/70
Train Loss: 0.1388, Accuracy: 0.9538, Precision: 0.9218, Recall: 0.9377, F1: 0.9284
Validation Loss: 0.7368, Accuracy: 0.8721, Precision: 0.8399, Recall: 0.8148, F1: 0.8254
Testing Loss: 0.7280, Accuracy: 0.8611, Precision: 0.8239, Recall: 0.8017, F1: 0.8115
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 0, 3, 1, 5, 3, 1, 3, 3, 5, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4049, Accuracy: 0.5238, Precision: 0.6583, Recall: 0.4657, F1: 0.4937
Epoch 26/70
Train Loss: 0.1414, Accuracy: 0.9566, Precision: 0.9309, Recall: 0.9300, F1: 0.9304
Validation Loss: 0.7398, Accuracy: 0.8763, Precision: 0.8416, Recall: 0.8105, F1: 0.8239
Testing Loss: 0.7324, Accuracy: 0.8623, Precision: 0.8172, Recall: 0.7961, F1: 0.8054
LM Predictions:  [0, 1, 2, 0, 2, 5, 0, 5, 5, 0, 5, 0, 2, 5, 3, 5, 2, 4, 0, 3, 0, 0, 3, 1, 0, 1, 0, 5, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3645, Accuracy: 0.5238, Precision: 0.6567, Recall: 0.4806, F1: 0.4741
Epoch 27/70
Train Loss: 0.1331, Accuracy: 0.9564, Precision: 0.9300, Recall: 0.9325, F1: 0.9309
Validation Loss: 0.6486, Accuracy: 0.8742, Precision: 0.8474, Recall: 0.8133, F1: 0.8276
Testing Loss: 0.6054, Accuracy: 0.8696, Precision: 0.8370, Recall: 0.8117, F1: 0.8226
LM Predictions:  [0, 1, 2, 0, 1, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 5, 2, 4, 0, 3, 3, 5, 3, 2, 0, 2, 0, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3321, Accuracy: 0.5476, Precision: 0.7064, Recall: 0.4847, F1: 0.5053
Epoch 28/70
Train Loss: 0.1291, Accuracy: 0.9554, Precision: 0.9282, Recall: 0.9332, F1: 0.9302
Validation Loss: 0.6709, Accuracy: 0.8785, Precision: 0.8491, Recall: 0.8309, F1: 0.8383
Testing Loss: 0.6854, Accuracy: 0.8611, Precision: 0.8231, Recall: 0.8117, F1: 0.8165
LM Predictions:  [0, 5, 2, 1, 5, 5, 0, 5, 5, 5, 5, 0, 2, 5, 3, 0, 2, 4, 0, 3, 1, 5, 3, 0, 0, 2, 0, 5, 2, 0, 4, 3, 3, 1, 1, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4554, Accuracy: 0.5238, Precision: 0.6667, Recall: 0.4681, F1: 0.4964
Epoch 29/70
Train Loss: 0.1236, Accuracy: 0.9564, Precision: 0.9277, Recall: 0.9357, F1: 0.9310
Validation Loss: 0.7344, Accuracy: 0.8614, Precision: 0.8275, Recall: 0.7861, F1: 0.8025
Testing Loss: 0.6581, Accuracy: 0.8635, Precision: 0.8283, Recall: 0.7864, F1: 0.8024
LM Predictions:  [0, 1, 2, 0, 2, 0, 0, 5, 5, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 0, 0, 2, 0, 5, 2, 0, 4, 3, 1, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4033, Accuracy: 0.5000, Precision: 0.6319, Recall: 0.4662, F1: 0.4347
Epoch 30/70
Train Loss: 0.1280, Accuracy: 0.9571, Precision: 0.9332, Recall: 0.9329, F1: 0.9327
Validation Loss: 0.8583, Accuracy: 0.8678, Precision: 0.8362, Recall: 0.8049, F1: 0.8186
Testing Loss: 0.8099, Accuracy: 0.8563, Precision: 0.8258, Recall: 0.7971, F1: 0.8091
LM Predictions:  [0, 1, 4, 0, 4, 0, 0, 5, 5, 0, 5, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 0, 3, 0, 5, 3, 0, 5, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2010, Accuracy: 0.4762, Precision: 0.6085, Recall: 0.4412, F1: 0.4298
Epoch 31/70
Train Loss: 0.1552, Accuracy: 0.9519, Precision: 0.9205, Recall: 0.9319, F1: 0.9257
Validation Loss: 0.7074, Accuracy: 0.8465, Precision: 0.7900, Recall: 0.7838, F1: 0.7817
Testing Loss: 0.6514, Accuracy: 0.8514, Precision: 0.7930, Recall: 0.7762, F1: 0.7758
LM Predictions:  [0, 1, 4, 2, 4, 2, 0, 0, 4, 0, 0, 0, 2, 0, 3, 2, 2, 4, 0, 3, 2, 0, 3, 2, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 2, 0, 0, 0, 3, 2, 3, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.3573, Accuracy: 0.5000, Precision: 0.6503, Recall: 0.5522, F1: 0.4721
Epoch 32/70
Train Loss: 0.1251, Accuracy: 0.9566, Precision: 0.9346, Recall: 0.9196, F1: 0.9267
Validation Loss: 0.7900, Accuracy: 0.8550, Precision: 0.8207, Recall: 0.7771, F1: 0.7937
Testing Loss: 0.7917, Accuracy: 0.8466, Precision: 0.8077, Recall: 0.7686, F1: 0.7831
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 1, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4644, Accuracy: 0.5476, Precision: 0.6712, Recall: 0.5032, F1: 0.4792
Epoch 33/70
Train Loss: 0.1094, Accuracy: 0.9609, Precision: 0.9441, Recall: 0.9292, F1: 0.9362
Validation Loss: 0.8669, Accuracy: 0.8657, Precision: 0.8359, Recall: 0.7943, F1: 0.8096
Testing Loss: 0.7542, Accuracy: 0.8647, Precision: 0.8383, Recall: 0.8075, F1: 0.8211
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 5, 5, 0, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 0, 3, 2, 0, 2, 5, 5, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1583, Accuracy: 0.5476, Precision: 0.6949, Recall: 0.5014, F1: 0.4855
Epoch 34/70
Train Loss: 0.1121, Accuracy: 0.9640, Precision: 0.9446, Recall: 0.9389, F1: 0.9417
Validation Loss: 0.8995, Accuracy: 0.8593, Precision: 0.8250, Recall: 0.8017, F1: 0.8123
Testing Loss: 0.8251, Accuracy: 0.8647, Precision: 0.8360, Recall: 0.8150, F1: 0.8240
LM Predictions:  [0, 1, 2, 5, 1, 3, 0, 5, 5, 5, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 2, 5, 3, 1, 0, 1, 5, 5, 2, 0, 4, 3, 3, 2, 0, 5, 2, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.2503, Accuracy: 0.5476, Precision: 0.6466, Recall: 0.4866, F1: 0.5009
Epoch 35/70
Train Loss: 0.1135, Accuracy: 0.9590, Precision: 0.9329, Recall: 0.9363, F1: 0.9343
Validation Loss: 0.8226, Accuracy: 0.8614, Precision: 0.8242, Recall: 0.8000, F1: 0.8107
Testing Loss: 0.6547, Accuracy: 0.8696, Precision: 0.8332, Recall: 0.8132, F1: 0.8211
LM Predictions:  [0, 1, 4, 5, 2, 3, 0, 5, 5, 0, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 5, 3, 1, 0, 2, 5, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4519, Accuracy: 0.5238, Precision: 0.6276, Recall: 0.4806, F1: 0.4782
Epoch 36/70
Train Loss: 0.1113, Accuracy: 0.9585, Precision: 0.9344, Recall: 0.9327, F1: 0.9334
Validation Loss: 0.8750, Accuracy: 0.8550, Precision: 0.8265, Recall: 0.7867, F1: 0.8035
Testing Loss: 0.7712, Accuracy: 0.8575, Precision: 0.8248, Recall: 0.7814, F1: 0.7984
LM Predictions:  [0, 1, 4, 0, 1, 0, 0, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 5, 3, 1, 0, 1, 0, 5, 2, 0, 4, 3, 3, 0, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1042, Accuracy: 0.5238, Precision: 0.6216, Recall: 0.4782, F1: 0.4692
Epoch 37/70
Train Loss: 0.1031, Accuracy: 0.9594, Precision: 0.9352, Recall: 0.9334, F1: 0.9342
Validation Loss: 0.8578, Accuracy: 0.8721, Precision: 0.8244, Recall: 0.7949, F1: 0.8038
Testing Loss: 0.7630, Accuracy: 0.8575, Precision: 0.8041, Recall: 0.7625, F1: 0.7742
LM Predictions:  [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1197, Accuracy: 0.5476, Precision: 0.8417, Recall: 0.5989, F1: 0.5801
Epoch 38/70
Train Loss: 0.1134, Accuracy: 0.9587, Precision: 0.9416, Recall: 0.9263, F1: 0.9336
Validation Loss: 0.8232, Accuracy: 0.8571, Precision: 0.7853, Recall: 0.7571, F1: 0.7496
Testing Loss: 0.7356, Accuracy: 0.8635, Precision: 0.8254, Recall: 0.7785, F1: 0.7869
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 3, 4, 0, 3, 0, 0, 3, 2, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0990, Accuracy: 0.5238, Precision: 0.7863, Recall: 0.5767, F1: 0.5393
Epoch 39/70
Train Loss: 0.1075, Accuracy: 0.9611, Precision: 0.9357, Recall: 0.9399, F1: 0.9373
Validation Loss: 0.7871, Accuracy: 0.8699, Precision: 0.8270, Recall: 0.8177, F1: 0.8220
Testing Loss: 0.7754, Accuracy: 0.8551, Precision: 0.8145, Recall: 0.7793, F1: 0.7930
LM Predictions:  [0, 1, 2, 0, 1, 3, 0, 0, 5, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 5, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7361, Accuracy: 0.6429, Precision: 0.7002, Recall: 0.5741, F1: 0.5445
Epoch 40/70
Train Loss: 0.1062, Accuracy: 0.9611, Precision: 0.9403, Recall: 0.9347, F1: 0.9373
Validation Loss: 0.9948, Accuracy: 0.8465, Precision: 0.8221, Recall: 0.7659, F1: 0.7854
Testing Loss: 0.8096, Accuracy: 0.8527, Precision: 0.8166, Recall: 0.7573, F1: 0.7744
LM Predictions:  [0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.4276, Accuracy: 0.5238, Precision: 0.8131, Recall: 0.5767, F1: 0.5456
Epoch 41/70
Train Loss: 0.1178, Accuracy: 0.9571, Precision: 0.9446, Recall: 0.9150, F1: 0.9279
Validation Loss: 0.7454, Accuracy: 0.8657, Precision: 0.8266, Recall: 0.8066, F1: 0.8118
Testing Loss: 0.6841, Accuracy: 0.8466, Precision: 0.7975, Recall: 0.7662, F1: 0.7755
LM Predictions:  [0, 1, 0, 3, 1, 3, 0, 0, 0, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 1, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 2, 3, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1667, Accuracy: 0.6190, Precision: 0.7558, Recall: 0.6639, F1: 0.6083
Epoch 42/70
Train Loss: 0.1101, Accuracy: 0.9625, Precision: 0.9459, Recall: 0.9342, F1: 0.9398
Validation Loss: 0.7697, Accuracy: 0.8763, Precision: 0.8405, Recall: 0.8133, F1: 0.8243
Testing Loss: 0.7780, Accuracy: 0.8527, Precision: 0.8159, Recall: 0.7683, F1: 0.7845
LM Predictions:  [0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 0, 2, 0, 4, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0217, Accuracy: 0.5952, Precision: 0.8254, Recall: 0.6489, F1: 0.6139
Epoch 43/70
Train Loss: 0.1122, Accuracy: 0.9585, Precision: 0.9381, Recall: 0.9326, F1: 0.9352
Validation Loss: 0.8542, Accuracy: 0.8699, Precision: 0.8303, Recall: 0.7988, F1: 0.8098
Testing Loss: 0.8031, Accuracy: 0.8611, Precision: 0.8240, Recall: 0.7924, F1: 0.8051
LM Predictions:  [0, 1, 2, 3, 1, 3, 0, 0, 0, 0, 3, 0, 2, 5, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 3, 2, 0, 5, 2, 0, 4, 3, 3, 2, 3, 0, 3, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7009, Accuracy: 0.6667, Precision: 0.6829, Recall: 0.5907, F1: 0.5540
Epoch 44/70
Train Loss: 0.1162, Accuracy: 0.9566, Precision: 0.9365, Recall: 0.9289, F1: 0.9326
Validation Loss: 0.8729, Accuracy: 0.8593, Precision: 0.8327, Recall: 0.7983, F1: 0.8132
Testing Loss: 0.7794, Accuracy: 0.8647, Precision: 0.8436, Recall: 0.8094, F1: 0.8227
LM Predictions:  [0, 1, 2, 0, 5, 2, 0, 5, 5, 0, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 0, 5, 3, 5, 0, 2, 0, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0563, Accuracy: 0.5238, Precision: 0.7014, Recall: 0.4829, F1: 0.4703
Epoch 45/70
Train Loss: 0.0910, Accuracy: 0.9644, Precision: 0.9509, Recall: 0.9349, F1: 0.9424
Validation Loss: 0.9208, Accuracy: 0.8486, Precision: 0.8252, Recall: 0.7653, F1: 0.7858
Testing Loss: 0.7331, Accuracy: 0.8551, Precision: 0.8157, Recall: 0.7690, F1: 0.7862
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 5, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 0, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1725, Accuracy: 0.6429, Precision: 0.7130, Recall: 0.5722, F1: 0.5359
Epoch 46/70
Train Loss: 0.0972, Accuracy: 0.9637, Precision: 0.9459, Recall: 0.9404, F1: 0.9431
Validation Loss: 0.8307, Accuracy: 0.8571, Precision: 0.8311, Recall: 0.7852, F1: 0.8028
Testing Loss: 0.8033, Accuracy: 0.8575, Precision: 0.8243, Recall: 0.7667, F1: 0.7863
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9058, Accuracy: 0.5714, Precision: 0.8435, Recall: 0.6239, F1: 0.5978
Epoch 47/70
Train Loss: 0.0857, Accuracy: 0.9661, Precision: 0.9492, Recall: 0.9413, F1: 0.9452
Validation Loss: 0.9384, Accuracy: 0.8657, Precision: 0.8230, Recall: 0.7957, F1: 0.8060
Testing Loss: 0.8907, Accuracy: 0.8599, Precision: 0.8176, Recall: 0.7776, F1: 0.7906
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 3, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6140, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7439, F1: 0.7301
Epoch 48/70
Train Loss: 0.0910, Accuracy: 0.9649, Precision: 0.9483, Recall: 0.9349, F1: 0.9412
Validation Loss: 0.8872, Accuracy: 0.8678, Precision: 0.8431, Recall: 0.8012, F1: 0.8183
Testing Loss: 0.8496, Accuracy: 0.8611, Precision: 0.8343, Recall: 0.7889, F1: 0.8067
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 3, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 1, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6494, Accuracy: 0.6667, Precision: 0.7130, Recall: 0.5903, F1: 0.5755
Epoch 49/70
Train Loss: 0.0892, Accuracy: 0.9637, Precision: 0.9460, Recall: 0.9298, F1: 0.9373
Validation Loss: 0.8479, Accuracy: 0.8614, Precision: 0.8312, Recall: 0.7924, F1: 0.8061
Testing Loss: 0.7920, Accuracy: 0.8635, Precision: 0.8337, Recall: 0.7883, F1: 0.8014
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 0, 2, 0, 4, 3, 3, 2, 0, 4, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6473, Accuracy: 0.6429, Precision: 0.8500, Recall: 0.6889, F1: 0.6674
Epoch 50/70
Train Loss: 0.0946, Accuracy: 0.9642, Precision: 0.9488, Recall: 0.9398, F1: 0.9442
Validation Loss: 0.8570, Accuracy: 0.8614, Precision: 0.8168, Recall: 0.8026, F1: 0.8082
Testing Loss: 0.8084, Accuracy: 0.8551, Precision: 0.8072, Recall: 0.7888, F1: 0.7957
LM Predictions:  [0, 1, 2, 0, 1, 2, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5252, Accuracy: 0.7619, Precision: 0.8464, Recall: 0.7839, F1: 0.7533
Epoch 51/70
Train Loss: 0.0895, Accuracy: 0.9632, Precision: 0.9389, Recall: 0.9420, F1: 0.9404
Validation Loss: 0.9330, Accuracy: 0.8486, Precision: 0.8161, Recall: 0.7746, F1: 0.7897
Testing Loss: 0.8255, Accuracy: 0.8563, Precision: 0.8260, Recall: 0.7760, F1: 0.7916
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 0, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5717, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7639, F1: 0.7445
Epoch 52/70
Train Loss: 0.1007, Accuracy: 0.9590, Precision: 0.9344, Recall: 0.9334, F1: 0.9339
Validation Loss: 0.8642, Accuracy: 0.8699, Precision: 0.8343, Recall: 0.8045, F1: 0.8172
Testing Loss: 0.7237, Accuracy: 0.8647, Precision: 0.8326, Recall: 0.7898, F1: 0.8063
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5332, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7839, F1: 0.7597
Epoch 53/70
Train Loss: 0.0951, Accuracy: 0.9663, Precision: 0.9531, Recall: 0.9343, F1: 0.9429
Validation Loss: 0.8335, Accuracy: 0.8614, Precision: 0.8118, Recall: 0.8236, F1: 0.8165
Testing Loss: 0.7281, Accuracy: 0.8659, Precision: 0.8255, Recall: 0.8248, F1: 0.8247
LM Predictions:  [0, 1, 2, 5, 3, 2, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 3, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 5, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5573, Accuracy: 0.7619, Precision: 0.7081, Recall: 0.6495, F1: 0.6262
Epoch 54/70
Train Loss: 0.1105, Accuracy: 0.9623, Precision: 0.9388, Recall: 0.9398, F1: 0.9393
Validation Loss: 0.8781, Accuracy: 0.8657, Precision: 0.8331, Recall: 0.7904, F1: 0.8054
Testing Loss: 0.7848, Accuracy: 0.8599, Precision: 0.8275, Recall: 0.7881, F1: 0.8044
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5054, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7639, F1: 0.7475
Epoch 55/70
Train Loss: 0.1012, Accuracy: 0.9597, Precision: 0.9393, Recall: 0.9351, F1: 0.9372
Validation Loss: 0.8705, Accuracy: 0.8486, Precision: 0.8142, Recall: 0.7930, F1: 0.7993
Testing Loss: 0.8439, Accuracy: 0.8551, Precision: 0.8300, Recall: 0.8048, F1: 0.8130
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 5, 4, 0, 3, 0, 2, 5, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 5, 2, 0, 4, 3, 3, 0, 0, 5, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9344, Accuracy: 0.6905, Precision: 0.7308, Recall: 0.6032, F1: 0.6100
Epoch 56/70
Train Loss: 0.0783, Accuracy: 0.9668, Precision: 0.9518, Recall: 0.9390, F1: 0.9450
Validation Loss: 1.0332, Accuracy: 0.8529, Precision: 0.8267, Recall: 0.7726, F1: 0.7938
Testing Loss: 0.9806, Accuracy: 0.8551, Precision: 0.8252, Recall: 0.7729, F1: 0.7929
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 0, 4, 0, 3, 0, 2, 4, 3, 3, 2, 2, 0, 3, 0, 0, 3, 1, 3, 2, 0, 0, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.9743, Accuracy: 0.6667, Precision: 0.8116, Recall: 0.7039, F1: 0.6717
Epoch 57/70
Train Loss: 0.0813, Accuracy: 0.9649, Precision: 0.9395, Recall: 0.9528, F1: 0.9455
Validation Loss: 0.9563, Accuracy: 0.8635, Precision: 0.8295, Recall: 0.7919, F1: 0.8076
Testing Loss: 0.9144, Accuracy: 0.8599, Precision: 0.8335, Recall: 0.7822, F1: 0.8012
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 3, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7115, Accuracy: 0.7619, Precision: 0.8464, Recall: 0.7839, F1: 0.7600
Epoch 58/70
Train Loss: 0.0780, Accuracy: 0.9649, Precision: 0.9474, Recall: 0.9325, F1: 0.9393
Validation Loss: 0.8764, Accuracy: 0.8657, Precision: 0.8124, Recall: 0.8190, F1: 0.8144
Testing Loss: 0.8872, Accuracy: 0.8623, Precision: 0.8183, Recall: 0.8162, F1: 0.8167
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5412, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8489, F1: 0.8185
Epoch 59/70
Train Loss: 0.0827, Accuracy: 0.9663, Precision: 0.9479, Recall: 0.9410, F1: 0.9441
Validation Loss: 0.8998, Accuracy: 0.8635, Precision: 0.8317, Recall: 0.7953, F1: 0.8107
Testing Loss: 0.9005, Accuracy: 0.8587, Precision: 0.8271, Recall: 0.7928, F1: 0.8071
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 0, 0, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5080, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7689, F1: 0.7326
Epoch 60/70
Train Loss: 0.1156, Accuracy: 0.9597, Precision: 0.9386, Recall: 0.9398, F1: 0.9392
Validation Loss: 0.9173, Accuracy: 0.8550, Precision: 0.8315, Recall: 0.7903, F1: 0.8076
Testing Loss: 0.8461, Accuracy: 0.8539, Precision: 0.8189, Recall: 0.7799, F1: 0.7938
LM Predictions:  [0, 1, 2, 0, 1, 2, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5607, Accuracy: 0.8095, Precision: 0.8611, Recall: 0.8289, F1: 0.7937
Epoch 61/70
Train Loss: 0.0865, Accuracy: 0.9685, Precision: 0.9557, Recall: 0.9449, F1: 0.9502
Validation Loss: 0.9815, Accuracy: 0.8550, Precision: 0.8082, Recall: 0.8026, F1: 0.8023
Testing Loss: 0.8385, Accuracy: 0.8647, Precision: 0.8298, Recall: 0.8105, F1: 0.8189
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 3, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.6394, Accuracy: 0.7619, Precision: 0.8464, Recall: 0.7789, F1: 0.7553
Epoch 62/70
Train Loss: 0.0769, Accuracy: 0.9673, Precision: 0.9434, Recall: 0.9525, F1: 0.9477
Validation Loss: 0.9135, Accuracy: 0.8635, Precision: 0.8147, Recall: 0.7979, F1: 0.8036
Testing Loss: 0.8697, Accuracy: 0.8563, Precision: 0.8069, Recall: 0.7708, F1: 0.7809
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4205, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8239, F1: 0.7986
Epoch 63/70
Train Loss: 0.0754, Accuracy: 0.9687, Precision: 0.9519, Recall: 0.9467, F1: 0.9491
Validation Loss: 0.8918, Accuracy: 0.8678, Precision: 0.8239, Recall: 0.8093, F1: 0.8151
Testing Loss: 0.8703, Accuracy: 0.8647, Precision: 0.8266, Recall: 0.7928, F1: 0.8065
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3783, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8661, F1: 0.8440
Epoch 64/70
Train Loss: 0.1045, Accuracy: 0.9594, Precision: 0.9337, Recall: 0.9349, F1: 0.9343
Validation Loss: 0.8517, Accuracy: 0.8614, Precision: 0.8034, Recall: 0.7850, F1: 0.7886
Testing Loss: 0.8628, Accuracy: 0.8623, Precision: 0.8140, Recall: 0.7765, F1: 0.7863
LM Predictions:  [0, 1, 2, 2, 1, 2, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 2, 3, 0, 4, 3, 1, 2, 2, 4, 4, 2, 0, 4, 3, 3, 0, 2, 4, 2, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5315, Accuracy: 0.7857, Precision: 0.8327, Recall: 0.8039, F1: 0.7750
Epoch 65/70
Train Loss: 0.0904, Accuracy: 0.9625, Precision: 0.9426, Recall: 0.9388, F1: 0.9406
Validation Loss: 0.9127, Accuracy: 0.8763, Precision: 0.8325, Recall: 0.8269, F1: 0.8290
Testing Loss: 0.9242, Accuracy: 0.8587, Precision: 0.8255, Recall: 0.8051, F1: 0.8141
LM Predictions:  [0, 1, 2, 1, 1, 0, 0, 4, 4, 0, 3, 0, 5, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4506, Accuracy: 0.8095, Precision: 0.7361, Recall: 0.6843, F1: 0.6748
Epoch 66/70
Train Loss: 0.0818, Accuracy: 0.9673, Precision: 0.9491, Recall: 0.9510, F1: 0.9500
Validation Loss: 0.8508, Accuracy: 0.8593, Precision: 0.8225, Recall: 0.8017, F1: 0.8106
Testing Loss: 0.8370, Accuracy: 0.8575, Precision: 0.8274, Recall: 0.7793, F1: 0.7975
LM Predictions:  [0, 1, 2, 0, 1, 2, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 2, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4395, Accuracy: 0.7857, Precision: 0.8389, Recall: 0.8039, F1: 0.7701
Epoch 67/70
Train Loss: 0.0772, Accuracy: 0.9654, Precision: 0.9475, Recall: 0.9416, F1: 0.9443
Validation Loss: 0.9031, Accuracy: 0.8657, Precision: 0.8249, Recall: 0.8245, F1: 0.8242
Testing Loss: 0.8450, Accuracy: 0.8599, Precision: 0.8186, Recall: 0.8138, F1: 0.8157
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 0, 2, 4, 1, 3, 0, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3112, Accuracy: 0.8571, Precision: 0.8800, Recall: 0.8800, F1: 0.8522
Epoch 68/70
Train Loss: 0.0689, Accuracy: 0.9675, Precision: 0.9488, Recall: 0.9487, F1: 0.9487
Validation Loss: 0.9553, Accuracy: 0.8742, Precision: 0.8295, Recall: 0.8285, F1: 0.8286
Testing Loss: 0.8994, Accuracy: 0.8623, Precision: 0.8140, Recall: 0.7908, F1: 0.7986
LM Predictions:  [0, 1, 2, 0, 1, 0, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3205, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8639, F1: 0.8347
Epoch 69/70
Train Loss: 0.0711, Accuracy: 0.9654, Precision: 0.9470, Recall: 0.9397, F1: 0.9430
Validation Loss: 0.9675, Accuracy: 0.8678, Precision: 0.8218, Recall: 0.8170, F1: 0.8190
Testing Loss: 0.9032, Accuracy: 0.8708, Precision: 0.8337, Recall: 0.8132, F1: 0.8225
LM Predictions:  [0, 1, 2, 1, 1, 2, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2685, Accuracy: 0.9524, Precision: 0.9444, Recall: 0.9578, F1: 0.9478
Epoch 70/70
Train Loss: 0.0739, Accuracy: 0.9651, Precision: 0.9491, Recall: 0.9400, F1: 0.9442
Validation Loss: 1.0477, Accuracy: 0.8529, Precision: 0.8085, Recall: 0.7899, F1: 0.7984
Testing Loss: 0.9060, Accuracy: 0.8587, Precision: 0.8277, Recall: 0.8074, F1: 0.8161
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2380, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9150, F1: 0.8942
For later layers:  [8, 9, 10, 11]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.1367, Accuracy: 0.5760, Precision: 0.5314, Recall: 0.4259, F1: 0.4277
Validation Loss: 0.5341, Accuracy: 0.8507, Precision: 0.8057, Recall: 0.7697, F1: 0.7649
Testing Loss: 0.5025, Accuracy: 0.8514, Precision: 0.7912, Recall: 0.7544, F1: 0.7490
LM Predictions:  [0, 3, 0, 0, 5, 0, 0, 0, 0, 4, 0, 0, 5, 0, 3, 0, 3, 4, 0, 5, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1822, Accuracy: 0.1429, Precision: 0.1194, Recall: 0.1667, F1: 0.0875
Epoch 2/70
Train Loss: 0.4837, Accuracy: 0.8575, Precision: 0.7969, Recall: 0.7684, F1: 0.7731
Validation Loss: 0.4950, Accuracy: 0.8657, Precision: 0.8896, Recall: 0.7832, F1: 0.7913
Testing Loss: 0.3828, Accuracy: 0.8684, Precision: 0.8740, Recall: 0.7725, F1: 0.7748
LM Predictions:  [0, 3, 0, 0, 5, 0, 5, 5, 0, 5, 0, 0, 0, 0, 0, 0, 3, 4, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 3, 5, 0, 3, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.9230, Accuracy: 0.1190, Precision: 0.1839, Recall: 0.1333, F1: 0.0850
Epoch 3/70
Train Loss: 0.3782, Accuracy: 0.8866, Precision: 0.8386, Recall: 0.8308, F1: 0.8344
Validation Loss: 0.4680, Accuracy: 0.8763, Precision: 0.8514, Recall: 0.8517, F1: 0.8483
Testing Loss: 0.3983, Accuracy: 0.8732, Precision: 0.8528, Recall: 0.8483, F1: 0.8466
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 4, 0, 5, 0, 5, 5, 0, 5, 0, 0, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.1198, Accuracy: 0.0952, Precision: 0.1970, Recall: 0.1000, F1: 0.0972
Epoch 4/70
Train Loss: 0.3255, Accuracy: 0.9025, Precision: 0.8597, Recall: 0.8650, F1: 0.8619
Validation Loss: 0.5142, Accuracy: 0.8785, Precision: 0.8682, Recall: 0.8294, F1: 0.8447
Testing Loss: 0.4106, Accuracy: 0.8744, Precision: 0.8519, Recall: 0.8030, F1: 0.8211
LM Predictions:  [0, 3, 5, 0, 5, 0, 5, 5, 0, 5, 0, 0, 3, 0, 0, 0, 3, 4, 0, 5, 0, 0, 5, 1, 0, 0, 0, 5, 5, 0, 4, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.2351, Accuracy: 0.1667, Precision: 0.5238, Recall: 0.1727, F1: 0.1644
Epoch 5/70
Train Loss: 0.2677, Accuracy: 0.9146, Precision: 0.8754, Recall: 0.8888, F1: 0.8803
Validation Loss: 0.5680, Accuracy: 0.8635, Precision: 0.8313, Recall: 0.8303, F1: 0.8272
Testing Loss: 0.4492, Accuracy: 0.8708, Precision: 0.8427, Recall: 0.8325, F1: 0.8335
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 0, 5, 0, 5, 4, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 3.0778, Accuracy: 0.1429, Precision: 0.3750, Recall: 0.1417, F1: 0.1735
Epoch 6/70
Train Loss: 0.2395, Accuracy: 0.9274, Precision: 0.8899, Recall: 0.9047, F1: 0.8956
Validation Loss: 0.5815, Accuracy: 0.8635, Precision: 0.8191, Recall: 0.8311, F1: 0.8233
Testing Loss: 0.4406, Accuracy: 0.8756, Precision: 0.8366, Recall: 0.8455, F1: 0.8395
LM Predictions:  [0, 4, 4, 5, 4, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 5, 0, 4, 0, 5, 0, 5, 5, 1, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5379, Accuracy: 0.2143, Precision: 0.6310, Recall: 0.1935, F1: 0.2484
Epoch 7/70
Train Loss: 0.2274, Accuracy: 0.9289, Precision: 0.8870, Recall: 0.9045, F1: 0.8937
Validation Loss: 0.5384, Accuracy: 0.8763, Precision: 0.8616, Recall: 0.8276, F1: 0.8423
Testing Loss: 0.4615, Accuracy: 0.8829, Precision: 0.8573, Recall: 0.8342, F1: 0.8439
LM Predictions:  [0, 4, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 2, 4, 0, 5, 0, 5, 2, 1, 0, 0, 0, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.5405, Accuracy: 0.2381, Precision: 0.6149, Recall: 0.2310, F1: 0.2608
Epoch 8/70
Train Loss: 0.2066, Accuracy: 0.9367, Precision: 0.8989, Recall: 0.9127, F1: 0.9043
Validation Loss: 0.5503, Accuracy: 0.8849, Precision: 0.8583, Recall: 0.8431, F1: 0.8500
Testing Loss: 0.4662, Accuracy: 0.8708, Precision: 0.8365, Recall: 0.8155, F1: 0.8244
LM Predictions:  [0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 2, 5, 3, 0, 5, 4, 0, 5, 0, 5, 2, 1, 0, 5, 0, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7018, Accuracy: 0.2143, Precision: 0.6611, Recall: 0.2102, F1: 0.2465
Epoch 9/70
Train Loss: 0.1928, Accuracy: 0.9431, Precision: 0.9063, Recall: 0.9234, F1: 0.9128
Validation Loss: 0.6374, Accuracy: 0.8635, Precision: 0.8527, Recall: 0.7852, F1: 0.8090
Testing Loss: 0.5552, Accuracy: 0.8623, Precision: 0.8416, Recall: 0.7609, F1: 0.7780
LM Predictions:  [0, 3, 5, 0, 0, 0, 5, 5, 0, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 5, 0, 0, 5, 1, 0, 0, 0, 5, 5, 0, 4, 0, 3, 0, 0, 0, 0, 5, 0, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7246, Accuracy: 0.2857, Precision: 0.6401, Recall: 0.2810, F1: 0.2787
Epoch 10/70
Train Loss: 0.1926, Accuracy: 0.9391, Precision: 0.9025, Recall: 0.9152, F1: 0.9076
Validation Loss: 0.7538, Accuracy: 0.8443, Precision: 0.8381, Recall: 0.7677, F1: 0.7926
Testing Loss: 0.5614, Accuracy: 0.8599, Precision: 0.8465, Recall: 0.7882, F1: 0.8108
LM Predictions:  [0, 4, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 2, 0, 2, 0, 2, 4, 0, 5, 0, 5, 2, 1, 0, 5, 5, 5, 5, 0, 4, 5, 3, 5, 0, 5, 0, 5, 2, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.7336, Accuracy: 0.2381, Precision: 0.5694, Recall: 0.2310, F1: 0.2452
Epoch 11/70
Train Loss: 0.1724, Accuracy: 0.9455, Precision: 0.9112, Recall: 0.9309, F1: 0.9188
Validation Loss: 0.6039, Accuracy: 0.8827, Precision: 0.8590, Recall: 0.8354, F1: 0.8455
Testing Loss: 0.5218, Accuracy: 0.8659, Precision: 0.8306, Recall: 0.7957, F1: 0.8083
LM Predictions:  [0, 1, 5, 0, 5, 0, 5, 5, 5, 5, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 0, 3, 0, 5, 5, 0, 4, 0, 3, 0, 0, 5, 0, 5, 3, 5, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 2.2367, Accuracy: 0.3810, Precision: 0.6741, Recall: 0.3329, F1: 0.3723
Epoch 12/70
Train Loss: 0.1480, Accuracy: 0.9502, Precision: 0.9188, Recall: 0.9295, F1: 0.9233
Validation Loss: 0.7002, Accuracy: 0.8721, Precision: 0.8428, Recall: 0.8028, F1: 0.8187
Testing Loss: 0.5715, Accuracy: 0.8732, Precision: 0.8476, Recall: 0.8088, F1: 0.8254
LM Predictions:  [0, 0, 3, 5, 3, 5, 5, 5, 4, 5, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 3, 0, 3, 1, 1, 3, 0, 5, 5, 0, 4, 3, 3, 5, 0, 0, 0, 5, 3, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7982, Accuracy: 0.4286, Precision: 0.6477, Recall: 0.3662, F1: 0.3984
Epoch 13/70
Train Loss: 0.1450, Accuracy: 0.9507, Precision: 0.9179, Recall: 0.9307, F1: 0.9231
Validation Loss: 0.6024, Accuracy: 0.8785, Precision: 0.8438, Recall: 0.8358, F1: 0.8374
Testing Loss: 0.4934, Accuracy: 0.8756, Precision: 0.8360, Recall: 0.8321, F1: 0.8336
LM Predictions:  [0, 1, 3, 0, 5, 5, 5, 5, 4, 5, 0, 0, 2, 5, 3, 0, 2, 4, 0, 3, 5, 4, 3, 1, 1, 1, 0, 5, 5, 0, 4, 3, 3, 5, 5, 4, 0, 5, 3, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.6529, Accuracy: 0.4762, Precision: 0.6389, Recall: 0.4014, F1: 0.4615
Epoch 14/70
Train Loss: 0.1271, Accuracy: 0.9559, Precision: 0.9251, Recall: 0.9389, F1: 0.9312
Validation Loss: 0.6922, Accuracy: 0.8657, Precision: 0.8500, Recall: 0.7960, F1: 0.8168
Testing Loss: 0.6275, Accuracy: 0.8635, Precision: 0.8345, Recall: 0.7755, F1: 0.7953
LM Predictions:  [0, 1, 5, 0, 0, 0, 0, 0, 4, 5, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 0, 0, 0, 5, 0, 4, 1, 3, 0, 0, 0, 0, 5, 3, 3, 5, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7275, Accuracy: 0.4048, Precision: 0.6136, Recall: 0.3662, F1: 0.3769
Epoch 15/70
Train Loss: 0.1176, Accuracy: 0.9576, Precision: 0.9266, Recall: 0.9424, F1: 0.9334
Validation Loss: 0.6319, Accuracy: 0.8742, Precision: 0.8368, Recall: 0.8151, F1: 0.8247
Testing Loss: 0.5470, Accuracy: 0.8647, Precision: 0.8242, Recall: 0.7973, F1: 0.8079
LM Predictions:  [0, 1, 5, 0, 1, 0, 0, 4, 4, 5, 0, 0, 2, 5, 3, 0, 2, 4, 3, 3, 0, 0, 3, 1, 1, 3, 0, 4, 5, 0, 4, 3, 3, 5, 0, 4, 0, 5, 3, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0919, Accuracy: 0.5952, Precision: 0.6971, Recall: 0.5051, F1: 0.5387
Epoch 16/70
Train Loss: 0.1177, Accuracy: 0.9568, Precision: 0.9274, Recall: 0.9403, F1: 0.9330
Validation Loss: 0.6906, Accuracy: 0.8742, Precision: 0.8413, Recall: 0.8371, F1: 0.8390
Testing Loss: 0.5694, Accuracy: 0.8768, Precision: 0.8377, Recall: 0.8267, F1: 0.8316
LM Predictions:  [0, 1, 5, 3, 1, 3, 0, 4, 4, 5, 0, 0, 2, 5, 3, 0, 2, 4, 3, 3, 0, 0, 3, 1, 3, 3, 0, 4, 5, 0, 4, 3, 3, 5, 5, 4, 1, 5, 3, 2, 4, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.0647, Accuracy: 0.5952, Precision: 0.6563, Recall: 0.5074, F1: 0.5377
Epoch 17/70
Train Loss: 0.1118, Accuracy: 0.9568, Precision: 0.9291, Recall: 0.9375, F1: 0.9329
Validation Loss: 0.7450, Accuracy: 0.8657, Precision: 0.8295, Recall: 0.8072, F1: 0.8168
Testing Loss: 0.6149, Accuracy: 0.8696, Precision: 0.8354, Recall: 0.8177, F1: 0.8258
LM Predictions:  [0, 1, 3, 0, 1, 3, 0, 4, 4, 5, 0, 0, 2, 5, 3, 0, 2, 4, 3, 3, 0, 0, 3, 1, 0, 5, 0, 4, 2, 0, 4, 3, 3, 5, 3, 4, 0, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.1337, Accuracy: 0.6190, Precision: 0.6722, Recall: 0.5282, F1: 0.5509
Epoch 18/70
Train Loss: 0.1027, Accuracy: 0.9568, Precision: 0.9306, Recall: 0.9377, F1: 0.9339
Validation Loss: 0.6942, Accuracy: 0.8699, Precision: 0.8325, Recall: 0.8322, F1: 0.8308
Testing Loss: 0.6002, Accuracy: 0.8659, Precision: 0.8338, Recall: 0.8204, F1: 0.8253
LM Predictions:  [0, 1, 5, 0, 1, 1, 0, 4, 4, 5, 0, 0, 2, 5, 3, 3, 2, 4, 1, 3, 0, 0, 3, 1, 0, 2, 5, 4, 5, 0, 4, 3, 3, 5, 1, 0, 5, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.8895, Accuracy: 0.6429, Precision: 0.7095, Recall: 0.5486, F1: 0.5905
Epoch 19/70
Train Loss: 0.1067, Accuracy: 0.9566, Precision: 0.9300, Recall: 0.9376, F1: 0.9333
Validation Loss: 0.7851, Accuracy: 0.8593, Precision: 0.8222, Recall: 0.7711, F1: 0.7874
Testing Loss: 0.6533, Accuracy: 0.8635, Precision: 0.8206, Recall: 0.7787, F1: 0.7904
LM Predictions:  [0, 1, 3, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 1, 3, 0, 4, 2, 0, 4, 3, 3, 0, 3, 0, 3, 3, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.7210, Accuracy: 0.7143, Precision: 0.8064, Recall: 0.7383, F1: 0.7206
Epoch 20/70
Train Loss: 0.0943, Accuracy: 0.9621, Precision: 0.9386, Recall: 0.9450, F1: 0.9416
Validation Loss: 0.7804, Accuracy: 0.8763, Precision: 0.8412, Recall: 0.8004, F1: 0.8160
Testing Loss: 0.6440, Accuracy: 0.8635, Precision: 0.8313, Recall: 0.7826, F1: 0.7975
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 1, 0, 2, 0, 3, 3, 2, 4, 3, 3, 0, 4, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 2, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3840, Accuracy: 0.8571, Precision: 0.8661, Recall: 0.8750, F1: 0.8498
Epoch 21/70
Train Loss: 0.0852, Accuracy: 0.9654, Precision: 0.9432, Recall: 0.9470, F1: 0.9450
Validation Loss: 0.8275, Accuracy: 0.8742, Precision: 0.8404, Recall: 0.8143, F1: 0.8254
Testing Loss: 0.7200, Accuracy: 0.8647, Precision: 0.8343, Recall: 0.7982, F1: 0.8127
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 5, 0, 4, 3, 3, 1, 0, 0, 1, 2, 3, 0, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5377, Accuracy: 0.6905, Precision: 0.6717, Recall: 0.5856, F1: 0.5896
Epoch 22/70
Train Loss: 0.0883, Accuracy: 0.9623, Precision: 0.9386, Recall: 0.9430, F1: 0.9406
Validation Loss: 0.8568, Accuracy: 0.8571, Precision: 0.8255, Recall: 0.7678, F1: 0.7876
Testing Loss: 0.7933, Accuracy: 0.8611, Precision: 0.8265, Recall: 0.7684, F1: 0.7848
LM Predictions:  [0, 1, 0, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3578, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8120
Epoch 23/70
Train Loss: 0.0791, Accuracy: 0.9635, Precision: 0.9398, Recall: 0.9448, F1: 0.9422
Validation Loss: 0.8896, Accuracy: 0.8678, Precision: 0.8287, Recall: 0.8071, F1: 0.8164
Testing Loss: 0.8487, Accuracy: 0.8659, Precision: 0.8292, Recall: 0.7976, F1: 0.8108
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 5, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4753, Accuracy: 0.8333, Precision: 0.7424, Recall: 0.7069, F1: 0.6970
Epoch 24/70
Train Loss: 0.0810, Accuracy: 0.9635, Precision: 0.9429, Recall: 0.9418, F1: 0.9423
Validation Loss: 0.7889, Accuracy: 0.8486, Precision: 0.8067, Recall: 0.7930, F1: 0.7982
Testing Loss: 0.6821, Accuracy: 0.8527, Precision: 0.8123, Recall: 0.7804, F1: 0.7920
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 1, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3506, Accuracy: 0.9286, Precision: 0.7815, Recall: 0.7606, F1: 0.7704
Epoch 25/70
Train Loss: 0.0761, Accuracy: 0.9675, Precision: 0.9476, Recall: 0.9503, F1: 0.9490
Validation Loss: 0.7884, Accuracy: 0.8763, Precision: 0.8497, Recall: 0.8213, F1: 0.8329
Testing Loss: 0.7063, Accuracy: 0.8732, Precision: 0.8524, Recall: 0.8174, F1: 0.8323
LM Predictions:  [0, 1, 2, 5, 1, 1, 0, 4, 4, 5, 0, 0, 2, 0, 5, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5792, Accuracy: 0.7619, Precision: 0.7273, Recall: 0.6463, F1: 0.6551
Epoch 26/70
Train Loss: 0.0766, Accuracy: 0.9632, Precision: 0.9402, Recall: 0.9424, F1: 0.9413
Validation Loss: 0.8186, Accuracy: 0.8593, Precision: 0.8085, Recall: 0.8031, F1: 0.8055
Testing Loss: 0.6716, Accuracy: 0.8696, Precision: 0.8341, Recall: 0.8061, F1: 0.8172
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3652, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7911, F1: 0.7706
Epoch 27/70
Train Loss: 0.0856, Accuracy: 0.9621, Precision: 0.9430, Recall: 0.9352, F1: 0.9389
Validation Loss: 0.7704, Accuracy: 0.8635, Precision: 0.8312, Recall: 0.8009, F1: 0.8136
Testing Loss: 0.6857, Accuracy: 0.8647, Precision: 0.8270, Recall: 0.8077, F1: 0.8162
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 0, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3942, Accuracy: 0.7619, Precision: 0.7179, Recall: 0.6403, F1: 0.6459
Epoch 28/70
Train Loss: 0.0808, Accuracy: 0.9647, Precision: 0.9432, Recall: 0.9501, F1: 0.9464
Validation Loss: 0.8716, Accuracy: 0.8571, Precision: 0.8038, Recall: 0.7670, F1: 0.7773
Testing Loss: 0.7422, Accuracy: 0.8708, Precision: 0.8455, Recall: 0.7846, F1: 0.8005
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3043, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8328, F1: 0.8138
Epoch 29/70
Train Loss: 0.0659, Accuracy: 0.9687, Precision: 0.9549, Recall: 0.9418, F1: 0.9478
Validation Loss: 0.8717, Accuracy: 0.8742, Precision: 0.8326, Recall: 0.8206, F1: 0.8257
Testing Loss: 0.7564, Accuracy: 0.8732, Precision: 0.8342, Recall: 0.8129, F1: 0.8212
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2482, Accuracy: 0.8571, Precision: 0.7407, Recall: 0.7125, F1: 0.7091
Epoch 30/70
Train Loss: 0.0667, Accuracy: 0.9677, Precision: 0.9477, Recall: 0.9529, F1: 0.9502
Validation Loss: 0.8525, Accuracy: 0.8465, Precision: 0.8206, Recall: 0.7932, F1: 0.8002
Testing Loss: 0.6525, Accuracy: 0.8623, Precision: 0.8330, Recall: 0.8026, F1: 0.8156
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4004, Accuracy: 0.7381, Precision: 0.7143, Recall: 0.6273, F1: 0.6326
Epoch 31/70
Train Loss: 0.0622, Accuracy: 0.9677, Precision: 0.9519, Recall: 0.9452, F1: 0.9484
Validation Loss: 0.9843, Accuracy: 0.8657, Precision: 0.8290, Recall: 0.8100, F1: 0.8179
Testing Loss: 0.8454, Accuracy: 0.8696, Precision: 0.8384, Recall: 0.8127, F1: 0.8242
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 5, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2677, Accuracy: 0.8333, Precision: 0.7333, Recall: 0.6963, F1: 0.6931
Epoch 32/70
Train Loss: 0.0583, Accuracy: 0.9635, Precision: 0.9442, Recall: 0.9398, F1: 0.9420
Validation Loss: 1.0453, Accuracy: 0.8593, Precision: 0.8182, Recall: 0.7900, F1: 0.8021
Testing Loss: 0.8504, Accuracy: 0.8780, Precision: 0.8468, Recall: 0.8143, F1: 0.8280
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2572, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 33/70
Train Loss: 0.0581, Accuracy: 0.9642, Precision: 0.9412, Recall: 0.9438, F1: 0.9425
Validation Loss: 1.0226, Accuracy: 0.8593, Precision: 0.8185, Recall: 0.7762, F1: 0.7922
Testing Loss: 0.9040, Accuracy: 0.8684, Precision: 0.8359, Recall: 0.7801, F1: 0.7955
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2816, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8283, F1: 0.8078
Epoch 34/70
Train Loss: 0.0683, Accuracy: 0.9628, Precision: 0.9399, Recall: 0.9449, F1: 0.9423
Validation Loss: 0.9601, Accuracy: 0.8529, Precision: 0.8302, Recall: 0.7790, F1: 0.7992
Testing Loss: 0.8279, Accuracy: 0.8635, Precision: 0.8346, Recall: 0.7974, F1: 0.8120
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 5, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2512, Accuracy: 0.9048, Precision: 0.7619, Recall: 0.7500, F1: 0.7484
Epoch 35/70
Train Loss: 0.0722, Accuracy: 0.9632, Precision: 0.9422, Recall: 0.9499, F1: 0.9458
Validation Loss: 0.9060, Accuracy: 0.8571, Precision: 0.8154, Recall: 0.7947, F1: 0.8031
Testing Loss: 0.7978, Accuracy: 0.8623, Precision: 0.8307, Recall: 0.7969, F1: 0.8112
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2151, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 36/70
Train Loss: 0.0584, Accuracy: 0.9701, Precision: 0.9512, Recall: 0.9551, F1: 0.9530
Validation Loss: 1.1287, Accuracy: 0.8614, Precision: 0.8234, Recall: 0.7809, F1: 0.7976
Testing Loss: 0.9194, Accuracy: 0.8732, Precision: 0.8421, Recall: 0.8009, F1: 0.8165
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2395, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8728, F1: 0.8499
Epoch 37/70
Train Loss: 0.0641, Accuracy: 0.9642, Precision: 0.9408, Recall: 0.9477, F1: 0.9440
Validation Loss: 0.9009, Accuracy: 0.8529, Precision: 0.8115, Recall: 0.7783, F1: 0.7921
Testing Loss: 0.7055, Accuracy: 0.8684, Precision: 0.8427, Recall: 0.7811, F1: 0.8007
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3601, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 38/70
Train Loss: 0.0540, Accuracy: 0.9673, Precision: 0.9530, Recall: 0.9452, F1: 0.9489
Validation Loss: 1.0317, Accuracy: 0.8699, Precision: 0.8397, Recall: 0.8166, F1: 0.8269
Testing Loss: 0.9064, Accuracy: 0.8696, Precision: 0.8436, Recall: 0.8222, F1: 0.8318
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2452, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8461, F1: 0.8249
Epoch 39/70
Train Loss: 0.0668, Accuracy: 0.9663, Precision: 0.9461, Recall: 0.9439, F1: 0.9450
Validation Loss: 1.0259, Accuracy: 0.8678, Precision: 0.8375, Recall: 0.7963, F1: 0.8134
Testing Loss: 0.8476, Accuracy: 0.8732, Precision: 0.8511, Recall: 0.8061, F1: 0.8240
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2809, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8728, F1: 0.8499
Epoch 40/70
Train Loss: 0.0617, Accuracy: 0.9659, Precision: 0.9465, Recall: 0.9445, F1: 0.9455
Validation Loss: 1.0582, Accuracy: 0.8699, Precision: 0.8431, Recall: 0.8024, F1: 0.8197
Testing Loss: 0.9548, Accuracy: 0.8575, Precision: 0.8328, Recall: 0.7758, F1: 0.7969
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 0, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3123, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7711, F1: 0.7492
Epoch 41/70
Train Loss: 0.0647, Accuracy: 0.9659, Precision: 0.9427, Recall: 0.9462, F1: 0.9444
Validation Loss: 1.0103, Accuracy: 0.8571, Precision: 0.8283, Recall: 0.7761, F1: 0.7965
Testing Loss: 0.9380, Accuracy: 0.8587, Precision: 0.8286, Recall: 0.7791, F1: 0.7969
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2446, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8883, F1: 0.8695
Epoch 42/70
Train Loss: 0.0752, Accuracy: 0.9609, Precision: 0.9397, Recall: 0.9364, F1: 0.9380
Validation Loss: 1.0104, Accuracy: 0.8657, Precision: 0.8297, Recall: 0.8015, F1: 0.8138
Testing Loss: 0.8878, Accuracy: 0.8623, Precision: 0.8235, Recall: 0.7860, F1: 0.8006
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1989, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 43/70
Train Loss: 0.0577, Accuracy: 0.9668, Precision: 0.9448, Recall: 0.9496, F1: 0.9471
Validation Loss: 1.0928, Accuracy: 0.8614, Precision: 0.8280, Recall: 0.7898, F1: 0.8054
Testing Loss: 1.0000, Accuracy: 0.8563, Precision: 0.8220, Recall: 0.7745, F1: 0.7906
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2444, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8728, F1: 0.8499
Epoch 44/70
Train Loss: 0.0685, Accuracy: 0.9694, Precision: 0.9511, Recall: 0.9567, F1: 0.9538
Validation Loss: 0.9803, Accuracy: 0.8529, Precision: 0.8194, Recall: 0.8009, F1: 0.8089
Testing Loss: 0.8889, Accuracy: 0.8527, Precision: 0.8212, Recall: 0.7939, F1: 0.8052
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2157, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8928, F1: 0.8755
Epoch 45/70
Train Loss: 0.0590, Accuracy: 0.9673, Precision: 0.9423, Recall: 0.9530, F1: 0.9472
Validation Loss: 0.9556, Accuracy: 0.8614, Precision: 0.8227, Recall: 0.7985, F1: 0.8092
Testing Loss: 0.7435, Accuracy: 0.8671, Precision: 0.8354, Recall: 0.7981, F1: 0.8138
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4560, Accuracy: 0.7143, Precision: 0.8588, Recall: 0.7511, F1: 0.7338
Epoch 46/70
Train Loss: 0.0529, Accuracy: 0.9689, Precision: 0.9540, Recall: 0.9424, F1: 0.9478
Validation Loss: 1.0799, Accuracy: 0.8529, Precision: 0.8355, Recall: 0.7863, F1: 0.8057
Testing Loss: 0.9345, Accuracy: 0.8539, Precision: 0.8234, Recall: 0.7725, F1: 0.7911
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 1, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2078, Accuracy: 0.8810, Precision: 0.8911, Recall: 0.9000, F1: 0.8718
Epoch 47/70
Train Loss: 0.0557, Accuracy: 0.9649, Precision: 0.9425, Recall: 0.9442, F1: 0.9433
Validation Loss: 1.0216, Accuracy: 0.8550, Precision: 0.8229, Recall: 0.7947, F1: 0.8057
Testing Loss: 0.9472, Accuracy: 0.8684, Precision: 0.8333, Recall: 0.8046, F1: 0.8167
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1883, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9800, F1: 0.9713
Epoch 48/70
Train Loss: 0.0517, Accuracy: 0.9647, Precision: 0.9416, Recall: 0.9501, F1: 0.9455
Validation Loss: 1.1924, Accuracy: 0.8507, Precision: 0.8239, Recall: 0.7775, F1: 0.7965
Testing Loss: 1.0356, Accuracy: 0.8575, Precision: 0.8220, Recall: 0.7825, F1: 0.7971
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2485, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8133, F1: 0.7930
Epoch 49/70
Train Loss: 0.0480, Accuracy: 0.9704, Precision: 0.9471, Recall: 0.9628, F1: 0.9543
Validation Loss: 1.1061, Accuracy: 0.8593, Precision: 0.8246, Recall: 0.7785, F1: 0.7948
Testing Loss: 1.0199, Accuracy: 0.8659, Precision: 0.8275, Recall: 0.7796, F1: 0.7933
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2181, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9000, F1: 0.8728
Epoch 50/70
Train Loss: 0.0464, Accuracy: 0.9680, Precision: 0.9523, Recall: 0.9417, F1: 0.9468
Validation Loss: 1.1330, Accuracy: 0.8657, Precision: 0.8327, Recall: 0.8065, F1: 0.8166
Testing Loss: 1.0279, Accuracy: 0.8744, Precision: 0.8329, Recall: 0.8140, F1: 0.8223
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1637, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9778, F1: 0.9701
Epoch 51/70
Train Loss: 0.0467, Accuracy: 0.9682, Precision: 0.9485, Recall: 0.9477, F1: 0.9481
Validation Loss: 1.2201, Accuracy: 0.8678, Precision: 0.8414, Recall: 0.8013, F1: 0.8165
Testing Loss: 1.1073, Accuracy: 0.8635, Precision: 0.8214, Recall: 0.7846, F1: 0.7957
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2292, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8533, F1: 0.8276
Epoch 52/70
Train Loss: 0.0488, Accuracy: 0.9682, Precision: 0.9469, Recall: 0.9524, F1: 0.9496
Validation Loss: 1.2210, Accuracy: 0.8593, Precision: 0.8244, Recall: 0.7926, F1: 0.8059
Testing Loss: 1.0632, Accuracy: 0.8623, Precision: 0.8195, Recall: 0.7770, F1: 0.7902
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2478, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8400, F1: 0.8111
Epoch 53/70
Train Loss: 0.0508, Accuracy: 0.9687, Precision: 0.9525, Recall: 0.9464, F1: 0.9493
Validation Loss: 1.2079, Accuracy: 0.8571, Precision: 0.8193, Recall: 0.7892, F1: 0.8014
Testing Loss: 1.1569, Accuracy: 0.8514, Precision: 0.8210, Recall: 0.7696, F1: 0.7892
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3148, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 54/70
Train Loss: 0.0584, Accuracy: 0.9659, Precision: 0.9440, Recall: 0.9476, F1: 0.9457
Validation Loss: 0.9978, Accuracy: 0.8571, Precision: 0.8197, Recall: 0.7899, F1: 0.8023
Testing Loss: 0.9228, Accuracy: 0.8671, Precision: 0.8378, Recall: 0.7856, F1: 0.8024
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 0, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2808, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7733, F1: 0.7552
Epoch 55/70
Train Loss: 0.0575, Accuracy: 0.9649, Precision: 0.9394, Recall: 0.9515, F1: 0.9447
Validation Loss: 1.2837, Accuracy: 0.8465, Precision: 0.8068, Recall: 0.7662, F1: 0.7787
Testing Loss: 1.1472, Accuracy: 0.8575, Precision: 0.8243, Recall: 0.7811, F1: 0.7968
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2860, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8040
Epoch 56/70
Train Loss: 0.0539, Accuracy: 0.9656, Precision: 0.9475, Recall: 0.9430, F1: 0.9452
Validation Loss: 1.1340, Accuracy: 0.8550, Precision: 0.8136, Recall: 0.8039, F1: 0.8072
Testing Loss: 0.9725, Accuracy: 0.8696, Precision: 0.8403, Recall: 0.8197, F1: 0.8290
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1993, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0511, Accuracy: 0.9654, Precision: 0.9461, Recall: 0.9427, F1: 0.9443
Validation Loss: 1.2101, Accuracy: 0.8614, Precision: 0.8198, Recall: 0.8100, F1: 0.8139
Testing Loss: 1.1179, Accuracy: 0.8659, Precision: 0.8330, Recall: 0.8027, F1: 0.8157
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2053, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9111, F1: 0.8857
Epoch 58/70
Train Loss: 0.0531, Accuracy: 0.9670, Precision: 0.9453, Recall: 0.9519, F1: 0.9483
Validation Loss: 1.1932, Accuracy: 0.8507, Precision: 0.8062, Recall: 0.7984, F1: 0.8012
Testing Loss: 1.0969, Accuracy: 0.8527, Precision: 0.8099, Recall: 0.7914, F1: 0.7988
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2405, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8800, F1: 0.8528
Epoch 59/70
Train Loss: 0.0503, Accuracy: 0.9696, Precision: 0.9510, Recall: 0.9504, F1: 0.9507
Validation Loss: 1.2225, Accuracy: 0.8571, Precision: 0.8217, Recall: 0.7847, F1: 0.7988
Testing Loss: 1.1382, Accuracy: 0.8599, Precision: 0.8329, Recall: 0.7865, F1: 0.8035
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 5, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 3, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.4099, Accuracy: 0.7619, Precision: 0.7070, Recall: 0.6509, F1: 0.6388
Epoch 60/70
Train Loss: 0.0617, Accuracy: 0.9654, Precision: 0.9488, Recall: 0.9457, F1: 0.9472
Validation Loss: 1.1772, Accuracy: 0.8486, Precision: 0.7985, Recall: 0.7665, F1: 0.7773
Testing Loss: 1.0717, Accuracy: 0.8478, Precision: 0.8219, Recall: 0.7608, F1: 0.7776
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 0, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2663, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.8950, F1: 0.8700
Epoch 61/70
Train Loss: 0.0628, Accuracy: 0.9654, Precision: 0.9515, Recall: 0.9350, F1: 0.9423
Validation Loss: 1.1410, Accuracy: 0.8571, Precision: 0.8316, Recall: 0.7908, F1: 0.8066
Testing Loss: 0.9777, Accuracy: 0.8551, Precision: 0.8362, Recall: 0.7916, F1: 0.8102
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 1, 2, 0, 4, 0, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 0, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2935, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8050, F1: 0.7869
Epoch 62/70
Train Loss: 0.0537, Accuracy: 0.9715, Precision: 0.9544, Recall: 0.9552, F1: 0.9548
Validation Loss: 1.1266, Accuracy: 0.8507, Precision: 0.8113, Recall: 0.7897, F1: 0.7992
Testing Loss: 0.9812, Accuracy: 0.8575, Precision: 0.8292, Recall: 0.7971, F1: 0.8108
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1992, Accuracy: 0.9524, Precision: 0.9429, Recall: 0.9600, F1: 0.9444
Epoch 63/70
Train Loss: 0.0547, Accuracy: 0.9644, Precision: 0.9406, Recall: 0.9496, F1: 0.9448
Validation Loss: 1.1331, Accuracy: 0.8465, Precision: 0.7947, Recall: 0.7558, F1: 0.7683
Testing Loss: 1.0301, Accuracy: 0.8394, Precision: 0.8123, Recall: 0.7460, F1: 0.7642
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3312, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7629
Epoch 64/70
Train Loss: 0.0473, Accuracy: 0.9685, Precision: 0.9475, Recall: 0.9562, F1: 0.9516
Validation Loss: 1.1586, Accuracy: 0.8593, Precision: 0.8151, Recall: 0.7953, F1: 0.8042
Testing Loss: 1.0728, Accuracy: 0.8514, Precision: 0.8176, Recall: 0.7782, F1: 0.7936
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 4, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2192, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8506, F1: 0.8293
Epoch 65/70
Train Loss: 0.0477, Accuracy: 0.9692, Precision: 0.9586, Recall: 0.9390, F1: 0.9475
Validation Loss: 1.1948, Accuracy: 0.8550, Precision: 0.7989, Recall: 0.7811, F1: 0.7890
Testing Loss: 1.0268, Accuracy: 0.8587, Precision: 0.8308, Recall: 0.7841, F1: 0.8016
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 2, 0, 4, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2395, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8311, F1: 0.8081
Epoch 66/70
Train Loss: 0.0682, Accuracy: 0.9659, Precision: 0.9528, Recall: 0.9310, F1: 0.9403
Validation Loss: 1.0912, Accuracy: 0.8422, Precision: 0.8119, Recall: 0.7807, F1: 0.7916
Testing Loss: 0.9905, Accuracy: 0.8514, Precision: 0.8167, Recall: 0.7846, F1: 0.7984
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 5, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 0, 5, 2, 0, 4, 3, 3, 2, 1, 0, 1, 5, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.5098, Accuracy: 0.7857, Precision: 0.7424, Recall: 0.6815, F1: 0.6711
Epoch 67/70
Train Loss: 0.0702, Accuracy: 0.9654, Precision: 0.9458, Recall: 0.9445, F1: 0.9451
Validation Loss: 1.0386, Accuracy: 0.8678, Precision: 0.8254, Recall: 0.8181, F1: 0.8212
Testing Loss: 0.9590, Accuracy: 0.8647, Precision: 0.8317, Recall: 0.8073, F1: 0.8174
LM Predictions:  [0, 1, 2, 0, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 0, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 0, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.3002, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.7861, F1: 0.7690
Epoch 68/70
Train Loss: 0.0508, Accuracy: 0.9687, Precision: 0.9465, Recall: 0.9555, F1: 0.9507
Validation Loss: 1.1494, Accuracy: 0.8657, Precision: 0.8242, Recall: 0.8027, F1: 0.8121
Testing Loss: 1.0091, Accuracy: 0.8563, Precision: 0.8205, Recall: 0.7820, F1: 0.7961
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 0, 2, 4, 0, 3, 0, 0, 3, 1, 0, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2391, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8528, F1: 0.8320
Epoch 69/70
Train Loss: 0.0477, Accuracy: 0.9694, Precision: 0.9509, Recall: 0.9500, F1: 0.9504
Validation Loss: 1.1363, Accuracy: 0.8507, Precision: 0.7913, Recall: 0.7775, F1: 0.7817
Testing Loss: 0.9602, Accuracy: 0.8539, Precision: 0.8137, Recall: 0.7686, F1: 0.7779
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 0, 3, 3, 2, 4, 3, 3, 3, 0, 3, 1, 0, 2, 0, 4, 2, 0, 4, 3, 3, 2, 0, 0, 0, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.2269, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8533, F1: 0.8276
Epoch 70/70
Train Loss: 0.0455, Accuracy: 0.9687, Precision: 0.9553, Recall: 0.9426, F1: 0.9486
Validation Loss: 1.2076, Accuracy: 0.8593, Precision: 0.8109, Recall: 0.8032, F1: 0.8065
Testing Loss: 1.0428, Accuracy: 0.8611, Precision: 0.8291, Recall: 0.8004, F1: 0.8129
LM Predictions:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 0, 0, 2, 4, 3, 3, 2, 4, 0, 3, 0, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 0, 1, 4, 1, 4, 3, 2, 1, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 0.1967, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9150, F1: 0.8942

