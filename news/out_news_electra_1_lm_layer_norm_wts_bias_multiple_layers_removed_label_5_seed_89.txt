Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 1: 1011
  Label 0: 1141
  Label 2: 966
  Label 4: 344
  Label 3: 495
  Label 5: 260
Label counts for Validation:
  Label 2: 107
  Label 0: 127
  Label 1: 113
  Label 5: 29
  Label 4: 38
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 1: 1025
  Label 0: 1150
  Label 2: 972
  Label 4: 351
  Label 3: 501
  Label 5: 218
For early layers:  [0, 1, 2, 3]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.6385, Accuracy: 0.3000, Precision: 0.1467, Recall: 0.1928, F1: 0.1538
Validation Loss: 1.5740, Accuracy: 0.3753, Precision: 0.1972, Recall: 0.2393, F1: 0.1769
Testing Loss: 1.5503, Accuracy: 0.3816, Precision: 0.1927, Recall: 0.2439, F1: 0.1797
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9358, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 2/70
Train Loss: 1.3611, Accuracy: 0.4458, Precision: 0.2762, Recall: 0.2922, F1: 0.2389
Validation Loss: 1.2571, Accuracy: 0.4797, Precision: 0.2462, Recall: 0.3387, F1: 0.2716
Testing Loss: 1.1725, Accuracy: 0.5036, Precision: 0.3681, Recall: 0.3591, F1: 0.2956
LM Predictions:  [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3137, Accuracy: 0.2143, Precision: 0.0462, Recall: 0.2000, F1: 0.0750
Epoch 3/70
Train Loss: 1.1349, Accuracy: 0.5649, Precision: 0.3853, Recall: 0.4300, F1: 0.4019
Validation Loss: 1.0514, Accuracy: 0.6588, Precision: 0.4518, Recall: 0.5126, F1: 0.4754
Testing Loss: 0.9358, Accuracy: 0.7101, Precision: 0.4970, Recall: 0.5598, F1: 0.5215
LM Predictions:  [0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 4, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4324, Accuracy: 0.1667, Precision: 0.2525, Recall: 0.1730, F1: 0.1270
Epoch 4/70
Train Loss: 0.8982, Accuracy: 0.7102, Precision: 0.4898, Recall: 0.5540, F1: 0.5161
Validation Loss: 0.9308, Accuracy: 0.7100, Precision: 0.5200, Recall: 0.5457, F1: 0.5195
Testing Loss: 0.8018, Accuracy: 0.7440, Precision: 0.5344, Recall: 0.5823, F1: 0.5497
LM Predictions:  [0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0588, Accuracy: 0.1905, Precision: 0.0778, Recall: 0.1889, F1: 0.0972
Epoch 5/70
Train Loss: 0.8314, Accuracy: 0.7282, Precision: 0.4864, Recall: 0.5805, F1: 0.5269
Validation Loss: 0.8861, Accuracy: 0.7122, Precision: 0.4678, Recall: 0.5802, F1: 0.5137
Testing Loss: 0.7961, Accuracy: 0.7415, Precision: 0.4860, Recall: 0.6040, F1: 0.5337
LM Predictions:  [0, 4, 4, 4, 1, 4, 0, 4, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4, 4, 0, 4, 0, 0, 0, 2, 0, 0, 2, 4, 0, 4, 2, 0, 4, 2, 2, 4, 2, 4, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8185, Accuracy: 0.1429, Precision: 0.0667, Recall: 0.1460, F1: 0.0914
Epoch 6/70
Train Loss: 0.7347, Accuracy: 0.7617, Precision: 0.6005, Recall: 0.6313, F1: 0.6074
Validation Loss: 0.7421, Accuracy: 0.7612, Precision: 0.6221, Recall: 0.6544, F1: 0.6321
Testing Loss: 0.6549, Accuracy: 0.7923, Precision: 0.6449, Recall: 0.6873, F1: 0.6629
LM Predictions:  [3, 2, 2, 2, 3, 2, 3, 3, 3, 0, 0, 0, 3, 3, 0, 3, 3, 3, 0, 3, 0, 3, 0, 3, 3, 3, 0, 0, 2, 3, 0, 3, 3, 0, 0, 3, 3, 3, 2, 3, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0568, Accuracy: 0.1190, Precision: 0.0801, Recall: 0.1444, F1: 0.0967
Epoch 7/70
Train Loss: 0.6066, Accuracy: 0.8205, Precision: 0.6676, Recall: 0.7057, F1: 0.6851
Validation Loss: 0.6096, Accuracy: 0.8102, Precision: 0.6701, Recall: 0.7005, F1: 0.6836
Testing Loss: 0.5316, Accuracy: 0.8490, Precision: 0.7047, Recall: 0.7441, F1: 0.7231
LM Predictions:  [0, 4, 4, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, 2, 4, 0, 4, 2, 0, 0, 0, 3, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6024, Accuracy: 0.1190, Precision: 0.1022, Recall: 0.1238, F1: 0.1000
Epoch 8/70
Train Loss: 0.5429, Accuracy: 0.8397, Precision: 0.7247, Recall: 0.7254, F1: 0.7114
Validation Loss: 0.6267, Accuracy: 0.8038, Precision: 0.6452, Recall: 0.6999, F1: 0.6700
Testing Loss: 0.4913, Accuracy: 0.8502, Precision: 0.6955, Recall: 0.7513, F1: 0.7210
LM Predictions:  [0, 4, 4, 4, 0, 4, 4, 4, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 4, 0, 0, 4, 0, 0, 0, 4, 0, 0, 4, 4, 0, 4, 4, 4, 0, 4, 3, 0, 2, 4, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7060, Accuracy: 0.2143, Precision: 0.0899, Recall: 0.2254, F1: 0.1285
Epoch 9/70
Train Loss: 0.4987, Accuracy: 0.8516, Precision: 0.7759, Recall: 0.7464, F1: 0.7444
Validation Loss: 0.6436, Accuracy: 0.8060, Precision: 0.7787, Recall: 0.7153, F1: 0.7351
Testing Loss: 0.5500, Accuracy: 0.8478, Precision: 0.8285, Recall: 0.7731, F1: 0.7885
LM Predictions:  [0, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 5, 5, 0, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 4, 0, 5, 0, 5, 0, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5893, Accuracy: 0.1905, Precision: 0.2067, Recall: 0.1587, F1: 0.1329
Epoch 10/70
Train Loss: 0.4681, Accuracy: 0.8646, Precision: 0.8075, Recall: 0.7904, F1: 0.7975
Validation Loss: 0.6097, Accuracy: 0.8252, Precision: 0.7785, Recall: 0.8019, F1: 0.7871
Testing Loss: 0.5153, Accuracy: 0.8563, Precision: 0.8032, Recall: 0.8192, F1: 0.8093
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4389, Accuracy: 0.0476, Precision: 0.0667, Recall: 0.0370, F1: 0.0476
Epoch 11/70
Train Loss: 0.4495, Accuracy: 0.8672, Precision: 0.8059, Recall: 0.7842, F1: 0.7926
Validation Loss: 0.7449, Accuracy: 0.8017, Precision: 0.7720, Recall: 0.7119, F1: 0.7254
Testing Loss: 0.6345, Accuracy: 0.8333, Precision: 0.8086, Recall: 0.7427, F1: 0.7536
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 5, 5, 0, 5, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8929, Accuracy: 0.1905, Precision: 0.0476, Recall: 0.1481, F1: 0.0721
Epoch 12/70
Train Loss: 0.4385, Accuracy: 0.8696, Precision: 0.8174, Recall: 0.7995, F1: 0.8074
Validation Loss: 0.6355, Accuracy: 0.8230, Precision: 0.7937, Recall: 0.7331, F1: 0.7486
Testing Loss: 0.5201, Accuracy: 0.8575, Precision: 0.8448, Recall: 0.7703, F1: 0.7880
LM Predictions:  [3, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 0, 5, 5, 5, 0, 5, 2, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8395, Accuracy: 0.0952, Precision: 0.0370, Recall: 0.0741, F1: 0.0494
Epoch 13/70
Train Loss: 0.3921, Accuracy: 0.8831, Precision: 0.8306, Recall: 0.8243, F1: 0.8271
Validation Loss: 0.6168, Accuracy: 0.8038, Precision: 0.7729, Recall: 0.7055, F1: 0.7130
Testing Loss: 0.4837, Accuracy: 0.8514, Precision: 0.8779, Recall: 0.7636, F1: 0.7763
LM Predictions:  [2, 0, 5, 4, 5, 5, 5, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 0, 0, 5, 0, 0, 0, 0, 5, 5, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7623, Accuracy: 0.2143, Precision: 0.2932, Recall: 0.1812, F1: 0.1495
Epoch 14/70
Train Loss: 0.3718, Accuracy: 0.8921, Precision: 0.8452, Recall: 0.8329, F1: 0.8385
Validation Loss: 0.5826, Accuracy: 0.8252, Precision: 0.7903, Recall: 0.7687, F1: 0.7780
Testing Loss: 0.4576, Accuracy: 0.8732, Precision: 0.8419, Recall: 0.8158, F1: 0.8262
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 5, 5, 0, 5, 3, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 3, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6449, Accuracy: 0.1190, Precision: 0.1346, Recall: 0.1019, F1: 0.1023
Epoch 15/70
Train Loss: 0.3520, Accuracy: 0.8992, Precision: 0.8513, Recall: 0.8493, F1: 0.8501
Validation Loss: 0.6082, Accuracy: 0.8443, Precision: 0.8112, Recall: 0.8123, F1: 0.8099
Testing Loss: 0.4676, Accuracy: 0.8792, Precision: 0.8375, Recall: 0.8313, F1: 0.8339
LM Predictions:  [2, 5, 5, 4, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3176, Accuracy: 0.0714, Precision: 0.1250, Recall: 0.0648, F1: 0.0809
Epoch 16/70
Train Loss: 0.3305, Accuracy: 0.9040, Precision: 0.8572, Recall: 0.8600, F1: 0.8578
Validation Loss: 0.6623, Accuracy: 0.8316, Precision: 0.8027, Recall: 0.7500, F1: 0.7597
Testing Loss: 0.4888, Accuracy: 0.8780, Precision: 0.8589, Recall: 0.8079, F1: 0.8232
LM Predictions:  [5, 5, 4, 4, 5, 5, 4, 0, 0, 0, 0, 0, 0, 4, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 4, 5, 0, 4, 0, 5, 0, 5, 0, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5820, Accuracy: 0.2143, Precision: 0.1139, Recall: 0.1772, F1: 0.1317
Epoch 17/70
Train Loss: 0.3513, Accuracy: 0.8976, Precision: 0.8507, Recall: 0.8379, F1: 0.8436
Validation Loss: 0.6420, Accuracy: 0.8252, Precision: 0.7944, Recall: 0.7361, F1: 0.7504
Testing Loss: 0.4702, Accuracy: 0.8647, Precision: 0.8500, Recall: 0.7798, F1: 0.7976
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 5, 0, 0, 5, 2, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4239, Accuracy: 0.2381, Precision: 0.2806, Recall: 0.2050, F1: 0.1916
Epoch 18/70
Train Loss: 0.3403, Accuracy: 0.9006, Precision: 0.8584, Recall: 0.8580, F1: 0.8578
Validation Loss: 0.6226, Accuracy: 0.8443, Precision: 0.7995, Recall: 0.8073, F1: 0.8022
Testing Loss: 0.4982, Accuracy: 0.8671, Precision: 0.8259, Recall: 0.8320, F1: 0.8287
LM Predictions:  [2, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 5, 0, 4, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 3, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5767, Accuracy: 0.1429, Precision: 0.3611, Recall: 0.1310, F1: 0.1810
Epoch 19/70
Train Loss: 0.3177, Accuracy: 0.9092, Precision: 0.8636, Recall: 0.8701, F1: 0.8661
Validation Loss: 0.6366, Accuracy: 0.8337, Precision: 0.8053, Recall: 0.7984, F1: 0.7998
Testing Loss: 0.5288, Accuracy: 0.8611, Precision: 0.8274, Recall: 0.8142, F1: 0.8183
LM Predictions:  [2, 5, 5, 4, 5, 5, 4, 5, 5, 0, 0, 5, 0, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4421, Accuracy: 0.1905, Precision: 0.3869, Recall: 0.1733, F1: 0.2219
Epoch 20/70
Train Loss: 0.2910, Accuracy: 0.9149, Precision: 0.8744, Recall: 0.8805, F1: 0.8762
Validation Loss: 0.6877, Accuracy: 0.8337, Precision: 0.8031, Recall: 0.7530, F1: 0.7675
Testing Loss: 0.5429, Accuracy: 0.8659, Precision: 0.8381, Recall: 0.7885, F1: 0.8054
LM Predictions:  [2, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 5, 0, 0, 5, 0, 0, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5432, Accuracy: 0.2143, Precision: 0.2847, Recall: 0.1865, F1: 0.1911
Epoch 21/70
Train Loss: 0.2792, Accuracy: 0.9206, Precision: 0.8814, Recall: 0.8855, F1: 0.8825
Validation Loss: 0.6726, Accuracy: 0.8316, Precision: 0.7781, Recall: 0.7417, F1: 0.7505
Testing Loss: 0.5397, Accuracy: 0.8720, Precision: 0.8489, Recall: 0.7959, F1: 0.8124
LM Predictions:  [2, 5, 5, 4, 5, 5, 4, 0, 5, 0, 0, 0, 2, 5, 0, 0, 4, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 0, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6940, Accuracy: 0.2619, Precision: 0.2242, Recall: 0.2288, F1: 0.2101
Epoch 22/70
Train Loss: 0.2861, Accuracy: 0.9187, Precision: 0.8754, Recall: 0.8798, F1: 0.8767
Validation Loss: 0.5862, Accuracy: 0.8550, Precision: 0.8188, Recall: 0.8045, F1: 0.8110
Testing Loss: 0.5141, Accuracy: 0.8599, Precision: 0.8227, Recall: 0.7996, F1: 0.8095
LM Predictions:  [2, 5, 5, 5, 1, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 5, 5, 1, 0, 5, 5, 5, 0, 5, 5, 1, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2719, Accuracy: 0.2381, Precision: 0.6597, Recall: 0.2077, F1: 0.2870
Epoch 23/70
Train Loss: 0.2646, Accuracy: 0.9189, Precision: 0.8741, Recall: 0.8791, F1: 0.8762
Validation Loss: 0.6167, Accuracy: 0.8380, Precision: 0.7952, Recall: 0.7655, F1: 0.7738
Testing Loss: 0.5399, Accuracy: 0.8587, Precision: 0.8134, Recall: 0.7914, F1: 0.7993
LM Predictions:  [2, 5, 5, 4, 0, 5, 4, 5, 5, 0, 0, 0, 1, 3, 5, 0, 4, 0, 5, 1, 0, 5, 0, 5, 0, 0, 5, 5, 2, 4, 0, 4, 5, 0, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3803, Accuracy: 0.2857, Precision: 0.2778, Recall: 0.2526, F1: 0.2457
Epoch 24/70
Train Loss: 0.2552, Accuracy: 0.9293, Precision: 0.8889, Recall: 0.8991, F1: 0.8933
Validation Loss: 0.6479, Accuracy: 0.8380, Precision: 0.7870, Recall: 0.7838, F1: 0.7841
Testing Loss: 0.5592, Accuracy: 0.8563, Precision: 0.8054, Recall: 0.7898, F1: 0.7965
LM Predictions:  [2, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 5, 1, 5, 5, 0, 4, 3, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 2, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3109, Accuracy: 0.2619, Precision: 0.5167, Recall: 0.2381, F1: 0.2945
Epoch 25/70
Train Loss: 0.2600, Accuracy: 0.9270, Precision: 0.8842, Recall: 0.8925, F1: 0.8879
Validation Loss: 0.6573, Accuracy: 0.8422, Precision: 0.7971, Recall: 0.7681, F1: 0.7666
Testing Loss: 0.5370, Accuracy: 0.8623, Precision: 0.8223, Recall: 0.7930, F1: 0.7948
LM Predictions:  [2, 4, 5, 5, 0, 5, 4, 0, 5, 0, 0, 0, 1, 5, 0, 0, 4, 0, 0, 3, 0, 5, 4, 5, 0, 0, 5, 2, 2, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3158, Accuracy: 0.2857, Precision: 0.2130, Recall: 0.2474, F1: 0.2127
Epoch 26/70
Train Loss: 0.2495, Accuracy: 0.9263, Precision: 0.8846, Recall: 0.8908, F1: 0.8874
Validation Loss: 0.6707, Accuracy: 0.8422, Precision: 0.8036, Recall: 0.8076, F1: 0.8039
Testing Loss: 0.5060, Accuracy: 0.8720, Precision: 0.8242, Recall: 0.8213, F1: 0.8219
LM Predictions:  [2, 4, 3, 4, 0, 5, 4, 5, 5, 0, 0, 0, 3, 4, 5, 0, 4, 3, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 4, 4, 5, 4, 5, 5, 5, 4, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1368, Accuracy: 0.3333, Precision: 0.4741, Recall: 0.3082, F1: 0.3298
Epoch 27/70
Train Loss: 0.2359, Accuracy: 0.9303, Precision: 0.8935, Recall: 0.9036, F1: 0.8977
Validation Loss: 0.6653, Accuracy: 0.8507, Precision: 0.8249, Recall: 0.8039, F1: 0.8133
Testing Loss: 0.5755, Accuracy: 0.8647, Precision: 0.8372, Recall: 0.8172, F1: 0.8258
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 1, 5, 5, 0, 4, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3927, Accuracy: 0.2619, Precision: 0.4500, Recall: 0.2288, F1: 0.2704
Epoch 28/70
Train Loss: 0.2472, Accuracy: 0.9279, Precision: 0.8877, Recall: 0.8932, F1: 0.8901
Validation Loss: 0.6246, Accuracy: 0.8443, Precision: 0.8088, Recall: 0.7813, F1: 0.7911
Testing Loss: 0.5243, Accuracy: 0.8684, Precision: 0.8335, Recall: 0.8097, F1: 0.8193
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 0, 0, 4, 3, 5, 2, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2137, Accuracy: 0.3333, Precision: 0.5972, Recall: 0.3122, F1: 0.3778
Epoch 29/70
Train Loss: 0.2144, Accuracy: 0.9350, Precision: 0.8925, Recall: 0.9077, F1: 0.8989
Validation Loss: 0.7141, Accuracy: 0.8380, Precision: 0.8176, Recall: 0.7642, F1: 0.7780
Testing Loss: 0.5977, Accuracy: 0.8671, Precision: 0.8385, Recall: 0.7941, F1: 0.8095
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 2, 2, 0, 0, 4, 3, 5, 2, 0, 5, 0, 5, 0, 0, 5, 2, 5, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2671, Accuracy: 0.3333, Precision: 0.4833, Recall: 0.3029, F1: 0.3149
Epoch 30/70
Train Loss: 0.2425, Accuracy: 0.9265, Precision: 0.8831, Recall: 0.8916, F1: 0.8864
Validation Loss: 0.6771, Accuracy: 0.8401, Precision: 0.8198, Recall: 0.7600, F1: 0.7782
Testing Loss: 0.6028, Accuracy: 0.8539, Precision: 0.8261, Recall: 0.7726, F1: 0.7912
LM Predictions:  [1, 5, 0, 5, 0, 5, 4, 5, 0, 0, 0, 0, 0, 5, 0, 0, 4, 3, 0, 1, 0, 5, 5, 5, 0, 0, 5, 2, 2, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5399, Accuracy: 0.2857, Precision: 0.3618, Recall: 0.2474, F1: 0.2338
Epoch 31/70
Train Loss: 0.2260, Accuracy: 0.9393, Precision: 0.9048, Recall: 0.9031, F1: 0.9038
Validation Loss: 0.7021, Accuracy: 0.8465, Precision: 0.8130, Recall: 0.8046, F1: 0.8085
Testing Loss: 0.6111, Accuracy: 0.8599, Precision: 0.8252, Recall: 0.8235, F1: 0.8216
LM Predictions:  [2, 5, 5, 5, 5, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 4, 3, 5, 1, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2915, Accuracy: 0.2619, Precision: 0.6667, Recall: 0.2474, F1: 0.3500
Epoch 32/70
Train Loss: 0.2178, Accuracy: 0.9367, Precision: 0.8972, Recall: 0.9104, F1: 0.9025
Validation Loss: 0.9125, Accuracy: 0.8060, Precision: 0.7711, Recall: 0.7564, F1: 0.7499
Testing Loss: 0.7710, Accuracy: 0.8370, Precision: 0.7917, Recall: 0.7864, F1: 0.7822
LM Predictions:  [2, 5, 3, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 3, 0, 0, 4, 3, 0, 5, 0, 5, 5, 5, 0, 0, 5, 5, 3, 4, 0, 4, 0, 0, 5, 3, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3642, Accuracy: 0.3571, Precision: 0.4444, Recall: 0.3267, F1: 0.3254
Epoch 33/70
Train Loss: 0.2614, Accuracy: 0.9293, Precision: 0.8909, Recall: 0.9136, F1: 0.8989
Validation Loss: 0.7753, Accuracy: 0.8294, Precision: 0.7877, Recall: 0.7629, F1: 0.7704
Testing Loss: 0.6114, Accuracy: 0.8587, Precision: 0.8279, Recall: 0.7972, F1: 0.8093
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 2, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3503, Accuracy: 0.3333, Precision: 0.4583, Recall: 0.3029, F1: 0.3226
Epoch 34/70
Train Loss: 0.3060, Accuracy: 0.9004, Precision: 0.8745, Recall: 0.8628, F1: 0.8620
Validation Loss: 0.9282, Accuracy: 0.7335, Precision: 0.5938, Recall: 0.6061, F1: 0.5786
Testing Loss: 0.7578, Accuracy: 0.7657, Precision: 0.7371, Recall: 0.6392, F1: 0.6208
LM Predictions:  [2, 0, 0, 5, 0, 5, 4, 5, 0, 0, 0, 0, 1, 5, 0, 0, 4, 1, 0, 5, 0, 5, 0, 5, 0, 0, 5, 2, 2, 5, 0, 4, 0, 0, 0, 5, 0, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5377, Accuracy: 0.2857, Precision: 0.2828, Recall: 0.2474, F1: 0.2231
Epoch 35/70
Train Loss: 0.2487, Accuracy: 0.9220, Precision: 0.8847, Recall: 0.8872, F1: 0.8835
Validation Loss: 0.7088, Accuracy: 0.8422, Precision: 0.7997, Recall: 0.7847, F1: 0.7906
Testing Loss: 0.6399, Accuracy: 0.8527, Precision: 0.8125, Recall: 0.7816, F1: 0.7944
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0389, Accuracy: 0.3095, Precision: 0.5833, Recall: 0.2791, F1: 0.3117
Epoch 36/70
Train Loss: 0.1959, Accuracy: 0.9391, Precision: 0.9001, Recall: 0.9038, F1: 0.9016
Validation Loss: 0.8234, Accuracy: 0.8380, Precision: 0.7992, Recall: 0.7781, F1: 0.7863
Testing Loss: 0.6727, Accuracy: 0.8575, Precision: 0.8172, Recall: 0.7903, F1: 0.8015
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 3, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2573, Accuracy: 0.3333, Precision: 0.5694, Recall: 0.3029, F1: 0.3488
Epoch 37/70
Train Loss: 0.2257, Accuracy: 0.9362, Precision: 0.8959, Recall: 0.9174, F1: 0.9037
Validation Loss: 0.7384, Accuracy: 0.8443, Precision: 0.8237, Recall: 0.7703, F1: 0.7889
Testing Loss: 0.6605, Accuracy: 0.8599, Precision: 0.8321, Recall: 0.7962, F1: 0.8115
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 5, 0, 3, 3, 0, 0, 4, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 3, 5, 5, 4, 0, 5, 0, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2130, Accuracy: 0.3095, Precision: 0.4425, Recall: 0.2751, F1: 0.2915
Epoch 38/70
Train Loss: 0.2562, Accuracy: 0.9277, Precision: 0.8843, Recall: 0.8970, F1: 0.8900
Validation Loss: 0.7492, Accuracy: 0.8380, Precision: 0.8121, Recall: 0.7682, F1: 0.7823
Testing Loss: 0.6240, Accuracy: 0.8490, Precision: 0.8011, Recall: 0.7659, F1: 0.7799
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 0, 0, 5, 3, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0882, Accuracy: 0.3095, Precision: 0.5784, Recall: 0.2791, F1: 0.3076
Epoch 39/70
Train Loss: 0.1920, Accuracy: 0.9417, Precision: 0.9034, Recall: 0.9196, F1: 0.9097
Validation Loss: 0.8445, Accuracy: 0.8380, Precision: 0.8222, Recall: 0.7567, F1: 0.7694
Testing Loss: 0.6640, Accuracy: 0.8623, Precision: 0.8367, Recall: 0.7746, F1: 0.7877
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 0, 0, 4, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 2, 2, 4, 0, 4, 0, 0, 0, 5, 5, 5, 0, 0, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3006, Accuracy: 0.3333, Precision: 0.4257, Recall: 0.2989, F1: 0.2910
Epoch 40/70
Train Loss: 0.1976, Accuracy: 0.9407, Precision: 0.9036, Recall: 0.9220, F1: 0.9107
Validation Loss: 0.7616, Accuracy: 0.8443, Precision: 0.8013, Recall: 0.7978, F1: 0.7994
Testing Loss: 0.6127, Accuracy: 0.8635, Precision: 0.8223, Recall: 0.8148, F1: 0.8174
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 0, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2577, Accuracy: 0.3333, Precision: 0.6481, Recall: 0.2989, F1: 0.3646
Epoch 41/70
Train Loss: 0.1774, Accuracy: 0.9457, Precision: 0.9094, Recall: 0.9332, F1: 0.9178
Validation Loss: 0.7901, Accuracy: 0.8465, Precision: 0.8095, Recall: 0.8047, F1: 0.8068
Testing Loss: 0.6450, Accuracy: 0.8647, Precision: 0.8256, Recall: 0.8183, F1: 0.8204
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 4, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2278, Accuracy: 0.3333, Precision: 0.5926, Recall: 0.2989, F1: 0.3460
Epoch 42/70
Train Loss: 0.1777, Accuracy: 0.9464, Precision: 0.9091, Recall: 0.9196, F1: 0.9136
Validation Loss: 0.7591, Accuracy: 0.8593, Precision: 0.8301, Recall: 0.8092, F1: 0.8184
Testing Loss: 0.6452, Accuracy: 0.8647, Precision: 0.8354, Recall: 0.8120, F1: 0.8221
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 3, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1547, Accuracy: 0.3333, Precision: 0.5167, Recall: 0.2989, F1: 0.3407
Epoch 43/70
Train Loss: 0.1799, Accuracy: 0.9455, Precision: 0.9079, Recall: 0.9343, F1: 0.9172
Validation Loss: 0.8378, Accuracy: 0.8443, Precision: 0.7999, Recall: 0.8016, F1: 0.8007
Testing Loss: 0.6830, Accuracy: 0.8599, Precision: 0.8228, Recall: 0.8139, F1: 0.8165
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0562, Accuracy: 0.3333, Precision: 0.6000, Recall: 0.2989, F1: 0.3467
Epoch 44/70
Train Loss: 0.1905, Accuracy: 0.9429, Precision: 0.9057, Recall: 0.9288, F1: 0.9131
Validation Loss: 0.7841, Accuracy: 0.8358, Precision: 0.7936, Recall: 0.8049, F1: 0.7972
Testing Loss: 0.6225, Accuracy: 0.8575, Precision: 0.8171, Recall: 0.8248, F1: 0.8162
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 5, 5, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1196, Accuracy: 0.3333, Precision: 0.6667, Recall: 0.2989, F1: 0.3733
Epoch 45/70
Train Loss: 0.1810, Accuracy: 0.9471, Precision: 0.9143, Recall: 0.9261, F1: 0.9188
Validation Loss: 0.8518, Accuracy: 0.8529, Precision: 0.8207, Recall: 0.8041, F1: 0.8117
Testing Loss: 0.6849, Accuracy: 0.8623, Precision: 0.8267, Recall: 0.8178, F1: 0.8208
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 5, 0, 0, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1362, Accuracy: 0.3095, Precision: 0.6212, Recall: 0.2751, F1: 0.3286
Epoch 46/70
Train Loss: 0.2029, Accuracy: 0.9402, Precision: 0.9036, Recall: 0.9260, F1: 0.9107
Validation Loss: 0.8098, Accuracy: 0.8358, Precision: 0.7972, Recall: 0.7838, F1: 0.7868
Testing Loss: 0.7169, Accuracy: 0.8466, Precision: 0.8030, Recall: 0.7830, F1: 0.7898
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 4, 4, 4, 5, 4, 0, 0, 5, 5, 4, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9068, Accuracy: 0.3333, Precision: 0.5379, Recall: 0.2989, F1: 0.3175
Epoch 47/70
Train Loss: 0.1811, Accuracy: 0.9445, Precision: 0.9059, Recall: 0.9303, F1: 0.9151
Validation Loss: 0.8213, Accuracy: 0.8486, Precision: 0.8174, Recall: 0.7940, F1: 0.8037
Testing Loss: 0.7043, Accuracy: 0.8539, Precision: 0.8128, Recall: 0.7877, F1: 0.7983
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2669, Accuracy: 0.3333, Precision: 0.6111, Recall: 0.2989, F1: 0.3434
Epoch 48/70
Train Loss: 0.1783, Accuracy: 0.9421, Precision: 0.9063, Recall: 0.9097, F1: 0.9076
Validation Loss: 0.7381, Accuracy: 0.8486, Precision: 0.8015, Recall: 0.7955, F1: 0.7983
Testing Loss: 0.6106, Accuracy: 0.8684, Precision: 0.8233, Recall: 0.8108, F1: 0.8165
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 0, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0439, Accuracy: 0.3095, Precision: 0.5952, Recall: 0.2751, F1: 0.3112
Epoch 49/70
Train Loss: 0.1715, Accuracy: 0.9490, Precision: 0.9132, Recall: 0.9299, F1: 0.9196
Validation Loss: 0.7950, Accuracy: 0.8507, Precision: 0.8202, Recall: 0.8074, F1: 0.8131
Testing Loss: 0.6725, Accuracy: 0.8575, Precision: 0.8298, Recall: 0.8119, F1: 0.8169
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 5, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9302, Accuracy: 0.3333, Precision: 0.6481, Recall: 0.2989, F1: 0.3646
Epoch 50/70
Train Loss: 0.2092, Accuracy: 0.9364, Precision: 0.8999, Recall: 0.9111, F1: 0.9044
Validation Loss: 0.7507, Accuracy: 0.8593, Precision: 0.8214, Recall: 0.8125, F1: 0.8168
Testing Loss: 0.7094, Accuracy: 0.8587, Precision: 0.8177, Recall: 0.8102, F1: 0.8125
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 5, 0, 4, 5, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0963, Accuracy: 0.3333, Precision: 0.5767, Recall: 0.2989, F1: 0.3386
Epoch 51/70
Train Loss: 0.1948, Accuracy: 0.9443, Precision: 0.9164, Recall: 0.9249, F1: 0.9200
Validation Loss: 0.7731, Accuracy: 0.8443, Precision: 0.8205, Recall: 0.8084, F1: 0.8116
Testing Loss: 0.7034, Accuracy: 0.8394, Precision: 0.7965, Recall: 0.7752, F1: 0.7821
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 2, 4, 0, 4, 3, 5, 5, 0, 5, 5, 5, 0, 0, 5, 5, 0, 4, 5, 4, 0, 0, 5, 5, 0, 5, 2, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8911, Accuracy: 0.3810, Precision: 0.5000, Recall: 0.3505, F1: 0.3862
Epoch 52/70
Train Loss: 0.1759, Accuracy: 0.9466, Precision: 0.9185, Recall: 0.9204, F1: 0.9191
Validation Loss: 0.7666, Accuracy: 0.8507, Precision: 0.8245, Recall: 0.7906, F1: 0.7995
Testing Loss: 0.6849, Accuracy: 0.8599, Precision: 0.8130, Recall: 0.7862, F1: 0.7937
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 3, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 0, 4, 5, 4, 5, 0, 0, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7310, Accuracy: 0.4048, Precision: 0.5148, Recall: 0.3783, F1: 0.3819
Epoch 53/70
Train Loss: 0.1749, Accuracy: 0.9478, Precision: 0.9184, Recall: 0.9197, F1: 0.9189
Validation Loss: 0.8186, Accuracy: 0.8443, Precision: 0.8181, Recall: 0.7758, F1: 0.7888
Testing Loss: 0.7375, Accuracy: 0.8587, Precision: 0.8173, Recall: 0.7852, F1: 0.7970
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 5, 0, 5, 0, 5, 0, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0036, Accuracy: 0.3571, Precision: 0.5507, Recall: 0.3228, F1: 0.3260
Epoch 54/70
Train Loss: 0.1745, Accuracy: 0.9464, Precision: 0.9129, Recall: 0.9242, F1: 0.9178
Validation Loss: 0.7663, Accuracy: 0.8593, Precision: 0.8367, Recall: 0.7907, F1: 0.8026
Testing Loss: 0.6270, Accuracy: 0.8684, Precision: 0.8230, Recall: 0.7949, F1: 0.8030
LM Predictions:  [2, 0, 5, 0, 0, 0, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 5, 0, 5, 0, 0, 0, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 0, 5, 4, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0541, Accuracy: 0.3571, Precision: 0.5042, Recall: 0.3228, F1: 0.2983
Epoch 55/70
Train Loss: 0.1703, Accuracy: 0.9459, Precision: 0.9123, Recall: 0.9285, F1: 0.9187
Validation Loss: 0.9745, Accuracy: 0.8252, Precision: 0.7819, Recall: 0.7906, F1: 0.7765
Testing Loss: 0.7915, Accuracy: 0.8454, Precision: 0.7921, Recall: 0.8055, F1: 0.7935
LM Predictions:  [2, 0, 5, 0, 0, 4, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 5, 5, 2, 0, 4, 5, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 5, 0, 5, 4, 5, 0, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0343, Accuracy: 0.3810, Precision: 0.5202, Recall: 0.3505, F1: 0.3505
Epoch 56/70
Train Loss: 0.1742, Accuracy: 0.9478, Precision: 0.9135, Recall: 0.9304, F1: 0.9203
Validation Loss: 0.8162, Accuracy: 0.8443, Precision: 0.8101, Recall: 0.7750, F1: 0.7867
Testing Loss: 0.6959, Accuracy: 0.8563, Precision: 0.8134, Recall: 0.7796, F1: 0.7923
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 5, 0, 3, 0, 5, 5, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9005, Accuracy: 0.3571, Precision: 0.4579, Recall: 0.3228, F1: 0.3194
Epoch 57/70
Train Loss: 0.1639, Accuracy: 0.9509, Precision: 0.9180, Recall: 0.9346, F1: 0.9251
Validation Loss: 0.7921, Accuracy: 0.8443, Precision: 0.8026, Recall: 0.7716, F1: 0.7818
Testing Loss: 0.6622, Accuracy: 0.8539, Precision: 0.8151, Recall: 0.7668, F1: 0.7839
LM Predictions:  [2, 5, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 0, 0, 4, 0, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 4, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9622, Accuracy: 0.3333, Precision: 0.5250, Recall: 0.3029, F1: 0.3138
Epoch 58/70
Train Loss: 0.1533, Accuracy: 0.9514, Precision: 0.9201, Recall: 0.9252, F1: 0.9224
Validation Loss: 0.8531, Accuracy: 0.8507, Precision: 0.8008, Recall: 0.8051, F1: 0.8023
Testing Loss: 0.7037, Accuracy: 0.8623, Precision: 0.8117, Recall: 0.8114, F1: 0.8113
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 5, 5, 1, 0, 5, 5, 2, 0, 0, 5, 2, 4, 4, 5, 4, 0, 5, 5, 5, 5, 2, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8678, Accuracy: 0.3571, Precision: 0.4755, Recall: 0.3228, F1: 0.3481
Epoch 59/70
Train Loss: 0.1476, Accuracy: 0.9509, Precision: 0.9184, Recall: 0.9291, F1: 0.9231
Validation Loss: 0.9072, Accuracy: 0.8507, Precision: 0.8175, Recall: 0.7956, F1: 0.8049
Testing Loss: 0.7821, Accuracy: 0.8563, Precision: 0.8159, Recall: 0.7942, F1: 0.8030
LM Predictions:  [2, 5, 5, 0, 0, 4, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 3, 5, 1, 0, 5, 5, 5, 0, 0, 5, 5, 0, 4, 5, 4, 0, 5, 0, 5, 0, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9519, Accuracy: 0.4048, Precision: 0.5678, Recall: 0.3690, F1: 0.3864
Epoch 60/70
Train Loss: 0.1597, Accuracy: 0.9528, Precision: 0.9201, Recall: 0.9420, F1: 0.9288
Validation Loss: 0.7445, Accuracy: 0.8486, Precision: 0.8121, Recall: 0.7847, F1: 0.7926
Testing Loss: 0.6442, Accuracy: 0.8647, Precision: 0.8289, Recall: 0.7899, F1: 0.8001
LM Predictions:  [2, 5, 0, 4, 0, 4, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 3, 0, 2, 0, 4, 0, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 4, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8974, Accuracy: 0.4048, Precision: 0.4924, Recall: 0.3783, F1: 0.3659
Epoch 61/70
Train Loss: 0.1700, Accuracy: 0.9488, Precision: 0.9185, Recall: 0.9253, F1: 0.9216
Validation Loss: 0.8308, Accuracy: 0.8550, Precision: 0.8128, Recall: 0.7780, F1: 0.7888
Testing Loss: 0.6915, Accuracy: 0.8611, Precision: 0.8268, Recall: 0.7799, F1: 0.7979
LM Predictions:  [2, 0, 0, 5, 0, 5, 4, 5, 5, 0, 0, 0, 3, 5, 0, 0, 4, 3, 0, 2, 0, 5, 0, 5, 0, 0, 5, 5, 5, 0, 0, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9960, Accuracy: 0.3571, Precision: 0.5667, Recall: 0.3307, F1: 0.3586
Epoch 62/70
Train Loss: 0.1571, Accuracy: 0.9488, Precision: 0.9177, Recall: 0.9265, F1: 0.9213
Validation Loss: 0.7552, Accuracy: 0.8550, Precision: 0.8196, Recall: 0.7997, F1: 0.8084
Testing Loss: 0.7167, Accuracy: 0.8623, Precision: 0.8337, Recall: 0.8084, F1: 0.8184
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 5, 0, 4, 0, 5, 2, 0, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0506, Accuracy: 0.3333, Precision: 0.6111, Recall: 0.3029, F1: 0.3579
Epoch 63/70
Train Loss: 0.1506, Accuracy: 0.9540, Precision: 0.9244, Recall: 0.9405, F1: 0.9307
Validation Loss: 0.7626, Accuracy: 0.8614, Precision: 0.8335, Recall: 0.8069, F1: 0.8169
Testing Loss: 0.6630, Accuracy: 0.8611, Precision: 0.8160, Recall: 0.7900, F1: 0.7994
LM Predictions:  [2, 4, 0, 5, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 3, 0, 2, 0, 5, 5, 5, 0, 0, 5, 5, 0, 4, 5, 4, 0, 0, 5, 5, 5, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9139, Accuracy: 0.4286, Precision: 0.5595, Recall: 0.3968, F1: 0.4161
Epoch 64/70
Train Loss: 0.1487, Accuracy: 0.9571, Precision: 0.9275, Recall: 0.9375, F1: 0.9321
Validation Loss: 0.8094, Accuracy: 0.8358, Precision: 0.8107, Recall: 0.7527, F1: 0.7693
Testing Loss: 0.6409, Accuracy: 0.8611, Precision: 0.8239, Recall: 0.7802, F1: 0.7952
LM Predictions:  [2, 4, 0, 0, 0, 0, 4, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 3, 0, 2, 0, 0, 0, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 0, 5, 1, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8421, Accuracy: 0.4286, Precision: 0.6708, Recall: 0.3902, F1: 0.3920
Epoch 65/70
Train Loss: 0.1449, Accuracy: 0.9535, Precision: 0.9253, Recall: 0.9312, F1: 0.9280
Validation Loss: 0.8202, Accuracy: 0.8422, Precision: 0.7993, Recall: 0.7947, F1: 0.7960
Testing Loss: 0.7038, Accuracy: 0.8671, Precision: 0.8251, Recall: 0.8229, F1: 0.8238
LM Predictions:  [2, 0, 5, 0, 0, 5, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 3, 5, 3, 0, 5, 5, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 5, 0, 5, 1, 5, 0, 5, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8021, Accuracy: 0.4048, Precision: 0.6859, Recall: 0.3624, F1: 0.3933
Epoch 66/70
Train Loss: 0.1522, Accuracy: 0.9519, Precision: 0.9220, Recall: 0.9357, F1: 0.9277
Validation Loss: 0.8978, Accuracy: 0.8422, Precision: 0.8070, Recall: 0.7807, F1: 0.7867
Testing Loss: 0.7196, Accuracy: 0.8563, Precision: 0.8059, Recall: 0.7815, F1: 0.7877
LM Predictions:  [2, 0, 0, 4, 0, 5, 4, 5, 3, 0, 0, 0, 3, 3, 4, 0, 4, 0, 0, 3, 0, 0, 0, 5, 0, 0, 5, 5, 4, 4, 0, 4, 0, 0, 0, 5, 0, 5, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1945, Accuracy: 0.3571, Precision: 0.3760, Recall: 0.3228, F1: 0.2810
Epoch 67/70
Train Loss: 0.1514, Accuracy: 0.9535, Precision: 0.9250, Recall: 0.9308, F1: 0.9276
Validation Loss: 0.8598, Accuracy: 0.8358, Precision: 0.7947, Recall: 0.7914, F1: 0.7926
Testing Loss: 0.7222, Accuracy: 0.8659, Precision: 0.8343, Recall: 0.8115, F1: 0.8205
LM Predictions:  [2, 4, 5, 4, 0, 4, 4, 5, 3, 0, 0, 0, 3, 3, 4, 0, 4, 0, 5, 3, 0, 4, 0, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 5, 4, 5, 4, 5, 0, 4, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4271, Accuracy: 0.3571, Precision: 0.3704, Recall: 0.3228, F1: 0.2815
Epoch 68/70
Train Loss: 0.1556, Accuracy: 0.9523, Precision: 0.9197, Recall: 0.9360, F1: 0.9261
Validation Loss: 0.8683, Accuracy: 0.8230, Precision: 0.7874, Recall: 0.7684, F1: 0.7754
Testing Loss: 0.7843, Accuracy: 0.8527, Precision: 0.8133, Recall: 0.7886, F1: 0.7983
LM Predictions:  [2, 5, 1, 5, 0, 5, 4, 5, 3, 0, 0, 0, 3, 3, 4, 0, 4, 0, 5, 1, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0509, Accuracy: 0.3571, Precision: 0.5748, Recall: 0.3108, F1: 0.3479
Epoch 69/70
Train Loss: 0.1713, Accuracy: 0.9459, Precision: 0.9142, Recall: 0.9324, F1: 0.9211
Validation Loss: 0.9101, Accuracy: 0.8230, Precision: 0.8005, Recall: 0.7565, F1: 0.7745
Testing Loss: 0.7699, Accuracy: 0.8466, Precision: 0.8180, Recall: 0.7692, F1: 0.7888
LM Predictions:  [2, 5, 5, 5, 0, 5, 4, 5, 3, 0, 0, 0, 3, 5, 5, 0, 4, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5, 5, 5, 5, 5, 4, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0547, Accuracy: 0.3095, Precision: 0.5278, Recall: 0.2751, F1: 0.3163
Epoch 70/70
Train Loss: 0.1530, Accuracy: 0.9516, Precision: 0.9197, Recall: 0.9390, F1: 0.9274
Validation Loss: 0.9245, Accuracy: 0.8465, Precision: 0.8130, Recall: 0.7944, F1: 0.8028
Testing Loss: 0.7756, Accuracy: 0.8599, Precision: 0.8174, Recall: 0.7930, F1: 0.8025
LM Predictions:  [2, 4, 4, 4, 0, 4, 4, 5, 5, 0, 0, 5, 3, 3, 4, 0, 4, 0, 5, 2, 0, 4, 0, 5, 0, 0, 5, 5, 4, 4, 5, 4, 0, 5, 0, 5, 4, 5, 0, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8666, Accuracy: 0.3810, Precision: 0.4252, Recall: 0.3505, F1: 0.3353
For middle layers:  [4, 5, 6, 7]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.2110, Accuracy: 0.5589, Precision: 0.4965, Recall: 0.4065, F1: 0.4035
Validation Loss: 0.7391, Accuracy: 0.7612, Precision: 0.5884, Recall: 0.6313, F1: 0.6039
Testing Loss: 0.6904, Accuracy: 0.7850, Precision: 0.6092, Recall: 0.6378, F1: 0.6140
LM Predictions:  [4, 3, 0, 4, 3, 3, 3, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 4, 3, 0, 0, 4, 4, 3, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 4, 2, 4, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6834, Accuracy: 0.1429, Precision: 0.0795, Recall: 0.1524, F1: 0.1030
Epoch 2/70
Train Loss: 0.6194, Accuracy: 0.8319, Precision: 0.6820, Recall: 0.7118, F1: 0.6958
Validation Loss: 0.5430, Accuracy: 0.8422, Precision: 0.7034, Recall: 0.7377, F1: 0.7183
Testing Loss: 0.5046, Accuracy: 0.8527, Precision: 0.7189, Recall: 0.7457, F1: 0.7306
LM Predictions:  [0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 3, 0, 2, 0, 3, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 3, 0, 3, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.9819, Accuracy: 0.1905, Precision: 0.1650, Recall: 0.1952, F1: 0.1346
Epoch 3/70
Train Loss: 0.4794, Accuracy: 0.8658, Precision: 0.7870, Recall: 0.7545, F1: 0.7462
Validation Loss: 0.5342, Accuracy: 0.8294, Precision: 0.7089, Recall: 0.7157, F1: 0.7093
Testing Loss: 0.4609, Accuracy: 0.8563, Precision: 0.7353, Recall: 0.7420, F1: 0.7336
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0216, Accuracy: 0.1667, Precision: 0.0350, Recall: 0.1556, F1: 0.0571
Epoch 4/70
Train Loss: 0.3942, Accuracy: 0.8810, Precision: 0.8200, Recall: 0.7955, F1: 0.8009
Validation Loss: 0.5695, Accuracy: 0.8507, Precision: 0.8148, Recall: 0.7845, F1: 0.7873
Testing Loss: 0.4145, Accuracy: 0.8768, Precision: 0.8298, Recall: 0.8201, F1: 0.8223
LM Predictions:  [5, 4, 3, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 3, 5, 0, 4, 3, 5, 3, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 0, 4, 5, 0, 0, 3, 3, 5, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7956, Accuracy: 0.1190, Precision: 0.1414, Recall: 0.1124, F1: 0.1217
Epoch 5/70
Train Loss: 0.3557, Accuracy: 0.8933, Precision: 0.8413, Recall: 0.8321, F1: 0.8361
Validation Loss: 0.5198, Accuracy: 0.8465, Precision: 0.8255, Recall: 0.7956, F1: 0.8062
Testing Loss: 0.4398, Accuracy: 0.8623, Precision: 0.8532, Recall: 0.8336, F1: 0.8392
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.0435, Accuracy: 0.0952, Precision: 0.0741, Recall: 0.0741, F1: 0.0741
Epoch 6/70
Train Loss: 0.3215, Accuracy: 0.9075, Precision: 0.8622, Recall: 0.8700, F1: 0.8649
Validation Loss: 0.5111, Accuracy: 0.8571, Precision: 0.8332, Recall: 0.7964, F1: 0.8115
Testing Loss: 0.4280, Accuracy: 0.8804, Precision: 0.8550, Recall: 0.8448, F1: 0.8472
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6407, Accuracy: 0.0952, Precision: 0.0667, Recall: 0.0741, F1: 0.0702
Epoch 7/70
Train Loss: 0.3114, Accuracy: 0.9049, Precision: 0.8581, Recall: 0.8682, F1: 0.8620
Validation Loss: 0.5351, Accuracy: 0.8507, Precision: 0.8188, Recall: 0.8596, F1: 0.8270
Testing Loss: 0.4465, Accuracy: 0.8744, Precision: 0.8377, Recall: 0.8637, F1: 0.8426
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5306, Accuracy: 0.0714, Precision: 0.3333, Recall: 0.0661, F1: 0.1074
Epoch 8/70
Train Loss: 0.2647, Accuracy: 0.9213, Precision: 0.8776, Recall: 0.8940, F1: 0.8839
Validation Loss: 0.5486, Accuracy: 0.8635, Precision: 0.8384, Recall: 0.8253, F1: 0.8313
Testing Loss: 0.4003, Accuracy: 0.8901, Precision: 0.8553, Recall: 0.8297, F1: 0.8405
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 0, 4, 0, 0, 0, 5, 5, 5, 0, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7881, Accuracy: 0.1667, Precision: 0.2361, Recall: 0.1402, F1: 0.1534
Epoch 9/70
Train Loss: 0.2500, Accuracy: 0.9225, Precision: 0.8795, Recall: 0.8966, F1: 0.8861
Validation Loss: 0.5783, Accuracy: 0.8614, Precision: 0.8293, Recall: 0.8196, F1: 0.8234
Testing Loss: 0.5029, Accuracy: 0.8816, Precision: 0.8504, Recall: 0.8442, F1: 0.8460
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 2, 5, 5, 4, 0, 5, 5, 5, 3, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4742, Accuracy: 0.1429, Precision: 0.2917, Recall: 0.1270, F1: 0.1769
Epoch 10/70
Train Loss: 0.2320, Accuracy: 0.9319, Precision: 0.8910, Recall: 0.9069, F1: 0.8973
Validation Loss: 0.5700, Accuracy: 0.8763, Precision: 0.8489, Recall: 0.8354, F1: 0.8415
Testing Loss: 0.5006, Accuracy: 0.8732, Precision: 0.8357, Recall: 0.8116, F1: 0.8214
LM Predictions:  [5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 0, 5, 0, 4, 0, 5, 0, 5, 5, 5, 5, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6712, Accuracy: 0.2143, Precision: 0.1994, Recall: 0.1878, F1: 0.1933
Epoch 11/70
Train Loss: 0.2161, Accuracy: 0.9374, Precision: 0.8986, Recall: 0.9218, F1: 0.9076
Validation Loss: 0.5342, Accuracy: 0.8635, Precision: 0.8350, Recall: 0.8169, F1: 0.8252
Testing Loss: 0.4746, Accuracy: 0.8684, Precision: 0.8339, Recall: 0.8213, F1: 0.8265
LM Predictions:  [5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 4, 0, 4, 3, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 2, 5, 5, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4228, Accuracy: 0.2143, Precision: 0.4111, Recall: 0.1971, F1: 0.2454
Epoch 12/70
Train Loss: 0.2071, Accuracy: 0.9410, Precision: 0.9033, Recall: 0.9234, F1: 0.9111
Validation Loss: 0.6761, Accuracy: 0.8571, Precision: 0.8330, Recall: 0.7916, F1: 0.8081
Testing Loss: 0.6456, Accuracy: 0.8611, Precision: 0.8403, Recall: 0.7918, F1: 0.8100
LM Predictions:  [5, 4, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 5, 5, 4, 0, 4, 0, 5, 5, 5, 5, 0, 4, 5, 0, 5, 5, 0, 5, 5, 4, 0, 5, 5, 5, 5, 5, 0, 4, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4903, Accuracy: 0.2619, Precision: 0.2190, Recall: 0.2302, F1: 0.2243
Epoch 13/70
Train Loss: 0.2159, Accuracy: 0.9374, Precision: 0.8978, Recall: 0.9135, F1: 0.9039
Validation Loss: 0.6458, Accuracy: 0.8593, Precision: 0.8256, Recall: 0.7952, F1: 0.8043
Testing Loss: 0.5686, Accuracy: 0.8647, Precision: 0.8250, Recall: 0.7960, F1: 0.8043
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 5, 5, 4, 0, 4, 3, 5, 5, 5, 5, 0, 4, 5, 0, 5, 1, 0, 0, 5, 4, 0, 0, 0, 5, 5, 5, 0, 4, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3538, Accuracy: 0.3095, Precision: 0.5357, Recall: 0.2698, F1: 0.2841
Epoch 14/70
Train Loss: 0.2012, Accuracy: 0.9402, Precision: 0.9033, Recall: 0.9275, F1: 0.9126
Validation Loss: 0.7126, Accuracy: 0.8550, Precision: 0.8242, Recall: 0.8019, F1: 0.8114
Testing Loss: 0.6140, Accuracy: 0.8587, Precision: 0.8260, Recall: 0.7982, F1: 0.8096
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 0, 4, 5, 0, 5, 1, 0, 5, 5, 4, 0, 5, 5, 5, 5, 5, 5, 4, 4, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4482, Accuracy: 0.2619, Precision: 0.3750, Recall: 0.2235, F1: 0.2314
Epoch 15/70
Train Loss: 0.1777, Accuracy: 0.9466, Precision: 0.9095, Recall: 0.9264, F1: 0.9167
Validation Loss: 0.6645, Accuracy: 0.8593, Precision: 0.8220, Recall: 0.8119, F1: 0.8165
Testing Loss: 0.6134, Accuracy: 0.8599, Precision: 0.8279, Recall: 0.8094, F1: 0.8168
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 5, 5, 5, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 1, 0, 5, 5, 4, 0, 5, 5, 5, 5, 1, 0, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.4666, Accuracy: 0.3095, Precision: 0.4583, Recall: 0.2421, F1: 0.2977
Epoch 16/70
Train Loss: 0.1766, Accuracy: 0.9469, Precision: 0.9100, Recall: 0.9323, F1: 0.9184
Validation Loss: 0.6952, Accuracy: 0.8614, Precision: 0.8198, Recall: 0.8318, F1: 0.8254
Testing Loss: 0.6253, Accuracy: 0.8623, Precision: 0.8191, Recall: 0.8307, F1: 0.8230
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 5, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 1, 0, 0, 5, 4, 0, 5, 5, 5, 5, 1, 5, 1, 1, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1391, Accuracy: 0.3095, Precision: 0.4107, Recall: 0.2474, F1: 0.2986
Epoch 17/70
Train Loss: 0.1834, Accuracy: 0.9476, Precision: 0.9108, Recall: 0.9358, F1: 0.9206
Validation Loss: 0.6074, Accuracy: 0.8635, Precision: 0.8257, Recall: 0.8295, F1: 0.8274
Testing Loss: 0.5758, Accuracy: 0.8671, Precision: 0.8248, Recall: 0.8283, F1: 0.8250
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 0, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 1, 0, 5, 5, 4, 0, 5, 5, 5, 5, 1, 5, 1, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0770, Accuracy: 0.2857, Precision: 0.4444, Recall: 0.2288, F1: 0.2866
Epoch 18/70
Train Loss: 0.1583, Accuracy: 0.9533, Precision: 0.9193, Recall: 0.9316, F1: 0.9246
Validation Loss: 0.6205, Accuracy: 0.8529, Precision: 0.8095, Recall: 0.8049, F1: 0.8067
Testing Loss: 0.5623, Accuracy: 0.8659, Precision: 0.8328, Recall: 0.8229, F1: 0.8263
LM Predictions:  [5, 5, 5, 5, 5, 5, 4, 5, 5, 0, 0, 5, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 0, 5, 0, 4, 0, 5, 5, 5, 5, 5, 0, 0, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3161, Accuracy: 0.2619, Precision: 0.2778, Recall: 0.2302, F1: 0.2500
Epoch 19/70
Train Loss: 0.1554, Accuracy: 0.9526, Precision: 0.9167, Recall: 0.9406, F1: 0.9261
Validation Loss: 0.6675, Accuracy: 0.8614, Precision: 0.8257, Recall: 0.7907, F1: 0.8040
Testing Loss: 0.6219, Accuracy: 0.8623, Precision: 0.8230, Recall: 0.7736, F1: 0.7905
LM Predictions:  [1, 5, 5, 5, 0, 3, 4, 0, 1, 0, 0, 0, 5, 5, 4, 0, 4, 3, 5, 1, 0, 5, 5, 4, 0, 0, 5, 1, 0, 0, 5, 4, 0, 0, 0, 5, 3, 1, 0, 1, 1, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.9799, Accuracy: 0.4524, Precision: 0.3897, Recall: 0.3611, F1: 0.3537
Epoch 20/70
Train Loss: 0.1479, Accuracy: 0.9500, Precision: 0.9138, Recall: 0.9259, F1: 0.9192
Validation Loss: 0.8487, Accuracy: 0.8571, Precision: 0.8253, Recall: 0.8088, F1: 0.8156
Testing Loss: 0.6953, Accuracy: 0.8720, Precision: 0.8381, Recall: 0.8268, F1: 0.8310
LM Predictions:  [1, 5, 2, 5, 5, 3, 4, 5, 5, 0, 0, 5, 5, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 3, 1, 0, 4, 0, 4, 0, 5, 5, 5, 3, 1, 0, 5, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2751, Accuracy: 0.3333, Precision: 0.3651, Recall: 0.2778, F1: 0.2932
Epoch 21/70
Train Loss: 0.1591, Accuracy: 0.9523, Precision: 0.9200, Recall: 0.9379, F1: 0.9277
Validation Loss: 0.8208, Accuracy: 0.8486, Precision: 0.8165, Recall: 0.7906, F1: 0.8012
Testing Loss: 0.7243, Accuracy: 0.8611, Precision: 0.8261, Recall: 0.7914, F1: 0.8056
LM Predictions:  [5, 5, 2, 5, 0, 1, 4, 5, 0, 0, 0, 0, 0, 5, 0, 0, 4, 5, 5, 1, 0, 5, 5, 4, 0, 0, 5, 1, 0, 4, 0, 4, 0, 0, 5, 5, 3, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.1387, Accuracy: 0.4524, Precision: 0.4771, Recall: 0.3611, F1: 0.3707
Epoch 22/70
Train Loss: 0.1568, Accuracy: 0.9540, Precision: 0.9232, Recall: 0.9363, F1: 0.9289
Validation Loss: 0.7065, Accuracy: 0.8443, Precision: 0.8134, Recall: 0.8021, F1: 0.8068
Testing Loss: 0.5819, Accuracy: 0.8563, Precision: 0.8224, Recall: 0.8128, F1: 0.8145
LM Predictions:  [5, 5, 2, 5, 5, 2, 4, 5, 5, 0, 0, 5, 1, 1, 0, 0, 4, 5, 5, 1, 5, 5, 5, 4, 5, 0, 1, 1, 5, 0, 5, 4, 0, 5, 5, 5, 5, 1, 0, 1, 1, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7832, Accuracy: 0.3333, Precision: 0.3634, Recall: 0.2474, F1: 0.2896
Epoch 23/70
Train Loss: 0.1574, Accuracy: 0.9490, Precision: 0.9159, Recall: 0.9312, F1: 0.9221
Validation Loss: 0.7157, Accuracy: 0.8593, Precision: 0.8300, Recall: 0.8124, F1: 0.8162
Testing Loss: 0.6466, Accuracy: 0.8671, Precision: 0.8192, Recall: 0.8189, F1: 0.8184
LM Predictions:  [5, 4, 0, 4, 0, 1, 4, 5, 5, 0, 0, 5, 3, 5, 4, 0, 4, 5, 5, 5, 0, 5, 5, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 5, 5, 5, 4, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8106, Accuracy: 0.5238, Precision: 0.5141, Recall: 0.4246, F1: 0.4294
Epoch 24/70
Train Loss: 0.1319, Accuracy: 0.9571, Precision: 0.9229, Recall: 0.9487, F1: 0.9338
Validation Loss: 0.7554, Accuracy: 0.8443, Precision: 0.8164, Recall: 0.7723, F1: 0.7839
Testing Loss: 0.6748, Accuracy: 0.8575, Precision: 0.8136, Recall: 0.7748, F1: 0.7870
LM Predictions:  [1, 4, 0, 5, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 5, 0, 5, 0, 1, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 5, 1, 1, 0, 1, 1, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4334, Accuracy: 0.5952, Precision: 0.5104, Recall: 0.4444, F1: 0.4163
Epoch 25/70
Train Loss: 0.1400, Accuracy: 0.9554, Precision: 0.9241, Recall: 0.9365, F1: 0.9297
Validation Loss: 0.8027, Accuracy: 0.8529, Precision: 0.8182, Recall: 0.8008, F1: 0.8081
Testing Loss: 0.6913, Accuracy: 0.8696, Precision: 0.8308, Recall: 0.8148, F1: 0.8216
LM Predictions:  [1, 5, 5, 5, 5, 1, 4, 3, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 3, 5, 5, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 5, 3, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5399, Accuracy: 0.5476, Precision: 0.5255, Recall: 0.4392, F1: 0.4729
Epoch 26/70
Train Loss: 0.1489, Accuracy: 0.9519, Precision: 0.9204, Recall: 0.9310, F1: 0.9253
Validation Loss: 0.7474, Accuracy: 0.8550, Precision: 0.8158, Recall: 0.8095, F1: 0.8122
Testing Loss: 0.6677, Accuracy: 0.8575, Precision: 0.8160, Recall: 0.8125, F1: 0.8130
LM Predictions:  [5, 5, 5, 5, 5, 3, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 0, 5, 5, 5, 5, 5, 4, 5, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 5, 5, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.5585, Accuracy: 0.5000, Precision: 0.5317, Recall: 0.3929, F1: 0.4392
Epoch 27/70
Train Loss: 0.1310, Accuracy: 0.9557, Precision: 0.9220, Recall: 0.9436, F1: 0.9311
Validation Loss: 0.8049, Accuracy: 0.8550, Precision: 0.8252, Recall: 0.8066, F1: 0.8143
Testing Loss: 0.6729, Accuracy: 0.8708, Precision: 0.8362, Recall: 0.8251, F1: 0.8290
LM Predictions:  [5, 4, 0, 5, 5, 3, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 0, 5, 5, 5, 5, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 5, 1, 1, 0, 3, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.4577, Accuracy: 0.5238, Precision: 0.4811, Recall: 0.4114, F1: 0.4278
Epoch 28/70
Train Loss: 0.1329, Accuracy: 0.9549, Precision: 0.9218, Recall: 0.9366, F1: 0.9284
Validation Loss: 0.7793, Accuracy: 0.8486, Precision: 0.8103, Recall: 0.7943, F1: 0.7997
Testing Loss: 0.6618, Accuracy: 0.8696, Precision: 0.8319, Recall: 0.8192, F1: 0.8246
LM Predictions:  [1, 4, 5, 2, 0, 3, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 5, 5, 0, 5, 5, 4, 0, 0, 1, 1, 0, 4, 2, 4, 0, 0, 5, 5, 2, 1, 0, 3, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.3235, Accuracy: 0.5476, Precision: 0.4666, Recall: 0.4365, F1: 0.4316
Epoch 29/70
Train Loss: 0.1262, Accuracy: 0.9592, Precision: 0.9316, Recall: 0.9402, F1: 0.9355
Validation Loss: 0.8297, Accuracy: 0.8486, Precision: 0.8086, Recall: 0.8076, F1: 0.8067
Testing Loss: 0.6829, Accuracy: 0.8671, Precision: 0.8285, Recall: 0.8284, F1: 0.8276
LM Predictions:  [1, 2, 2, 2, 5, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 1, 5, 2, 5, 4, 0, 0, 1, 1, 0, 4, 3, 4, 0, 5, 5, 2, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1247, Accuracy: 0.6667, Precision: 0.6572, Recall: 0.5463, F1: 0.5944
Epoch 30/70
Train Loss: 0.1359, Accuracy: 0.9568, Precision: 0.9287, Recall: 0.9381, F1: 0.9330
Validation Loss: 0.7976, Accuracy: 0.8486, Precision: 0.8146, Recall: 0.7939, F1: 0.8011
Testing Loss: 0.7062, Accuracy: 0.8635, Precision: 0.8315, Recall: 0.7914, F1: 0.8056
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 1, 0, 5, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 2, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0128, Accuracy: 0.6905, Precision: 0.6221, Recall: 0.5238, F1: 0.5190
Epoch 31/70
Train Loss: 0.1135, Accuracy: 0.9630, Precision: 0.9402, Recall: 0.9398, F1: 0.9399
Validation Loss: 0.8665, Accuracy: 0.8571, Precision: 0.8255, Recall: 0.8226, F1: 0.8240
Testing Loss: 0.7601, Accuracy: 0.8647, Precision: 0.8248, Recall: 0.8236, F1: 0.8224
LM Predictions:  [1, 2, 1, 1, 5, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 2, 5, 2, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 2, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9969, Accuracy: 0.7381, Precision: 0.7491, Recall: 0.5979, F1: 0.6568
Epoch 32/70
Train Loss: 0.1289, Accuracy: 0.9552, Precision: 0.9239, Recall: 0.9415, F1: 0.9314
Validation Loss: 0.8025, Accuracy: 0.8422, Precision: 0.7963, Recall: 0.7734, F1: 0.7821
Testing Loss: 0.6857, Accuracy: 0.8671, Precision: 0.8330, Recall: 0.7990, F1: 0.8134
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 1, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 5, 3, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1279, Accuracy: 0.7857, Precision: 0.6766, Recall: 0.6349, F1: 0.6383
Epoch 33/70
Train Loss: 0.1289, Accuracy: 0.9566, Precision: 0.9317, Recall: 0.9279, F1: 0.9297
Validation Loss: 0.8592, Accuracy: 0.8465, Precision: 0.8028, Recall: 0.7907, F1: 0.7935
Testing Loss: 0.8184, Accuracy: 0.8599, Precision: 0.8176, Recall: 0.7914, F1: 0.8015
LM Predictions:  [2, 4, 3, 4, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 3, 0, 4, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 2, 4, 0, 0, 5, 3, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1394, Accuracy: 0.7143, Precision: 0.6099, Recall: 0.5833, F1: 0.5768
Epoch 34/70
Train Loss: 0.1146, Accuracy: 0.9592, Precision: 0.9351, Recall: 0.9363, F1: 0.9356
Validation Loss: 0.9012, Accuracy: 0.8571, Precision: 0.8072, Recall: 0.8170, F1: 0.8111
Testing Loss: 0.7580, Accuracy: 0.8684, Precision: 0.8237, Recall: 0.8354, F1: 0.8285
LM Predictions:  [5, 2, 3, 1, 5, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 5, 5, 2, 5, 4, 0, 0, 1, 1, 0, 4, 2, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0578, Accuracy: 0.7143, Precision: 0.7292, Recall: 0.5701, F1: 0.6378
Epoch 35/70
Train Loss: 0.1222, Accuracy: 0.9578, Precision: 0.9292, Recall: 0.9394, F1: 0.9339
Validation Loss: 0.8084, Accuracy: 0.8529, Precision: 0.8111, Recall: 0.8117, F1: 0.8104
Testing Loss: 0.7475, Accuracy: 0.8659, Precision: 0.8229, Recall: 0.8244, F1: 0.8228
LM Predictions:  [2, 2, 3, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 5, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1063, Accuracy: 0.7857, Precision: 0.7611, Recall: 0.6349, F1: 0.6861
Epoch 36/70
Train Loss: 0.1100, Accuracy: 0.9635, Precision: 0.9454, Recall: 0.9351, F1: 0.9400
Validation Loss: 0.8143, Accuracy: 0.8529, Precision: 0.8167, Recall: 0.8057, F1: 0.8099
Testing Loss: 0.7245, Accuracy: 0.8659, Precision: 0.8246, Recall: 0.8138, F1: 0.8186
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9383, Accuracy: 0.8095, Precision: 0.7917, Recall: 0.6468, F1: 0.6854
Epoch 37/70
Train Loss: 0.1029, Accuracy: 0.9623, Precision: 0.9373, Recall: 0.9419, F1: 0.9394
Validation Loss: 0.7826, Accuracy: 0.8486, Precision: 0.8154, Recall: 0.8038, F1: 0.8092
Testing Loss: 0.7395, Accuracy: 0.8671, Precision: 0.8288, Recall: 0.8145, F1: 0.8208
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 0, 5, 5, 0, 2, 5, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 5, 5, 1, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0683, Accuracy: 0.7143, Precision: 0.7405, Recall: 0.5714, F1: 0.6128
Epoch 38/70
Train Loss: 0.0960, Accuracy: 0.9661, Precision: 0.9482, Recall: 0.9416, F1: 0.9448
Validation Loss: 0.7563, Accuracy: 0.8614, Precision: 0.8241, Recall: 0.8119, F1: 0.8165
Testing Loss: 0.7598, Accuracy: 0.8623, Precision: 0.8201, Recall: 0.7914, F1: 0.8028
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 5, 2, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9732, Accuracy: 0.7381, Precision: 0.7271, Recall: 0.5992, F1: 0.6154
Epoch 39/70
Train Loss: 0.1020, Accuracy: 0.9611, Precision: 0.9372, Recall: 0.9385, F1: 0.9378
Validation Loss: 0.7940, Accuracy: 0.8593, Precision: 0.8228, Recall: 0.8013, F1: 0.8090
Testing Loss: 0.8057, Accuracy: 0.8647, Precision: 0.8224, Recall: 0.7904, F1: 0.8027
LM Predictions:  [2, 1, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 5, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.1213, Accuracy: 0.7381, Precision: 0.7410, Recall: 0.5833, F1: 0.6120
Epoch 40/70
Train Loss: 0.1036, Accuracy: 0.9623, Precision: 0.9400, Recall: 0.9318, F1: 0.9357
Validation Loss: 0.7550, Accuracy: 0.8571, Precision: 0.8142, Recall: 0.8177, F1: 0.8145
Testing Loss: 0.7661, Accuracy: 0.8587, Precision: 0.8151, Recall: 0.8156, F1: 0.8149
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 5, 0, 4, 0, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 5, 5, 2, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0132, Accuracy: 0.7381, Precision: 0.7543, Recall: 0.5873, F1: 0.6265
Epoch 41/70
Train Loss: 0.1099, Accuracy: 0.9632, Precision: 0.9409, Recall: 0.9392, F1: 0.9400
Validation Loss: 0.7593, Accuracy: 0.8614, Precision: 0.8231, Recall: 0.8307, F1: 0.8266
Testing Loss: 0.7065, Accuracy: 0.8563, Precision: 0.8191, Recall: 0.8106, F1: 0.8134
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 5, 0, 4, 0, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9719, Accuracy: 0.8095, Precision: 0.8030, Recall: 0.6349, F1: 0.6842
Epoch 42/70
Train Loss: 0.1217, Accuracy: 0.9573, Precision: 0.9301, Recall: 0.9348, F1: 0.9323
Validation Loss: 0.8732, Accuracy: 0.8252, Precision: 0.7764, Recall: 0.7560, F1: 0.7604
Testing Loss: 0.7624, Accuracy: 0.8418, Precision: 0.7837, Recall: 0.7695, F1: 0.7750
LM Predictions:  [1, 2, 2, 1, 0, 1, 4, 1, 1, 0, 0, 4, 1, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9546, Accuracy: 0.7857, Precision: 0.7167, Recall: 0.6151, F1: 0.6088
Epoch 43/70
Train Loss: 0.1301, Accuracy: 0.9564, Precision: 0.9397, Recall: 0.9235, F1: 0.9307
Validation Loss: 0.7289, Accuracy: 0.8550, Precision: 0.8141, Recall: 0.8088, F1: 0.8085
Testing Loss: 0.7122, Accuracy: 0.8539, Precision: 0.8022, Recall: 0.7854, F1: 0.7927
LM Predictions:  [1, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 5, 1, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 2, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9961, Accuracy: 0.7381, Precision: 0.7028, Recall: 0.5635, F1: 0.5920
Epoch 44/70
Train Loss: 0.1102, Accuracy: 0.9592, Precision: 0.9340, Recall: 0.9351, F1: 0.9344
Validation Loss: 0.8000, Accuracy: 0.8550, Precision: 0.8209, Recall: 0.8037, F1: 0.8104
Testing Loss: 0.8099, Accuracy: 0.8527, Precision: 0.8090, Recall: 0.7820, F1: 0.7927
LM Predictions:  [1, 1, 0, 3, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 1, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 2, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.2301, Accuracy: 0.6429, Precision: 0.5802, Recall: 0.4881, F1: 0.4987
Epoch 45/70
Train Loss: 0.1336, Accuracy: 0.9585, Precision: 0.9254, Recall: 0.9508, F1: 0.9360
Validation Loss: 0.7894, Accuracy: 0.8529, Precision: 0.8123, Recall: 0.8075, F1: 0.8082
Testing Loss: 0.7931, Accuracy: 0.8466, Precision: 0.7900, Recall: 0.7764, F1: 0.7812
LM Predictions:  [2, 1, 2, 2, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 5, 3, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 2, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9967, Accuracy: 0.7143, Precision: 0.6107, Recall: 0.5516, F1: 0.5646
Epoch 46/70
Train Loss: 0.1295, Accuracy: 0.9597, Precision: 0.9368, Recall: 0.9350, F1: 0.9358
Validation Loss: 0.8157, Accuracy: 0.8571, Precision: 0.8247, Recall: 0.8168, F1: 0.8205
Testing Loss: 0.7873, Accuracy: 0.8587, Precision: 0.8228, Recall: 0.8067, F1: 0.8139
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8525, Accuracy: 0.8571, Precision: 0.8167, Recall: 0.6865, F1: 0.7349
Epoch 47/70
Train Loss: 0.1201, Accuracy: 0.9613, Precision: 0.9399, Recall: 0.9362, F1: 0.9380
Validation Loss: 0.8593, Accuracy: 0.8550, Precision: 0.8230, Recall: 0.8151, F1: 0.8187
Testing Loss: 0.7858, Accuracy: 0.8563, Precision: 0.8164, Recall: 0.8006, F1: 0.8076
LM Predictions:  [2, 2, 3, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 0, 0, 4, 0, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9294, Accuracy: 0.7857, Precision: 0.7361, Recall: 0.6230, F1: 0.6612
Epoch 48/70
Train Loss: 0.0916, Accuracy: 0.9651, Precision: 0.9401, Recall: 0.9455, F1: 0.9427
Validation Loss: 0.8845, Accuracy: 0.8486, Precision: 0.8156, Recall: 0.7936, F1: 0.8004
Testing Loss: 0.8761, Accuracy: 0.8551, Precision: 0.8119, Recall: 0.7818, F1: 0.7918
LM Predictions:  [2, 1, 3, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9886, Accuracy: 0.7619, Precision: 0.8317, Recall: 0.7286, F1: 0.7449
Epoch 49/70
Train Loss: 0.1008, Accuracy: 0.9651, Precision: 0.9521, Recall: 0.9350, F1: 0.9427
Validation Loss: 0.8851, Accuracy: 0.8507, Precision: 0.8189, Recall: 0.8063, F1: 0.8114
Testing Loss: 0.8312, Accuracy: 0.8599, Precision: 0.8201, Recall: 0.8042, F1: 0.8114
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 1, 2, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8916, Accuracy: 0.8095, Precision: 0.7543, Recall: 0.6746, F1: 0.6919
Epoch 50/70
Train Loss: 0.0872, Accuracy: 0.9647, Precision: 0.9437, Recall: 0.9396, F1: 0.9416
Validation Loss: 0.9395, Accuracy: 0.8550, Precision: 0.8271, Recall: 0.8109, F1: 0.8169
Testing Loss: 0.8412, Accuracy: 0.8575, Precision: 0.8086, Recall: 0.7885, F1: 0.7957
LM Predictions:  [5, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8031, Accuracy: 0.8333, Precision: 0.7667, Recall: 0.6706, F1: 0.6900
Epoch 51/70
Train Loss: 0.0869, Accuracy: 0.9654, Precision: 0.9423, Recall: 0.9444, F1: 0.9433
Validation Loss: 0.9482, Accuracy: 0.8465, Precision: 0.8015, Recall: 0.7781, F1: 0.7835
Testing Loss: 0.8166, Accuracy: 0.8527, Precision: 0.8041, Recall: 0.7752, F1: 0.7839
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7924, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 52/70
Train Loss: 0.0905, Accuracy: 0.9699, Precision: 0.9515, Recall: 0.9505, F1: 0.9510
Validation Loss: 0.9108, Accuracy: 0.8550, Precision: 0.8285, Recall: 0.7975, F1: 0.8094
Testing Loss: 0.8754, Accuracy: 0.8527, Precision: 0.8157, Recall: 0.7736, F1: 0.7896
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8770, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7619, F1: 0.7795
Epoch 53/70
Train Loss: 0.0906, Accuracy: 0.9659, Precision: 0.9432, Recall: 0.9414, F1: 0.9423
Validation Loss: 0.9266, Accuracy: 0.8486, Precision: 0.8024, Recall: 0.8013, F1: 0.8012
Testing Loss: 0.8613, Accuracy: 0.8575, Precision: 0.8134, Recall: 0.8034, F1: 0.8081
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7067, Accuracy: 0.8810, Precision: 0.8167, Recall: 0.7103, F1: 0.7477
Epoch 54/70
Train Loss: 0.0912, Accuracy: 0.9661, Precision: 0.9444, Recall: 0.9410, F1: 0.9426
Validation Loss: 0.9175, Accuracy: 0.8571, Precision: 0.8122, Recall: 0.7928, F1: 0.7968
Testing Loss: 0.8727, Accuracy: 0.8563, Precision: 0.8098, Recall: 0.7820, F1: 0.7886
LM Predictions:  [5, 5, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 5, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8302, Accuracy: 0.7619, Precision: 0.7667, Recall: 0.5714, F1: 0.5831
Epoch 55/70
Train Loss: 0.0933, Accuracy: 0.9670, Precision: 0.9500, Recall: 0.9385, F1: 0.9438
Validation Loss: 0.8550, Accuracy: 0.8614, Precision: 0.8238, Recall: 0.8197, F1: 0.8215
Testing Loss: 0.8289, Accuracy: 0.8599, Precision: 0.8216, Recall: 0.8078, F1: 0.8139
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7528, Accuracy: 0.8571, Precision: 0.8030, Recall: 0.6825, F1: 0.7120
Epoch 56/70
Train Loss: 0.0972, Accuracy: 0.9637, Precision: 0.9384, Recall: 0.9433, F1: 0.9407
Validation Loss: 0.8251, Accuracy: 0.8571, Precision: 0.8286, Recall: 0.8234, F1: 0.8246
Testing Loss: 0.8961, Accuracy: 0.8430, Precision: 0.8102, Recall: 0.7946, F1: 0.8003
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8454, Accuracy: 0.8333, Precision: 0.8030, Recall: 0.6746, F1: 0.7203
Epoch 57/70
Train Loss: 0.0968, Accuracy: 0.9640, Precision: 0.9418, Recall: 0.9411, F1: 0.9414
Validation Loss: 0.8512, Accuracy: 0.8614, Precision: 0.8271, Recall: 0.8115, F1: 0.8166
Testing Loss: 0.8307, Accuracy: 0.8599, Precision: 0.8136, Recall: 0.7912, F1: 0.7982
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8400, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7619, F1: 0.7758
Epoch 58/70
Train Loss: 0.0947, Accuracy: 0.9651, Precision: 0.9442, Recall: 0.9384, F1: 0.9411
Validation Loss: 0.8398, Accuracy: 0.8657, Precision: 0.8277, Recall: 0.8244, F1: 0.8252
Testing Loss: 0.8329, Accuracy: 0.8611, Precision: 0.8152, Recall: 0.8029, F1: 0.8080
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 3, 5, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8561, Accuracy: 0.7857, Precision: 0.7667, Recall: 0.6389, F1: 0.6732
Epoch 59/70
Train Loss: 0.0915, Accuracy: 0.9656, Precision: 0.9431, Recall: 0.9378, F1: 0.9403
Validation Loss: 0.7864, Accuracy: 0.8571, Precision: 0.8076, Recall: 0.7951, F1: 0.7976
Testing Loss: 0.7503, Accuracy: 0.8502, Precision: 0.7839, Recall: 0.7621, F1: 0.7634
LM Predictions:  [2, 2, 1, 0, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 5, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0100, Accuracy: 0.8095, Precision: 0.7604, Recall: 0.6587, F1: 0.6682
Epoch 60/70
Train Loss: 0.1027, Accuracy: 0.9642, Precision: 0.9486, Recall: 0.9321, F1: 0.9388
Validation Loss: 0.8578, Accuracy: 0.8529, Precision: 0.8273, Recall: 0.7930, F1: 0.8058
Testing Loss: 0.8094, Accuracy: 0.8611, Precision: 0.8228, Recall: 0.7891, F1: 0.8008
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7903, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7905, F1: 0.7963
Epoch 61/70
Train Loss: 0.1071, Accuracy: 0.9644, Precision: 0.9463, Recall: 0.9376, F1: 0.9417
Validation Loss: 0.8990, Accuracy: 0.8486, Precision: 0.8020, Recall: 0.8014, F1: 0.8012
Testing Loss: 0.7599, Accuracy: 0.8684, Precision: 0.8263, Recall: 0.8202, F1: 0.8225
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 5, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7913, Accuracy: 0.8333, Precision: 0.7917, Recall: 0.6865, F1: 0.7188
Epoch 62/70
Train Loss: 0.0921, Accuracy: 0.9621, Precision: 0.9357, Recall: 0.9417, F1: 0.9386
Validation Loss: 0.8680, Accuracy: 0.8550, Precision: 0.8080, Recall: 0.8143, F1: 0.8110
Testing Loss: 0.7458, Accuracy: 0.8744, Precision: 0.8397, Recall: 0.8303, F1: 0.8344
LM Predictions:  [2, 2, 0, 1, 5, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 5, 4, 0, 5, 2, 0, 2, 5, 4, 5, 0, 1, 1, 0, 0, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9471, Accuracy: 0.6667, Precision: 0.7436, Recall: 0.5317, F1: 0.5936
Epoch 63/70
Train Loss: 0.0913, Accuracy: 0.9630, Precision: 0.9495, Recall: 0.9250, F1: 0.9353
Validation Loss: 0.8137, Accuracy: 0.8550, Precision: 0.8219, Recall: 0.7981, F1: 0.8058
Testing Loss: 0.7534, Accuracy: 0.8599, Precision: 0.8164, Recall: 0.7948, F1: 0.8024
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7284, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8048, F1: 0.8104
Epoch 64/70
Train Loss: 0.0822, Accuracy: 0.9666, Precision: 0.9494, Recall: 0.9360, F1: 0.9422
Validation Loss: 0.8479, Accuracy: 0.8550, Precision: 0.8182, Recall: 0.8041, F1: 0.8103
Testing Loss: 0.7698, Accuracy: 0.8671, Precision: 0.8300, Recall: 0.8135, F1: 0.8207
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7781, Accuracy: 0.8095, Precision: 0.7667, Recall: 0.6587, F1: 0.6732
Epoch 65/70
Train Loss: 0.0765, Accuracy: 0.9670, Precision: 0.9471, Recall: 0.9455, F1: 0.9462
Validation Loss: 0.8670, Accuracy: 0.8571, Precision: 0.8258, Recall: 0.7952, F1: 0.8056
Testing Loss: 0.8030, Accuracy: 0.8696, Precision: 0.8245, Recall: 0.7937, F1: 0.8052
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 5, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7002, Accuracy: 0.8333, Precision: 0.7667, Recall: 0.6587, F1: 0.6742
Epoch 66/70
Train Loss: 0.0774, Accuracy: 0.9659, Precision: 0.9509, Recall: 0.9329, F1: 0.9408
Validation Loss: 0.8396, Accuracy: 0.8550, Precision: 0.8137, Recall: 0.8109, F1: 0.8107
Testing Loss: 0.7682, Accuracy: 0.8635, Precision: 0.8264, Recall: 0.8227, F1: 0.8235
LM Predictions:  [2, 2, 1, 1, 5, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 5, 4, 3, 5, 2, 0, 2, 5, 4, 5, 0, 1, 1, 0, 4, 1, 4, 0, 0, 5, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8940, Accuracy: 0.8095, Precision: 0.7917, Recall: 0.6548, F1: 0.7074
Epoch 67/70
Train Loss: 0.0922, Accuracy: 0.9630, Precision: 0.9417, Recall: 0.9380, F1: 0.9397
Validation Loss: 0.8554, Accuracy: 0.8465, Precision: 0.8071, Recall: 0.7849, F1: 0.7903
Testing Loss: 0.7570, Accuracy: 0.8551, Precision: 0.8104, Recall: 0.7917, F1: 0.7988
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7143, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7333, F1: 0.7532
Epoch 68/70
Train Loss: 0.0927, Accuracy: 0.9644, Precision: 0.9494, Recall: 0.9319, F1: 0.9398
Validation Loss: 0.9039, Accuracy: 0.8465, Precision: 0.8134, Recall: 0.7735, F1: 0.7865
Testing Loss: 0.7715, Accuracy: 0.8587, Precision: 0.8165, Recall: 0.7722, F1: 0.7844
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 5, 0, 5, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8992, Accuracy: 0.7381, Precision: 0.7549, Recall: 0.5794, F1: 0.6103
Epoch 69/70
Train Loss: 0.0805, Accuracy: 0.9656, Precision: 0.9453, Recall: 0.9382, F1: 0.9415
Validation Loss: 1.0016, Accuracy: 0.8486, Precision: 0.8067, Recall: 0.7875, F1: 0.7957
Testing Loss: 0.8180, Accuracy: 0.8684, Precision: 0.8323, Recall: 0.8068, F1: 0.8178
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 5, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 5, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8980, Accuracy: 0.8333, Precision: 0.7821, Recall: 0.6706, F1: 0.7013
Epoch 70/70
Train Loss: 0.0874, Accuracy: 0.9680, Precision: 0.9469, Recall: 0.9451, F1: 0.9459
Validation Loss: 0.8459, Accuracy: 0.8507, Precision: 0.8099, Recall: 0.7806, F1: 0.7888
Testing Loss: 0.7396, Accuracy: 0.8671, Precision: 0.8269, Recall: 0.7876, F1: 0.7995
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 0, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 0, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8368, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7905, F1: 0.7963
For later layers:  [8, 9, 10, 11]
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.1846, Accuracy: 0.5639, Precision: 0.4910, Recall: 0.4133, F1: 0.4106
Validation Loss: 0.6139, Accuracy: 0.8060, Precision: 0.6560, Recall: 0.7113, F1: 0.6782
Testing Loss: 0.5251, Accuracy: 0.8418, Precision: 0.6808, Recall: 0.7382, F1: 0.7061
LM Predictions:  [3, 4, 0, 3, 3, 4, 3, 3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 3, 3, 0, 0, 3, 3, 3, 0, 0, 4, 3, 0, 3, 0, 0, 0, 3, 3, 3, 2, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.3410, Accuracy: 0.0714, Precision: 0.0311, Recall: 0.0778, F1: 0.0443
Epoch 2/70
Train Loss: 0.4975, Accuracy: 0.8501, Precision: 0.7690, Recall: 0.7512, F1: 0.7528
Validation Loss: 0.4970, Accuracy: 0.8380, Precision: 0.7147, Recall: 0.7256, F1: 0.7176
Testing Loss: 0.4446, Accuracy: 0.8587, Precision: 0.8961, Recall: 0.7481, F1: 0.7395
LM Predictions:  [0, 4, 2, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.6978, Accuracy: 0.1905, Precision: 0.0370, Recall: 0.1481, F1: 0.0593
Epoch 3/70
Train Loss: 0.3794, Accuracy: 0.8814, Precision: 0.8266, Recall: 0.8206, F1: 0.8232
Validation Loss: 0.4417, Accuracy: 0.8571, Precision: 0.8229, Recall: 0.8574, F1: 0.8268
Testing Loss: 0.3897, Accuracy: 0.8877, Precision: 0.8549, Recall: 0.8800, F1: 0.8594
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.8962, Accuracy: 0.0000, Precision: 0.0000, Recall: 0.0000, F1: 0.0000
Epoch 4/70
Train Loss: 0.3352, Accuracy: 0.8945, Precision: 0.8433, Recall: 0.8487, F1: 0.8451
Validation Loss: 0.4797, Accuracy: 0.8657, Precision: 0.8505, Recall: 0.8148, F1: 0.8278
Testing Loss: 0.4554, Accuracy: 0.8696, Precision: 0.8451, Recall: 0.8069, F1: 0.8175
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 5, 0, 5, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7175, Accuracy: 0.0714, Precision: 0.0312, Recall: 0.0556, F1: 0.0400
Epoch 5/70
Train Loss: 0.2811, Accuracy: 0.9158, Precision: 0.8705, Recall: 0.8777, F1: 0.8732
Validation Loss: 0.4948, Accuracy: 0.8657, Precision: 0.8376, Recall: 0.8201, F1: 0.8272
Testing Loss: 0.4408, Accuracy: 0.8780, Precision: 0.8400, Recall: 0.8181, F1: 0.8273
LM Predictions:  [5, 5, 0, 5, 5, 5, 5, 5, 0, 0, 0, 0, 3, 5, 0, 0, 5, 0, 5, 5, 0, 5, 0, 5, 5, 5, 5, 0, 3, 5, 0, 5, 0, 0, 0, 3, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.7800, Accuracy: 0.1190, Precision: 0.0948, Recall: 0.1019, F1: 0.0883
Epoch 6/70
Train Loss: 0.2431, Accuracy: 0.9277, Precision: 0.8876, Recall: 0.8964, F1: 0.8911
Validation Loss: 0.5365, Accuracy: 0.8763, Precision: 0.8510, Recall: 0.8473, F1: 0.8483
Testing Loss: 0.4711, Accuracy: 0.8780, Precision: 0.8423, Recall: 0.8380, F1: 0.8383
LM Predictions:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 3, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.5480, Accuracy: 0.1190, Precision: 0.2500, Recall: 0.1019, F1: 0.1261
Epoch 7/70
Train Loss: 0.2383, Accuracy: 0.9300, Precision: 0.8928, Recall: 0.9071, F1: 0.8984
Validation Loss: 0.5610, Accuracy: 0.8742, Precision: 0.8508, Recall: 0.8476, F1: 0.8487
Testing Loss: 0.4610, Accuracy: 0.8732, Precision: 0.8304, Recall: 0.8274, F1: 0.8279
LM Predictions:  [5, 2, 2, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 5, 5, 0, 4, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 4, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.2324, Accuracy: 0.2143, Precision: 0.5093, Recall: 0.1958, F1: 0.2560
Epoch 8/70
Train Loss: 0.2110, Accuracy: 0.9346, Precision: 0.8959, Recall: 0.9050, F1: 0.8996
Validation Loss: 0.5561, Accuracy: 0.8635, Precision: 0.8299, Recall: 0.8360, F1: 0.8325
Testing Loss: 0.4636, Accuracy: 0.8780, Precision: 0.8332, Recall: 0.8371, F1: 0.8346
LM Predictions:  [5, 2, 0, 5, 5, 5, 5, 5, 5, 0, 0, 4, 3, 5, 4, 0, 4, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 2, 0, 5, 5, 4, 0, 5, 5, 5, 5, 5, 5, 5, 5, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3054, Accuracy: 0.2619, Precision: 0.5024, Recall: 0.2434, F1: 0.3046
Epoch 9/70
Train Loss: 0.1961, Accuracy: 0.9379, Precision: 0.8996, Recall: 0.9072, F1: 0.9026
Validation Loss: 0.5785, Accuracy: 0.8571, Precision: 0.8249, Recall: 0.8248, F1: 0.8244
Testing Loss: 0.4797, Accuracy: 0.8708, Precision: 0.8276, Recall: 0.8233, F1: 0.8253
LM Predictions:  [5, 2, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 0, 5, 5, 0, 5, 5, 5, 0, 0, 5, 1, 0, 5, 0, 4, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.3422, Accuracy: 0.3095, Precision: 0.7500, Recall: 0.2685, F1: 0.3189
Epoch 10/70
Train Loss: 0.1771, Accuracy: 0.9450, Precision: 0.9086, Recall: 0.9180, F1: 0.9127
Validation Loss: 0.5584, Accuracy: 0.8550, Precision: 0.8207, Recall: 0.8404, F1: 0.8275
Testing Loss: 0.5120, Accuracy: 0.8623, Precision: 0.8351, Recall: 0.8356, F1: 0.8312
LM Predictions:  [5, 2, 0, 5, 5, 5, 5, 5, 5, 0, 0, 0, 3, 5, 4, 0, 4, 5, 5, 1, 5, 5, 5, 5, 5, 0, 5, 1, 0, 4, 5, 4, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0225, Accuracy: 0.2857, Precision: 0.6667, Recall: 0.2553, F1: 0.3250
Epoch 11/70
Train Loss: 0.1647, Accuracy: 0.9471, Precision: 0.9099, Recall: 0.9294, F1: 0.9176
Validation Loss: 0.6910, Accuracy: 0.8593, Precision: 0.8337, Recall: 0.8088, F1: 0.8193
Testing Loss: 0.6073, Accuracy: 0.8635, Precision: 0.8299, Recall: 0.8014, F1: 0.8132
LM Predictions:  [5, 2, 0, 5, 5, 5, 4, 5, 0, 0, 0, 4, 3, 5, 4, 0, 4, 0, 4, 2, 0, 5, 0, 5, 5, 0, 5, 1, 0, 4, 5, 4, 0, 5, 5, 5, 5, 5, 0, 2, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.8693, Accuracy: 0.4048, Precision: 0.6592, Recall: 0.3677, F1: 0.3833
Epoch 12/70
Train Loss: 0.1564, Accuracy: 0.9447, Precision: 0.9073, Recall: 0.9203, F1: 0.9128
Validation Loss: 0.7002, Accuracy: 0.8529, Precision: 0.8296, Recall: 0.7840, F1: 0.7942
Testing Loss: 0.6345, Accuracy: 0.8623, Precision: 0.8472, Recall: 0.7806, F1: 0.7978
LM Predictions:  [5, 2, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 3, 5, 4, 0, 4, 0, 0, 2, 0, 5, 0, 4, 0, 0, 0, 1, 0, 4, 0, 4, 0, 0, 0, 5, 5, 5, 0, 3, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 2.0283, Accuracy: 0.4524, Precision: 0.6486, Recall: 0.4087, F1: 0.4016
Epoch 13/70
Train Loss: 0.1564, Accuracy: 0.9459, Precision: 0.9099, Recall: 0.9240, F1: 0.9161
Validation Loss: 0.6911, Accuracy: 0.8486, Precision: 0.8241, Recall: 0.7800, F1: 0.7952
Testing Loss: 0.6586, Accuracy: 0.8647, Precision: 0.8500, Recall: 0.7859, F1: 0.8063
LM Predictions:  [5, 2, 0, 4, 0, 5, 4, 0, 5, 0, 0, 4, 3, 5, 4, 0, 4, 0, 4, 5, 0, 5, 0, 4, 0, 0, 5, 1, 0, 4, 0, 4, 0, 0, 0, 5, 5, 5, 0, 1, 0, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7945, Accuracy: 0.4762, Precision: 0.7000, Recall: 0.4127, F1: 0.3853
Epoch 14/70
Train Loss: 0.1583, Accuracy: 0.9450, Precision: 0.9171, Recall: 0.9136, F1: 0.9153
Validation Loss: 0.6806, Accuracy: 0.8550, Precision: 0.8270, Recall: 0.8022, F1: 0.8118
Testing Loss: 0.5962, Accuracy: 0.8611, Precision: 0.8240, Recall: 0.8004, F1: 0.8101
LM Predictions:  [5, 2, 0, 5, 5, 5, 4, 0, 0, 0, 0, 4, 3, 5, 4, 0, 4, 5, 5, 2, 0, 5, 0, 4, 0, 0, 0, 1, 0, 4, 0, 4, 0, 0, 0, 5, 0, 5, 0, 1, 1, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.2695, Accuracy: 0.4762, Precision: 0.6852, Recall: 0.4220, F1: 0.4356
Epoch 15/70
Train Loss: 0.1269, Accuracy: 0.9540, Precision: 0.9210, Recall: 0.9379, F1: 0.9283
Validation Loss: 0.7054, Accuracy: 0.8571, Precision: 0.8334, Recall: 0.8007, F1: 0.8124
Testing Loss: 0.6457, Accuracy: 0.8659, Precision: 0.8362, Recall: 0.7960, F1: 0.8107
LM Predictions:  [2, 2, 1, 5, 0, 5, 0, 0, 1, 0, 1, 0, 3, 5, 4, 0, 4, 0, 0, 2, 0, 5, 0, 4, 0, 0, 5, 1, 0, 0, 5, 4, 0, 0, 0, 5, 0, 5, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0551, Accuracy: 0.5238, Precision: 0.6789, Recall: 0.4484, F1: 0.4829
Epoch 16/70
Train Loss: 0.1353, Accuracy: 0.9540, Precision: 0.9248, Recall: 0.9303, F1: 0.9273
Validation Loss: 0.7438, Accuracy: 0.8507, Precision: 0.8205, Recall: 0.7908, F1: 0.8013
Testing Loss: 0.6794, Accuracy: 0.8659, Precision: 0.8327, Recall: 0.7972, F1: 0.8107
LM Predictions:  [2, 2, 0, 5, 0, 5, 4, 0, 0, 0, 0, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 5, 0, 4, 0, 0, 5, 1, 0, 0, 0, 4, 0, 5, 0, 1, 0, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9669, Accuracy: 0.5952, Precision: 0.7218, Recall: 0.5079, F1: 0.5322
Epoch 17/70
Train Loss: 0.1262, Accuracy: 0.9523, Precision: 0.9227, Recall: 0.9294, F1: 0.9258
Validation Loss: 0.6999, Accuracy: 0.8635, Precision: 0.8353, Recall: 0.8193, F1: 0.8266
Testing Loss: 0.6779, Accuracy: 0.8708, Precision: 0.8422, Recall: 0.8147, F1: 0.8267
LM Predictions:  [2, 2, 5, 5, 0, 5, 4, 5, 1, 0, 1, 0, 3, 5, 0, 0, 4, 0, 5, 2, 0, 5, 0, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 5, 5, 5, 5, 5, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0704, Accuracy: 0.5476, Precision: 0.7127, Recall: 0.4603, F1: 0.5193
Epoch 18/70
Train Loss: 0.1074, Accuracy: 0.9590, Precision: 0.9317, Recall: 0.9379, F1: 0.9346
Validation Loss: 0.7642, Accuracy: 0.8571, Precision: 0.8273, Recall: 0.8182, F1: 0.8224
Testing Loss: 0.6810, Accuracy: 0.8732, Precision: 0.8378, Recall: 0.8179, F1: 0.8268
LM Predictions:  [2, 2, 0, 2, 5, 1, 4, 1, 1, 0, 1, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 5, 0, 4, 0, 0, 1, 1, 0, 5, 1, 4, 0, 1, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9186, Accuracy: 0.7381, Precision: 0.6986, Recall: 0.5728, F1: 0.6024
Epoch 19/70
Train Loss: 0.1015, Accuracy: 0.9599, Precision: 0.9325, Recall: 0.9375, F1: 0.9349
Validation Loss: 0.8033, Accuracy: 0.8635, Precision: 0.8310, Recall: 0.8325, F1: 0.8313
Testing Loss: 0.6903, Accuracy: 0.8671, Precision: 0.8249, Recall: 0.8184, F1: 0.8215
LM Predictions:  [2, 2, 1, 5, 0, 2, 4, 1, 1, 0, 1, 0, 3, 1, 4, 0, 4, 3, 5, 2, 0, 5, 0, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5961, Accuracy: 0.7857, Precision: 0.7134, Recall: 0.6190, F1: 0.6450
Epoch 20/70
Train Loss: 0.1010, Accuracy: 0.9592, Precision: 0.9340, Recall: 0.9355, F1: 0.9348
Validation Loss: 0.9052, Accuracy: 0.8593, Precision: 0.8294, Recall: 0.8218, F1: 0.8251
Testing Loss: 0.7804, Accuracy: 0.8768, Precision: 0.8340, Recall: 0.8187, F1: 0.8258
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 5, 2, 0, 2, 3, 4, 0, 0, 5, 1, 0, 4, 1, 4, 0, 1, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4131, Accuracy: 0.9048, Precision: 0.8048, Recall: 0.7381, F1: 0.7642
Epoch 21/70
Train Loss: 0.0968, Accuracy: 0.9632, Precision: 0.9374, Recall: 0.9450, F1: 0.9411
Validation Loss: 0.9320, Accuracy: 0.8443, Precision: 0.8056, Recall: 0.7770, F1: 0.7849
Testing Loss: 0.7835, Accuracy: 0.8732, Precision: 0.8469, Recall: 0.8057, F1: 0.8213
LM Predictions:  [2, 2, 0, 0, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 5, 2, 0, 2, 3, 4, 0, 0, 2, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5995, Accuracy: 0.7857, Precision: 0.7326, Recall: 0.6429, F1: 0.6601
Epoch 22/70
Train Loss: 0.0838, Accuracy: 0.9625, Precision: 0.9380, Recall: 0.9408, F1: 0.9394
Validation Loss: 0.8979, Accuracy: 0.8273, Precision: 0.7597, Recall: 0.7317, F1: 0.7301
Testing Loss: 0.7987, Accuracy: 0.8599, Precision: 0.8287, Recall: 0.7586, F1: 0.7681
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5088, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.7810, F1: 0.7895
Epoch 23/70
Train Loss: 0.0767, Accuracy: 0.9651, Precision: 0.9454, Recall: 0.9369, F1: 0.9410
Validation Loss: 1.0815, Accuracy: 0.8401, Precision: 0.8152, Recall: 0.7508, F1: 0.7638
Testing Loss: 0.9868, Accuracy: 0.8406, Precision: 0.7593, Recall: 0.7264, F1: 0.7309
LM Predictions:  [5, 2, 1, 2, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 1, 3, 4, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8353, Accuracy: 0.7619, Precision: 0.7132, Recall: 0.6111, F1: 0.6322
Epoch 24/70
Train Loss: 0.0926, Accuracy: 0.9628, Precision: 0.9406, Recall: 0.9445, F1: 0.9425
Validation Loss: 1.0001, Accuracy: 0.8507, Precision: 0.8227, Recall: 0.8129, F1: 0.8161
Testing Loss: 0.8209, Accuracy: 0.8623, Precision: 0.8240, Recall: 0.8086, F1: 0.8148
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3507, Accuracy: 0.8810, Precision: 0.7917, Recall: 0.7024, F1: 0.7262
Epoch 25/70
Train Loss: 0.0785, Accuracy: 0.9647, Precision: 0.9416, Recall: 0.9456, F1: 0.9435
Validation Loss: 0.7910, Accuracy: 0.8614, Precision: 0.8398, Recall: 0.8110, F1: 0.8224
Testing Loss: 0.7374, Accuracy: 0.8635, Precision: 0.8258, Recall: 0.7965, F1: 0.8079
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 1, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3868, Accuracy: 0.8333, Precision: 0.7619, Recall: 0.6667, F1: 0.6913
Epoch 26/70
Train Loss: 0.0950, Accuracy: 0.9604, Precision: 0.9334, Recall: 0.9389, F1: 0.9360
Validation Loss: 0.8919, Accuracy: 0.8593, Precision: 0.8326, Recall: 0.8158, F1: 0.8233
Testing Loss: 0.7854, Accuracy: 0.8768, Precision: 0.8475, Recall: 0.8194, F1: 0.8320
LM Predictions:  [2, 2, 0, 1, 0, 1, 5, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6430, Accuracy: 0.8095, Precision: 0.7604, Recall: 0.6429, F1: 0.6653
Epoch 27/70
Train Loss: 0.0753, Accuracy: 0.9666, Precision: 0.9433, Recall: 0.9468, F1: 0.9450
Validation Loss: 0.9914, Accuracy: 0.8529, Precision: 0.8249, Recall: 0.7979, F1: 0.8093
Testing Loss: 0.8774, Accuracy: 0.8720, Precision: 0.8442, Recall: 0.8084, F1: 0.8235
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3313, Accuracy: 0.8810, Precision: 0.7917, Recall: 0.7063, F1: 0.7307
Epoch 28/70
Train Loss: 0.0753, Accuracy: 0.9654, Precision: 0.9441, Recall: 0.9438, F1: 0.9439
Validation Loss: 0.8926, Accuracy: 0.8657, Precision: 0.8356, Recall: 0.8260, F1: 0.8305
Testing Loss: 0.8227, Accuracy: 0.8671, Precision: 0.8346, Recall: 0.8167, F1: 0.8250
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3502, Accuracy: 0.8571, Precision: 0.7821, Recall: 0.6944, F1: 0.7181
Epoch 29/70
Train Loss: 0.0696, Accuracy: 0.9644, Precision: 0.9414, Recall: 0.9380, F1: 0.9397
Validation Loss: 1.0229, Accuracy: 0.8614, Precision: 0.8306, Recall: 0.8229, F1: 0.8259
Testing Loss: 0.8760, Accuracy: 0.8684, Precision: 0.8344, Recall: 0.8244, F1: 0.8290
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1669, Accuracy: 0.9524, Precision: 0.8167, Recall: 0.7778, F1: 0.7912
Epoch 30/70
Train Loss: 0.0632, Accuracy: 0.9668, Precision: 0.9427, Recall: 0.9483, F1: 0.9455
Validation Loss: 0.9679, Accuracy: 0.8465, Precision: 0.8122, Recall: 0.7737, F1: 0.7862
Testing Loss: 0.9379, Accuracy: 0.8611, Precision: 0.8350, Recall: 0.7747, F1: 0.7950
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3812, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917
Epoch 31/70
Train Loss: 0.0666, Accuracy: 0.9666, Precision: 0.9441, Recall: 0.9440, F1: 0.9440
Validation Loss: 1.0659, Accuracy: 0.8635, Precision: 0.8354, Recall: 0.8224, F1: 0.8275
Testing Loss: 0.9262, Accuracy: 0.8684, Precision: 0.8353, Recall: 0.8176, F1: 0.8257
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2327, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9132
Epoch 32/70
Train Loss: 0.0775, Accuracy: 0.9651, Precision: 0.9427, Recall: 0.9415, F1: 0.9421
Validation Loss: 1.0232, Accuracy: 0.8678, Precision: 0.8374, Recall: 0.8259, F1: 0.8307
Testing Loss: 0.9507, Accuracy: 0.8623, Precision: 0.8268, Recall: 0.8062, F1: 0.8154
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2594, Accuracy: 0.9048, Precision: 0.7917, Recall: 0.7381, F1: 0.7549
Epoch 33/70
Train Loss: 0.0657, Accuracy: 0.9677, Precision: 0.9468, Recall: 0.9473, F1: 0.9470
Validation Loss: 1.0025, Accuracy: 0.8614, Precision: 0.8251, Recall: 0.8138, F1: 0.8173
Testing Loss: 0.8745, Accuracy: 0.8671, Precision: 0.8325, Recall: 0.8135, F1: 0.8222
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1800, Accuracy: 0.9286, Precision: 0.8030, Recall: 0.7579, F1: 0.7737
Epoch 34/70
Train Loss: 0.0772, Accuracy: 0.9613, Precision: 0.9390, Recall: 0.9393, F1: 0.9391
Validation Loss: 0.8790, Accuracy: 0.8593, Precision: 0.8289, Recall: 0.8255, F1: 0.8241
Testing Loss: 0.7949, Accuracy: 0.8551, Precision: 0.8214, Recall: 0.8170, F1: 0.8170
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 2, 4, 2, 4, 0, 0, 1, 1, 1, 1, 2, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2823, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9079, F1: 0.8991
Epoch 35/70
Train Loss: 0.0680, Accuracy: 0.9680, Precision: 0.9498, Recall: 0.9482, F1: 0.9490
Validation Loss: 0.9465, Accuracy: 0.8550, Precision: 0.8146, Recall: 0.7945, F1: 0.8009
Testing Loss: 0.8980, Accuracy: 0.8696, Precision: 0.8375, Recall: 0.8007, F1: 0.8144
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1145, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 36/70
Train Loss: 0.0683, Accuracy: 0.9675, Precision: 0.9480, Recall: 0.9468, F1: 0.9474
Validation Loss: 1.0198, Accuracy: 0.8571, Precision: 0.8287, Recall: 0.8185, F1: 0.8216
Testing Loss: 0.8508, Accuracy: 0.8671, Precision: 0.8337, Recall: 0.8124, F1: 0.8216
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 1, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1492, Accuracy: 0.9762, Precision: 0.9867, Recall: 0.9667, F1: 0.9749
Epoch 37/70
Train Loss: 0.0785, Accuracy: 0.9632, Precision: 0.9383, Recall: 0.9478, F1: 0.9428
Validation Loss: 1.0121, Accuracy: 0.8401, Precision: 0.8049, Recall: 0.7771, F1: 0.7855
Testing Loss: 0.8236, Accuracy: 0.8684, Precision: 0.8343, Recall: 0.7918, F1: 0.8062
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2717, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 38/70
Train Loss: 0.0656, Accuracy: 0.9654, Precision: 0.9498, Recall: 0.9324, F1: 0.9403
Validation Loss: 1.0243, Accuracy: 0.8486, Precision: 0.8154, Recall: 0.8028, F1: 0.8070
Testing Loss: 0.7693, Accuracy: 0.8744, Precision: 0.8389, Recall: 0.8214, F1: 0.8295
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1467, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8857, F1: 0.8896
Epoch 39/70
Train Loss: 0.0566, Accuracy: 0.9689, Precision: 0.9467, Recall: 0.9529, F1: 0.9497
Validation Loss: 1.0544, Accuracy: 0.8614, Precision: 0.8359, Recall: 0.8138, F1: 0.8233
Testing Loss: 0.8914, Accuracy: 0.8744, Precision: 0.8500, Recall: 0.8081, F1: 0.8253
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2853, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8238, F1: 0.8424
Epoch 40/70
Train Loss: 0.0576, Accuracy: 0.9689, Precision: 0.9491, Recall: 0.9475, F1: 0.9483
Validation Loss: 1.0593, Accuracy: 0.8614, Precision: 0.8419, Recall: 0.8152, F1: 0.8268
Testing Loss: 0.8936, Accuracy: 0.8708, Precision: 0.8367, Recall: 0.8009, F1: 0.8157
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1839, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 41/70
Train Loss: 0.0523, Accuracy: 0.9692, Precision: 0.9481, Recall: 0.9525, F1: 0.9502
Validation Loss: 0.9178, Accuracy: 0.8550, Precision: 0.8325, Recall: 0.8260, F1: 0.8279
Testing Loss: 0.7876, Accuracy: 0.8696, Precision: 0.8409, Recall: 0.8224, F1: 0.8294
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1617, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9524, F1: 0.9544
Epoch 42/70
Train Loss: 0.0674, Accuracy: 0.9659, Precision: 0.9405, Recall: 0.9471, F1: 0.9437
Validation Loss: 1.0553, Accuracy: 0.8465, Precision: 0.8035, Recall: 0.7802, F1: 0.7869
Testing Loss: 0.8751, Accuracy: 0.8756, Precision: 0.8421, Recall: 0.8021, F1: 0.8170
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2113, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 43/70
Train Loss: 0.0529, Accuracy: 0.9682, Precision: 0.9542, Recall: 0.9398, F1: 0.9465
Validation Loss: 1.1178, Accuracy: 0.8443, Precision: 0.8114, Recall: 0.7888, F1: 0.7979
Testing Loss: 0.9579, Accuracy: 0.8708, Precision: 0.8391, Recall: 0.8017, F1: 0.8170
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1590, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 44/70
Train Loss: 0.0577, Accuracy: 0.9668, Precision: 0.9450, Recall: 0.9428, F1: 0.9439
Validation Loss: 0.9767, Accuracy: 0.8401, Precision: 0.7938, Recall: 0.7566, F1: 0.7613
Testing Loss: 0.7309, Accuracy: 0.8732, Precision: 0.8180, Recall: 0.7759, F1: 0.7853
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2651, Accuracy: 0.8095, Precision: 0.7604, Recall: 0.6548, F1: 0.6799
Epoch 45/70
Train Loss: 0.0566, Accuracy: 0.9656, Precision: 0.9448, Recall: 0.9391, F1: 0.9418
Validation Loss: 1.1688, Accuracy: 0.8465, Precision: 0.8128, Recall: 0.7720, F1: 0.7846
Testing Loss: 0.9694, Accuracy: 0.8659, Precision: 0.8372, Recall: 0.7774, F1: 0.7944
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1894, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7810, F1: 0.8049
Epoch 46/70
Train Loss: 0.0618, Accuracy: 0.9689, Precision: 0.9489, Recall: 0.9447, F1: 0.9467
Validation Loss: 0.9271, Accuracy: 0.8337, Precision: 0.7753, Recall: 0.7612, F1: 0.7648
Testing Loss: 0.7366, Accuracy: 0.8587, Precision: 0.8132, Recall: 0.7867, F1: 0.7954
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 3, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2229, Accuracy: 0.8571, Precision: 0.9000, Recall: 0.8524, F1: 0.8530
Epoch 47/70
Train Loss: 0.0556, Accuracy: 0.9659, Precision: 0.9390, Recall: 0.9506, F1: 0.9443
Validation Loss: 1.1433, Accuracy: 0.8486, Precision: 0.8114, Recall: 0.7812, F1: 0.7898
Testing Loss: 0.9319, Accuracy: 0.8756, Precision: 0.8394, Recall: 0.7978, F1: 0.8108
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1593, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8667, F1: 0.8764
Epoch 48/70
Train Loss: 0.0561, Accuracy: 0.9680, Precision: 0.9500, Recall: 0.9410, F1: 0.9452
Validation Loss: 1.0426, Accuracy: 0.8486, Precision: 0.8155, Recall: 0.7913, F1: 0.8004
Testing Loss: 0.8766, Accuracy: 0.8708, Precision: 0.8362, Recall: 0.8031, F1: 0.8162
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2344, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.7810, F1: 0.8046
Epoch 49/70
Train Loss: 0.0507, Accuracy: 0.9675, Precision: 0.9451, Recall: 0.9484, F1: 0.9467
Validation Loss: 1.1830, Accuracy: 0.8465, Precision: 0.8055, Recall: 0.7769, F1: 0.7849
Testing Loss: 0.9819, Accuracy: 0.8720, Precision: 0.8274, Recall: 0.7819, F1: 0.7937
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2052, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8095, F1: 0.8284
Epoch 50/70
Train Loss: 0.0498, Accuracy: 0.9706, Precision: 0.9493, Recall: 0.9542, F1: 0.9517
Validation Loss: 1.1108, Accuracy: 0.8550, Precision: 0.8238, Recall: 0.7927, F1: 0.8035
Testing Loss: 0.8988, Accuracy: 0.8804, Precision: 0.8495, Recall: 0.8076, F1: 0.8227
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2127, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 51/70
Train Loss: 0.0571, Accuracy: 0.9699, Precision: 0.9591, Recall: 0.9383, F1: 0.9476
Validation Loss: 0.9704, Accuracy: 0.8529, Precision: 0.8218, Recall: 0.8014, F1: 0.8091
Testing Loss: 0.7792, Accuracy: 0.8829, Precision: 0.8496, Recall: 0.8194, F1: 0.8317
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1981, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8857, F1: 0.8866
Epoch 52/70
Train Loss: 0.0590, Accuracy: 0.9677, Precision: 0.9432, Recall: 0.9514, F1: 0.9472
Validation Loss: 0.9007, Accuracy: 0.8657, Precision: 0.8353, Recall: 0.8172, F1: 0.8250
Testing Loss: 0.7895, Accuracy: 0.8684, Precision: 0.8428, Recall: 0.8050, F1: 0.8212
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2090, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8000, F1: 0.8154
Epoch 53/70
Train Loss: 0.0673, Accuracy: 0.9642, Precision: 0.9400, Recall: 0.9451, F1: 0.9424
Validation Loss: 1.0721, Accuracy: 0.8550, Precision: 0.8319, Recall: 0.8164, F1: 0.8232
Testing Loss: 0.8480, Accuracy: 0.8756, Precision: 0.8415, Recall: 0.8238, F1: 0.8318
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2078, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 54/70
Train Loss: 0.0689, Accuracy: 0.9661, Precision: 0.9405, Recall: 0.9505, F1: 0.9451
Validation Loss: 1.0858, Accuracy: 0.8465, Precision: 0.8240, Recall: 0.7725, F1: 0.7866
Testing Loss: 0.9021, Accuracy: 0.8696, Precision: 0.8378, Recall: 0.7810, F1: 0.7970
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2494, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8095, F1: 0.8284
Epoch 55/70
Train Loss: 0.0543, Accuracy: 0.9682, Precision: 0.9519, Recall: 0.9409, F1: 0.9461
Validation Loss: 1.0866, Accuracy: 0.8614, Precision: 0.8330, Recall: 0.8206, F1: 0.8262
Testing Loss: 0.8441, Accuracy: 0.8744, Precision: 0.8401, Recall: 0.8117, F1: 0.8239
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2672, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 56/70
Train Loss: 0.0723, Accuracy: 0.9656, Precision: 0.9441, Recall: 0.9501, F1: 0.9468
Validation Loss: 0.9459, Accuracy: 0.8465, Precision: 0.8064, Recall: 0.7721, F1: 0.7819
Testing Loss: 0.8412, Accuracy: 0.8659, Precision: 0.8346, Recall: 0.7878, F1: 0.8034
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2352, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 57/70
Train Loss: 0.0639, Accuracy: 0.9656, Precision: 0.9485, Recall: 0.9374, F1: 0.9423
Validation Loss: 0.9745, Accuracy: 0.8529, Precision: 0.8267, Recall: 0.7852, F1: 0.7986
Testing Loss: 0.8671, Accuracy: 0.8659, Precision: 0.8437, Recall: 0.7903, F1: 0.8093
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1701, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 58/70
Train Loss: 0.0578, Accuracy: 0.9663, Precision: 0.9428, Recall: 0.9441, F1: 0.9435
Validation Loss: 1.0870, Accuracy: 0.8571, Precision: 0.8198, Recall: 0.8193, F1: 0.8192
Testing Loss: 0.8756, Accuracy: 0.8865, Precision: 0.8532, Recall: 0.8399, F1: 0.8452
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1341, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0530, Accuracy: 0.9713, Precision: 0.9518, Recall: 0.9562, F1: 0.9539
Validation Loss: 1.1015, Accuracy: 0.8422, Precision: 0.7871, Recall: 0.7538, F1: 0.7564
Testing Loss: 0.8975, Accuracy: 0.8708, Precision: 0.8350, Recall: 0.7767, F1: 0.7907
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1321, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9429, F1: 0.9467
Epoch 60/70
Train Loss: 0.0536, Accuracy: 0.9682, Precision: 0.9481, Recall: 0.9432, F1: 0.9455
Validation Loss: 1.1195, Accuracy: 0.8614, Precision: 0.8370, Recall: 0.8143, F1: 0.8240
Testing Loss: 0.9424, Accuracy: 0.8647, Precision: 0.8340, Recall: 0.7963, F1: 0.8118
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2274, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 61/70
Train Loss: 0.0552, Accuracy: 0.9677, Precision: 0.9521, Recall: 0.9382, F1: 0.9447
Validation Loss: 1.1064, Accuracy: 0.8571, Precision: 0.8138, Recall: 0.8075, F1: 0.8101
Testing Loss: 0.9307, Accuracy: 0.8732, Precision: 0.8318, Recall: 0.8077, F1: 0.8176
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 1, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 1, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4722, Accuracy: 0.9286, Precision: 0.9550, Recall: 0.9000, F1: 0.9095
Epoch 62/70
Train Loss: 0.0495, Accuracy: 0.9699, Precision: 0.9452, Recall: 0.9572, F1: 0.9508
Validation Loss: 1.0676, Accuracy: 0.8571, Precision: 0.8278, Recall: 0.8069, F1: 0.8155
Testing Loss: 0.9199, Accuracy: 0.8671, Precision: 0.8390, Recall: 0.8037, F1: 0.8183
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2012, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917
Epoch 63/70
Train Loss: 0.0541, Accuracy: 0.9696, Precision: 0.9480, Recall: 0.9523, F1: 0.9500
Validation Loss: 1.0353, Accuracy: 0.8614, Precision: 0.8314, Recall: 0.8122, F1: 0.8198
Testing Loss: 0.9306, Accuracy: 0.8780, Precision: 0.8446, Recall: 0.8161, F1: 0.8287
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1568, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8857, F1: 0.8866
Epoch 64/70
Train Loss: 0.0528, Accuracy: 0.9687, Precision: 0.9473, Recall: 0.9500, F1: 0.9486
Validation Loss: 1.0956, Accuracy: 0.8593, Precision: 0.8217, Recall: 0.8148, F1: 0.8173
Testing Loss: 0.9694, Accuracy: 0.8708, Precision: 0.8287, Recall: 0.8140, F1: 0.8205
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1512, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 65/70
Train Loss: 0.0517, Accuracy: 0.9723, Precision: 0.9533, Recall: 0.9560, F1: 0.9546
Validation Loss: 1.1290, Accuracy: 0.8614, Precision: 0.8317, Recall: 0.8105, F1: 0.8191
Testing Loss: 0.9436, Accuracy: 0.8720, Precision: 0.8277, Recall: 0.7992, F1: 0.8097
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1668, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 66/70
Train Loss: 0.0508, Accuracy: 0.9711, Precision: 0.9551, Recall: 0.9447, F1: 0.9496
Validation Loss: 0.9653, Accuracy: 0.8571, Precision: 0.8236, Recall: 0.8219, F1: 0.8224
Testing Loss: 0.7757, Accuracy: 0.8756, Precision: 0.8395, Recall: 0.8275, F1: 0.8328
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 5, 3, 5, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2904, Accuracy: 0.8095, Precision: 0.7667, Recall: 0.6468, F1: 0.6771
Epoch 67/70
Train Loss: 0.0607, Accuracy: 0.9694, Precision: 0.9497, Recall: 0.9438, F1: 0.9466
Validation Loss: 1.1743, Accuracy: 0.8550, Precision: 0.8200, Recall: 0.8217, F1: 0.8200
Testing Loss: 0.9302, Accuracy: 0.8780, Precision: 0.8342, Recall: 0.8193, F1: 0.8262
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 5, 0, 5, 3, 5, 5, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4416, Accuracy: 0.8571, Precision: 0.8000, Recall: 0.6799, F1: 0.7275
Epoch 68/70
Train Loss: 0.0700, Accuracy: 0.9642, Precision: 0.9396, Recall: 0.9483, F1: 0.9436
Validation Loss: 0.9966, Accuracy: 0.8486, Precision: 0.8129, Recall: 0.8018, F1: 0.8049
Testing Loss: 0.9462, Accuracy: 0.8635, Precision: 0.8256, Recall: 0.8019, F1: 0.8102
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2058, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 69/70
Train Loss: 0.0578, Accuracy: 0.9689, Precision: 0.9511, Recall: 0.9412, F1: 0.9455
Validation Loss: 1.0907, Accuracy: 0.8571, Precision: 0.8297, Recall: 0.8003, F1: 0.8115
Testing Loss: 1.0121, Accuracy: 0.8720, Precision: 0.8451, Recall: 0.7919, F1: 0.8103
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1438, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9095, F1: 0.9199
Epoch 70/70
Train Loss: 0.0537, Accuracy: 0.9694, Precision: 0.9502, Recall: 0.9509, F1: 0.9505
Validation Loss: 1.0786, Accuracy: 0.8465, Precision: 0.8070, Recall: 0.7773, F1: 0.7870
Testing Loss: 0.9689, Accuracy: 0.8696, Precision: 0.8389, Recall: 0.7884, F1: 0.8045
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1692, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498

