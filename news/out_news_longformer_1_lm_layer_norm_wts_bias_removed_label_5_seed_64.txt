Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:1
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 2: 966
  Label 3: 495
  Label 1: 1011
  Label 4: 344
  Label 5: 260
Label counts for Validation:
  Label 3: 55
  Label 2: 107
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 2: 977
  Label 3: 505
  Label 1: 1016
  Label 4: 355
  Label 5: 218
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.4535, Accuracy: 0.4385, Precision: 0.3128, Recall: 0.2971, F1: 0.2680
Validation Loss: 1.2013, Accuracy: 0.6311, Precision: 0.3334, Recall: 0.4264, F1: 0.3709
Testing Loss: 1.1893, Accuracy: 0.6341, Precision: 0.3353, Recall: 0.4275, F1: 0.3719
LM Predictions:  [2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0002, Accuracy: 0.1190, Precision: 0.0562, Recall: 0.1782, F1: 0.0635
Epoch 2/70
Train Loss: 1.0350, Accuracy: 0.6308, Precision: 0.4403, Recall: 0.4666, F1: 0.4419
Validation Loss: 0.8156, Accuracy: 0.7015, Precision: 0.5035, Recall: 0.5207, F1: 0.4939
Testing Loss: 0.7910, Accuracy: 0.7234, Precision: 0.5183, Recall: 0.5483, F1: 0.5199
LM Predictions:  [0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7475, Accuracy: 0.0952, Precision: 0.0205, Recall: 0.1600, F1: 0.0364
Epoch 3/70
Train Loss: 0.8365, Accuracy: 0.7209, Precision: 0.5780, Recall: 0.5841, F1: 0.5719
Validation Loss: 0.6656, Accuracy: 0.7719, Precision: 0.6244, Recall: 0.6289, F1: 0.6196
Testing Loss: 0.6642, Accuracy: 0.7693, Precision: 0.6269, Recall: 0.6384, F1: 0.6259
LM Predictions:  [4, 3, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 2, 3, 3, 3, 0, 3, 3, 0, 0, 0, 3, 3, 3, 0, 2, 0, 3, 3, 0, 0, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7748, Accuracy: 0.1905, Precision: 0.1794, Recall: 0.2182, F1: 0.1376
Epoch 4/70
Train Loss: 0.7249, Accuracy: 0.7657, Precision: 0.6607, Recall: 0.6455, F1: 0.6397
Validation Loss: 0.6253, Accuracy: 0.7996, Precision: 0.6897, Recall: 0.6661, F1: 0.6609
Testing Loss: 0.6469, Accuracy: 0.7911, Precision: 0.6922, Recall: 0.6579, F1: 0.6475
LM Predictions:  [4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8546, Accuracy: 0.1429, Precision: 0.1889, Recall: 0.1982, F1: 0.1006
Epoch 5/70
Train Loss: 0.6604, Accuracy: 0.8010, Precision: 0.7122, Recall: 0.6822, F1: 0.6732
Validation Loss: 0.6143, Accuracy: 0.8060, Precision: 0.6970, Recall: 0.6638, F1: 0.6681
Testing Loss: 0.6338, Accuracy: 0.8043, Precision: 0.6976, Recall: 0.6648, F1: 0.6687
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.5074, Accuracy: 0.1429, Precision: 0.1263, Recall: 0.2200, F1: 0.0798
Epoch 6/70
Train Loss: 0.6086, Accuracy: 0.8091, Precision: 0.7352, Recall: 0.7091, F1: 0.7137
Validation Loss: 0.4909, Accuracy: 0.8443, Precision: 0.7890, Recall: 0.7427, F1: 0.7282
Testing Loss: 0.5059, Accuracy: 0.8370, Precision: 0.7061, Recall: 0.7248, F1: 0.7110
LM Predictions:  [0, 3, 3, 0, 0, 0, 0, 2, 5, 0, 0, 0, 0, 5, 0, 4, 0, 2, 0, 0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.1707, Accuracy: 0.1429, Precision: 0.1326, Recall: 0.1667, F1: 0.0883
Epoch 7/70
Train Loss: 0.5542, Accuracy: 0.8269, Precision: 0.7360, Recall: 0.7162, F1: 0.7109
Validation Loss: 0.4816, Accuracy: 0.8507, Precision: 0.8090, Recall: 0.7871, F1: 0.7963
Testing Loss: 0.5114, Accuracy: 0.8454, Precision: 0.8145, Recall: 0.7727, F1: 0.7885
LM Predictions:  [5, 5, 3, 0, 0, 0, 5, 2, 5, 0, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 3, 0, 0, 5, 5, 0, 0, 5, 5, 0, 0, 0, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8082, Accuracy: 0.1429, Precision: 0.1296, Recall: 0.1833, F1: 0.1002
Epoch 8/70
Train Loss: 0.5211, Accuracy: 0.8342, Precision: 0.7696, Recall: 0.7531, F1: 0.7592
Validation Loss: 0.4329, Accuracy: 0.8507, Precision: 0.7149, Recall: 0.7463, F1: 0.7286
Testing Loss: 0.4952, Accuracy: 0.8442, Precision: 0.7176, Recall: 0.7348, F1: 0.7240
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7337, Accuracy: 0.1429, Precision: 0.2256, Recall: 0.2200, F1: 0.0818
Epoch 9/70
Train Loss: 0.4984, Accuracy: 0.8385, Precision: 0.7756, Recall: 0.7542, F1: 0.7613
Validation Loss: 0.4163, Accuracy: 0.8550, Precision: 0.7900, Recall: 0.7833, F1: 0.7824
Testing Loss: 0.4774, Accuracy: 0.8514, Precision: 0.8153, Recall: 0.7940, F1: 0.8017
LM Predictions:  [5, 5, 3, 0, 5, 0, 5, 2, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9473, Accuracy: 0.0952, Precision: 0.1218, Recall: 0.1167, F1: 0.0833
Epoch 10/70
Train Loss: 0.4745, Accuracy: 0.8535, Precision: 0.7948, Recall: 0.7806, F1: 0.7867
Validation Loss: 0.4080, Accuracy: 0.8465, Precision: 0.8082, Recall: 0.7922, F1: 0.7992
Testing Loss: 0.4414, Accuracy: 0.8635, Precision: 0.8415, Recall: 0.8147, F1: 0.8259
LM Predictions:  [5, 5, 5, 0, 0, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8034, Accuracy: 0.0714, Precision: 0.1970, Recall: 0.0833, F1: 0.0720
Epoch 11/70
Train Loss: 0.4449, Accuracy: 0.8606, Precision: 0.8071, Recall: 0.7981, F1: 0.8020
Validation Loss: 0.4409, Accuracy: 0.8571, Precision: 0.8099, Recall: 0.7955, F1: 0.8021
Testing Loss: 0.4532, Accuracy: 0.8744, Precision: 0.8518, Recall: 0.8196, F1: 0.8330
LM Predictions:  [5, 5, 0, 0, 0, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.1852, Accuracy: 0.1190, Precision: 0.3690, Recall: 0.1500, F1: 0.1385
Epoch 12/70
Train Loss: 0.4348, Accuracy: 0.8644, Precision: 0.8135, Recall: 0.8066, F1: 0.8097
Validation Loss: 0.4496, Accuracy: 0.8614, Precision: 0.8091, Recall: 0.8235, F1: 0.8153
Testing Loss: 0.4527, Accuracy: 0.8684, Precision: 0.8295, Recall: 0.8339, F1: 0.8308
LM Predictions:  [5, 5, 5, 5, 0, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 4, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9149, Accuracy: 0.1667, Precision: 0.3417, Recall: 0.1621, F1: 0.1636
Epoch 13/70
Train Loss: 0.4371, Accuracy: 0.8658, Precision: 0.8144, Recall: 0.8119, F1: 0.8129
Validation Loss: 0.4576, Accuracy: 0.8550, Precision: 0.8026, Recall: 0.7935, F1: 0.7920
Testing Loss: 0.4791, Accuracy: 0.8539, Precision: 0.8280, Recall: 0.7852, F1: 0.7972
LM Predictions:  [5, 5, 5, 5, 0, 0, 5, 2, 4, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 4, 0, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0252, Accuracy: 0.2143, Precision: 0.5139, Recall: 0.2288, F1: 0.2310
Epoch 14/70
Train Loss: 0.4270, Accuracy: 0.8625, Precision: 0.8139, Recall: 0.8088, F1: 0.8109
Validation Loss: 0.4937, Accuracy: 0.8443, Precision: 0.8004, Recall: 0.8007, F1: 0.7982
Testing Loss: 0.4970, Accuracy: 0.8623, Precision: 0.8316, Recall: 0.8195, F1: 0.8231
LM Predictions:  [5, 5, 5, 5, 0, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 4, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9510, Accuracy: 0.1429, Precision: 0.3232, Recall: 0.1470, F1: 0.1404
Epoch 15/70
Train Loss: 0.4064, Accuracy: 0.8781, Precision: 0.8344, Recall: 0.8409, F1: 0.8364
Validation Loss: 0.4100, Accuracy: 0.8550, Precision: 0.8110, Recall: 0.8088, F1: 0.8095
Testing Loss: 0.4337, Accuracy: 0.8756, Precision: 0.8438, Recall: 0.8439, F1: 0.8424
LM Predictions:  [5, 5, 5, 5, 0, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9017, Accuracy: 0.1190, Precision: 0.3833, Recall: 0.1500, F1: 0.1525
Epoch 16/70
Train Loss: 0.3914, Accuracy: 0.8817, Precision: 0.8370, Recall: 0.8330, F1: 0.8343
Validation Loss: 0.4395, Accuracy: 0.8571, Precision: 0.8115, Recall: 0.8173, F1: 0.8127
Testing Loss: 0.4419, Accuracy: 0.8756, Precision: 0.8428, Recall: 0.8465, F1: 0.8423
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0381, Accuracy: 0.0952, Precision: 0.1389, Recall: 0.1167, F1: 0.0992
Epoch 17/70
Train Loss: 0.3769, Accuracy: 0.8876, Precision: 0.8486, Recall: 0.8436, F1: 0.8456
Validation Loss: 0.4714, Accuracy: 0.8593, Precision: 0.8195, Recall: 0.8525, F1: 0.8277
Testing Loss: 0.4445, Accuracy: 0.8756, Precision: 0.8433, Recall: 0.8721, F1: 0.8495
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6737, Accuracy: 0.0714, Precision: 0.1667, Recall: 0.0833, F1: 0.1019
Epoch 18/70
Train Loss: 0.3728, Accuracy: 0.8831, Precision: 0.8303, Recall: 0.8312, F1: 0.8306
Validation Loss: 0.4287, Accuracy: 0.8550, Precision: 0.8178, Recall: 0.7987, F1: 0.8053
Testing Loss: 0.4579, Accuracy: 0.8599, Precision: 0.8374, Recall: 0.8042, F1: 0.8144
LM Predictions:  [5, 5, 5, 5, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.3041, Accuracy: 0.0952, Precision: 0.2024, Recall: 0.1167, F1: 0.0829
Epoch 19/70
Train Loss: 0.3566, Accuracy: 0.8926, Precision: 0.8505, Recall: 0.8525, F1: 0.8511
Validation Loss: 0.5856, Accuracy: 0.8571, Precision: 0.8258, Recall: 0.7817, F1: 0.7885
Testing Loss: 0.4532, Accuracy: 0.8792, Precision: 0.8701, Recall: 0.7949, F1: 0.8079
LM Predictions:  [5, 5, 5, 0, 0, 0, 0, 2, 4, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 0, 4, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 0, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9897, Accuracy: 0.1667, Precision: 0.5294, Recall: 0.1803, F1: 0.1826
Epoch 20/70
Train Loss: 0.3419, Accuracy: 0.8976, Precision: 0.8565, Recall: 0.8578, F1: 0.8567
Validation Loss: 0.4590, Accuracy: 0.8678, Precision: 0.8215, Recall: 0.8259, F1: 0.8235
Testing Loss: 0.4270, Accuracy: 0.8780, Precision: 0.8422, Recall: 0.8436, F1: 0.8408
LM Predictions:  [5, 5, 5, 4, 2, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 5, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6696, Accuracy: 0.1905, Precision: 0.4125, Recall: 0.1773, F1: 0.1954
Epoch 21/70
Train Loss: 0.3630, Accuracy: 0.8881, Precision: 0.8396, Recall: 0.8441, F1: 0.8416
Validation Loss: 0.5005, Accuracy: 0.8657, Precision: 0.8212, Recall: 0.8128, F1: 0.8160
Testing Loss: 0.4703, Accuracy: 0.8792, Precision: 0.8451, Recall: 0.8279, F1: 0.8351
LM Predictions:  [5, 5, 5, 5, 0, 0, 0, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 2, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8901, Accuracy: 0.1429, Precision: 0.5417, Recall: 0.1652, F1: 0.1725
Epoch 22/70
Train Loss: 0.3299, Accuracy: 0.8900, Precision: 0.8435, Recall: 0.8456, F1: 0.8442
Validation Loss: 0.4739, Accuracy: 0.8593, Precision: 0.8114, Recall: 0.8122, F1: 0.8114
Testing Loss: 0.4780, Accuracy: 0.8720, Precision: 0.8359, Recall: 0.8306, F1: 0.8322
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9471, Accuracy: 0.1905, Precision: 0.4667, Recall: 0.1955, F1: 0.2214
Epoch 23/70
Train Loss: 0.3145, Accuracy: 0.9049, Precision: 0.8631, Recall: 0.8624, F1: 0.8626
Validation Loss: 0.5000, Accuracy: 0.8614, Precision: 0.8153, Recall: 0.8203, F1: 0.8166
Testing Loss: 0.4659, Accuracy: 0.8708, Precision: 0.8400, Recall: 0.8379, F1: 0.8384
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 5, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0094, Accuracy: 0.1429, Precision: 0.2167, Recall: 0.1470, F1: 0.1399
Epoch 24/70
Train Loss: 0.3250, Accuracy: 0.9037, Precision: 0.8658, Recall: 0.8678, F1: 0.8663
Validation Loss: 0.4915, Accuracy: 0.8593, Precision: 0.8105, Recall: 0.7730, F1: 0.7743
Testing Loss: 0.4559, Accuracy: 0.8768, Precision: 0.8478, Recall: 0.8016, F1: 0.8076
LM Predictions:  [5, 5, 5, 4, 3, 0, 0, 3, 4, 0, 0, 5, 5, 5, 5, 4, 0, 0, 5, 1, 3, 0, 4, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6828, Accuracy: 0.1905, Precision: 0.3535, Recall: 0.1955, F1: 0.1913
Epoch 25/70
Train Loss: 0.3073, Accuracy: 0.9056, Precision: 0.8631, Recall: 0.8681, F1: 0.8652
Validation Loss: 0.4551, Accuracy: 0.8550, Precision: 0.8054, Recall: 0.8353, F1: 0.8158
Testing Loss: 0.4891, Accuracy: 0.8611, Precision: 0.8138, Recall: 0.8448, F1: 0.8257
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7010, Accuracy: 0.1429, Precision: 0.4603, Recall: 0.1652, F1: 0.1923
Epoch 26/70
Train Loss: 0.2924, Accuracy: 0.9089, Precision: 0.8665, Recall: 0.8725, F1: 0.8688
Validation Loss: 0.4990, Accuracy: 0.8550, Precision: 0.8065, Recall: 0.7932, F1: 0.7987
Testing Loss: 0.5259, Accuracy: 0.8599, Precision: 0.8313, Recall: 0.8008, F1: 0.8109
LM Predictions:  [5, 5, 5, 0, 0, 0, 0, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 4, 3, 0, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8217, Accuracy: 0.1429, Precision: 0.3135, Recall: 0.1470, F1: 0.1306
Epoch 27/70
Train Loss: 0.3054, Accuracy: 0.9014, Precision: 0.8559, Recall: 0.8637, F1: 0.8590
Validation Loss: 0.4344, Accuracy: 0.8571, Precision: 0.8099, Recall: 0.8047, F1: 0.8068
Testing Loss: 0.4395, Accuracy: 0.8756, Precision: 0.8437, Recall: 0.8312, F1: 0.8357
LM Predictions:  [5, 3, 5, 4, 3, 0, 0, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 0, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6026, Accuracy: 0.1905, Precision: 0.4343, Recall: 0.1970, F1: 0.2170
Epoch 28/70
Train Loss: 0.2782, Accuracy: 0.9182, Precision: 0.8816, Recall: 0.8933, F1: 0.8864
Validation Loss: 0.4670, Accuracy: 0.8678, Precision: 0.8180, Recall: 0.8324, F1: 0.8233
Testing Loss: 0.4782, Accuracy: 0.8744, Precision: 0.8349, Recall: 0.8305, F1: 0.8313
LM Predictions:  [5, 5, 5, 4, 3, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7066, Accuracy: 0.1905, Precision: 0.5069, Recall: 0.1955, F1: 0.2335
Epoch 29/70
Train Loss: 0.2903, Accuracy: 0.9113, Precision: 0.8744, Recall: 0.8818, F1: 0.8773
Validation Loss: 0.5210, Accuracy: 0.8593, Precision: 0.8244, Recall: 0.8154, F1: 0.8191
Testing Loss: 0.5113, Accuracy: 0.8684, Precision: 0.8360, Recall: 0.8223, F1: 0.8276
LM Predictions:  [5, 5, 5, 4, 0, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 2, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6471, Accuracy: 0.1905, Precision: 0.5903, Recall: 0.1955, F1: 0.2360
Epoch 30/70
Train Loss: 0.2763, Accuracy: 0.9180, Precision: 0.8812, Recall: 0.8798, F1: 0.8803
Validation Loss: 0.4516, Accuracy: 0.8593, Precision: 0.8122, Recall: 0.8345, F1: 0.8210
Testing Loss: 0.4536, Accuracy: 0.8792, Precision: 0.8429, Recall: 0.8470, F1: 0.8444
LM Predictions:  [5, 5, 5, 5, 3, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 3, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8437, Accuracy: 0.1667, Precision: 0.4603, Recall: 0.1803, F1: 0.2158
Epoch 31/70
Train Loss: 0.2953, Accuracy: 0.9127, Precision: 0.8763, Recall: 0.8768, F1: 0.8764
Validation Loss: 0.4967, Accuracy: 0.8593, Precision: 0.8147, Recall: 0.8154, F1: 0.8146
Testing Loss: 0.5354, Accuracy: 0.8623, Precision: 0.8323, Recall: 0.8107, F1: 0.8174
LM Predictions:  [5, 5, 5, 4, 0, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0488, Accuracy: 0.1667, Precision: 0.4829, Recall: 0.1803, F1: 0.1890
Epoch 32/70
Train Loss: 0.2639, Accuracy: 0.9234, Precision: 0.8855, Recall: 0.8954, F1: 0.8900
Validation Loss: 0.5271, Accuracy: 0.8294, Precision: 0.7854, Recall: 0.7561, F1: 0.7680
Testing Loss: 0.4939, Accuracy: 0.8756, Precision: 0.8463, Recall: 0.8200, F1: 0.8313
LM Predictions:  [5, 5, 5, 0, 3, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0307, Accuracy: 0.1905, Precision: 0.6288, Recall: 0.1955, F1: 0.2249
Epoch 33/70
Train Loss: 0.2561, Accuracy: 0.9208, Precision: 0.8852, Recall: 0.8867, F1: 0.8855
Validation Loss: 0.5043, Accuracy: 0.8507, Precision: 0.7992, Recall: 0.7908, F1: 0.7939
Testing Loss: 0.4682, Accuracy: 0.8744, Precision: 0.8369, Recall: 0.8187, F1: 0.8246
LM Predictions:  [5, 3, 5, 4, 3, 0, 0, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 0, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4544, Accuracy: 0.2381, Precision: 0.5917, Recall: 0.2273, F1: 0.2643
Epoch 34/70
Train Loss: 0.2554, Accuracy: 0.9241, Precision: 0.8851, Recall: 0.8936, F1: 0.8889
Validation Loss: 0.5611, Accuracy: 0.8507, Precision: 0.8082, Recall: 0.7617, F1: 0.7739
Testing Loss: 0.5254, Accuracy: 0.8623, Precision: 0.8251, Recall: 0.7789, F1: 0.7894
LM Predictions:  [0, 5, 5, 0, 3, 0, 0, 5, 4, 0, 0, 0, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 1, 5, 5, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.1879, Accuracy: 0.2143, Precision: 0.3730, Recall: 0.2470, F1: 0.1908
Epoch 35/70
Train Loss: 0.2384, Accuracy: 0.9289, Precision: 0.8940, Recall: 0.9004, F1: 0.8962
Validation Loss: 0.5501, Accuracy: 0.8337, Precision: 0.7876, Recall: 0.7760, F1: 0.7801
Testing Loss: 0.5481, Accuracy: 0.8671, Precision: 0.8354, Recall: 0.8177, F1: 0.8254
LM Predictions:  [5, 3, 3, 0, 3, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4718, Accuracy: 0.2381, Precision: 0.6222, Recall: 0.2273, F1: 0.2706
Epoch 36/70
Train Loss: 0.2436, Accuracy: 0.9286, Precision: 0.8953, Recall: 0.8923, F1: 0.8936
Validation Loss: 0.5515, Accuracy: 0.8486, Precision: 0.8015, Recall: 0.8137, F1: 0.8062
Testing Loss: 0.5364, Accuracy: 0.8647, Precision: 0.8251, Recall: 0.8287, F1: 0.8266
LM Predictions:  [5, 3, 5, 0, 3, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7452, Accuracy: 0.2381, Precision: 0.6389, Recall: 0.2273, F1: 0.2738
Epoch 37/70
Train Loss: 0.2452, Accuracy: 0.9272, Precision: 0.8896, Recall: 0.8988, F1: 0.8930
Validation Loss: 0.5700, Accuracy: 0.8294, Precision: 0.7579, Recall: 0.7695, F1: 0.7600
Testing Loss: 0.5717, Accuracy: 0.8563, Precision: 0.8018, Recall: 0.7909, F1: 0.7919
LM Predictions:  [5, 3, 5, 4, 2, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5024, Accuracy: 0.2619, Precision: 0.6635, Recall: 0.2424, F1: 0.2846
Epoch 38/70
Train Loss: 0.2470, Accuracy: 0.9284, Precision: 0.8902, Recall: 0.8898, F1: 0.8898
Validation Loss: 0.5545, Accuracy: 0.8550, Precision: 0.8162, Recall: 0.7823, F1: 0.7885
Testing Loss: 0.5481, Accuracy: 0.8587, Precision: 0.8201, Recall: 0.7710, F1: 0.7824
LM Predictions:  [0, 3, 5, 0, 2, 0, 0, 3, 4, 0, 0, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 0, 5, 5, 5, 5, 0, 5, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9676, Accuracy: 0.2619, Precision: 0.6481, Recall: 0.2606, F1: 0.2674
Epoch 39/70
Train Loss: 0.2420, Accuracy: 0.9305, Precision: 0.8958, Recall: 0.9048, F1: 0.8994
Validation Loss: 0.5769, Accuracy: 0.8401, Precision: 0.7988, Recall: 0.8053, F1: 0.7989
Testing Loss: 0.5071, Accuracy: 0.8575, Precision: 0.8220, Recall: 0.8283, F1: 0.8238
LM Predictions:  [5, 3, 5, 0, 3, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9292, Accuracy: 0.1905, Precision: 0.4792, Recall: 0.1970, F1: 0.2314
Epoch 40/70
Train Loss: 0.2204, Accuracy: 0.9300, Precision: 0.8922, Recall: 0.9004, F1: 0.8954
Validation Loss: 0.6108, Accuracy: 0.8252, Precision: 0.7723, Recall: 0.7633, F1: 0.7670
Testing Loss: 0.5671, Accuracy: 0.8635, Precision: 0.8237, Recall: 0.8055, F1: 0.8124
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8592, Accuracy: 0.2619, Precision: 0.7292, Recall: 0.2424, F1: 0.3107
Epoch 41/70
Train Loss: 0.2285, Accuracy: 0.9341, Precision: 0.8970, Recall: 0.9131, F1: 0.9033
Validation Loss: 0.6018, Accuracy: 0.8380, Precision: 0.7878, Recall: 0.8027, F1: 0.7923
Testing Loss: 0.5606, Accuracy: 0.8611, Precision: 0.8157, Recall: 0.8211, F1: 0.8179
LM Predictions:  [5, 3, 3, 4, 2, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 2, 5, 5, 5, 0, 5, 5, 0, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3704, Accuracy: 0.2381, Precision: 0.5437, Recall: 0.2273, F1: 0.2817
Epoch 42/70
Train Loss: 0.2193, Accuracy: 0.9341, Precision: 0.9012, Recall: 0.9081, F1: 0.9039
Validation Loss: 0.6034, Accuracy: 0.8465, Precision: 0.7822, Recall: 0.7739, F1: 0.7732
Testing Loss: 0.5627, Accuracy: 0.8671, Precision: 0.8251, Recall: 0.7970, F1: 0.8045
LM Predictions:  [5, 1, 5, 0, 2, 0, 0, 3, 4, 0, 0, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 0, 2, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7202, Accuracy: 0.2619, Precision: 0.4907, Recall: 0.2773, F1: 0.2468
Epoch 43/70
Train Loss: 0.2366, Accuracy: 0.9279, Precision: 0.8896, Recall: 0.8982, F1: 0.8932
Validation Loss: 0.5311, Accuracy: 0.8571, Precision: 0.8400, Recall: 0.7750, F1: 0.7853
Testing Loss: 0.5132, Accuracy: 0.8623, Precision: 0.8371, Recall: 0.7794, F1: 0.7957
LM Predictions:  [0, 3, 5, 0, 2, 0, 0, 3, 4, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 2, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6935, Accuracy: 0.2619, Precision: 0.5675, Recall: 0.2788, F1: 0.2479
Epoch 44/70
Train Loss: 0.2063, Accuracy: 0.9417, Precision: 0.9121, Recall: 0.9147, F1: 0.9132
Validation Loss: 0.5613, Accuracy: 0.8571, Precision: 0.8223, Recall: 0.8210, F1: 0.8198
Testing Loss: 0.5592, Accuracy: 0.8659, Precision: 0.8383, Recall: 0.8242, F1: 0.8279
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 2, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9662, Accuracy: 0.2381, Precision: 0.7111, Recall: 0.2273, F1: 0.3100
Epoch 45/70
Train Loss: 0.2172, Accuracy: 0.9346, Precision: 0.8987, Recall: 0.9112, F1: 0.9031
Validation Loss: 0.6482, Accuracy: 0.8571, Precision: 0.8893, Recall: 0.7577, F1: 0.7606
Testing Loss: 0.6162, Accuracy: 0.8671, Precision: 0.8707, Recall: 0.7658, F1: 0.7795
LM Predictions:  [0, 3, 3, 0, 2, 0, 0, 2, 4, 0, 0, 0, 0, 5, 5, 5, 2, 0, 5, 1, 3, 0, 4, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0443, Accuracy: 0.3095, Precision: 0.5934, Recall: 0.3091, F1: 0.2876
Epoch 46/70
Train Loss: 0.2247, Accuracy: 0.9310, Precision: 0.8933, Recall: 0.8889, F1: 0.8910
Validation Loss: 0.6023, Accuracy: 0.8571, Precision: 0.8178, Recall: 0.8443, F1: 0.8255
Testing Loss: 0.5432, Accuracy: 0.8708, Precision: 0.8312, Recall: 0.8467, F1: 0.8364
LM Predictions:  [5, 3, 5, 0, 2, 5, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5224, Accuracy: 0.2619, Precision: 0.6944, Recall: 0.2424, F1: 0.3205
Epoch 47/70
Train Loss: 0.2099, Accuracy: 0.9367, Precision: 0.9022, Recall: 0.9107, F1: 0.9057
Validation Loss: 0.6915, Accuracy: 0.8465, Precision: 0.7890, Recall: 0.7816, F1: 0.7813
Testing Loss: 0.6261, Accuracy: 0.8551, Precision: 0.8085, Recall: 0.7784, F1: 0.7847
LM Predictions:  [5, 3, 5, 0, 3, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 4, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4725, Accuracy: 0.2619, Precision: 0.4883, Recall: 0.2788, F1: 0.2477
Epoch 48/70
Train Loss: 0.2084, Accuracy: 0.9374, Precision: 0.9035, Recall: 0.9034, F1: 0.9033
Validation Loss: 0.6078, Accuracy: 0.8443, Precision: 0.8044, Recall: 0.8256, F1: 0.8108
Testing Loss: 0.5418, Accuracy: 0.8696, Precision: 0.8289, Recall: 0.8325, F1: 0.8278
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4138, Accuracy: 0.2381, Precision: 0.7381, Recall: 0.2273, F1: 0.2937
Epoch 49/70
Train Loss: 0.1963, Accuracy: 0.9407, Precision: 0.9070, Recall: 0.9166, F1: 0.9108
Validation Loss: 0.5962, Accuracy: 0.8486, Precision: 0.8104, Recall: 0.8213, F1: 0.8132
Testing Loss: 0.5130, Accuracy: 0.8696, Precision: 0.8312, Recall: 0.8270, F1: 0.8277
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8562, Accuracy: 0.2381, Precision: 0.6944, Recall: 0.2273, F1: 0.2970
Epoch 50/70
Train Loss: 0.1889, Accuracy: 0.9372, Precision: 0.9002, Recall: 0.9134, F1: 0.9053
Validation Loss: 0.6635, Accuracy: 0.8486, Precision: 0.8106, Recall: 0.8056, F1: 0.8078
Testing Loss: 0.6416, Accuracy: 0.8623, Precision: 0.8255, Recall: 0.8148, F1: 0.8181
LM Predictions:  [5, 3, 5, 4, 2, 5, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 3, 5, 5, 5, 0, 5, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5608, Accuracy: 0.2143, Precision: 0.6111, Recall: 0.2121, F1: 0.2695
Epoch 51/70
Train Loss: 0.1957, Accuracy: 0.9414, Precision: 0.9056, Recall: 0.9188, F1: 0.9111
Validation Loss: 0.6706, Accuracy: 0.8380, Precision: 0.7943, Recall: 0.7809, F1: 0.7871
Testing Loss: 0.6198, Accuracy: 0.8696, Precision: 0.8344, Recall: 0.8047, F1: 0.8151
LM Predictions:  [5, 3, 5, 0, 2, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 0, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.7813, Accuracy: 0.2857, Precision: 0.6752, Recall: 0.2939, F1: 0.2986
Epoch 52/70
Train Loss: 0.1977, Accuracy: 0.9474, Precision: 0.9161, Recall: 0.9246, F1: 0.9197
Validation Loss: 0.7324, Accuracy: 0.8401, Precision: 0.8031, Recall: 0.8036, F1: 0.8006
Testing Loss: 0.6157, Accuracy: 0.8599, Precision: 0.8258, Recall: 0.8156, F1: 0.8183
LM Predictions:  [5, 3, 5, 4, 2, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 2, 5, 5, 5, 0, 2, 5, 0, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5332, Accuracy: 0.2381, Precision: 0.5992, Recall: 0.2273, F1: 0.2865
Epoch 53/70
Train Loss: 0.1904, Accuracy: 0.9431, Precision: 0.9087, Recall: 0.9221, F1: 0.9143
Validation Loss: 0.7469, Accuracy: 0.8401, Precision: 0.7957, Recall: 0.7779, F1: 0.7860
Testing Loss: 0.6788, Accuracy: 0.8563, Precision: 0.8281, Recall: 0.7804, F1: 0.7984
LM Predictions:  [5, 3, 5, 0, 2, 0, 5, 2, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 2, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5839, Accuracy: 0.2857, Precision: 0.6528, Recall: 0.2939, F1: 0.3049
Epoch 54/70
Train Loss: 0.1983, Accuracy: 0.9438, Precision: 0.9118, Recall: 0.9141, F1: 0.9127
Validation Loss: 0.6635, Accuracy: 0.8529, Precision: 0.8124, Recall: 0.8179, F1: 0.8149
Testing Loss: 0.6057, Accuracy: 0.8671, Precision: 0.8279, Recall: 0.8195, F1: 0.8220
LM Predictions:  [5, 3, 5, 4, 2, 0, 5, 3, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 5, 5, 0, 2, 5, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5353, Accuracy: 0.2857, Precision: 0.6435, Recall: 0.2758, F1: 0.3200
Epoch 55/70
Train Loss: 0.1901, Accuracy: 0.9469, Precision: 0.9126, Recall: 0.9236, F1: 0.9173
Validation Loss: 0.6496, Accuracy: 0.8422, Precision: 0.7884, Recall: 0.7953, F1: 0.7908
Testing Loss: 0.5914, Accuracy: 0.8659, Precision: 0.8204, Recall: 0.8095, F1: 0.8130
LM Predictions:  [5, 3, 5, 3, 2, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4614, Accuracy: 0.3333, Precision: 0.7308, Recall: 0.3258, F1: 0.3478
Epoch 56/70
Train Loss: 0.1863, Accuracy: 0.9424, Precision: 0.9085, Recall: 0.9176, F1: 0.9122
Validation Loss: 0.6746, Accuracy: 0.8337, Precision: 0.7881, Recall: 0.7839, F1: 0.7852
Testing Loss: 0.6080, Accuracy: 0.8623, Precision: 0.8305, Recall: 0.8043, F1: 0.8160
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 3, 0, 0, 5, 1, 3, 5, 2, 5, 5, 5, 0, 2, 5, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4286, Accuracy: 0.2857, Precision: 0.6717, Recall: 0.2773, F1: 0.3147
Epoch 57/70
Train Loss: 0.1846, Accuracy: 0.9447, Precision: 0.9095, Recall: 0.9190, F1: 0.9136
Validation Loss: 0.6654, Accuracy: 0.8465, Precision: 0.7932, Recall: 0.7886, F1: 0.7905
Testing Loss: 0.6569, Accuracy: 0.8563, Precision: 0.8163, Recall: 0.7834, F1: 0.7960
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 2, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6603, Accuracy: 0.2857, Precision: 0.6706, Recall: 0.2939, F1: 0.2977
Epoch 58/70
Train Loss: 0.2203, Accuracy: 0.9334, Precision: 0.8988, Recall: 0.9167, F1: 0.9056
Validation Loss: 0.6821, Accuracy: 0.8358, Precision: 0.7820, Recall: 0.7779, F1: 0.7794
Testing Loss: 0.6079, Accuracy: 0.8623, Precision: 0.8293, Recall: 0.8093, F1: 0.8176
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 5, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5886, Accuracy: 0.2857, Precision: 0.7262, Recall: 0.2939, F1: 0.2980
Epoch 59/70
Train Loss: 0.1935, Accuracy: 0.9424, Precision: 0.9069, Recall: 0.9213, F1: 0.9124
Validation Loss: 0.6591, Accuracy: 0.8358, Precision: 0.7743, Recall: 0.7669, F1: 0.7690
Testing Loss: 0.6243, Accuracy: 0.8659, Precision: 0.8323, Recall: 0.8008, F1: 0.8137
LM Predictions:  [5, 3, 5, 4, 2, 0, 0, 3, 4, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2002, Accuracy: 0.3095, Precision: 0.6085, Recall: 0.3091, F1: 0.3132
Epoch 60/70
Train Loss: 0.1796, Accuracy: 0.9476, Precision: 0.9149, Recall: 0.9232, F1: 0.9183
Validation Loss: 0.6607, Accuracy: 0.8401, Precision: 0.7991, Recall: 0.8004, F1: 0.7955
Testing Loss: 0.5883, Accuracy: 0.8623, Precision: 0.8323, Recall: 0.8323, F1: 0.8289
LM Predictions:  [5, 3, 5, 0, 2, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 2, 5, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5708, Accuracy: 0.2619, Precision: 0.7407, Recall: 0.2606, F1: 0.3089
Epoch 61/70
Train Loss: 0.1668, Accuracy: 0.9502, Precision: 0.9190, Recall: 0.9227, F1: 0.9206
Validation Loss: 0.6963, Accuracy: 0.8337, Precision: 0.7639, Recall: 0.7738, F1: 0.7653
Testing Loss: 0.6034, Accuracy: 0.8671, Precision: 0.8181, Recall: 0.8133, F1: 0.8130
LM Predictions:  [5, 3, 5, 4, 2, 0, 0, 4, 4, 5, 5, 5, 5, 5, 5, 4, 0, 0, 5, 1, 3, 5, 4, 5, 4, 0, 0, 2, 4, 0, 0, 4, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0640, Accuracy: 0.3571, Precision: 0.6567, Recall: 0.3394, F1: 0.3383
Epoch 62/70
Train Loss: 0.1829, Accuracy: 0.9440, Precision: 0.9123, Recall: 0.9201, F1: 0.9153
Validation Loss: 0.6534, Accuracy: 0.8401, Precision: 0.7788, Recall: 0.7813, F1: 0.7788
Testing Loss: 0.5995, Accuracy: 0.8659, Precision: 0.8299, Recall: 0.8065, F1: 0.8160
LM Predictions:  [5, 3, 5, 4, 2, 0, 0, 3, 4, 5, 5, 5, 5, 5, 5, 3, 2, 0, 5, 1, 3, 5, 4, 5, 4, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0336, Accuracy: 0.3571, Precision: 0.6278, Recall: 0.3409, F1: 0.3590
Epoch 63/70
Train Loss: 0.1699, Accuracy: 0.9478, Precision: 0.9142, Recall: 0.9314, F1: 0.9212
Validation Loss: 0.6375, Accuracy: 0.8401, Precision: 0.7967, Recall: 0.7883, F1: 0.7905
Testing Loss: 0.6025, Accuracy: 0.8647, Precision: 0.8364, Recall: 0.8196, F1: 0.8258
LM Predictions:  [5, 3, 5, 0, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3487, Accuracy: 0.3333, Precision: 0.7424, Recall: 0.3242, F1: 0.3581
Epoch 64/70
Train Loss: 0.1611, Accuracy: 0.9500, Precision: 0.9200, Recall: 0.9303, F1: 0.9244
Validation Loss: 0.8062, Accuracy: 0.8358, Precision: 0.7756, Recall: 0.7565, F1: 0.7579
Testing Loss: 0.7689, Accuracy: 0.8442, Precision: 0.7889, Recall: 0.7652, F1: 0.7704
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 5, 3, 0, 5, 1, 3, 0, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9111, Accuracy: 0.3095, Precision: 0.6574, Recall: 0.3091, F1: 0.3020
Epoch 65/70
Train Loss: 0.1726, Accuracy: 0.9485, Precision: 0.9164, Recall: 0.9242, F1: 0.9196
Validation Loss: 0.6609, Accuracy: 0.8443, Precision: 0.8034, Recall: 0.8060, F1: 0.8035
Testing Loss: 0.5833, Accuracy: 0.8696, Precision: 0.8356, Recall: 0.8244, F1: 0.8284
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 5, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3827, Accuracy: 0.3333, Precision: 0.7593, Recall: 0.3242, F1: 0.3730
Epoch 66/70
Train Loss: 0.1513, Accuracy: 0.9504, Precision: 0.9181, Recall: 0.9258, F1: 0.9213
Validation Loss: 0.7341, Accuracy: 0.8422, Precision: 0.7959, Recall: 0.7972, F1: 0.7950
Testing Loss: 0.6690, Accuracy: 0.8551, Precision: 0.8206, Recall: 0.8081, F1: 0.8131
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4639, Accuracy: 0.3095, Precision: 0.7308, Recall: 0.3091, F1: 0.3264
Epoch 67/70
Train Loss: 0.1829, Accuracy: 0.9457, Precision: 0.9125, Recall: 0.9297, F1: 0.9195
Validation Loss: 0.7788, Accuracy: 0.8401, Precision: 0.7904, Recall: 0.7780, F1: 0.7819
Testing Loss: 0.6171, Accuracy: 0.8587, Precision: 0.8230, Recall: 0.8035, F1: 0.8122
LM Predictions:  [5, 3, 5, 5, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 3, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3349, Accuracy: 0.3571, Precision: 0.7593, Recall: 0.3409, F1: 0.3944
Epoch 68/70
Train Loss: 0.1580, Accuracy: 0.9502, Precision: 0.9202, Recall: 0.9327, F1: 0.9250
Validation Loss: 0.7414, Accuracy: 0.8316, Precision: 0.7807, Recall: 0.7532, F1: 0.7641
Testing Loss: 0.6533, Accuracy: 0.8514, Precision: 0.8150, Recall: 0.7617, F1: 0.7798
LM Predictions:  [5, 3, 5, 5, 2, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2156, Accuracy: 0.3333, Precision: 0.7222, Recall: 0.3242, F1: 0.3373
Epoch 69/70
Train Loss: 0.1536, Accuracy: 0.9481, Precision: 0.9150, Recall: 0.9229, F1: 0.9180
Validation Loss: 0.7559, Accuracy: 0.8443, Precision: 0.7999, Recall: 0.7614, F1: 0.7729
Testing Loss: 0.6607, Accuracy: 0.8611, Precision: 0.8189, Recall: 0.7704, F1: 0.7847
LM Predictions:  [0, 3, 5, 0, 2, 0, 0, 5, 4, 0, 0, 5, 5, 5, 5, 0, 2, 0, 5, 1, 3, 5, 4, 5, 0, 0, 0, 2, 0, 0, 0, 5, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3341, Accuracy: 0.3333, Precision: 0.7063, Recall: 0.3242, F1: 0.3181
Epoch 70/70
Train Loss: 0.1507, Accuracy: 0.9512, Precision: 0.9217, Recall: 0.9317, F1: 0.9253
Validation Loss: 0.7285, Accuracy: 0.8443, Precision: 0.7943, Recall: 0.7925, F1: 0.7930
Testing Loss: 0.6424, Accuracy: 0.8623, Precision: 0.8233, Recall: 0.8075, F1: 0.8138
LM Predictions:  [5, 3, 5, 4, 2, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1748, Accuracy: 0.3333, Precision: 0.7176, Recall: 0.3242, F1: 0.3683
Label Memorization Analysis: 
LM Predictions:  [5, 3, 5, 4, 2, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 4, 5, 5, 0, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 0, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1748, Accuracy: 0.3333, Precision: 0.7176, Recall: 0.3242, F1: 0.3683

