Model: google/electra-base-discriminator, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 1: 1011
  Label 2: 966
  Label 5: 260
  Label 4: 344
  Label 3: 495
Label counts for Validation:
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
  Label 2: 107
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 1: 1020
  Label 2: 974
  Label 5: 218
  Label 4: 354
  Label 3: 505
Layer: backbone.electra.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.electra.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.electra.embeddings.token_type_embeddings.weight, Size: torch.Size([2, 768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.electra.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.electra.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.classifier.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.classifier.out_proj.weight, Size: torch.Size([6, 768]), req grad: True
Layer: backbone.classifier.out_proj.bias, Size: torch.Size([6]), req grad: True
Epoch 1/70
Train Loss: 1.6655, Accuracy: 0.2594, Precision: 0.2245, Recall: 0.1691, F1: 0.1402
Validation Loss: 1.6962, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6910, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7900, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 2/70
Train Loss: 1.6638, Accuracy: 0.2547, Precision: 0.1216, Recall: 0.1649, F1: 0.1337
Validation Loss: 1.6952, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6905, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7789, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 3/70
Train Loss: 1.6613, Accuracy: 0.2528, Precision: 0.1211, Recall: 0.1618, F1: 0.1251
Validation Loss: 1.6935, Accuracy: 0.2281, Precision: 0.0380, Recall: 0.1667, F1: 0.0619
Testing Loss: 1.6912, Accuracy: 0.2295, Precision: 0.0382, Recall: 0.1667, F1: 0.0622
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7680, Accuracy: 0.1905, Precision: 0.0381, Recall: 0.2000, F1: 0.0640
Epoch 4/70
Train Loss: 1.6605, Accuracy: 0.2473, Precision: 0.1175, Recall: 0.1593, F1: 0.1262
Validation Loss: 1.6737, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6684, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8298, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 5/70
Train Loss: 1.6573, Accuracy: 0.2580, Precision: 0.1224, Recall: 0.1642, F1: 0.1229
Validation Loss: 1.6823, Accuracy: 0.2281, Precision: 0.0380, Recall: 0.1667, F1: 0.0619
Testing Loss: 1.6782, Accuracy: 0.2295, Precision: 0.0382, Recall: 0.1667, F1: 0.0622
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8350, Accuracy: 0.1905, Precision: 0.0381, Recall: 0.2000, F1: 0.0640
Epoch 6/70
Train Loss: 1.6568, Accuracy: 0.2632, Precision: 0.1268, Recall: 0.1687, F1: 0.1312
Validation Loss: 1.6858, Accuracy: 0.2281, Precision: 0.0380, Recall: 0.1667, F1: 0.0619
Testing Loss: 1.6824, Accuracy: 0.2295, Precision: 0.0382, Recall: 0.1667, F1: 0.0622
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8900, Accuracy: 0.1905, Precision: 0.0381, Recall: 0.2000, F1: 0.0640
Epoch 7/70
Train Loss: 1.6580, Accuracy: 0.2630, Precision: 0.1258, Recall: 0.1691, F1: 0.1332
Validation Loss: 1.6688, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6647, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8349, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 8/70
Train Loss: 1.6582, Accuracy: 0.2578, Precision: 0.1216, Recall: 0.1644, F1: 0.1249
Validation Loss: 1.6670, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6624, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8434, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 9/70
Train Loss: 1.6588, Accuracy: 0.2689, Precision: 0.1305, Recall: 0.1722, F1: 0.1327
Validation Loss: 1.6686, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6658, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8219, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 10/70
Train Loss: 1.6560, Accuracy: 0.2594, Precision: 0.1231, Recall: 0.1653, F1: 0.1240
Validation Loss: 1.6746, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6684, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8849, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 11/70
Train Loss: 1.6556, Accuracy: 0.2729, Precision: 0.1313, Recall: 0.1737, F1: 0.1300
Validation Loss: 1.6669, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6623, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8431, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 12/70
Train Loss: 1.6580, Accuracy: 0.2540, Precision: 0.1207, Recall: 0.1623, F1: 0.1244
Validation Loss: 1.6675, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6624, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8300, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 13/70
Train Loss: 1.6550, Accuracy: 0.2718, Precision: 0.1300, Recall: 0.1733, F1: 0.1314
Validation Loss: 1.6713, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6670, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8067, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 14/70
Train Loss: 1.6561, Accuracy: 0.2578, Precision: 0.1198, Recall: 0.1642, F1: 0.1215
Validation Loss: 1.6675, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6637, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8167, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 15/70
Train Loss: 1.6563, Accuracy: 0.2537, Precision: 0.1134, Recall: 0.1604, F1: 0.1137
Validation Loss: 1.6693, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6641, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8558, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 16/70
Train Loss: 1.6557, Accuracy: 0.2571, Precision: 0.1118, Recall: 0.1608, F1: 0.1038
Validation Loss: 1.6705, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6661, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8100, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 17/70
Train Loss: 1.6584, Accuracy: 0.2542, Precision: 0.1198, Recall: 0.1612, F1: 0.1178
Validation Loss: 1.6694, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6646, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8653, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 18/70
Train Loss: 1.6574, Accuracy: 0.2618, Precision: 0.1226, Recall: 0.1659, F1: 0.1182
Validation Loss: 1.6678, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6630, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8358, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 19/70
Train Loss: 1.6561, Accuracy: 0.2646, Precision: 0.1285, Recall: 0.1691, F1: 0.1298
Validation Loss: 1.6726, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6696, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8334, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 20/70
Train Loss: 1.6553, Accuracy: 0.2661, Precision: 0.1230, Recall: 0.1674, F1: 0.1140
Validation Loss: 1.6671, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6628, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8287, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 21/70
Train Loss: 1.6558, Accuracy: 0.2656, Precision: 0.1231, Recall: 0.1669, F1: 0.1121
Validation Loss: 1.6691, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6648, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8512, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 22/70
Train Loss: 1.6550, Accuracy: 0.2590, Precision: 0.1204, Recall: 0.1632, F1: 0.1134
Validation Loss: 1.6681, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6638, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8282, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 23/70
Train Loss: 1.6548, Accuracy: 0.2713, Precision: 0.1323, Recall: 0.1714, F1: 0.1212
Validation Loss: 1.6686, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6655, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7934, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 24/70
Train Loss: 1.6551, Accuracy: 0.2670, Precision: 0.1255, Recall: 0.1681, F1: 0.1150
Validation Loss: 1.6684, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6634, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8257, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 25/70
Train Loss: 1.6537, Accuracy: 0.2675, Precision: 0.1298, Recall: 0.1686, F1: 0.1174
Validation Loss: 1.6778, Accuracy: 0.2281, Precision: 0.0380, Recall: 0.1667, F1: 0.0619
Testing Loss: 1.6730, Accuracy: 0.2295, Precision: 0.0382, Recall: 0.1667, F1: 0.0622
LM Predictions:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8520, Accuracy: 0.1905, Precision: 0.0381, Recall: 0.2000, F1: 0.0640
Epoch 26/70
Train Loss: 1.6563, Accuracy: 0.2540, Precision: 0.1172, Recall: 0.1612, F1: 0.1183
Validation Loss: 1.6691, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6646, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8067, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 27/70
Train Loss: 1.6553, Accuracy: 0.2654, Precision: 0.1283, Recall: 0.1679, F1: 0.1203
Validation Loss: 1.6671, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6626, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8523, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 28/70
Train Loss: 1.6549, Accuracy: 0.2639, Precision: 0.1204, Recall: 0.1645, F1: 0.1004
Validation Loss: 1.6674, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6624, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8535, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 29/70
Train Loss: 1.6539, Accuracy: 0.2646, Precision: 0.1236, Recall: 0.1673, F1: 0.1197
Validation Loss: 1.6676, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6621, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8431, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 30/70
Train Loss: 1.6555, Accuracy: 0.2644, Precision: 0.1277, Recall: 0.1666, F1: 0.1154
Validation Loss: 1.6686, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6641, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8425, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 31/70
Train Loss: 1.6550, Accuracy: 0.2673, Precision: 0.1182, Recall: 0.1681, F1: 0.1121
Validation Loss: 1.6661, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6618, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8292, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 32/70
Train Loss: 1.6540, Accuracy: 0.2535, Precision: 0.1118, Recall: 0.1593, F1: 0.1079
Validation Loss: 1.6678, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6629, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8420, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 33/70
Train Loss: 1.6540, Accuracy: 0.2737, Precision: 0.1334, Recall: 0.1714, F1: 0.1124
Validation Loss: 1.6681, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6636, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8303, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 34/70
Train Loss: 1.6549, Accuracy: 0.2618, Precision: 0.1183, Recall: 0.1640, F1: 0.1074
Validation Loss: 1.6686, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6645, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8311, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 35/70
Train Loss: 1.6527, Accuracy: 0.2649, Precision: 0.1198, Recall: 0.1700, F1: 0.1178
Validation Loss: 1.6697, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6655, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8366, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 36/70
Train Loss: 1.6546, Accuracy: 0.2687, Precision: 0.1233, Recall: 0.1677, F1: 0.1050
Validation Loss: 1.6679, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6630, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8301, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 37/70
Train Loss: 1.6547, Accuracy: 0.2649, Precision: 0.1220, Recall: 0.1654, F1: 0.1043
Validation Loss: 1.6751, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6696, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8087, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 38/70
Train Loss: 1.6545, Accuracy: 0.2637, Precision: 0.1205, Recall: 0.1655, F1: 0.1104
Validation Loss: 1.6681, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6630, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8304, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 39/70
Train Loss: 1.6547, Accuracy: 0.2682, Precision: 0.1257, Recall: 0.1694, F1: 0.1204
Validation Loss: 1.6683, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6631, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8282, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 40/70
Train Loss: 1.6556, Accuracy: 0.2668, Precision: 0.1230, Recall: 0.1666, F1: 0.1029
Validation Loss: 1.6663, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6621, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8199, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 41/70
Train Loss: 1.6544, Accuracy: 0.2644, Precision: 0.1233, Recall: 0.1657, F1: 0.1083
Validation Loss: 1.6665, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6621, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8423, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 42/70
Train Loss: 1.6530, Accuracy: 0.2677, Precision: 0.1281, Recall: 0.1692, F1: 0.1169
Validation Loss: 1.6666, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6618, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8311, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 43/70
Train Loss: 1.6538, Accuracy: 0.2665, Precision: 0.1245, Recall: 0.1676, F1: 0.1117
Validation Loss: 1.6676, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6631, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8192, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 44/70
Train Loss: 1.6527, Accuracy: 0.2722, Precision: 0.1269, Recall: 0.1688, F1: 0.0960
Validation Loss: 1.6698, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6666, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.7912, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 45/70
Train Loss: 1.6552, Accuracy: 0.2582, Precision: 0.1340, Recall: 0.1631, F1: 0.1115
Validation Loss: 1.6663, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6615, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8449, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 46/70
Train Loss: 1.6557, Accuracy: 0.2649, Precision: 0.1274, Recall: 0.1655, F1: 0.1049
Validation Loss: 1.6679, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6636, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8214, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 47/70
Train Loss: 1.6549, Accuracy: 0.2625, Precision: 0.1215, Recall: 0.1658, F1: 0.1155
Validation Loss: 1.6669, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6627, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8299, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 48/70
Train Loss: 1.6543, Accuracy: 0.2751, Precision: 0.1293, Recall: 0.1719, F1: 0.1087
Validation Loss: 1.6697, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6654, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8427, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 49/70
Train Loss: 1.6561, Accuracy: 0.2601, Precision: 0.1214, Recall: 0.1632, F1: 0.1087
Validation Loss: 1.6664, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6624, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8268, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 50/70
Train Loss: 1.6552, Accuracy: 0.2632, Precision: 0.1150, Recall: 0.1632, F1: 0.0929
Validation Loss: 1.6666, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6623, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8311, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 51/70
Train Loss: 1.6546, Accuracy: 0.2665, Precision: 0.1192, Recall: 0.1664, F1: 0.1050
Validation Loss: 1.6671, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6628, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8387, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 52/70
Train Loss: 1.6530, Accuracy: 0.2729, Precision: 0.1270, Recall: 0.1704, F1: 0.1057
Validation Loss: 1.6672, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6627, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8494, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 53/70
Train Loss: 1.6538, Accuracy: 0.2687, Precision: 0.1310, Recall: 0.1678, F1: 0.1058
Validation Loss: 1.6661, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6619, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8267, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 54/70
Train Loss: 1.6528, Accuracy: 0.2682, Precision: 0.1256, Recall: 0.1702, F1: 0.1234
Validation Loss: 1.6704, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6657, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8610, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 55/70
Train Loss: 1.6537, Accuracy: 0.2680, Precision: 0.1283, Recall: 0.1673, F1: 0.1049
Validation Loss: 1.6667, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6625, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8485, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 56/70
Train Loss: 1.6546, Accuracy: 0.2642, Precision: 0.1171, Recall: 0.1643, F1: 0.0972
Validation Loss: 1.6657, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6614, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8258, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 57/70
Train Loss: 1.6552, Accuracy: 0.2613, Precision: 0.1198, Recall: 0.1641, F1: 0.1106
Validation Loss: 1.6668, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6624, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8252, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 58/70
Train Loss: 1.6551, Accuracy: 0.2608, Precision: 0.1132, Recall: 0.1627, F1: 0.1006
Validation Loss: 1.6677, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6633, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8137, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 59/70
Train Loss: 1.6542, Accuracy: 0.2611, Precision: 0.1227, Recall: 0.1632, F1: 0.1044
Validation Loss: 1.6672, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6631, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8228, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 60/70
Train Loss: 1.6537, Accuracy: 0.2646, Precision: 0.1048, Recall: 0.1650, F1: 0.0995
Validation Loss: 1.6666, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6621, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8374, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 61/70
Train Loss: 1.6547, Accuracy: 0.2611, Precision: 0.1228, Recall: 0.1631, F1: 0.1034
Validation Loss: 1.6661, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6628, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8179, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 62/70
Train Loss: 1.6521, Accuracy: 0.2691, Precision: 0.1148, Recall: 0.1673, F1: 0.0978
Validation Loss: 1.6676, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6625, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8452, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 63/70
Train Loss: 1.6545, Accuracy: 0.2654, Precision: 0.1178, Recall: 0.1641, F1: 0.0888
Validation Loss: 1.6676, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6629, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8515, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 64/70
Train Loss: 1.6545, Accuracy: 0.2710, Precision: 0.1297, Recall: 0.1678, F1: 0.0905
Validation Loss: 1.6687, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6645, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8117, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 65/70
Train Loss: 1.6535, Accuracy: 0.2668, Precision: 0.1230, Recall: 0.1658, F1: 0.0979
Validation Loss: 1.6664, Accuracy: 0.2409, Precision: 0.0402, Recall: 0.1667, F1: 0.0647
Testing Loss: 1.6630, Accuracy: 0.2403, Precision: 0.0401, Recall: 0.1667, F1: 0.0646
LM Predictions:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8162, Accuracy: 0.2143, Precision: 0.0429, Recall: 0.2000, F1: 0.0706
Epoch 66/70
Train Loss: 1.6527, Accuracy: 0.2689, Precision: 0.1269, Recall: 0.1685, F1: 0.1105
Validation Loss: 1.6673, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6626, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8393, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 67/70
Train Loss: 1.6538, Accuracy: 0.2661, Precision: 0.1208, Recall: 0.1652, F1: 0.0959
Validation Loss: 1.6658, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6618, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8315, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 68/70
Train Loss: 1.6538, Accuracy: 0.2644, Precision: 0.1211, Recall: 0.1657, F1: 0.1086
Validation Loss: 1.6659, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6621, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8313, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 69/70
Train Loss: 1.6545, Accuracy: 0.2601, Precision: 0.1141, Recall: 0.1630, F1: 0.1047
Validation Loss: 1.6664, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6617, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8394, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Epoch 70/70
Train Loss: 1.6544, Accuracy: 0.2646, Precision: 0.1183, Recall: 0.1647, F1: 0.0978
Validation Loss: 1.6657, Accuracy: 0.2708, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
Testing Loss: 1.6613, Accuracy: 0.2705, Precision: 0.0451, Recall: 0.1667, F1: 0.0710
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8329, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426
Label Memorization Analysis: 
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [0, 1, 2, 1, 1, 1, 0, 4, 4, 0, 3, 0, 2, 4, 3, 3, 2, 4, 3, 3, 3, 4, 3, 1, 1, 2, 4, 4, 2, 0, 4, 3, 3, 2, 1, 4, 1, 4, 3, 2, 1, 2]
LM Loss: 1.8329, Accuracy: 0.1190, Precision: 0.0238, Recall: 0.2000, F1: 0.0426

