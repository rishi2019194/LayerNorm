Model: Qwen/Qwen2-0.5B-Instruct, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 1: 1011
  Label 0: 1141
  Label 2: 966
  Label 4: 344
  Label 3: 495
  Label 5: 260
Label counts for Validation:
  Label 2: 107
  Label 0: 127
  Label 1: 113
  Label 5: 29
  Label 4: 38
  Label 3: 55
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 1: 1025
  Label 0: 1150
  Label 2: 972
  Label 4: 351
  Label 3: 501
  Label 5: 218
For early layers:  [0, 1, 2, 3, 4, 5, 6, 7]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 1.6230, Accuracy: 0.4204, Precision: 0.3253, Recall: 0.3043, F1: 0.2962
Validation Loss: 1.1348, Accuracy: 0.5800, Precision: 0.4120, Recall: 0.4391, F1: 0.4105
Testing Loss: 1.1299, Accuracy: 0.5966, Precision: 0.5894, Recall: 0.4498, F1: 0.4316
LM Predictions:  [3, 0, 0, 2, 0, 1, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 3, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 1, 3, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.7111, Accuracy: 0.3095, Precision: 0.3052, Recall: 0.2873, F1: 0.2119
Epoch 2/70
Train Loss: 0.7325, Accuracy: 0.7539, Precision: 0.6872, Recall: 0.6559, F1: 0.6649
Validation Loss: 0.8666, Accuracy: 0.7079, Precision: 0.5808, Recall: 0.6094, F1: 0.5884
Testing Loss: 0.8622, Accuracy: 0.6981, Precision: 0.5856, Recall: 0.5970, F1: 0.5835
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 3, 1, 0, 0, 0, 3, 1, 0, 0, 3, 0, 0, 2, 0, 2, 3, 4, 0, 0, 0, 0, 0, 0, 0, 4, 0, 3, 0, 0, 3, 0, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0306, Accuracy: 0.6190, Precision: 0.7961, Recall: 0.6571, F1: 0.6463
Epoch 3/70
Train Loss: 0.2825, Accuracy: 0.9120, Precision: 0.8721, Recall: 0.8606, F1: 0.8660
Validation Loss: 1.1741, Accuracy: 0.6759, Precision: 0.5808, Recall: 0.5742, F1: 0.5652
Testing Loss: 1.1026, Accuracy: 0.6908, Precision: 0.6080, Recall: 0.5723, F1: 0.5761
LM Predictions:  [2, 2, 0, 0, 0, 1, 4, 0, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 0, 3, 4, 0, 0, 1, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8965, Accuracy: 0.6429, Precision: 0.8750, Recall: 0.6667, F1: 0.6945
Epoch 4/70
Train Loss: 0.1735, Accuracy: 0.9424, Precision: 0.9117, Recall: 0.9099, F1: 0.9108
Validation Loss: 1.4267, Accuracy: 0.6994, Precision: 0.6592, Recall: 0.6530, F1: 0.6474
Testing Loss: 1.1232, Accuracy: 0.7101, Precision: 0.6466, Recall: 0.6547, F1: 0.6460
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 3, 4, 0, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 5, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5167, Accuracy: 0.7619, Precision: 0.7139, Recall: 0.6085, F1: 0.6377
Epoch 5/70
Train Loss: 0.1446, Accuracy: 0.9481, Precision: 0.9193, Recall: 0.9178, F1: 0.9185
Validation Loss: 1.3107, Accuracy: 0.7122, Precision: 0.6800, Recall: 0.6419, F1: 0.6536
Testing Loss: 1.2382, Accuracy: 0.7234, Precision: 0.6894, Recall: 0.6413, F1: 0.6567
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 5, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 5, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4958, Accuracy: 0.8571, Precision: 0.7821, Recall: 0.7063, F1: 0.7291
Epoch 6/70
Train Loss: 0.1040, Accuracy: 0.9580, Precision: 0.9320, Recall: 0.9348, F1: 0.9333
Validation Loss: 1.6482, Accuracy: 0.7079, Precision: 0.6556, Recall: 0.6250, F1: 0.6188
Testing Loss: 1.4840, Accuracy: 0.7005, Precision: 0.6460, Recall: 0.6104, F1: 0.6049
LM Predictions:  [2, 2, 0, 1, 0, 2, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 4, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3121, Accuracy: 0.8571, Precision: 0.8849, Recall: 0.8571, F1: 0.8443
Epoch 7/70
Train Loss: 0.1054, Accuracy: 0.9590, Precision: 0.9373, Recall: 0.9345, F1: 0.9359
Validation Loss: 1.5940, Accuracy: 0.7207, Precision: 0.6669, Recall: 0.5950, F1: 0.6017
Testing Loss: 1.5591, Accuracy: 0.7186, Precision: 0.6435, Recall: 0.5870, F1: 0.5936
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 0, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4464, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7634
Epoch 8/70
Train Loss: 0.1082, Accuracy: 0.9528, Precision: 0.9265, Recall: 0.9287, F1: 0.9276
Validation Loss: 1.5821, Accuracy: 0.6780, Precision: 0.6360, Recall: 0.6128, F1: 0.5915
Testing Loss: 1.5175, Accuracy: 0.6763, Precision: 0.5926, Recall: 0.5997, F1: 0.5727
LM Predictions:  [3, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 3, 0, 3, 0, 2, 3, 4, 0, 0, 4, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4742, Accuracy: 0.7857, Precision: 0.8248, Recall: 0.7619, F1: 0.7641
Epoch 9/70
Train Loss: 0.1194, Accuracy: 0.9535, Precision: 0.9280, Recall: 0.9260, F1: 0.9270
Validation Loss: 1.7030, Accuracy: 0.6866, Precision: 0.6701, Recall: 0.6328, F1: 0.6286
Testing Loss: 1.5384, Accuracy: 0.7150, Precision: 0.6917, Recall: 0.6491, F1: 0.6561
LM Predictions:  [5, 2, 0, 1, 0, 1, 4, 5, 1, 0, 5, 5, 3, 1, 0, 0, 5, 5, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6616, Accuracy: 0.7143, Precision: 0.7821, Recall: 0.5635, F1: 0.6275
Epoch 10/70
Train Loss: 0.0962, Accuracy: 0.9616, Precision: 0.9390, Recall: 0.9345, F1: 0.9367
Validation Loss: 1.6586, Accuracy: 0.7079, Precision: 0.6645, Recall: 0.6168, F1: 0.6310
Testing Loss: 1.5140, Accuracy: 0.7476, Precision: 0.7074, Recall: 0.6529, F1: 0.6703
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4140, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917
Epoch 11/70
Train Loss: 0.1108, Accuracy: 0.9580, Precision: 0.9371, Recall: 0.9374, F1: 0.9372
Validation Loss: 1.3390, Accuracy: 0.7186, Precision: 0.6993, Recall: 0.6019, F1: 0.6193
Testing Loss: 1.2210, Accuracy: 0.7428, Precision: 0.6798, Recall: 0.6025, F1: 0.6128
LM Predictions:  [2, 0, 1, 1, 0, 1, 4, 1, 0, 0, 0, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3078, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8476, F1: 0.8613
Epoch 12/70
Train Loss: 0.0946, Accuracy: 0.9599, Precision: 0.9392, Recall: 0.9349, F1: 0.9370
Validation Loss: 1.3994, Accuracy: 0.7036, Precision: 0.6769, Recall: 0.6122, F1: 0.6304
Testing Loss: 1.2995, Accuracy: 0.7440, Precision: 0.7063, Recall: 0.6394, F1: 0.6598
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 5, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3976, Accuracy: 0.7857, Precision: 0.7549, Recall: 0.6310, F1: 0.6470
Epoch 13/70
Train Loss: 0.0699, Accuracy: 0.9644, Precision: 0.9424, Recall: 0.9423, F1: 0.9423
Validation Loss: 1.9008, Accuracy: 0.7313, Precision: 0.6705, Recall: 0.6749, F1: 0.6712
Testing Loss: 1.9711, Accuracy: 0.7415, Precision: 0.6880, Recall: 0.6734, F1: 0.6790
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 3, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1960, Accuracy: 0.9762, Precision: 0.9714, Recall: 0.9714, F1: 0.9692
Epoch 14/70
Train Loss: 0.0694, Accuracy: 0.9642, Precision: 0.9380, Recall: 0.9448, F1: 0.9412
Validation Loss: 1.6964, Accuracy: 0.7399, Precision: 0.6824, Recall: 0.6852, F1: 0.6823
Testing Loss: 1.6085, Accuracy: 0.7488, Precision: 0.6777, Recall: 0.6728, F1: 0.6729
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3226, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 15/70
Train Loss: 0.0595, Accuracy: 0.9694, Precision: 0.9483, Recall: 0.9482, F1: 0.9483
Validation Loss: 2.7179, Accuracy: 0.7122, Precision: 0.6757, Recall: 0.6228, F1: 0.6398
Testing Loss: 2.3899, Accuracy: 0.7307, Precision: 0.6792, Recall: 0.6288, F1: 0.6459
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1760, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 16/70
Train Loss: 0.0609, Accuracy: 0.9673, Precision: 0.9430, Recall: 0.9459, F1: 0.9445
Validation Loss: 1.9091, Accuracy: 0.7313, Precision: 0.6747, Recall: 0.6635, F1: 0.6665
Testing Loss: 1.8273, Accuracy: 0.7403, Precision: 0.6799, Recall: 0.6605, F1: 0.6678
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3336, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 17/70
Train Loss: 0.0546, Accuracy: 0.9720, Precision: 0.9554, Recall: 0.9520, F1: 0.9537
Validation Loss: 2.6033, Accuracy: 0.7292, Precision: 0.6878, Recall: 0.6535, F1: 0.6662
Testing Loss: 2.5520, Accuracy: 0.7428, Precision: 0.6993, Recall: 0.6586, F1: 0.6728
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3459, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8341
Epoch 18/70
Train Loss: 0.0523, Accuracy: 0.9692, Precision: 0.9467, Recall: 0.9530, F1: 0.9497
Validation Loss: 2.9270, Accuracy: 0.7463, Precision: 0.7068, Recall: 0.6581, F1: 0.6749
Testing Loss: 2.9583, Accuracy: 0.7343, Precision: 0.6995, Recall: 0.6391, F1: 0.6581
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2764, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8341
Epoch 19/70
Train Loss: 0.0511, Accuracy: 0.9687, Precision: 0.9483, Recall: 0.9510, F1: 0.9496
Validation Loss: 2.8411, Accuracy: 0.7356, Precision: 0.6871, Recall: 0.6667, F1: 0.6750
Testing Loss: 2.7612, Accuracy: 0.7415, Precision: 0.6884, Recall: 0.6561, F1: 0.6681
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2413, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8286, F1: 0.8426
Epoch 20/70
Train Loss: 0.0997, Accuracy: 0.9578, Precision: 0.9337, Recall: 0.9367, F1: 0.9351
Validation Loss: 1.4572, Accuracy: 0.6951, Precision: 0.6892, Recall: 0.6362, F1: 0.6099
Testing Loss: 1.3688, Accuracy: 0.6993, Precision: 0.6330, Recall: 0.6249, F1: 0.6046
LM Predictions:  [3, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 3, 3, 4, 0, 0, 4, 1, 0, 4, 1, 4, 0, 3, 1, 4, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4910, Accuracy: 0.8810, Precision: 0.8856, Recall: 0.8714, F1: 0.8539
Epoch 21/70
Train Loss: 0.2232, Accuracy: 0.9274, Precision: 0.8945, Recall: 0.8914, F1: 0.8929
Validation Loss: 1.0347, Accuracy: 0.7335, Precision: 0.7000, Recall: 0.6906, F1: 0.6810
Testing Loss: 0.8912, Accuracy: 0.7609, Precision: 0.7162, Recall: 0.6960, F1: 0.6967
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 5, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 5, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4770, Accuracy: 0.8333, Precision: 0.7738, Recall: 0.6706, F1: 0.6909
Epoch 22/70
Train Loss: 0.0877, Accuracy: 0.9675, Precision: 0.9450, Recall: 0.9525, F1: 0.9486
Validation Loss: 1.2737, Accuracy: 0.7463, Precision: 0.7153, Recall: 0.6564, F1: 0.6729
Testing Loss: 1.1620, Accuracy: 0.7500, Precision: 0.7107, Recall: 0.6454, F1: 0.6645
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 2, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2534, Accuracy: 0.8810, Precision: 0.9099, Recall: 0.8476, F1: 0.8537
Epoch 23/70
Train Loss: 0.0630, Accuracy: 0.9673, Precision: 0.9458, Recall: 0.9445, F1: 0.9452
Validation Loss: 1.3287, Accuracy: 0.6972, Precision: 0.6621, Recall: 0.6075, F1: 0.6157
Testing Loss: 1.1713, Accuracy: 0.7560, Precision: 0.7233, Recall: 0.6689, F1: 0.6800
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3227, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 24/70
Train Loss: 0.0571, Accuracy: 0.9706, Precision: 0.9514, Recall: 0.9517, F1: 0.9515
Validation Loss: 1.5230, Accuracy: 0.7591, Precision: 0.7095, Recall: 0.7035, F1: 0.7025
Testing Loss: 1.3844, Accuracy: 0.7742, Precision: 0.7176, Recall: 0.7010, F1: 0.7064
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 3, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0690, Accuracy: 0.9762, Precision: 0.9714, Recall: 0.9667, F1: 0.9664
Epoch 25/70
Train Loss: 0.0624, Accuracy: 0.9675, Precision: 0.9459, Recall: 0.9461, F1: 0.9460
Validation Loss: 1.5326, Accuracy: 0.7804, Precision: 0.7367, Recall: 0.7208, F1: 0.7265
Testing Loss: 1.4936, Accuracy: 0.7681, Precision: 0.6957, Recall: 0.6805, F1: 0.6867
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1162, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 26/70
Train Loss: 0.0496, Accuracy: 0.9704, Precision: 0.9504, Recall: 0.9501, F1: 0.9502
Validation Loss: 1.4722, Accuracy: 0.7548, Precision: 0.7103, Recall: 0.6927, F1: 0.6999
Testing Loss: 1.3441, Accuracy: 0.7633, Precision: 0.7089, Recall: 0.6764, F1: 0.6893
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2240, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 27/70
Train Loss: 0.0486, Accuracy: 0.9713, Precision: 0.9497, Recall: 0.9540, F1: 0.9518
Validation Loss: 1.7975, Accuracy: 0.7676, Precision: 0.7189, Recall: 0.7274, F1: 0.7214
Testing Loss: 1.6770, Accuracy: 0.7645, Precision: 0.7044, Recall: 0.6980, F1: 0.6999
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2517, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 28/70
Train Loss: 0.0485, Accuracy: 0.9701, Precision: 0.9508, Recall: 0.9457, F1: 0.9482
Validation Loss: 1.9138, Accuracy: 0.7676, Precision: 0.7191, Recall: 0.7134, F1: 0.7140
Testing Loss: 1.7608, Accuracy: 0.7729, Precision: 0.7084, Recall: 0.6998, F1: 0.7026
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1516, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 29/70
Train Loss: 0.0484, Accuracy: 0.9694, Precision: 0.9453, Recall: 0.9545, F1: 0.9496
Validation Loss: 1.9162, Accuracy: 0.7740, Precision: 0.7277, Recall: 0.7160, F1: 0.7196
Testing Loss: 1.7937, Accuracy: 0.7717, Precision: 0.7115, Recall: 0.6965, F1: 0.7031
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1829, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9143, F1: 0.9149
Epoch 30/70
Train Loss: 0.0488, Accuracy: 0.9696, Precision: 0.9506, Recall: 0.9494, F1: 0.9500
Validation Loss: 1.9773, Accuracy: 0.7697, Precision: 0.7213, Recall: 0.7069, F1: 0.7124
Testing Loss: 1.9025, Accuracy: 0.7705, Precision: 0.7155, Recall: 0.6854, F1: 0.6975
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2166, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7667, F1: 0.7917
Epoch 31/70
Train Loss: 0.0483, Accuracy: 0.9675, Precision: 0.9448, Recall: 0.9479, F1: 0.9463
Validation Loss: 2.1228, Accuracy: 0.7655, Precision: 0.7260, Recall: 0.6962, F1: 0.7030
Testing Loss: 1.9850, Accuracy: 0.7669, Precision: 0.7043, Recall: 0.6874, F1: 0.6923
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0575, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0480, Accuracy: 0.9730, Precision: 0.9544, Recall: 0.9560, F1: 0.9552
Validation Loss: 1.7657, Accuracy: 0.7633, Precision: 0.7172, Recall: 0.6659, F1: 0.6782
Testing Loss: 1.7110, Accuracy: 0.7693, Precision: 0.7004, Recall: 0.6545, F1: 0.6667
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1486, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 33/70
Train Loss: 0.0472, Accuracy: 0.9711, Precision: 0.9526, Recall: 0.9501, F1: 0.9513
Validation Loss: 2.1450, Accuracy: 0.7740, Precision: 0.7256, Recall: 0.7114, F1: 0.7167
Testing Loss: 2.0979, Accuracy: 0.7717, Precision: 0.7220, Recall: 0.6940, F1: 0.7050
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1486, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 34/70
Train Loss: 0.0467, Accuracy: 0.9713, Precision: 0.9514, Recall: 0.9542, F1: 0.9528
Validation Loss: 1.9848, Accuracy: 0.7655, Precision: 0.7014, Recall: 0.6731, F1: 0.6793
Testing Loss: 1.9385, Accuracy: 0.7717, Precision: 0.7005, Recall: 0.6673, F1: 0.6769
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2424, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 35/70
Train Loss: 0.0469, Accuracy: 0.9732, Precision: 0.9562, Recall: 0.9528, F1: 0.9545
Validation Loss: 2.1601, Accuracy: 0.7740, Precision: 0.7297, Recall: 0.7154, F1: 0.7214
Testing Loss: 2.1236, Accuracy: 0.7742, Precision: 0.7188, Recall: 0.6876, F1: 0.7001
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0643, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 36/70
Train Loss: 0.1695, Accuracy: 0.9447, Precision: 0.9202, Recall: 0.9163, F1: 0.9182
Validation Loss: 1.0954, Accuracy: 0.7697, Precision: 0.7038, Recall: 0.6701, F1: 0.6764
Testing Loss: 1.0990, Accuracy: 0.7754, Precision: 0.6926, Recall: 0.6620, F1: 0.6680
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 0, 1, 1, 1, 0, 0, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5745, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7905, F1: 0.8055
Epoch 37/70
Train Loss: 0.1158, Accuracy: 0.9530, Precision: 0.9311, Recall: 0.9214, F1: 0.9261
Validation Loss: 1.1409, Accuracy: 0.7697, Precision: 0.7264, Recall: 0.7121, F1: 0.7149
Testing Loss: 1.0645, Accuracy: 0.7717, Precision: 0.7218, Recall: 0.7034, F1: 0.7101
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2728, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 38/70
Train Loss: 0.0706, Accuracy: 0.9654, Precision: 0.9435, Recall: 0.9395, F1: 0.9414
Validation Loss: 1.1646, Accuracy: 0.7697, Precision: 0.6902, Recall: 0.6633, F1: 0.6656
Testing Loss: 1.1387, Accuracy: 0.7681, Precision: 0.6980, Recall: 0.6603, F1: 0.6671
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 4, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3777, Accuracy: 0.7619, Precision: 0.8600, Recall: 0.7381, F1: 0.7578
Epoch 39/70
Train Loss: 0.0599, Accuracy: 0.9677, Precision: 0.9441, Recall: 0.9496, F1: 0.9466
Validation Loss: 1.3067, Accuracy: 0.7655, Precision: 0.6957, Recall: 0.6793, F1: 0.6811
Testing Loss: 1.3660, Accuracy: 0.7681, Precision: 0.7206, Recall: 0.6849, F1: 0.6960
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1814, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8830
Epoch 40/70
Train Loss: 0.0584, Accuracy: 0.9694, Precision: 0.9476, Recall: 0.9500, F1: 0.9488
Validation Loss: 1.4122, Accuracy: 0.7719, Precision: 0.7208, Recall: 0.7181, F1: 0.7152
Testing Loss: 1.3704, Accuracy: 0.7729, Precision: 0.7250, Recall: 0.7079, F1: 0.7108
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2094, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 41/70
Train Loss: 0.0482, Accuracy: 0.9704, Precision: 0.9509, Recall: 0.9522, F1: 0.9515
Validation Loss: 1.6153, Accuracy: 0.7719, Precision: 0.6822, Recall: 0.6698, F1: 0.6671
Testing Loss: 1.6987, Accuracy: 0.7802, Precision: 0.7267, Recall: 0.6771, F1: 0.6842
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1850, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8000, F1: 0.8239
Epoch 42/70
Train Loss: 0.0464, Accuracy: 0.9699, Precision: 0.9487, Recall: 0.9529, F1: 0.9507
Validation Loss: 1.7024, Accuracy: 0.7697, Precision: 0.6812, Recall: 0.6785, F1: 0.6727
Testing Loss: 1.6904, Accuracy: 0.7838, Precision: 0.7255, Recall: 0.6911, F1: 0.6981
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1800, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 43/70
Train Loss: 0.0468, Accuracy: 0.9696, Precision: 0.9479, Recall: 0.9499, F1: 0.9489
Validation Loss: 1.7245, Accuracy: 0.7783, Precision: 0.7200, Recall: 0.7058, F1: 0.7078
Testing Loss: 1.7325, Accuracy: 0.7790, Precision: 0.7246, Recall: 0.7017, F1: 0.7094
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1499, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 44/70
Train Loss: 0.0439, Accuracy: 0.9701, Precision: 0.9488, Recall: 0.9515, F1: 0.9501
Validation Loss: 1.8571, Accuracy: 0.7804, Precision: 0.7091, Recall: 0.6844, F1: 0.6876
Testing Loss: 1.8838, Accuracy: 0.7766, Precision: 0.7197, Recall: 0.6740, F1: 0.6839
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2791, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 45/70
Train Loss: 0.0454, Accuracy: 0.9701, Precision: 0.9506, Recall: 0.9474, F1: 0.9489
Validation Loss: 1.6918, Accuracy: 0.7804, Precision: 0.7281, Recall: 0.7086, F1: 0.7160
Testing Loss: 1.7028, Accuracy: 0.7729, Precision: 0.7230, Recall: 0.6946, F1: 0.7067
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2101, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 46/70
Train Loss: 0.0433, Accuracy: 0.9682, Precision: 0.9465, Recall: 0.9516, F1: 0.9488
Validation Loss: 1.8620, Accuracy: 0.7719, Precision: 0.6989, Recall: 0.6908, F1: 0.6876
Testing Loss: 1.8825, Accuracy: 0.7862, Precision: 0.7319, Recall: 0.7031, F1: 0.7102
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1746, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 47/70
Train Loss: 0.0450, Accuracy: 0.9692, Precision: 0.9452, Recall: 0.9533, F1: 0.9491
Validation Loss: 1.7920, Accuracy: 0.7740, Precision: 0.7087, Recall: 0.6843, F1: 0.6888
Testing Loss: 1.8026, Accuracy: 0.7802, Precision: 0.7256, Recall: 0.6855, F1: 0.6986
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1862, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8810, F1: 0.8909
Epoch 48/70
Train Loss: 0.0438, Accuracy: 0.9701, Precision: 0.9504, Recall: 0.9484, F1: 0.9494
Validation Loss: 1.8811, Accuracy: 0.7825, Precision: 0.7227, Recall: 0.7127, F1: 0.7102
Testing Loss: 1.8898, Accuracy: 0.7838, Precision: 0.7265, Recall: 0.7074, F1: 0.7113
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1743, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 49/70
Train Loss: 0.0588, Accuracy: 0.9704, Precision: 0.9501, Recall: 0.9504, F1: 0.9502
Validation Loss: 2.9342, Accuracy: 0.7825, Precision: 0.7369, Recall: 0.7184, F1: 0.7204
Testing Loss: 3.0375, Accuracy: 0.7717, Precision: 0.7195, Recall: 0.6820, F1: 0.6954
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3757, Accuracy: 0.8571, Precision: 0.7738, Recall: 0.7103, F1: 0.7237
Epoch 50/70
Train Loss: 0.0997, Accuracy: 0.9592, Precision: 0.9315, Recall: 0.9304, F1: 0.9310
Validation Loss: 1.7057, Accuracy: 0.7655, Precision: 0.6947, Recall: 0.6980, F1: 0.6889
Testing Loss: 1.6089, Accuracy: 0.7488, Precision: 0.6764, Recall: 0.6760, F1: 0.6719
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 5, 0, 0, 1, 1, 0, 0, 1, 3, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3596, Accuracy: 0.8333, Precision: 0.7405, Recall: 0.6548, F1: 0.6698
Epoch 51/70
Train Loss: 0.0932, Accuracy: 0.9606, Precision: 0.9355, Recall: 0.9335, F1: 0.9345
Validation Loss: 1.2124, Accuracy: 0.7868, Precision: 0.7401, Recall: 0.7325, F1: 0.7346
Testing Loss: 1.1257, Accuracy: 0.7754, Precision: 0.7220, Recall: 0.7086, F1: 0.7146
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 1, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0708, Accuracy: 0.9762, Precision: 0.9867, Recall: 0.9667, F1: 0.9749
Epoch 52/70
Train Loss: 0.0557, Accuracy: 0.9689, Precision: 0.9495, Recall: 0.9453, F1: 0.9473
Validation Loss: 1.3431, Accuracy: 0.7719, Precision: 0.7120, Recall: 0.7085, F1: 0.7089
Testing Loss: 1.3358, Accuracy: 0.7717, Precision: 0.7134, Recall: 0.7013, F1: 0.7062
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1175, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 53/70
Train Loss: 0.0528, Accuracy: 0.9675, Precision: 0.9462, Recall: 0.9446, F1: 0.9454
Validation Loss: 1.2265, Accuracy: 0.7910, Precision: 0.7496, Recall: 0.7400, F1: 0.7444
Testing Loss: 1.2356, Accuracy: 0.7742, Precision: 0.7201, Recall: 0.7041, F1: 0.7111
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1398, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9048, F1: 0.9083
Epoch 54/70
Train Loss: 0.0451, Accuracy: 0.9687, Precision: 0.9461, Recall: 0.9510, F1: 0.9485
Validation Loss: 1.4009, Accuracy: 0.7889, Precision: 0.7436, Recall: 0.7139, F1: 0.7233
Testing Loss: 1.4271, Accuracy: 0.7705, Precision: 0.6991, Recall: 0.6708, F1: 0.6786
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1161, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 55/70
Train Loss: 0.0435, Accuracy: 0.9723, Precision: 0.9538, Recall: 0.9534, F1: 0.9536
Validation Loss: 1.4445, Accuracy: 0.7932, Precision: 0.7448, Recall: 0.7266, F1: 0.7327
Testing Loss: 1.5002, Accuracy: 0.7717, Precision: 0.7146, Recall: 0.6927, F1: 0.7006
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1291, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 56/70
Train Loss: 0.0433, Accuracy: 0.9694, Precision: 0.9475, Recall: 0.9510, F1: 0.9492
Validation Loss: 1.5109, Accuracy: 0.7868, Precision: 0.7415, Recall: 0.7099, F1: 0.7200
Testing Loss: 1.5747, Accuracy: 0.7729, Precision: 0.7199, Recall: 0.6859, F1: 0.6979
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1460, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 57/70
Train Loss: 0.0425, Accuracy: 0.9704, Precision: 0.9493, Recall: 0.9507, F1: 0.9500
Validation Loss: 1.5845, Accuracy: 0.7868, Precision: 0.7248, Recall: 0.7009, F1: 0.7053
Testing Loss: 1.5857, Accuracy: 0.7766, Precision: 0.7069, Recall: 0.6741, F1: 0.6836
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1147, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 58/70
Train Loss: 0.0441, Accuracy: 0.9715, Precision: 0.9511, Recall: 0.9544, F1: 0.9527
Validation Loss: 1.6185, Accuracy: 0.7783, Precision: 0.7214, Recall: 0.6857, F1: 0.6916
Testing Loss: 1.6608, Accuracy: 0.7754, Precision: 0.7102, Recall: 0.6674, F1: 0.6785
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2972, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 59/70
Train Loss: 0.0447, Accuracy: 0.9692, Precision: 0.9499, Recall: 0.9462, F1: 0.9479
Validation Loss: 1.6236, Accuracy: 0.7825, Precision: 0.7323, Recall: 0.6919, F1: 0.7018
Testing Loss: 1.6918, Accuracy: 0.7778, Precision: 0.7037, Recall: 0.6683, F1: 0.6750
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3598, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 60/70
Train Loss: 0.0443, Accuracy: 0.9713, Precision: 0.9535, Recall: 0.9525, F1: 0.9530
Validation Loss: 1.5463, Accuracy: 0.7910, Precision: 0.7491, Recall: 0.7331, F1: 0.7395
Testing Loss: 1.5745, Accuracy: 0.7729, Precision: 0.7166, Recall: 0.6911, F1: 0.7010
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0831, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0430, Accuracy: 0.9706, Precision: 0.9490, Recall: 0.9529, F1: 0.9508
Validation Loss: 1.6092, Accuracy: 0.7910, Precision: 0.7376, Recall: 0.7123, F1: 0.7180
Testing Loss: 1.6594, Accuracy: 0.7790, Precision: 0.7181, Recall: 0.6915, F1: 0.6989
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1421, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 62/70
Train Loss: 0.0428, Accuracy: 0.9725, Precision: 0.9513, Recall: 0.9564, F1: 0.9537
Validation Loss: 1.6532, Accuracy: 0.7825, Precision: 0.7251, Recall: 0.6908, F1: 0.6963
Testing Loss: 1.6766, Accuracy: 0.7778, Precision: 0.7050, Recall: 0.6681, F1: 0.6761
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1666, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 63/70
Train Loss: 0.0449, Accuracy: 0.9701, Precision: 0.9506, Recall: 0.9489, F1: 0.9497
Validation Loss: 1.5731, Accuracy: 0.7910, Precision: 0.7476, Recall: 0.7197, F1: 0.7258
Testing Loss: 1.6235, Accuracy: 0.7754, Precision: 0.7122, Recall: 0.6923, F1: 0.6987
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1245, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 64/70
Train Loss: 0.0448, Accuracy: 0.9734, Precision: 0.9562, Recall: 0.9585, F1: 0.9573
Validation Loss: 1.5842, Accuracy: 0.7740, Precision: 0.7365, Recall: 0.7224, F1: 0.7266
Testing Loss: 1.5902, Accuracy: 0.7742, Precision: 0.7272, Recall: 0.7016, F1: 0.7114
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 5, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2812, Accuracy: 0.7857, Precision: 0.7549, Recall: 0.6270, F1: 0.6531
Epoch 65/70
Train Loss: 0.1154, Accuracy: 0.9568, Precision: 0.9328, Recall: 0.9335, F1: 0.9331
Validation Loss: 1.0852, Accuracy: 0.7996, Precision: 0.7751, Recall: 0.7356, F1: 0.7486
Testing Loss: 1.1185, Accuracy: 0.7754, Precision: 0.7320, Recall: 0.6913, F1: 0.7061
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 5, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2268, Accuracy: 0.9048, Precision: 0.7917, Recall: 0.7381, F1: 0.7478
Epoch 66/70
Train Loss: 0.0702, Accuracy: 0.9661, Precision: 0.9467, Recall: 0.9465, F1: 0.9466
Validation Loss: 1.3258, Accuracy: 0.7633, Precision: 0.6447, Recall: 0.6638, F1: 0.6525
Testing Loss: 1.2000, Accuracy: 0.7874, Precision: 0.6697, Recall: 0.6756, F1: 0.6697
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1256, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 67/70
Train Loss: 0.0509, Accuracy: 0.9718, Precision: 0.9535, Recall: 0.9528, F1: 0.9531
Validation Loss: 1.3180, Accuracy: 0.7612, Precision: 0.7178, Recall: 0.7203, F1: 0.7175
Testing Loss: 1.2085, Accuracy: 0.7886, Precision: 0.7371, Recall: 0.7176, F1: 0.7254
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1271, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 68/70
Train Loss: 0.0452, Accuracy: 0.9711, Precision: 0.9493, Recall: 0.9551, F1: 0.9520
Validation Loss: 1.2796, Accuracy: 0.7548, Precision: 0.7071, Recall: 0.7008, F1: 0.7034
Testing Loss: 1.1628, Accuracy: 0.7862, Precision: 0.7211, Recall: 0.7010, F1: 0.7091
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1238, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 69/70
Train Loss: 0.0440, Accuracy: 0.9673, Precision: 0.9446, Recall: 0.9462, F1: 0.9454
Validation Loss: 1.4266, Accuracy: 0.7527, Precision: 0.6989, Recall: 0.6801, F1: 0.6856
Testing Loss: 1.3513, Accuracy: 0.7899, Precision: 0.7299, Recall: 0.6982, F1: 0.7086
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1959, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 70/70
Train Loss: 0.0422, Accuracy: 0.9711, Precision: 0.9519, Recall: 0.9532, F1: 0.9526
Validation Loss: 1.5116, Accuracy: 0.7591, Precision: 0.7164, Recall: 0.6887, F1: 0.6982
Testing Loss: 1.4387, Accuracy: 0.7886, Precision: 0.7301, Recall: 0.6942, F1: 0.7061
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2238, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
For middle layers:  [8, 9, 10, 11, 12, 13, 14, 15]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.16.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.6666, Accuracy: 0.8269, Precision: 0.7658, Recall: 0.7528, F1: 0.7581
Validation Loss: 0.4462, Accuracy: 0.8635, Precision: 0.8344, Recall: 0.7999, F1: 0.8119
Testing Loss: 0.3847, Accuracy: 0.8732, Precision: 0.8620, Recall: 0.8035, F1: 0.8248
LM Predictions:  [5, 0, 0, 5, 0, 5, 4, 5, 0, 0, 0, 0, 0, 5, 0, 0, 5, 5, 5, 5, 0, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 5, 0, 5, 5, 5, 0, 5, 0, 0, 5, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.6235, Accuracy: 0.2143, Precision: 0.2302, Recall: 0.1720, F1: 0.1306
Epoch 2/70
Train Loss: 0.2338, Accuracy: 0.9248, Precision: 0.8890, Recall: 0.8826, F1: 0.8856
Validation Loss: 0.4605, Accuracy: 0.8699, Precision: 0.8464, Recall: 0.8255, F1: 0.8326
Testing Loss: 0.3932, Accuracy: 0.8804, Precision: 0.8469, Recall: 0.8152, F1: 0.8267
LM Predictions:  [2, 2, 1, 0, 0, 5, 4, 1, 0, 0, 0, 0, 3, 1, 0, 0, 4, 0, 5, 2, 0, 2, 0, 4, 0, 0, 0, 1, 0, 5, 0, 4, 0, 3, 0, 3, 0, 1, 0, 0, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.9385, Accuracy: 0.6190, Precision: 0.6964, Recall: 0.5437, F1: 0.5604
Epoch 3/70
Train Loss: 0.1590, Accuracy: 0.9455, Precision: 0.9139, Recall: 0.9119, F1: 0.9128
Validation Loss: 0.7329, Accuracy: 0.8614, Precision: 0.8270, Recall: 0.7753, F1: 0.7871
Testing Loss: 0.6454, Accuracy: 0.8720, Precision: 0.8529, Recall: 0.7875, F1: 0.8058
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 0, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7747, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7476, F1: 0.7664
Epoch 4/70
Train Loss: 0.1063, Accuracy: 0.9566, Precision: 0.9288, Recall: 0.9269, F1: 0.9278
Validation Loss: 0.4985, Accuracy: 0.8635, Precision: 0.8392, Recall: 0.8195, F1: 0.8240
Testing Loss: 0.4810, Accuracy: 0.8659, Precision: 0.8435, Recall: 0.8185, F1: 0.8276
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 5, 0, 5, 0, 5, 2, 0, 5, 3, 4, 0, 0, 1, 1, 0, 5, 0, 5, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7264, Accuracy: 0.7381, Precision: 0.7821, Recall: 0.5635, F1: 0.6154
Epoch 5/70
Train Loss: 0.1059, Accuracy: 0.9580, Precision: 0.9330, Recall: 0.9334, F1: 0.9332
Validation Loss: 0.7822, Accuracy: 0.8593, Precision: 0.8241, Recall: 0.8035, F1: 0.8094
Testing Loss: 0.7551, Accuracy: 0.8720, Precision: 0.8489, Recall: 0.8202, F1: 0.8315
LM Predictions:  [2, 2, 1, 0, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 0, 5, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 5, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5052, Accuracy: 0.8095, Precision: 0.7821, Recall: 0.6508, F1: 0.6917
Epoch 6/70
Train Loss: 0.1104, Accuracy: 0.9559, Precision: 0.9354, Recall: 0.9320, F1: 0.9336
Validation Loss: 0.7653, Accuracy: 0.8593, Precision: 0.8239, Recall: 0.8197, F1: 0.8188
Testing Loss: 0.5973, Accuracy: 0.8635, Precision: 0.8166, Recall: 0.7995, F1: 0.8038
LM Predictions:  [5, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 0, 4, 0, 3, 1, 1, 1, 1, 2, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4308, Accuracy: 0.8095, Precision: 0.7359, Recall: 0.6442, F1: 0.6751
Epoch 7/70
Train Loss: 0.1245, Accuracy: 0.9493, Precision: 0.9252, Recall: 0.9175, F1: 0.9212
Validation Loss: 0.9570, Accuracy: 0.8529, Precision: 0.8095, Recall: 0.8038, F1: 0.8019
Testing Loss: 0.7042, Accuracy: 0.8623, Precision: 0.8229, Recall: 0.8091, F1: 0.8133
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2899, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9238, F1: 0.9305
Epoch 8/70
Train Loss: 0.1230, Accuracy: 0.9542, Precision: 0.9324, Recall: 0.9256, F1: 0.9289
Validation Loss: 0.7487, Accuracy: 0.8657, Precision: 0.8352, Recall: 0.8411, F1: 0.8373
Testing Loss: 0.6123, Accuracy: 0.8696, Precision: 0.8369, Recall: 0.8206, F1: 0.8279
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5165, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8000, F1: 0.8239
Epoch 9/70
Train Loss: 0.0770, Accuracy: 0.9666, Precision: 0.9477, Recall: 0.9423, F1: 0.9449
Validation Loss: 0.9496, Accuracy: 0.8593, Precision: 0.8244, Recall: 0.8338, F1: 0.8284
Testing Loss: 0.7229, Accuracy: 0.8768, Precision: 0.8457, Recall: 0.8387, F1: 0.8412
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 5, 0, 5, 3, 3, 2, 0, 2, 3, 4, 0, 0, 5, 1, 0, 5, 5, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4237, Accuracy: 0.8333, Precision: 0.8030, Recall: 0.7024, F1: 0.7300
Epoch 10/70
Train Loss: 0.0664, Accuracy: 0.9644, Precision: 0.9399, Recall: 0.9409, F1: 0.9404
Validation Loss: 1.0754, Accuracy: 0.8486, Precision: 0.8209, Recall: 0.8113, F1: 0.8138
Testing Loss: 1.0629, Accuracy: 0.8406, Precision: 0.8284, Recall: 0.7984, F1: 0.8085
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3164, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 11/70
Train Loss: 0.0774, Accuracy: 0.9623, Precision: 0.9392, Recall: 0.9400, F1: 0.9396
Validation Loss: 0.9635, Accuracy: 0.8614, Precision: 0.8258, Recall: 0.7978, F1: 0.8043
Testing Loss: 0.8142, Accuracy: 0.8696, Precision: 0.8360, Recall: 0.7876, F1: 0.8036
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2268, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8333, F1: 0.8517
Epoch 12/70
Train Loss: 0.0628, Accuracy: 0.9692, Precision: 0.9446, Recall: 0.9519, F1: 0.9481
Validation Loss: 1.0068, Accuracy: 0.8550, Precision: 0.8208, Recall: 0.7792, F1: 0.7920
Testing Loss: 0.9878, Accuracy: 0.8490, Precision: 0.8312, Recall: 0.7571, F1: 0.7793
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3818, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 13/70
Train Loss: 0.0631, Accuracy: 0.9704, Precision: 0.9508, Recall: 0.9475, F1: 0.9491
Validation Loss: 1.2709, Accuracy: 0.8550, Precision: 0.8222, Recall: 0.7957, F1: 0.8046
Testing Loss: 1.0543, Accuracy: 0.8708, Precision: 0.8482, Recall: 0.7969, F1: 0.8151
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4648, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 14/70
Train Loss: 0.0572, Accuracy: 0.9673, Precision: 0.9456, Recall: 0.9407, F1: 0.9431
Validation Loss: 1.2701, Accuracy: 0.8614, Precision: 0.8302, Recall: 0.8293, F1: 0.8292
Testing Loss: 1.0419, Accuracy: 0.8696, Precision: 0.8425, Recall: 0.8146, F1: 0.8271
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2560, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8857, F1: 0.8866
Epoch 15/70
Train Loss: 0.0543, Accuracy: 0.9699, Precision: 0.9469, Recall: 0.9539, F1: 0.9503
Validation Loss: 1.3533, Accuracy: 0.8507, Precision: 0.8028, Recall: 0.7784, F1: 0.7838
Testing Loss: 1.1281, Accuracy: 0.8659, Precision: 0.8401, Recall: 0.7788, F1: 0.7950
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2508, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 16/70
Train Loss: 0.0506, Accuracy: 0.9689, Precision: 0.9475, Recall: 0.9466, F1: 0.9471
Validation Loss: 1.1647, Accuracy: 0.8593, Precision: 0.8249, Recall: 0.8276, F1: 0.8257
Testing Loss: 1.0198, Accuracy: 0.8635, Precision: 0.8457, Recall: 0.8184, F1: 0.8298
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3750, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 17/70
Train Loss: 0.0493, Accuracy: 0.9734, Precision: 0.9552, Recall: 0.9599, F1: 0.9575
Validation Loss: 1.2904, Accuracy: 0.8550, Precision: 0.8159, Recall: 0.8280, F1: 0.8215
Testing Loss: 1.0445, Accuracy: 0.8684, Precision: 0.8402, Recall: 0.8189, F1: 0.8285
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0855, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 18/70
Train Loss: 0.0501, Accuracy: 0.9713, Precision: 0.9522, Recall: 0.9549, F1: 0.9535
Validation Loss: 1.2635, Accuracy: 0.8550, Precision: 0.8137, Recall: 0.7913, F1: 0.7958
Testing Loss: 1.0832, Accuracy: 0.8732, Precision: 0.8525, Recall: 0.8002, F1: 0.8159
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1824, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 19/70
Train Loss: 0.0509, Accuracy: 0.9680, Precision: 0.9477, Recall: 0.9427, F1: 0.9451
Validation Loss: 1.4146, Accuracy: 0.8550, Precision: 0.8206, Recall: 0.8045, F1: 0.8082
Testing Loss: 1.1040, Accuracy: 0.8696, Precision: 0.8391, Recall: 0.8056, F1: 0.8180
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1209, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 20/70
Train Loss: 0.0493, Accuracy: 0.9677, Precision: 0.9477, Recall: 0.9420, F1: 0.9447
Validation Loss: 1.2243, Accuracy: 0.8529, Precision: 0.8098, Recall: 0.7782, F1: 0.7857
Testing Loss: 1.0299, Accuracy: 0.8671, Precision: 0.8420, Recall: 0.7819, F1: 0.7982
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1186, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9524, F1: 0.9544
Epoch 21/70
Train Loss: 0.0810, Accuracy: 0.9597, Precision: 0.9344, Recall: 0.9284, F1: 0.9313
Validation Loss: 0.8622, Accuracy: 0.8422, Precision: 0.8172, Recall: 0.8215, F1: 0.8154
Testing Loss: 0.6998, Accuracy: 0.8623, Precision: 0.8299, Recall: 0.8226, F1: 0.8240
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 5, 4, 3, 5, 2, 0, 2, 3, 0, 5, 0, 1, 1, 0, 4, 1, 4, 2, 3, 1, 1, 1, 1, 0, 2, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5038, Accuracy: 0.8571, Precision: 0.7679, Recall: 0.7143, F1: 0.7337
Epoch 22/70
Train Loss: 0.2013, Accuracy: 0.9350, Precision: 0.9088, Recall: 0.8987, F1: 0.9035
Validation Loss: 0.7879, Accuracy: 0.8401, Precision: 0.8007, Recall: 0.7675, F1: 0.7709
Testing Loss: 0.7677, Accuracy: 0.8442, Precision: 0.8029, Recall: 0.7628, F1: 0.7681
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2301, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8429, F1: 0.8526
Epoch 23/70
Train Loss: 0.0799, Accuracy: 0.9632, Precision: 0.9423, Recall: 0.9393, F1: 0.9408
Validation Loss: 0.9681, Accuracy: 0.8571, Precision: 0.8303, Recall: 0.8232, F1: 0.8260
Testing Loss: 0.8843, Accuracy: 0.8514, Precision: 0.8091, Recall: 0.7965, F1: 0.8018
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1987, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 24/70
Train Loss: 0.0585, Accuracy: 0.9701, Precision: 0.9509, Recall: 0.9478, F1: 0.9493
Validation Loss: 0.9064, Accuracy: 0.8593, Precision: 0.8324, Recall: 0.8047, F1: 0.8119
Testing Loss: 0.8072, Accuracy: 0.8551, Precision: 0.8109, Recall: 0.7759, F1: 0.7860
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1738, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9095, F1: 0.9147
Epoch 25/70
Train Loss: 0.0483, Accuracy: 0.9696, Precision: 0.9496, Recall: 0.9471, F1: 0.9483
Validation Loss: 1.1512, Accuracy: 0.8486, Precision: 0.8229, Recall: 0.7888, F1: 0.7999
Testing Loss: 1.0837, Accuracy: 0.8527, Precision: 0.8239, Recall: 0.7720, F1: 0.7900
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3176, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 26/70
Train Loss: 0.0473, Accuracy: 0.9692, Precision: 0.9521, Recall: 0.9425, F1: 0.9470
Validation Loss: 1.1263, Accuracy: 0.8593, Precision: 0.8386, Recall: 0.8011, F1: 0.8139
Testing Loss: 1.0508, Accuracy: 0.8551, Precision: 0.8210, Recall: 0.7741, F1: 0.7912
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3171, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 27/70
Train Loss: 0.0475, Accuracy: 0.9699, Precision: 0.9499, Recall: 0.9511, F1: 0.9505
Validation Loss: 1.0796, Accuracy: 0.8550, Precision: 0.8172, Recall: 0.8201, F1: 0.8182
Testing Loss: 0.9931, Accuracy: 0.8599, Precision: 0.8267, Recall: 0.8106, F1: 0.8172
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1897, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 28/70
Train Loss: 0.0467, Accuracy: 0.9725, Precision: 0.9550, Recall: 0.9534, F1: 0.9542
Validation Loss: 1.2484, Accuracy: 0.8614, Precision: 0.8258, Recall: 0.8216, F1: 0.8230
Testing Loss: 1.1502, Accuracy: 0.8587, Precision: 0.8284, Recall: 0.8099, F1: 0.8177
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1428, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9095, F1: 0.9199
Epoch 29/70
Train Loss: 0.0480, Accuracy: 0.9696, Precision: 0.9469, Recall: 0.9506, F1: 0.9487
Validation Loss: 1.2661, Accuracy: 0.8422, Precision: 0.7917, Recall: 0.7607, F1: 0.7619
Testing Loss: 1.1486, Accuracy: 0.8551, Precision: 0.8114, Recall: 0.7739, F1: 0.7799
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0968, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 30/70
Train Loss: 0.0484, Accuracy: 0.9711, Precision: 0.9509, Recall: 0.9515, F1: 0.9512
Validation Loss: 1.3027, Accuracy: 0.8507, Precision: 0.8198, Recall: 0.7783, F1: 0.7876
Testing Loss: 1.2549, Accuracy: 0.8623, Precision: 0.8211, Recall: 0.7784, F1: 0.7901
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2344, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.7810, F1: 0.8046
Epoch 31/70
Train Loss: 0.0490, Accuracy: 0.9715, Precision: 0.9519, Recall: 0.9526, F1: 0.9522
Validation Loss: 1.3943, Accuracy: 0.8486, Precision: 0.8119, Recall: 0.7739, F1: 0.7802
Testing Loss: 1.2894, Accuracy: 0.8551, Precision: 0.8230, Recall: 0.7727, F1: 0.7839
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1998, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8857, F1: 0.8866
Epoch 32/70
Train Loss: 0.0553, Accuracy: 0.9694, Precision: 0.9489, Recall: 0.9498, F1: 0.9493
Validation Loss: 1.4589, Accuracy: 0.8252, Precision: 0.7374, Recall: 0.7226, F1: 0.7222
Testing Loss: 1.4217, Accuracy: 0.8345, Precision: 0.7926, Recall: 0.7334, F1: 0.7449
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 0, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2172, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8571, F1: 0.8593
Epoch 33/70
Train Loss: 0.1054, Accuracy: 0.9559, Precision: 0.9353, Recall: 0.9358, F1: 0.9355
Validation Loss: 1.1677, Accuracy: 0.8401, Precision: 0.8294, Recall: 0.7781, F1: 0.7967
Testing Loss: 1.2396, Accuracy: 0.8152, Precision: 0.7858, Recall: 0.7264, F1: 0.7430
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 0, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.6256, Accuracy: 0.7857, Precision: 0.8916, Recall: 0.7333, F1: 0.7630
Epoch 34/70
Train Loss: 0.1078, Accuracy: 0.9559, Precision: 0.9323, Recall: 0.9318, F1: 0.9320
Validation Loss: 0.9124, Accuracy: 0.8465, Precision: 0.8056, Recall: 0.7999, F1: 0.7991
Testing Loss: 0.9770, Accuracy: 0.8333, Precision: 0.7825, Recall: 0.7680, F1: 0.7697
LM Predictions:  [2, 2, 1, 1, 0, 5, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1681, Accuracy: 0.9286, Precision: 0.8030, Recall: 0.7659, F1: 0.7802
Epoch 35/70
Train Loss: 0.0653, Accuracy: 0.9666, Precision: 0.9446, Recall: 0.9493, F1: 0.9469
Validation Loss: 1.0281, Accuracy: 0.8294, Precision: 0.7871, Recall: 0.7475, F1: 0.7562
Testing Loss: 1.0620, Accuracy: 0.8321, Precision: 0.7917, Recall: 0.7471, F1: 0.7623
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1819, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9000, F1: 0.9017
Epoch 36/70
Train Loss: 0.0468, Accuracy: 0.9725, Precision: 0.9515, Recall: 0.9549, F1: 0.9532
Validation Loss: 1.1218, Accuracy: 0.8443, Precision: 0.7975, Recall: 0.7752, F1: 0.7793
Testing Loss: 1.2358, Accuracy: 0.8418, Precision: 0.8056, Recall: 0.7632, F1: 0.7741
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2722, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 37/70
Train Loss: 0.0439, Accuracy: 0.9723, Precision: 0.9549, Recall: 0.9518, F1: 0.9533
Validation Loss: 1.1671, Accuracy: 0.8465, Precision: 0.7922, Recall: 0.7714, F1: 0.7735
Testing Loss: 1.2335, Accuracy: 0.8454, Precision: 0.8017, Recall: 0.7634, F1: 0.7720
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2244, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 38/70
Train Loss: 0.0463, Accuracy: 0.9715, Precision: 0.9518, Recall: 0.9516, F1: 0.9517
Validation Loss: 1.2216, Accuracy: 0.8507, Precision: 0.7947, Recall: 0.7668, F1: 0.7698
Testing Loss: 1.3015, Accuracy: 0.8454, Precision: 0.8075, Recall: 0.7624, F1: 0.7742
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1377, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8429, F1: 0.8565
Epoch 39/70
Train Loss: 0.0462, Accuracy: 0.9701, Precision: 0.9531, Recall: 0.9416, F1: 0.9469
Validation Loss: 1.1623, Accuracy: 0.8465, Precision: 0.8030, Recall: 0.7706, F1: 0.7769
Testing Loss: 1.2986, Accuracy: 0.8418, Precision: 0.8049, Recall: 0.7581, F1: 0.7711
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3087, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 40/70
Train Loss: 0.0451, Accuracy: 0.9689, Precision: 0.9493, Recall: 0.9481, F1: 0.9487
Validation Loss: 1.1901, Accuracy: 0.8657, Precision: 0.8301, Recall: 0.8292, F1: 0.8295
Testing Loss: 1.2639, Accuracy: 0.8502, Precision: 0.8184, Recall: 0.7981, F1: 0.8069
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1541, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9429, F1: 0.9407
Epoch 41/70
Train Loss: 0.0488, Accuracy: 0.9701, Precision: 0.9504, Recall: 0.9506, F1: 0.9505
Validation Loss: 1.1508, Accuracy: 0.8614, Precision: 0.8216, Recall: 0.8289, F1: 0.8245
Testing Loss: 1.2145, Accuracy: 0.8478, Precision: 0.8151, Recall: 0.7986, F1: 0.8060
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2043, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 42/70
Train Loss: 0.0450, Accuracy: 0.9725, Precision: 0.9534, Recall: 0.9534, F1: 0.9534
Validation Loss: 1.2864, Accuracy: 0.8507, Precision: 0.7978, Recall: 0.7782, F1: 0.7802
Testing Loss: 1.3704, Accuracy: 0.8454, Precision: 0.8054, Recall: 0.7660, F1: 0.7758
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0783, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 43/70
Train Loss: 0.0470, Accuracy: 0.9708, Precision: 0.9505, Recall: 0.9509, F1: 0.9507
Validation Loss: 1.1899, Accuracy: 0.8678, Precision: 0.8405, Recall: 0.8230, F1: 0.8303
Testing Loss: 1.3049, Accuracy: 0.8454, Precision: 0.8125, Recall: 0.7839, F1: 0.7954
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2381, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 44/70
Train Loss: 0.0456, Accuracy: 0.9706, Precision: 0.9527, Recall: 0.9525, F1: 0.9525
Validation Loss: 1.2173, Accuracy: 0.8486, Precision: 0.7939, Recall: 0.7711, F1: 0.7740
Testing Loss: 1.3124, Accuracy: 0.8418, Precision: 0.8019, Recall: 0.7576, F1: 0.7672
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1891, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8143, F1: 0.8371
Epoch 45/70
Train Loss: 0.0457, Accuracy: 0.9708, Precision: 0.9530, Recall: 0.9526, F1: 0.9528
Validation Loss: 1.1896, Accuracy: 0.8699, Precision: 0.8350, Recall: 0.8361, F1: 0.8353
Testing Loss: 1.2899, Accuracy: 0.8466, Precision: 0.8132, Recall: 0.7915, F1: 0.8004
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1957, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 46/70
Train Loss: 0.0440, Accuracy: 0.9673, Precision: 0.9469, Recall: 0.9449, F1: 0.9459
Validation Loss: 1.3351, Accuracy: 0.8571, Precision: 0.8151, Recall: 0.7915, F1: 0.7971
Testing Loss: 1.4028, Accuracy: 0.8430, Precision: 0.7965, Recall: 0.7611, F1: 0.7683
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1165, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 47/70
Train Loss: 0.0457, Accuracy: 0.9692, Precision: 0.9490, Recall: 0.9457, F1: 0.9473
Validation Loss: 1.2782, Accuracy: 0.8550, Precision: 0.8163, Recall: 0.7885, F1: 0.7962
Testing Loss: 1.3391, Accuracy: 0.8454, Precision: 0.8082, Recall: 0.7626, F1: 0.7735
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1758, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8667, F1: 0.8764
Epoch 48/70
Train Loss: 0.0514, Accuracy: 0.9696, Precision: 0.9475, Recall: 0.9482, F1: 0.9478
Validation Loss: 1.1294, Accuracy: 0.8507, Precision: 0.8095, Recall: 0.8220, F1: 0.8144
Testing Loss: 1.1275, Accuracy: 0.8418, Precision: 0.7970, Recall: 0.7978, F1: 0.7968
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1589, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 49/70
Train Loss: 0.1395, Accuracy: 0.9507, Precision: 0.9294, Recall: 0.9226, F1: 0.9258
Validation Loss: 0.7424, Accuracy: 0.8145, Precision: 0.7675, Recall: 0.8023, F1: 0.7765
Testing Loss: 0.7034, Accuracy: 0.8273, Precision: 0.7853, Recall: 0.7971, F1: 0.7843
LM Predictions:  [1, 2, 5, 1, 0, 1, 4, 1, 1, 5, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 5, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 0, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4029, Accuracy: 0.7857, Precision: 0.7511, Recall: 0.6336, F1: 0.6608
Epoch 50/70
Train Loss: 0.0985, Accuracy: 0.9590, Precision: 0.9320, Recall: 0.9334, F1: 0.9327
Validation Loss: 1.1018, Accuracy: 0.8209, Precision: 0.8006, Recall: 0.7455, F1: 0.7622
Testing Loss: 0.9460, Accuracy: 0.8502, Precision: 0.8065, Recall: 0.7562, F1: 0.7685
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1494, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 51/70
Train Loss: 0.0644, Accuracy: 0.9675, Precision: 0.9451, Recall: 0.9459, F1: 0.9455
Validation Loss: 0.9162, Accuracy: 0.8486, Precision: 0.8163, Recall: 0.7970, F1: 0.8052
Testing Loss: 0.9262, Accuracy: 0.8430, Precision: 0.8210, Recall: 0.7717, F1: 0.7903
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3346, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 52/70
Train Loss: 0.0526, Accuracy: 0.9701, Precision: 0.9510, Recall: 0.9516, F1: 0.9513
Validation Loss: 1.2393, Accuracy: 0.8252, Precision: 0.8026, Recall: 0.7509, F1: 0.7684
Testing Loss: 1.1215, Accuracy: 0.8599, Precision: 0.8177, Recall: 0.7809, F1: 0.7942
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1195, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 53/70
Train Loss: 0.0488, Accuracy: 0.9715, Precision: 0.9540, Recall: 0.9506, F1: 0.9523
Validation Loss: 1.1456, Accuracy: 0.8380, Precision: 0.8120, Recall: 0.7760, F1: 0.7896
Testing Loss: 1.0158, Accuracy: 0.8599, Precision: 0.8211, Recall: 0.7787, F1: 0.7943
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1858, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 54/70
Train Loss: 0.0427, Accuracy: 0.9685, Precision: 0.9494, Recall: 0.9437, F1: 0.9465
Validation Loss: 1.2883, Accuracy: 0.8443, Precision: 0.8186, Recall: 0.7952, F1: 0.8041
Testing Loss: 1.1776, Accuracy: 0.8563, Precision: 0.8144, Recall: 0.7882, F1: 0.7994
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1281, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 55/70
Train Loss: 0.0429, Accuracy: 0.9732, Precision: 0.9573, Recall: 0.9519, F1: 0.9545
Validation Loss: 1.2722, Accuracy: 0.8465, Precision: 0.8187, Recall: 0.7963, F1: 0.8055
Testing Loss: 1.1951, Accuracy: 0.8599, Precision: 0.8181, Recall: 0.7860, F1: 0.7992
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2465, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 56/70
Train Loss: 0.0438, Accuracy: 0.9718, Precision: 0.9511, Recall: 0.9554, F1: 0.9531
Validation Loss: 1.3657, Accuracy: 0.8358, Precision: 0.8082, Recall: 0.7748, F1: 0.7871
Testing Loss: 1.2465, Accuracy: 0.8611, Precision: 0.8188, Recall: 0.7863, F1: 0.7973
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0877, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 57/70
Train Loss: 0.0439, Accuracy: 0.9699, Precision: 0.9491, Recall: 0.9529, F1: 0.9509
Validation Loss: 1.3294, Accuracy: 0.8422, Precision: 0.8139, Recall: 0.7834, F1: 0.7950
Testing Loss: 1.2220, Accuracy: 0.8575, Precision: 0.8106, Recall: 0.7779, F1: 0.7896
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1109, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 58/70
Train Loss: 0.0431, Accuracy: 0.9718, Precision: 0.9546, Recall: 0.9512, F1: 0.9529
Validation Loss: 1.2887, Accuracy: 0.8401, Precision: 0.8127, Recall: 0.7801, F1: 0.7929
Testing Loss: 1.2109, Accuracy: 0.8635, Precision: 0.8278, Recall: 0.7875, F1: 0.8007
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2280, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 59/70
Train Loss: 0.0423, Accuracy: 0.9725, Precision: 0.9517, Recall: 0.9576, F1: 0.9545
Validation Loss: 1.3110, Accuracy: 0.8507, Precision: 0.8208, Recall: 0.8034, F1: 0.8106
Testing Loss: 1.2415, Accuracy: 0.8599, Precision: 0.8192, Recall: 0.7978, F1: 0.8075
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1863, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 60/70
Train Loss: 0.0433, Accuracy: 0.9723, Precision: 0.9555, Recall: 0.9534, F1: 0.9544
Validation Loss: 1.2988, Accuracy: 0.8443, Precision: 0.8171, Recall: 0.7933, F1: 0.8031
Testing Loss: 1.2111, Accuracy: 0.8575, Precision: 0.8104, Recall: 0.7924, F1: 0.8001
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1254, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 61/70
Train Loss: 0.0435, Accuracy: 0.9715, Precision: 0.9528, Recall: 0.9537, F1: 0.9532
Validation Loss: 1.3272, Accuracy: 0.8422, Precision: 0.8100, Recall: 0.7964, F1: 0.8018
Testing Loss: 1.2436, Accuracy: 0.8599, Precision: 0.8164, Recall: 0.8006, F1: 0.8079
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1241, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 62/70
Train Loss: 0.0420, Accuracy: 0.9723, Precision: 0.9524, Recall: 0.9544, F1: 0.9534
Validation Loss: 1.4628, Accuracy: 0.8401, Precision: 0.8111, Recall: 0.7850, F1: 0.7949
Testing Loss: 1.3431, Accuracy: 0.8539, Precision: 0.8114, Recall: 0.7857, F1: 0.7968
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1968, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 63/70
Train Loss: 0.0450, Accuracy: 0.9694, Precision: 0.9485, Recall: 0.9507, F1: 0.9496
Validation Loss: 1.2076, Accuracy: 0.8507, Precision: 0.8192, Recall: 0.8095, F1: 0.8136
Testing Loss: 1.1674, Accuracy: 0.8611, Precision: 0.8146, Recall: 0.7954, F1: 0.8039
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1800, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8714, F1: 0.8745
Epoch 64/70
Train Loss: 0.0425, Accuracy: 0.9727, Precision: 0.9520, Recall: 0.9607, F1: 0.9562
Validation Loss: 1.3755, Accuracy: 0.8358, Precision: 0.8079, Recall: 0.7537, F1: 0.7685
Testing Loss: 1.3601, Accuracy: 0.8587, Precision: 0.8145, Recall: 0.7650, F1: 0.7795
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3749, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 65/70
Train Loss: 0.0426, Accuracy: 0.9730, Precision: 0.9547, Recall: 0.9526, F1: 0.9536
Validation Loss: 1.4197, Accuracy: 0.8422, Precision: 0.8213, Recall: 0.7683, F1: 0.7848
Testing Loss: 1.3416, Accuracy: 0.8563, Precision: 0.8170, Recall: 0.7683, F1: 0.7818
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2022, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 66/70
Train Loss: 0.0435, Accuracy: 0.9692, Precision: 0.9474, Recall: 0.9479, F1: 0.9476
Validation Loss: 1.4060, Accuracy: 0.8443, Precision: 0.8175, Recall: 0.7857, F1: 0.7975
Testing Loss: 1.3747, Accuracy: 0.8647, Precision: 0.8260, Recall: 0.7908, F1: 0.8033
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1673, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 67/70
Train Loss: 0.0456, Accuracy: 0.9718, Precision: 0.9569, Recall: 0.9476, F1: 0.9520
Validation Loss: 1.3750, Accuracy: 0.8252, Precision: 0.7876, Recall: 0.7691, F1: 0.7719
Testing Loss: 1.1870, Accuracy: 0.8611, Precision: 0.8116, Recall: 0.7993, F1: 0.8036
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0779, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0698, Accuracy: 0.9654, Precision: 0.9434, Recall: 0.9478, F1: 0.9455
Validation Loss: 1.0029, Accuracy: 0.7783, Precision: 0.7552, Recall: 0.6965, F1: 0.7169
Testing Loss: 0.7997, Accuracy: 0.8128, Precision: 0.7572, Recall: 0.7081, F1: 0.7233
LM Predictions:  [2, 2, 0, 1, 0, 5, 4, 0, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 0, 1, 5, 0, 2, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5266, Accuracy: 0.6667, Precision: 0.7139, Recall: 0.5675, F1: 0.5858
Epoch 69/70
Train Loss: 0.1196, Accuracy: 0.9576, Precision: 0.9378, Recall: 0.9259, F1: 0.9314
Validation Loss: 0.9508, Accuracy: 0.8166, Precision: 0.7976, Recall: 0.7520, F1: 0.7657
Testing Loss: 0.8528, Accuracy: 0.8370, Precision: 0.7998, Recall: 0.7595, F1: 0.7709
LM Predictions:  [3, 2, 4, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 5, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 0, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4873, Accuracy: 0.8571, Precision: 0.7440, Recall: 0.7183, F1: 0.7196
Epoch 70/70
Train Loss: 0.0693, Accuracy: 0.9689, Precision: 0.9541, Recall: 0.9446, F1: 0.9491
Validation Loss: 0.9986, Accuracy: 0.8102, Precision: 0.7630, Recall: 0.7722, F1: 0.7654
Testing Loss: 0.8729, Accuracy: 0.8442, Precision: 0.7877, Recall: 0.7899, F1: 0.7871
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1083, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
For later layers:  [16, 17, 18, 19, 20, 21, 22, 23]
Layer: backbone.model.embed_tokens.weight, Size: torch.Size([151936, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.0.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.0.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.0.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.0.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.0.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.1.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.1.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.1.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.1.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.1.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.2.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.2.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.2.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.2.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.2.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.3.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.3.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.3.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.3.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.3.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.4.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.4.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.4.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.4.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.4.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.5.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.5.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.5.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.5.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.5.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.6.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.6.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.6.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.6.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.6.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.7.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.7.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.7.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.7.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.7.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.8.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.8.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.8.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.8.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.8.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.9.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.9.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.9.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.9.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.9.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.10.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.10.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.10.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.10.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.10.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.11.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.11.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.11.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.11.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.11.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.12.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.12.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.12.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.12.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.12.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.13.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.13.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.13.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.13.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.13.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.14.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.14.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.14.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.14.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.14.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.15.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.15.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.15.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.15.input_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.15.post_attention_layernorm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.16.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.16.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.16.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.17.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.17.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.17.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.18.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.18.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.18.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.19.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.19.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.19.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.20.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.20.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.20.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.21.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.21.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.21.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.22.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.22.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.22.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.q_proj.bias, Size: torch.Size([896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.k_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.weight, Size: torch.Size([128, 896]), req grad: True
Layer: backbone.model.layers.23.self_attn.v_proj.bias, Size: torch.Size([128]), req grad: True
Layer: backbone.model.layers.23.self_attn.o_proj.weight, Size: torch.Size([896, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.gate_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.up_proj.weight, Size: torch.Size([4864, 896]), req grad: True
Layer: backbone.model.layers.23.mlp.down_proj.weight, Size: torch.Size([896, 4864]), req grad: True
Layer: backbone.model.norm.weight, Size: torch.Size([896]), req grad: True
Layer: backbone.score.weight, Size: torch.Size([6, 896]), req grad: True
Epoch 1/70
Train Loss: 0.7734, Accuracy: 0.8191, Precision: 0.7508, Recall: 0.7450, F1: 0.7476
Validation Loss: 0.5455, Accuracy: 0.8188, Precision: 0.8191, Recall: 0.8155, F1: 0.7915
Testing Loss: 0.5390, Accuracy: 0.8406, Precision: 0.8382, Recall: 0.8201, F1: 0.8021
LM Predictions:  [5, 5, 5, 5, 0, 5, 5, 5, 5, 0, 0, 5, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 3.4481, Accuracy: 0.0714, Precision: 0.2500, Recall: 0.0489, F1: 0.0735
Epoch 2/70
Train Loss: 0.2356, Accuracy: 0.9229, Precision: 0.8862, Recall: 0.8779, F1: 0.8819
Validation Loss: 0.4509, Accuracy: 0.8785, Precision: 0.8442, Recall: 0.8609, F1: 0.8496
Testing Loss: 0.4560, Accuracy: 0.8732, Precision: 0.8389, Recall: 0.8443, F1: 0.8396
LM Predictions:  [2, 5, 0, 5, 0, 1, 4, 1, 1, 5, 5, 5, 3, 1, 4, 0, 5, 5, 0, 2, 5, 1, 5, 4, 0, 0, 1, 1, 0, 5, 1, 5, 0, 5, 5, 1, 1, 1, 5, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 1.0511, Accuracy: 0.5714, Precision: 0.7778, Recall: 0.4246, F1: 0.5253
Epoch 3/70
Train Loss: 0.1670, Accuracy: 0.9512, Precision: 0.9202, Recall: 0.9209, F1: 0.9205
Validation Loss: 0.6505, Accuracy: 0.8465, Precision: 0.8182, Recall: 0.7663, F1: 0.7825
Testing Loss: 0.5964, Accuracy: 0.8623, Precision: 0.8376, Recall: 0.7760, F1: 0.7984
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 0, 3, 0, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5532, Accuracy: 0.8095, Precision: 0.7556, Recall: 0.6362, F1: 0.6626
Epoch 4/70
Train Loss: 0.1230, Accuracy: 0.9592, Precision: 0.9315, Recall: 0.9322, F1: 0.9318
Validation Loss: 0.5609, Accuracy: 0.8742, Precision: 0.8419, Recall: 0.8316, F1: 0.8363
Testing Loss: 0.4948, Accuracy: 0.8768, Precision: 0.8484, Recall: 0.8315, F1: 0.8388
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 1, 0, 3, 1, 5, 0, 5, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 5, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.7467, Accuracy: 0.7619, Precision: 0.7692, Recall: 0.6032, F1: 0.6471
Epoch 5/70
Train Loss: 0.1076, Accuracy: 0.9623, Precision: 0.9356, Recall: 0.9384, F1: 0.9369
Validation Loss: 0.6331, Accuracy: 0.8721, Precision: 0.8419, Recall: 0.8500, F1: 0.8446
Testing Loss: 0.4530, Accuracy: 0.8768, Precision: 0.8474, Recall: 0.8341, F1: 0.8401
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 5, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 5, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.5514, Accuracy: 0.8095, Precision: 0.7738, Recall: 0.6548, F1: 0.6903
Epoch 6/70
Train Loss: 0.0978, Accuracy: 0.9599, Precision: 0.9357, Recall: 0.9345, F1: 0.9351
Validation Loss: 0.7392, Accuracy: 0.8507, Precision: 0.8375, Recall: 0.8088, F1: 0.8204
Testing Loss: 0.5890, Accuracy: 0.8623, Precision: 0.8321, Recall: 0.8035, F1: 0.8155
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4451, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 7/70
Train Loss: 0.1012, Accuracy: 0.9628, Precision: 0.9403, Recall: 0.9418, F1: 0.9410
Validation Loss: 0.8626, Accuracy: 0.8635, Precision: 0.8295, Recall: 0.8014, F1: 0.8107
Testing Loss: 0.8407, Accuracy: 0.8635, Precision: 0.8239, Recall: 0.7805, F1: 0.7920
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4797, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8143, F1: 0.8371
Epoch 8/70
Train Loss: 0.0893, Accuracy: 0.9630, Precision: 0.9401, Recall: 0.9409, F1: 0.9405
Validation Loss: 0.7951, Accuracy: 0.8657, Precision: 0.8397, Recall: 0.8277, F1: 0.8329
Testing Loss: 0.7370, Accuracy: 0.8575, Precision: 0.8181, Recall: 0.8033, F1: 0.8099
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 5, 0, 4, 0, 5, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3115, Accuracy: 0.8333, Precision: 0.7738, Recall: 0.6786, F1: 0.7009
Epoch 9/70
Train Loss: 0.0871, Accuracy: 0.9630, Precision: 0.9388, Recall: 0.9393, F1: 0.9390
Validation Loss: 0.8504, Accuracy: 0.8443, Precision: 0.7648, Recall: 0.7494, F1: 0.7449
Testing Loss: 0.7416, Accuracy: 0.8659, Precision: 0.8204, Recall: 0.7746, F1: 0.7813
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3098, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7810, F1: 0.8049
Epoch 10/70
Train Loss: 0.0899, Accuracy: 0.9623, Precision: 0.9416, Recall: 0.9379, F1: 0.9397
Validation Loss: 1.0599, Accuracy: 0.8635, Precision: 0.8340, Recall: 0.8287, F1: 0.8290
Testing Loss: 0.8796, Accuracy: 0.8684, Precision: 0.8301, Recall: 0.8227, F1: 0.8258
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 5, 0, 4, 3, 0, 2, 5, 2, 3, 4, 0, 0, 1, 1, 0, 5, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3050, Accuracy: 0.8571, Precision: 0.7879, Recall: 0.6878, F1: 0.7242
Epoch 11/70
Train Loss: 0.1025, Accuracy: 0.9594, Precision: 0.9414, Recall: 0.9375, F1: 0.9394
Validation Loss: 0.7827, Accuracy: 0.8529, Precision: 0.8220, Recall: 0.8033, F1: 0.8094
Testing Loss: 0.7455, Accuracy: 0.8635, Precision: 0.8337, Recall: 0.8091, F1: 0.8196
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 5, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4711, Accuracy: 0.7857, Precision: 0.7549, Recall: 0.6111, F1: 0.6415
Epoch 12/70
Train Loss: 0.0968, Accuracy: 0.9564, Precision: 0.9337, Recall: 0.9309, F1: 0.9323
Validation Loss: 0.8813, Accuracy: 0.8443, Precision: 0.8203, Recall: 0.7942, F1: 0.8052
Testing Loss: 0.7977, Accuracy: 0.8527, Precision: 0.8182, Recall: 0.8045, F1: 0.8106
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 4, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3340, Accuracy: 0.8571, Precision: 0.8831, Recall: 0.8254, F1: 0.8388
Epoch 13/70
Train Loss: 0.1088, Accuracy: 0.9557, Precision: 0.9326, Recall: 0.9301, F1: 0.9313
Validation Loss: 0.8578, Accuracy: 0.8486, Precision: 0.8138, Recall: 0.7956, F1: 0.8025
Testing Loss: 0.7020, Accuracy: 0.8647, Precision: 0.8310, Recall: 0.8092, F1: 0.8190
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1789, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 14/70
Train Loss: 0.0797, Accuracy: 0.9625, Precision: 0.9379, Recall: 0.9368, F1: 0.9373
Validation Loss: 0.9644, Accuracy: 0.8529, Precision: 0.8304, Recall: 0.7948, F1: 0.8080
Testing Loss: 0.8899, Accuracy: 0.8647, Precision: 0.8393, Recall: 0.8041, F1: 0.8187
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2742, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8103
Epoch 15/70
Train Loss: 0.0603, Accuracy: 0.9685, Precision: 0.9446, Recall: 0.9473, F1: 0.9459
Validation Loss: 0.9843, Accuracy: 0.8550, Precision: 0.8198, Recall: 0.7853, F1: 0.7958
Testing Loss: 0.9064, Accuracy: 0.8647, Precision: 0.8347, Recall: 0.7859, F1: 0.8020
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3513, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 16/70
Train Loss: 0.0575, Accuracy: 0.9663, Precision: 0.9425, Recall: 0.9420, F1: 0.9423
Validation Loss: 1.0557, Accuracy: 0.8550, Precision: 0.8262, Recall: 0.7980, F1: 0.8097
Testing Loss: 0.9756, Accuracy: 0.8659, Precision: 0.8392, Recall: 0.8020, F1: 0.8179
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4586, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8377
Epoch 17/70
Train Loss: 0.0634, Accuracy: 0.9677, Precision: 0.9452, Recall: 0.9448, F1: 0.9450
Validation Loss: 0.8612, Accuracy: 0.8635, Precision: 0.8356, Recall: 0.8212, F1: 0.8274
Testing Loss: 0.8330, Accuracy: 0.8647, Precision: 0.8330, Recall: 0.8136, F1: 0.8223
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2176, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8429, F1: 0.8650
Epoch 18/70
Train Loss: 0.0534, Accuracy: 0.9704, Precision: 0.9509, Recall: 0.9514, F1: 0.9511
Validation Loss: 1.2102, Accuracy: 0.8443, Precision: 0.8050, Recall: 0.8095, F1: 0.8071
Testing Loss: 1.0831, Accuracy: 0.8647, Precision: 0.8261, Recall: 0.8185, F1: 0.8220
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0514, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 19/70
Train Loss: 0.0575, Accuracy: 0.9694, Precision: 0.9482, Recall: 0.9525, F1: 0.9503
Validation Loss: 1.0910, Accuracy: 0.8422, Precision: 0.8040, Recall: 0.7636, F1: 0.7753
Testing Loss: 1.0417, Accuracy: 0.8539, Precision: 0.8307, Recall: 0.7702, F1: 0.7887
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2929, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8143, F1: 0.8286
Epoch 20/70
Train Loss: 0.0544, Accuracy: 0.9701, Precision: 0.9479, Recall: 0.9515, F1: 0.9497
Validation Loss: 1.1339, Accuracy: 0.8486, Precision: 0.8075, Recall: 0.7808, F1: 0.7877
Testing Loss: 1.0745, Accuracy: 0.8696, Precision: 0.8462, Recall: 0.7959, F1: 0.8121
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1981, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 21/70
Train Loss: 0.0531, Accuracy: 0.9737, Precision: 0.9576, Recall: 0.9540, F1: 0.9557
Validation Loss: 1.1169, Accuracy: 0.8614, Precision: 0.8306, Recall: 0.7992, F1: 0.8111
Testing Loss: 1.0562, Accuracy: 0.8671, Precision: 0.8408, Recall: 0.7975, F1: 0.8144
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3028, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 22/70
Train Loss: 0.0565, Accuracy: 0.9673, Precision: 0.9461, Recall: 0.9468, F1: 0.9465
Validation Loss: 1.0553, Accuracy: 0.8593, Precision: 0.8296, Recall: 0.8109, F1: 0.8190
Testing Loss: 1.0015, Accuracy: 0.8635, Precision: 0.8340, Recall: 0.8121, F1: 0.8220
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2991, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 23/70
Train Loss: 0.0517, Accuracy: 0.9694, Precision: 0.9491, Recall: 0.9474, F1: 0.9483
Validation Loss: 1.2298, Accuracy: 0.8529, Precision: 0.8228, Recall: 0.7772, F1: 0.7925
Testing Loss: 1.2008, Accuracy: 0.8587, Precision: 0.8471, Recall: 0.7740, F1: 0.7972
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4811, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 24/70
Train Loss: 0.0518, Accuracy: 0.9706, Precision: 0.9534, Recall: 0.9518, F1: 0.9526
Validation Loss: 1.0808, Accuracy: 0.8571, Precision: 0.8370, Recall: 0.8084, F1: 0.8208
Testing Loss: 1.0130, Accuracy: 0.8527, Precision: 0.8302, Recall: 0.7946, F1: 0.8097
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3352, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 25/70
Train Loss: 0.0518, Accuracy: 0.9730, Precision: 0.9548, Recall: 0.9563, F1: 0.9556
Validation Loss: 1.2552, Accuracy: 0.8571, Precision: 0.8259, Recall: 0.8244, F1: 0.8246
Testing Loss: 1.1815, Accuracy: 0.8587, Precision: 0.8315, Recall: 0.8080, F1: 0.8182
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4703, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 26/70
Train Loss: 0.0481, Accuracy: 0.9706, Precision: 0.9515, Recall: 0.9501, F1: 0.9508
Validation Loss: 1.5776, Accuracy: 0.8401, Precision: 0.7928, Recall: 0.7623, F1: 0.7696
Testing Loss: 1.4932, Accuracy: 0.8647, Precision: 0.8493, Recall: 0.7837, F1: 0.8032
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1149, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9714, F1: 0.9741
Epoch 27/70
Train Loss: 0.0508, Accuracy: 0.9723, Precision: 0.9549, Recall: 0.9548, F1: 0.9548
Validation Loss: 1.4081, Accuracy: 0.8550, Precision: 0.8208, Recall: 0.7955, F1: 0.8054
Testing Loss: 1.3050, Accuracy: 0.8708, Precision: 0.8523, Recall: 0.8027, F1: 0.8215
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1804, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9429, F1: 0.9407
Epoch 28/70
Train Loss: 0.0526, Accuracy: 0.9711, Precision: 0.9503, Recall: 0.9546, F1: 0.9524
Validation Loss: 1.4888, Accuracy: 0.8614, Precision: 0.8300, Recall: 0.8197, F1: 0.8244
Testing Loss: 1.4422, Accuracy: 0.8671, Precision: 0.8401, Recall: 0.8169, F1: 0.8274
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2465, Accuracy: 0.8333, Precision: 0.9125, Recall: 0.8190, F1: 0.8377
Epoch 29/70
Train Loss: 0.0478, Accuracy: 0.9723, Precision: 0.9522, Recall: 0.9571, F1: 0.9546
Validation Loss: 1.3755, Accuracy: 0.8614, Precision: 0.8355, Recall: 0.8182, F1: 0.8262
Testing Loss: 1.3577, Accuracy: 0.8587, Precision: 0.8317, Recall: 0.8037, F1: 0.8159
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2355, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8429, F1: 0.8526
Epoch 30/70
Train Loss: 0.0487, Accuracy: 0.9699, Precision: 0.9500, Recall: 0.9490, F1: 0.9495
Validation Loss: 1.5369, Accuracy: 0.8657, Precision: 0.8351, Recall: 0.8214, F1: 0.8277
Testing Loss: 1.5092, Accuracy: 0.8708, Precision: 0.8455, Recall: 0.8203, F1: 0.8317
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1827, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8667, F1: 0.8788
Epoch 31/70
Train Loss: 0.0479, Accuracy: 0.9730, Precision: 0.9522, Recall: 0.9595, F1: 0.9556
Validation Loss: 1.7017, Accuracy: 0.8529, Precision: 0.8180, Recall: 0.7913, F1: 0.8016
Testing Loss: 1.6506, Accuracy: 0.8659, Precision: 0.8414, Recall: 0.8013, F1: 0.8180
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0744, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 32/70
Train Loss: 0.0493, Accuracy: 0.9739, Precision: 0.9588, Recall: 0.9542, F1: 0.9565
Validation Loss: 2.4649, Accuracy: 0.8614, Precision: 0.8307, Recall: 0.8261, F1: 0.8279
Testing Loss: 2.1334, Accuracy: 0.8671, Precision: 0.8344, Recall: 0.8184, F1: 0.8258
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1323, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9333, F1: 0.9400
Epoch 33/70
Train Loss: 0.2015, Accuracy: 0.9350, Precision: 0.9085, Recall: 0.9036, F1: 0.9059
Validation Loss: 0.9502, Accuracy: 0.8401, Precision: 0.8065, Recall: 0.8117, F1: 0.8087
Testing Loss: 0.7616, Accuracy: 0.8575, Precision: 0.8175, Recall: 0.8086, F1: 0.8126
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 1, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3393, Accuracy: 0.8810, Precision: 0.9251, Recall: 0.8524, F1: 0.8586
Epoch 34/70
Train Loss: 0.1029, Accuracy: 0.9602, Precision: 0.9402, Recall: 0.9371, F1: 0.9386
Validation Loss: 0.9322, Accuracy: 0.8316, Precision: 0.8095, Recall: 0.7530, F1: 0.7731
Testing Loss: 0.8213, Accuracy: 0.8514, Precision: 0.8230, Recall: 0.7806, F1: 0.7982
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1447, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.8810, F1: 0.8909
Epoch 35/70
Train Loss: 0.0578, Accuracy: 0.9706, Precision: 0.9531, Recall: 0.9475, F1: 0.9502
Validation Loss: 0.8596, Accuracy: 0.8380, Precision: 0.8066, Recall: 0.7713, F1: 0.7830
Testing Loss: 0.7997, Accuracy: 0.8551, Precision: 0.8257, Recall: 0.7767, F1: 0.7910
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3949, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 36/70
Train Loss: 0.0511, Accuracy: 0.9706, Precision: 0.9522, Recall: 0.9495, F1: 0.9508
Validation Loss: 1.0364, Accuracy: 0.8401, Precision: 0.8080, Recall: 0.7859, F1: 0.7944
Testing Loss: 0.9736, Accuracy: 0.8599, Precision: 0.8196, Recall: 0.7955, F1: 0.8038
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1427, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 37/70
Train Loss: 0.0457, Accuracy: 0.9685, Precision: 0.9467, Recall: 0.9477, F1: 0.9472
Validation Loss: 1.0601, Accuracy: 0.8422, Precision: 0.8107, Recall: 0.8048, F1: 0.8075
Testing Loss: 1.0089, Accuracy: 0.8623, Precision: 0.8287, Recall: 0.8205, F1: 0.8243
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1964, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9095, F1: 0.9147
Epoch 38/70
Train Loss: 0.0454, Accuracy: 0.9713, Precision: 0.9521, Recall: 0.9526, F1: 0.9524
Validation Loss: 1.1302, Accuracy: 0.8422, Precision: 0.8077, Recall: 0.7822, F1: 0.7917
Testing Loss: 1.0705, Accuracy: 0.8502, Precision: 0.8045, Recall: 0.7935, F1: 0.7974
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1095, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9000, F1: 0.9048
Epoch 39/70
Train Loss: 0.0461, Accuracy: 0.9687, Precision: 0.9466, Recall: 0.9478, F1: 0.9472
Validation Loss: 1.1071, Accuracy: 0.8486, Precision: 0.8189, Recall: 0.8050, F1: 0.8115
Testing Loss: 1.0690, Accuracy: 0.8551, Precision: 0.8230, Recall: 0.8020, F1: 0.8110
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3186, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 40/70
Train Loss: 0.0464, Accuracy: 0.9685, Precision: 0.9477, Recall: 0.9493, F1: 0.9485
Validation Loss: 1.0806, Accuracy: 0.8465, Precision: 0.8124, Recall: 0.8099, F1: 0.8110
Testing Loss: 1.0204, Accuracy: 0.8635, Precision: 0.8275, Recall: 0.8182, F1: 0.8224
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1153, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 41/70
Train Loss: 0.0468, Accuracy: 0.9727, Precision: 0.9532, Recall: 0.9558, F1: 0.9545
Validation Loss: 1.1719, Accuracy: 0.8380, Precision: 0.8082, Recall: 0.7752, F1: 0.7862
Testing Loss: 1.1324, Accuracy: 0.8599, Precision: 0.8331, Recall: 0.7815, F1: 0.7951
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4040, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 42/70
Train Loss: 0.0489, Accuracy: 0.9692, Precision: 0.9480, Recall: 0.9496, F1: 0.9488
Validation Loss: 1.0893, Accuracy: 0.8422, Precision: 0.8120, Recall: 0.7787, F1: 0.7903
Testing Loss: 1.0483, Accuracy: 0.8599, Precision: 0.8272, Recall: 0.7846, F1: 0.7978
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2340, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 43/70
Train Loss: 0.0441, Accuracy: 0.9708, Precision: 0.9539, Recall: 0.9484, F1: 0.9511
Validation Loss: 1.1849, Accuracy: 0.8422, Precision: 0.8072, Recall: 0.7894, F1: 0.7964
Testing Loss: 1.1151, Accuracy: 0.8575, Precision: 0.8215, Recall: 0.8018, F1: 0.8108
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1535, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 44/70
Train Loss: 0.0448, Accuracy: 0.9706, Precision: 0.9493, Recall: 0.9499, F1: 0.9496
Validation Loss: 1.2399, Accuracy: 0.8465, Precision: 0.8161, Recall: 0.7851, F1: 0.7971
Testing Loss: 1.1972, Accuracy: 0.8527, Precision: 0.8098, Recall: 0.7789, F1: 0.7882
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2324, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 45/70
Train Loss: 0.0450, Accuracy: 0.9727, Precision: 0.9548, Recall: 0.9543, F1: 0.9545
Validation Loss: 1.1969, Accuracy: 0.8422, Precision: 0.8091, Recall: 0.8024, F1: 0.8054
Testing Loss: 1.1234, Accuracy: 0.8647, Precision: 0.8264, Recall: 0.8150, F1: 0.8204
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0828, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 46/70
Train Loss: 0.0472, Accuracy: 0.9704, Precision: 0.9518, Recall: 0.9509, F1: 0.9513
Validation Loss: 1.1730, Accuracy: 0.8443, Precision: 0.8129, Recall: 0.8037, F1: 0.8078
Testing Loss: 1.0966, Accuracy: 0.8563, Precision: 0.8223, Recall: 0.8062, F1: 0.8137
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1312, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 47/70
Train Loss: 0.0459, Accuracy: 0.9687, Precision: 0.9490, Recall: 0.9477, F1: 0.9483
Validation Loss: 1.2778, Accuracy: 0.8465, Precision: 0.8149, Recall: 0.8096, F1: 0.8120
Testing Loss: 1.1989, Accuracy: 0.8635, Precision: 0.8319, Recall: 0.8183, F1: 0.8245
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1267, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9143, F1: 0.9169
Epoch 48/70
Train Loss: 0.0466, Accuracy: 0.9711, Precision: 0.9547, Recall: 0.9487, F1: 0.9516
Validation Loss: 1.2787, Accuracy: 0.8465, Precision: 0.8176, Recall: 0.8079, F1: 0.8125
Testing Loss: 1.2221, Accuracy: 0.8623, Precision: 0.8320, Recall: 0.8165, F1: 0.8235
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2027, Accuracy: 0.8810, Precision: 0.9286, Recall: 0.8667, F1: 0.8764
Epoch 49/70
Train Loss: 0.0446, Accuracy: 0.9699, Precision: 0.9507, Recall: 0.9506, F1: 0.9506
Validation Loss: 1.2324, Accuracy: 0.8422, Precision: 0.8138, Recall: 0.7858, F1: 0.7972
Testing Loss: 1.1907, Accuracy: 0.8611, Precision: 0.8281, Recall: 0.7938, F1: 0.8073
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2044, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8524, F1: 0.8619
Epoch 50/70
Train Loss: 0.0460, Accuracy: 0.9711, Precision: 0.9512, Recall: 0.9525, F1: 0.9518
Validation Loss: 1.2986, Accuracy: 0.8422, Precision: 0.8102, Recall: 0.8006, F1: 0.8050
Testing Loss: 1.2225, Accuracy: 0.8611, Precision: 0.8217, Recall: 0.8123, F1: 0.8164
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1528, Accuracy: 0.9286, Precision: 0.9500, Recall: 0.9381, F1: 0.9379
Epoch 51/70
Train Loss: 0.0456, Accuracy: 0.9694, Precision: 0.9467, Recall: 0.9514, F1: 0.9490
Validation Loss: 1.2576, Accuracy: 0.8422, Precision: 0.8122, Recall: 0.7859, F1: 0.7965
Testing Loss: 1.2385, Accuracy: 0.8587, Precision: 0.8267, Recall: 0.7896, F1: 0.8031
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2206, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 52/70
Train Loss: 0.0580, Accuracy: 0.9706, Precision: 0.9561, Recall: 0.9502, F1: 0.9531
Validation Loss: 0.9976, Accuracy: 0.8124, Precision: 0.7640, Recall: 0.7662, F1: 0.7624
Testing Loss: 0.8199, Accuracy: 0.8297, Precision: 0.7857, Recall: 0.7655, F1: 0.7704
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 5, 1, 0, 0, 0, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 0]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.4442, Accuracy: 0.7857, Precision: 0.7549, Recall: 0.6349, F1: 0.6603
Epoch 53/70
Train Loss: 0.1676, Accuracy: 0.9433, Precision: 0.9158, Recall: 0.9162, F1: 0.9160
Validation Loss: 0.8854, Accuracy: 0.8294, Precision: 0.7970, Recall: 0.7620, F1: 0.7752
Testing Loss: 0.7651, Accuracy: 0.8430, Precision: 0.8059, Recall: 0.7680, F1: 0.7816
LM Predictions:  [2, 2, 0, 1, 0, 2, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 5, 2, 0, 2, 3, 4, 0, 0, 1, 5, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 4]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.3916, Accuracy: 0.7857, Precision: 0.7196, Recall: 0.6468, F1: 0.6572
Epoch 54/70
Train Loss: 0.0885, Accuracy: 0.9613, Precision: 0.9417, Recall: 0.9364, F1: 0.9390
Validation Loss: 0.9559, Accuracy: 0.8337, Precision: 0.8026, Recall: 0.7809, F1: 0.7893
Testing Loss: 0.9093, Accuracy: 0.8478, Precision: 0.8309, Recall: 0.7693, F1: 0.7902
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2779, Accuracy: 0.8095, Precision: 0.9059, Recall: 0.7857, F1: 0.8019
Epoch 55/70
Train Loss: 0.0528, Accuracy: 0.9689, Precision: 0.9470, Recall: 0.9494, F1: 0.9481
Validation Loss: 1.0170, Accuracy: 0.8422, Precision: 0.8018, Recall: 0.7957, F1: 0.7982
Testing Loss: 0.8894, Accuracy: 0.8466, Precision: 0.8092, Recall: 0.7797, F1: 0.7915
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 0, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1725, Accuracy: 0.9048, Precision: 0.9385, Recall: 0.9095, F1: 0.9147
Epoch 56/70
Train Loss: 0.0469, Accuracy: 0.9699, Precision: 0.9497, Recall: 0.9507, F1: 0.9502
Validation Loss: 1.0949, Accuracy: 0.8422, Precision: 0.8055, Recall: 0.7902, F1: 0.7964
Testing Loss: 1.0094, Accuracy: 0.8514, Precision: 0.8090, Recall: 0.7728, F1: 0.7854
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2129, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 57/70
Train Loss: 0.0442, Accuracy: 0.9723, Precision: 0.9526, Recall: 0.9564, F1: 0.9545
Validation Loss: 1.1566, Accuracy: 0.8486, Precision: 0.8123, Recall: 0.8123, F1: 0.8121
Testing Loss: 1.0881, Accuracy: 0.8514, Precision: 0.8102, Recall: 0.7874, F1: 0.7977
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1715, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 58/70
Train Loss: 0.0433, Accuracy: 0.9711, Precision: 0.9525, Recall: 0.9524, F1: 0.9524
Validation Loss: 1.1673, Accuracy: 0.8507, Precision: 0.8157, Recall: 0.8164, F1: 0.8160
Testing Loss: 1.0954, Accuracy: 0.8527, Precision: 0.8128, Recall: 0.7939, F1: 0.8025
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1334, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 59/70
Train Loss: 0.0451, Accuracy: 0.9711, Precision: 0.9530, Recall: 0.9520, F1: 0.9525
Validation Loss: 1.2203, Accuracy: 0.8507, Precision: 0.8158, Recall: 0.7960, F1: 0.8038
Testing Loss: 1.1496, Accuracy: 0.8502, Precision: 0.8044, Recall: 0.7704, F1: 0.7811
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0832, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 60/70
Train Loss: 0.0440, Accuracy: 0.9704, Precision: 0.9532, Recall: 0.9507, F1: 0.9519
Validation Loss: 1.2408, Accuracy: 0.8529, Precision: 0.8172, Recall: 0.8079, F1: 0.8116
Testing Loss: 1.1557, Accuracy: 0.8551, Precision: 0.8073, Recall: 0.7875, F1: 0.7958
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0544, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 61/70
Train Loss: 0.0463, Accuracy: 0.9694, Precision: 0.9482, Recall: 0.9503, F1: 0.9492
Validation Loss: 1.1820, Accuracy: 0.8507, Precision: 0.8117, Recall: 0.8107, F1: 0.8110
Testing Loss: 1.1311, Accuracy: 0.8478, Precision: 0.8086, Recall: 0.7880, F1: 0.7973
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1642, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8381, F1: 0.8498
Epoch 62/70
Train Loss: 0.0447, Accuracy: 0.9713, Precision: 0.9527, Recall: 0.9488, F1: 0.9507
Validation Loss: 1.2247, Accuracy: 0.8401, Precision: 0.7935, Recall: 0.7730, F1: 0.7793
Testing Loss: 1.1485, Accuracy: 0.8478, Precision: 0.8127, Recall: 0.7651, F1: 0.7813
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2308, Accuracy: 0.7857, Precision: 0.9000, Recall: 0.7524, F1: 0.7786
Epoch 63/70
Train Loss: 0.0427, Accuracy: 0.9732, Precision: 0.9571, Recall: 0.9534, F1: 0.9552
Validation Loss: 1.2083, Accuracy: 0.8486, Precision: 0.8171, Recall: 0.7933, F1: 0.8033
Testing Loss: 1.1768, Accuracy: 0.8563, Precision: 0.8158, Recall: 0.7720, F1: 0.7872
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 0, 3, 1, 0, 0, 4, 0, 0, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 0, 1, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1431, Accuracy: 0.8571, Precision: 0.9200, Recall: 0.8143, F1: 0.8288
Epoch 64/70
Train Loss: 0.0433, Accuracy: 0.9727, Precision: 0.9561, Recall: 0.9543, F1: 0.9552
Validation Loss: 1.2801, Accuracy: 0.8443, Precision: 0.8066, Recall: 0.8038, F1: 0.8040
Testing Loss: 1.1863, Accuracy: 0.8502, Precision: 0.8000, Recall: 0.7852, F1: 0.7918
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1217, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 65/70
Train Loss: 0.0456, Accuracy: 0.9675, Precision: 0.9478, Recall: 0.9435, F1: 0.9456
Validation Loss: 1.2471, Accuracy: 0.8486, Precision: 0.8131, Recall: 0.8063, F1: 0.8093
Testing Loss: 1.1928, Accuracy: 0.8478, Precision: 0.8003, Recall: 0.7792, F1: 0.7885
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1334, Accuracy: 0.9524, Precision: 0.9636, Recall: 0.9714, F1: 0.9646
Epoch 66/70
Train Loss: 0.0441, Accuracy: 0.9677, Precision: 0.9476, Recall: 0.9433, F1: 0.9454
Validation Loss: 1.2129, Accuracy: 0.8486, Precision: 0.8092, Recall: 0.8023, F1: 0.8056
Testing Loss: 1.1744, Accuracy: 0.8442, Precision: 0.7976, Recall: 0.7766, F1: 0.7860
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.1263, Accuracy: 0.9762, Precision: 0.9800, Recall: 0.9667, F1: 0.9713
Epoch 67/70
Train Loss: 0.0421, Accuracy: 0.9730, Precision: 0.9530, Recall: 0.9588, F1: 0.9558
Validation Loss: 1.2700, Accuracy: 0.8486, Precision: 0.8197, Recall: 0.7995, F1: 0.8077
Testing Loss: 1.2475, Accuracy: 0.8502, Precision: 0.8018, Recall: 0.7791, F1: 0.7883
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.0899, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 68/70
Train Loss: 0.0781, Accuracy: 0.9699, Precision: 0.9531, Recall: 0.9511, F1: 0.9521
Validation Loss: 1.2987, Accuracy: 0.7761, Precision: 0.7736, Recall: 0.7388, F1: 0.7385
Testing Loss: 1.0203, Accuracy: 0.8164, Precision: 0.7714, Recall: 0.7525, F1: 0.7529
LM Predictions:  [2, 2, 0, 1, 4, 5, 4, 1, 1, 0, 2, 4, 3, 5, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 2, 0, 0, 4, 0, 3, 1, 5, 1, 0, 0, 1, 3, 2]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.8181, Accuracy: 0.7857, Precision: 0.7251, Recall: 0.7011, F1: 0.7012
Epoch 69/70
Train Loss: 0.1240, Accuracy: 0.9554, Precision: 0.9319, Recall: 0.9278, F1: 0.9297
Validation Loss: 0.9377, Accuracy: 0.8230, Precision: 0.7936, Recall: 0.7325, F1: 0.7455
Testing Loss: 0.8924, Accuracy: 0.8478, Precision: 0.8199, Recall: 0.7543, F1: 0.7702
LM Predictions:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 0, 0, 3, 1, 0, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 1]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2732, Accuracy: 0.8571, Precision: 0.9143, Recall: 0.8286, F1: 0.8507
Epoch 70/70
Train Loss: 0.0634, Accuracy: 0.9654, Precision: 0.9459, Recall: 0.9432, F1: 0.9445
Validation Loss: 0.8426, Accuracy: 0.8273, Precision: 0.7953, Recall: 0.7457, F1: 0.7617
Testing Loss: 0.7421, Accuracy: 0.8514, Precision: 0.8295, Recall: 0.7676, F1: 0.7882
LM Predictions:  [2, 2, 0, 1, 0, 1, 4, 1, 1, 0, 0, 4, 3, 1, 4, 0, 4, 0, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 0, 4, 0, 0, 1, 1, 1, 1, 0, 1, 3, 5]
LM Labels:  [2, 2, 1, 1, 0, 1, 4, 1, 1, 0, 2, 4, 3, 1, 4, 0, 4, 3, 3, 2, 0, 2, 3, 4, 0, 0, 1, 1, 0, 4, 1, 4, 0, 3, 1, 1, 1, 1, 0, 1, 3, 2]
LM Loss: 0.2174, Accuracy: 0.8571, Precision: 0.7738, Recall: 0.6984, F1: 0.7176

