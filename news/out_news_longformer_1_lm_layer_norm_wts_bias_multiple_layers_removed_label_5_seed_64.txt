Model: allenai/longformer-base-4096, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:2
Noise: 1% with label 5
Label counts for Train:
  Label 0: 1141
  Label 2: 966
  Label 3: 495
  Label 1: 1011
  Label 4: 344
  Label 5: 260
Label counts for Validation:
  Label 3: 55
  Label 2: 107
  Label 1: 113
  Label 0: 127
  Label 4: 38
  Label 5: 29
Label counts for Test:
  Label 2: 190
  Label 0: 224
  Label 3: 97
  Label 1: 199
  Label 5: 51
  Label 4: 67
42
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 1146
  Label 2: 977
  Label 3: 505
  Label 1: 1016
  Label 4: 355
  Label 5: 218
For early layers:  [0, 1, 2, 3]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 1.0515, Accuracy: 0.6265, Precision: 0.5063, Recall: 0.4765, F1: 0.4703
Validation Loss: 0.6375, Accuracy: 0.8145, Precision: 0.6753, Recall: 0.6972, F1: 0.6847
Testing Loss: 0.5912, Accuracy: 0.8225, Precision: 0.6913, Recall: 0.7061, F1: 0.6956
LM Predictions:  [4, 3, 3, 0, 3, 0, 0, 2, 4, 0, 0, 0, 0, 0, 3, 4, 0, 4, 3, 3, 3, 0, 3, 0, 0, 0, 0, 0, 4, 3, 0, 4, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8595, Accuracy: 0.2143, Precision: 0.2045, Recall: 0.2127, F1: 0.1788
Epoch 2/70
Train Loss: 0.6153, Accuracy: 0.8167, Precision: 0.7517, Recall: 0.7120, F1: 0.7156
Validation Loss: 0.4881, Accuracy: 0.8294, Precision: 0.7168, Recall: 0.7116, F1: 0.7084
Testing Loss: 0.5200, Accuracy: 0.8273, Precision: 0.7315, Recall: 0.7023, F1: 0.7066
LM Predictions:  [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.1818, Accuracy: 0.1429, Precision: 0.1250, Recall: 0.2200, F1: 0.0778
Epoch 3/70
Train Loss: 0.4955, Accuracy: 0.8504, Precision: 0.7900, Recall: 0.7633, F1: 0.7719
Validation Loss: 0.4623, Accuracy: 0.8465, Precision: 0.8113, Recall: 0.8289, F1: 0.8143
Testing Loss: 0.4891, Accuracy: 0.8587, Precision: 0.8365, Recall: 0.8296, F1: 0.8275
LM Predictions:  [5, 5, 3, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 0, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0522, Accuracy: 0.0476, Precision: 0.1111, Recall: 0.0500, F1: 0.0581
Epoch 4/70
Train Loss: 0.4531, Accuracy: 0.8601, Precision: 0.8056, Recall: 0.7941, F1: 0.7993
Validation Loss: 0.5191, Accuracy: 0.8507, Precision: 0.7980, Recall: 0.7527, F1: 0.7472
Testing Loss: 0.4788, Accuracy: 0.8732, Precision: 0.8532, Recall: 0.7788, F1: 0.7862
LM Predictions:  [5, 5, 3, 0, 5, 0, 0, 2, 5, 0, 0, 0, 5, 5, 5, 4, 0, 0, 0, 0, 3, 0, 2, 5, 0, 5, 0, 5, 5, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.3372, Accuracy: 0.1190, Precision: 0.1111, Recall: 0.1500, F1: 0.0738
Epoch 5/70
Train Loss: 0.4109, Accuracy: 0.8731, Precision: 0.8238, Recall: 0.8150, F1: 0.8190
Validation Loss: 0.4562, Accuracy: 0.8614, Precision: 0.8286, Recall: 0.8445, F1: 0.8318
Testing Loss: 0.4208, Accuracy: 0.8841, Precision: 0.8643, Recall: 0.8499, F1: 0.8522
LM Predictions:  [5, 5, 3, 5, 5, 0, 0, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 4, 3, 0, 2, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.2059, Accuracy: 0.0714, Precision: 0.1250, Recall: 0.0833, F1: 0.0791
Epoch 6/70
Train Loss: 0.3878, Accuracy: 0.8824, Precision: 0.8360, Recall: 0.8329, F1: 0.8341
Validation Loss: 0.4679, Accuracy: 0.8507, Precision: 0.8118, Recall: 0.7923, F1: 0.7907
Testing Loss: 0.3982, Accuracy: 0.8877, Precision: 0.8611, Recall: 0.8323, F1: 0.8400
LM Predictions:  [5, 5, 3, 4, 5, 0, 0, 5, 4, 0, 0, 5, 5, 5, 2, 4, 5, 0, 5, 4, 3, 0, 2, 5, 5, 5, 0, 2, 5, 5, 0, 4, 5, 5, 5, 0, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9626, Accuracy: 0.1667, Precision: 0.2868, Recall: 0.1439, F1: 0.1541
Epoch 7/70
Train Loss: 0.3572, Accuracy: 0.8968, Precision: 0.8553, Recall: 0.8518, F1: 0.8531
Validation Loss: 0.4638, Accuracy: 0.8571, Precision: 0.7137, Recall: 0.7625, F1: 0.7364
Testing Loss: 0.4003, Accuracy: 0.8816, Precision: 0.8668, Recall: 0.7903, F1: 0.7887
LM Predictions:  [0, 5, 3, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, 1, 3, 0, 2, 0, 0, 0, 0, 5, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0498, Accuracy: 0.1667, Precision: 0.3542, Recall: 0.1985, F1: 0.1450
Epoch 8/70
Train Loss: 0.3407, Accuracy: 0.8968, Precision: 0.8579, Recall: 0.8494, F1: 0.8534
Validation Loss: 0.4714, Accuracy: 0.8635, Precision: 0.8190, Recall: 0.8096, F1: 0.8103
Testing Loss: 0.3718, Accuracy: 0.9046, Precision: 0.8831, Recall: 0.8543, F1: 0.8650
LM Predictions:  [5, 5, 3, 5, 5, 0, 0, 5, 5, 0, 0, 5, 5, 5, 2, 5, 5, 0, 5, 1, 3, 0, 2, 5, 5, 5, 0, 5, 5, 5, 0, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6120, Accuracy: 0.1667, Precision: 0.5278, Recall: 0.1621, F1: 0.1995
Epoch 9/70
Train Loss: 0.3121, Accuracy: 0.9040, Precision: 0.8645, Recall: 0.8596, F1: 0.8617
Validation Loss: 0.4659, Accuracy: 0.8721, Precision: 0.8273, Recall: 0.8393, F1: 0.8327
Testing Loss: 0.4071, Accuracy: 0.8901, Precision: 0.8619, Recall: 0.8571, F1: 0.8581
LM Predictions:  [5, 5, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 1, 5, 5, 0, 5, 1, 3, 5, 2, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0154, Accuracy: 0.0952, Precision: 0.3056, Recall: 0.1167, F1: 0.1385
Epoch 10/70
Train Loss: 0.2986, Accuracy: 0.9108, Precision: 0.8696, Recall: 0.8754, F1: 0.8715
Validation Loss: 0.4485, Accuracy: 0.8721, Precision: 0.8301, Recall: 0.8333, F1: 0.8284
Testing Loss: 0.3967, Accuracy: 0.8986, Precision: 0.8744, Recall: 0.8532, F1: 0.8610
LM Predictions:  [5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 0, 5, 5, 5, 2, 4, 2, 0, 5, 1, 3, 0, 2, 5, 5, 5, 0, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5407, Accuracy: 0.1905, Precision: 0.5889, Recall: 0.1773, F1: 0.2255
Epoch 11/70
Train Loss: 0.2699, Accuracy: 0.9170, Precision: 0.8773, Recall: 0.8844, F1: 0.8801
Validation Loss: 0.4690, Accuracy: 0.8657, Precision: 0.8175, Recall: 0.8262, F1: 0.8210
Testing Loss: 0.4588, Accuracy: 0.8816, Precision: 0.8565, Recall: 0.8462, F1: 0.8480
LM Predictions:  [5, 5, 5, 4, 5, 0, 5, 5, 4, 5, 5, 5, 5, 5, 2, 4, 5, 0, 5, 1, 3, 0, 2, 5, 5, 5, 0, 5, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6017, Accuracy: 0.2381, Precision: 0.5903, Recall: 0.2258, F1: 0.2669
Epoch 12/70
Train Loss: 0.2734, Accuracy: 0.9187, Precision: 0.8828, Recall: 0.8875, F1: 0.8846
Validation Loss: 0.4648, Accuracy: 0.8806, Precision: 0.8684, Recall: 0.8032, F1: 0.8136
Testing Loss: 0.4365, Accuracy: 0.8792, Precision: 0.8625, Recall: 0.8060, F1: 0.8223
LM Predictions:  [5, 5, 5, 0, 5, 0, 0, 5, 4, 0, 0, 5, 5, 5, 2, 0, 2, 0, 5, 1, 3, 0, 2, 5, 0, 5, 0, 2, 0, 0, 0, 4, 5, 0, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5999, Accuracy: 0.2619, Precision: 0.6528, Recall: 0.2409, F1: 0.2674
Epoch 13/70
Train Loss: 0.2510, Accuracy: 0.9244, Precision: 0.8886, Recall: 0.8829, F1: 0.8856
Validation Loss: 0.5635, Accuracy: 0.8593, Precision: 0.8035, Recall: 0.8445, F1: 0.8189
Testing Loss: 0.4739, Accuracy: 0.8792, Precision: 0.8279, Recall: 0.8456, F1: 0.8348
LM Predictions:  [5, 5, 5, 4, 5, 0, 0, 5, 4, 5, 5, 5, 5, 5, 2, 4, 2, 0, 4, 1, 3, 0, 2, 4, 5, 5, 0, 2, 4, 5, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2155, Accuracy: 0.2857, Precision: 0.6042, Recall: 0.2379, F1: 0.2915
Epoch 14/70
Train Loss: 0.2563, Accuracy: 0.9232, Precision: 0.8825, Recall: 0.8918, F1: 0.8861
Validation Loss: 0.5110, Accuracy: 0.8678, Precision: 0.8632, Recall: 0.8089, F1: 0.8138
Testing Loss: 0.4436, Accuracy: 0.8841, Precision: 0.8545, Recall: 0.8136, F1: 0.8247
LM Predictions:  [5, 5, 5, 0, 5, 0, 0, 5, 5, 0, 0, 5, 5, 5, 2, 5, 2, 0, 5, 1, 3, 0, 2, 5, 0, 5, 0, 2, 5, 0, 0, 4, 5, 0, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.4041, Accuracy: 0.2381, Precision: 0.6562, Recall: 0.2258, F1: 0.2514
Epoch 15/70
Train Loss: 0.2342, Accuracy: 0.9334, Precision: 0.9003, Recall: 0.8957, F1: 0.8978
Validation Loss: 0.5323, Accuracy: 0.8529, Precision: 0.8306, Recall: 0.7717, F1: 0.7745
Testing Loss: 0.4573, Accuracy: 0.8768, Precision: 0.8612, Recall: 0.8045, F1: 0.8150
LM Predictions:  [4, 5, 5, 0, 5, 0, 0, 5, 4, 0, 0, 1, 4, 5, 2, 4, 2, 0, 1, 1, 3, 0, 2, 4, 0, 0, 0, 2, 4, 0, 0, 4, 5, 0, 5, 4, 0, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3131, Accuracy: 0.4286, Precision: 0.6250, Recall: 0.4015, F1: 0.3966
Epoch 16/70
Train Loss: 0.2137, Accuracy: 0.9357, Precision: 0.9033, Recall: 0.9047, F1: 0.9038
Validation Loss: 0.5034, Accuracy: 0.8614, Precision: 0.8210, Recall: 0.8083, F1: 0.8114
Testing Loss: 0.4444, Accuracy: 0.8732, Precision: 0.8524, Recall: 0.8137, F1: 0.8277
LM Predictions:  [4, 5, 5, 0, 5, 0, 0, 5, 4, 0, 0, 1, 5, 5, 2, 0, 0, 0, 5, 1, 3, 0, 2, 4, 0, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2450, Accuracy: 0.3095, Precision: 0.6424, Recall: 0.2894, F1: 0.3249
Epoch 17/70
Train Loss: 0.2109, Accuracy: 0.9383, Precision: 0.9070, Recall: 0.9103, F1: 0.9081
Validation Loss: 0.5801, Accuracy: 0.8507, Precision: 0.7880, Recall: 0.7661, F1: 0.7474
Testing Loss: 0.5407, Accuracy: 0.8671, Precision: 0.8333, Recall: 0.7742, F1: 0.7786
LM Predictions:  [4, 1, 1, 4, 1, 0, 0, 3, 4, 0, 0, 1, 0, 3, 2, 4, 2, 0, 1, 1, 3, 0, 2, 4, 0, 0, 0, 2, 4, 0, 0, 4, 1, 0, 1, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7402, Accuracy: 0.4286, Precision: 0.4606, Recall: 0.4030, F1: 0.3581
Epoch 18/70
Train Loss: 0.2123, Accuracy: 0.9360, Precision: 0.9025, Recall: 0.9022, F1: 0.9021
Validation Loss: 0.6142, Accuracy: 0.8486, Precision: 0.8316, Recall: 0.7707, F1: 0.7802
Testing Loss: 0.5457, Accuracy: 0.8575, Precision: 0.8218, Recall: 0.7686, F1: 0.7809
LM Predictions:  [5, 5, 5, 0, 0, 0, 0, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 2, 1, 3, 0, 2, 4, 0, 0, 0, 2, 0, 0, 0, 4, 5, 0, 2, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1306, Accuracy: 0.3095, Precision: 0.5984, Recall: 0.3076, F1: 0.3074
Epoch 19/70
Train Loss: 0.1929, Accuracy: 0.9424, Precision: 0.9119, Recall: 0.9105, F1: 0.9109
Validation Loss: 0.6980, Accuracy: 0.8380, Precision: 0.7981, Recall: 0.7776, F1: 0.7855
Testing Loss: 0.6387, Accuracy: 0.8611, Precision: 0.8245, Recall: 0.7841, F1: 0.7994
LM Predictions:  [5, 5, 5, 0, 2, 0, 5, 5, 4, 5, 0, 2, 5, 5, 2, 5, 2, 0, 1, 1, 3, 0, 4, 4, 5, 5, 0, 2, 5, 0, 0, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3097, Accuracy: 0.3571, Precision: 0.6750, Recall: 0.3197, F1: 0.3719
Epoch 20/70
Train Loss: 0.2053, Accuracy: 0.9374, Precision: 0.9056, Recall: 0.9033, F1: 0.9044
Validation Loss: 0.6831, Accuracy: 0.8507, Precision: 0.8031, Recall: 0.7779, F1: 0.7850
Testing Loss: 0.5811, Accuracy: 0.8635, Precision: 0.8290, Recall: 0.7821, F1: 0.7981
LM Predictions:  [0, 5, 5, 5, 5, 0, 0, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 1, 1, 3, 0, 5, 4, 5, 5, 0, 2, 0, 0, 5, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6185, Accuracy: 0.3095, Precision: 0.7000, Recall: 0.3076, F1: 0.3455
Epoch 21/70
Train Loss: 0.1955, Accuracy: 0.9457, Precision: 0.9166, Recall: 0.9138, F1: 0.9150
Validation Loss: 0.6614, Accuracy: 0.8550, Precision: 0.8152, Recall: 0.7921, F1: 0.7964
Testing Loss: 0.5927, Accuracy: 0.8635, Precision: 0.8192, Recall: 0.7834, F1: 0.7959
LM Predictions:  [4, 5, 5, 0, 5, 0, 5, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 1, 1, 3, 0, 4, 4, 0, 5, 0, 2, 4, 0, 5, 4, 5, 5, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7740, Accuracy: 0.3810, Precision: 0.7024, Recall: 0.3530, F1: 0.3888
Epoch 22/70
Train Loss: 0.1791, Accuracy: 0.9443, Precision: 0.9121, Recall: 0.9112, F1: 0.9116
Validation Loss: 0.6032, Accuracy: 0.8614, Precision: 0.8093, Recall: 0.7839, F1: 0.7860
Testing Loss: 0.5396, Accuracy: 0.8792, Precision: 0.8565, Recall: 0.8071, F1: 0.8197
LM Predictions:  [4, 5, 5, 0, 5, 0, 0, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 5, 1, 3, 0, 4, 4, 0, 0, 0, 2, 5, 0, 0, 4, 0, 0, 5, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0204, Accuracy: 0.3810, Precision: 0.7083, Recall: 0.3712, F1: 0.3611
Epoch 23/70
Train Loss: 0.1843, Accuracy: 0.9402, Precision: 0.9068, Recall: 0.9069, F1: 0.9067
Validation Loss: 0.6087, Accuracy: 0.8422, Precision: 0.8025, Recall: 0.7724, F1: 0.7829
Testing Loss: 0.5307, Accuracy: 0.8635, Precision: 0.8211, Recall: 0.7805, F1: 0.7954
LM Predictions:  [0, 5, 5, 0, 5, 0, 0, 5, 4, 5, 0, 1, 5, 5, 2, 5, 0, 0, 1, 1, 3, 0, 4, 5, 5, 0, 0, 2, 5, 0, 0, 4, 0, 0, 5, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0080, Accuracy: 0.3571, Precision: 0.7130, Recall: 0.3742, F1: 0.3679
Epoch 24/70
Train Loss: 0.1775, Accuracy: 0.9459, Precision: 0.9144, Recall: 0.9157, F1: 0.9149
Validation Loss: 0.7265, Accuracy: 0.8401, Precision: 0.8012, Recall: 0.7698, F1: 0.7791
Testing Loss: 0.6407, Accuracy: 0.8514, Precision: 0.8112, Recall: 0.7692, F1: 0.7844
LM Predictions:  [4, 5, 5, 0, 0, 0, 5, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 1, 1, 3, 0, 4, 5, 5, 0, 0, 2, 5, 0, 0, 4, 0, 0, 1, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0919, Accuracy: 0.3810, Precision: 0.6713, Recall: 0.3894, F1: 0.3693
Epoch 25/70
Train Loss: 0.1710, Accuracy: 0.9426, Precision: 0.9095, Recall: 0.9158, F1: 0.9122
Validation Loss: 0.7053, Accuracy: 0.8380, Precision: 0.7796, Recall: 0.7917, F1: 0.7841
Testing Loss: 0.5717, Accuracy: 0.8659, Precision: 0.8251, Recall: 0.8041, F1: 0.8126
LM Predictions:  [4, 5, 5, 3, 5, 0, 0, 5, 4, 0, 0, 1, 5, 5, 2, 5, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 5, 0, 5, 4, 0, 5, 4, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.6818, Accuracy: 0.4286, Precision: 0.6949, Recall: 0.4212, F1: 0.4223
Epoch 26/70
Train Loss: 0.1839, Accuracy: 0.9438, Precision: 0.9146, Recall: 0.9083, F1: 0.9114
Validation Loss: 0.5830, Accuracy: 0.8443, Precision: 0.7943, Recall: 0.8211, F1: 0.8054
Testing Loss: 0.5294, Accuracy: 0.8720, Precision: 0.8384, Recall: 0.8377, F1: 0.8369
LM Predictions:  [4, 5, 5, 0, 5, 5, 5, 5, 4, 5, 0, 1, 5, 5, 2, 5, 2, 0, 1, 1, 3, 5, 4, 4, 3, 5, 0, 2, 4, 0, 5, 4, 0, 5, 4, 4, 3, 5, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7302, Accuracy: 0.4762, Precision: 0.7199, Recall: 0.4348, F1: 0.4914
Epoch 27/70
Train Loss: 0.1503, Accuracy: 0.9535, Precision: 0.9282, Recall: 0.9285, F1: 0.9282
Validation Loss: 0.7465, Accuracy: 0.8422, Precision: 0.8138, Recall: 0.7703, F1: 0.7805
Testing Loss: 0.6760, Accuracy: 0.8490, Precision: 0.8270, Recall: 0.7652, F1: 0.7807
LM Predictions:  [0, 3, 5, 3, 2, 0, 0, 5, 4, 0, 0, 1, 0, 5, 1, 0, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 0, 5, 4, 3, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7069, Accuracy: 0.4762, Precision: 0.6647, Recall: 0.4545, F1: 0.4394
Epoch 28/70
Train Loss: 0.1488, Accuracy: 0.9530, Precision: 0.9276, Recall: 0.9277, F1: 0.9275
Validation Loss: 0.7304, Accuracy: 0.8529, Precision: 0.8066, Recall: 0.7922, F1: 0.7924
Testing Loss: 0.6259, Accuracy: 0.8635, Precision: 0.8197, Recall: 0.7828, F1: 0.7907
LM Predictions:  [4, 3, 4, 3, 2, 0, 5, 4, 4, 0, 0, 1, 5, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 4, 0, 0, 2, 4, 0, 0, 4, 0, 0, 4, 4, 3, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.4330, Accuracy: 0.5952, Precision: 0.6733, Recall: 0.5333, F1: 0.5220
Epoch 29/70
Train Loss: 0.1605, Accuracy: 0.9485, Precision: 0.9170, Recall: 0.9193, F1: 0.9181
Validation Loss: 0.6695, Accuracy: 0.8316, Precision: 0.7948, Recall: 0.8143, F1: 0.7983
Testing Loss: 0.5762, Accuracy: 0.8659, Precision: 0.8352, Recall: 0.8363, F1: 0.8330
LM Predictions:  [4, 5, 5, 3, 2, 5, 5, 1, 4, 5, 5, 1, 5, 1, 2, 5, 2, 0, 1, 1, 3, 5, 4, 4, 5, 5, 5, 2, 4, 0, 5, 4, 5, 5, 4, 4, 5, 5, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.5832, Accuracy: 0.4762, Precision: 0.7792, Recall: 0.4333, F1: 0.5256
Epoch 30/70
Train Loss: 0.1500, Accuracy: 0.9533, Precision: 0.9237, Recall: 0.9328, F1: 0.9279
Validation Loss: 0.7434, Accuracy: 0.8401, Precision: 0.8020, Recall: 0.7586, F1: 0.7686
Testing Loss: 0.6436, Accuracy: 0.8587, Precision: 0.8411, Recall: 0.7726, F1: 0.7935
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 2, 4, 0, 0, 1, 0, 2, 2, 0, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 4, 4, 3, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.8969, Accuracy: 0.4762, Precision: 0.6124, Recall: 0.4530, F1: 0.4351
Epoch 31/70
Train Loss: 0.1321, Accuracy: 0.9542, Precision: 0.9331, Recall: 0.9273, F1: 0.9301
Validation Loss: 0.7451, Accuracy: 0.8401, Precision: 0.8002, Recall: 0.8109, F1: 0.8035
Testing Loss: 0.6655, Accuracy: 0.8671, Precision: 0.8351, Recall: 0.8281, F1: 0.8306
LM Predictions:  [4, 3, 4, 0, 2, 5, 5, 5, 4, 5, 5, 1, 0, 1, 2, 5, 0, 0, 1, 1, 3, 5, 4, 4, 0, 5, 5, 2, 0, 0, 5, 4, 0, 5, 3, 4, 3, 5, 5, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.4055, Accuracy: 0.5238, Precision: 0.6917, Recall: 0.4682, F1: 0.5122
Epoch 32/70
Train Loss: 0.1453, Accuracy: 0.9507, Precision: 0.9226, Recall: 0.9264, F1: 0.9243
Validation Loss: 0.7491, Accuracy: 0.8230, Precision: 0.7722, Recall: 0.7645, F1: 0.7671
Testing Loss: 0.6550, Accuracy: 0.8575, Precision: 0.8168, Recall: 0.7856, F1: 0.7981
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 1, 4, 0, 5, 1, 0, 2, 2, 3, 0, 0, 1, 1, 3, 5, 4, 4, 3, 5, 0, 2, 3, 0, 5, 4, 0, 0, 3, 4, 3, 5, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.1698, Accuracy: 0.6190, Precision: 0.6578, Recall: 0.5515, F1: 0.5589
Epoch 33/70
Train Loss: 0.1452, Accuracy: 0.9552, Precision: 0.9290, Recall: 0.9344, F1: 0.9313
Validation Loss: 0.7892, Accuracy: 0.8380, Precision: 0.7803, Recall: 0.7681, F1: 0.7671
Testing Loss: 0.7315, Accuracy: 0.8575, Precision: 0.8160, Recall: 0.7827, F1: 0.7896
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 5, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 3, 0, 0, 2, 0, 0, 0, 4, 0, 0, 4, 4, 3, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.2784, Accuracy: 0.6429, Precision: 0.6944, Recall: 0.5667, F1: 0.5504
Epoch 34/70
Train Loss: 0.1364, Accuracy: 0.9566, Precision: 0.9299, Recall: 0.9370, F1: 0.9333
Validation Loss: 0.7448, Accuracy: 0.8380, Precision: 0.7731, Recall: 0.7708, F1: 0.7643
Testing Loss: 0.5985, Accuracy: 0.8684, Precision: 0.8231, Recall: 0.7837, F1: 0.7907
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 5, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 3, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 3, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.0071, Accuracy: 0.6667, Precision: 0.7130, Recall: 0.5833, F1: 0.5671
Epoch 35/70
Train Loss: 0.1405, Accuracy: 0.9519, Precision: 0.9295, Recall: 0.9245, F1: 0.9270
Validation Loss: 0.6947, Accuracy: 0.8550, Precision: 0.8074, Recall: 0.7898, F1: 0.7965
Testing Loss: 0.6385, Accuracy: 0.8623, Precision: 0.8347, Recall: 0.7908, F1: 0.8079
LM Predictions:  [0, 3, 4, 0, 2, 0, 5, 5, 4, 0, 0, 1, 0, 2, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 5, 0, 0, 4, 0, 0, 2, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.6769, Accuracy: 0.4762, Precision: 0.6397, Recall: 0.4530, F1: 0.4462
Epoch 36/70
Train Loss: 0.1491, Accuracy: 0.9523, Precision: 0.9265, Recall: 0.9278, F1: 0.9271
Validation Loss: 0.8242, Accuracy: 0.8316, Precision: 0.7826, Recall: 0.7572, F1: 0.7609
Testing Loss: 0.7565, Accuracy: 0.8599, Precision: 0.8280, Recall: 0.7809, F1: 0.7934
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 5, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 3, 0, 0, 2, 5, 0, 0, 4, 0, 0, 3, 4, 3, 0, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.9381, Accuracy: 0.6905, Precision: 0.7188, Recall: 0.5985, F1: 0.5837
Epoch 37/70
Train Loss: 0.1282, Accuracy: 0.9566, Precision: 0.9318, Recall: 0.9329, F1: 0.9323
Validation Loss: 0.8534, Accuracy: 0.8401, Precision: 0.7892, Recall: 0.7646, F1: 0.7663
Testing Loss: 0.7499, Accuracy: 0.8696, Precision: 0.8267, Recall: 0.7808, F1: 0.7902
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 5, 4, 0, 0, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 3, 0, 0, 2, 3, 0, 0, 4, 0, 0, 3, 4, 3, 0, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.0911, Accuracy: 0.7143, Precision: 0.7056, Recall: 0.6136, F1: 0.5972
Epoch 38/70
Train Loss: 0.1204, Accuracy: 0.9604, Precision: 0.9405, Recall: 0.9340, F1: 0.9372
Validation Loss: 0.9147, Accuracy: 0.8273, Precision: 0.7627, Recall: 0.7726, F1: 0.7638
Testing Loss: 0.7326, Accuracy: 0.8635, Precision: 0.8134, Recall: 0.8029, F1: 0.8028
LM Predictions:  [4, 3, 4, 3, 2, 5, 4, 5, 4, 4, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 4, 5, 0, 2, 4, 0, 5, 4, 0, 3, 3, 4, 3, 3, 5, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.0991, Accuracy: 0.6905, Precision: 0.6910, Recall: 0.5788, F1: 0.5851
Epoch 39/70
Train Loss: 0.1230, Accuracy: 0.9592, Precision: 0.9381, Recall: 0.9346, F1: 0.9363
Validation Loss: 0.8476, Accuracy: 0.8422, Precision: 0.7882, Recall: 0.7754, F1: 0.7760
Testing Loss: 0.7206, Accuracy: 0.8659, Precision: 0.8302, Recall: 0.7840, F1: 0.7974
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 3, 2, 0, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.0002, Accuracy: 0.6190, Precision: 0.8476, Recall: 0.6782, F1: 0.6631
Epoch 40/70
Train Loss: 0.1253, Accuracy: 0.9564, Precision: 0.9356, Recall: 0.9288, F1: 0.9321
Validation Loss: 0.8225, Accuracy: 0.8230, Precision: 0.7721, Recall: 0.7399, F1: 0.7475
Testing Loss: 0.7228, Accuracy: 0.8539, Precision: 0.8131, Recall: 0.7643, F1: 0.7760
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.1798, Accuracy: 0.5952, Precision: 0.8455, Recall: 0.6618, F1: 0.6431
Epoch 41/70
Train Loss: 0.1234, Accuracy: 0.9576, Precision: 0.9306, Recall: 0.9323, F1: 0.9314
Validation Loss: 0.8348, Accuracy: 0.8145, Precision: 0.7766, Recall: 0.7922, F1: 0.7786
Testing Loss: 0.6712, Accuracy: 0.8575, Precision: 0.8250, Recall: 0.8210, F1: 0.8219
LM Predictions:  [5, 3, 4, 3, 2, 5, 5, 1, 4, 5, 5, 5, 4, 3, 2, 3, 0, 0, 1, 1, 3, 5, 4, 4, 3, 5, 0, 2, 3, 0, 5, 4, 5, 5, 3, 4, 3, 5, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.0327, Accuracy: 0.5952, Precision: 0.7214, Recall: 0.5015, F1: 0.5594
Epoch 42/70
Train Loss: 0.1372, Accuracy: 0.9516, Precision: 0.9261, Recall: 0.9264, F1: 0.9262
Validation Loss: 0.8545, Accuracy: 0.8337, Precision: 0.7832, Recall: 0.7966, F1: 0.7881
Testing Loss: 0.6919, Accuracy: 0.8696, Precision: 0.8328, Recall: 0.8206, F1: 0.8261
LM Predictions:  [4, 3, 4, 3, 2, 0, 5, 1, 4, 5, 0, 1, 4, 2, 2, 3, 0, 0, 1, 1, 3, 5, 4, 4, 3, 0, 3, 2, 3, 0, 5, 4, 0, 5, 3, 4, 3, 5, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.2110, Accuracy: 0.7143, Precision: 0.7009, Recall: 0.6318, F1: 0.6339
Epoch 43/70
Train Loss: 0.1172, Accuracy: 0.9609, Precision: 0.9372, Recall: 0.9477, F1: 0.9421
Validation Loss: 0.9078, Accuracy: 0.8337, Precision: 0.7667, Recall: 0.7617, F1: 0.7587
Testing Loss: 0.7644, Accuracy: 0.8671, Precision: 0.8291, Recall: 0.7875, F1: 0.7993
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 2, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 3, 0, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.9220, Accuracy: 0.6667, Precision: 0.8156, Recall: 0.7164, F1: 0.6947
Epoch 44/70
Train Loss: 0.1070, Accuracy: 0.9616, Precision: 0.9394, Recall: 0.9412, F1: 0.9402
Validation Loss: 0.9897, Accuracy: 0.8294, Precision: 0.7776, Recall: 0.7901, F1: 0.7827
Testing Loss: 0.7659, Accuracy: 0.8647, Precision: 0.8261, Recall: 0.8030, F1: 0.8131
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 0, 0, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 0, 5, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6985, Accuracy: 0.6429, Precision: 0.7105, Recall: 0.5833, F1: 0.5734
Epoch 45/70
Train Loss: 0.1163, Accuracy: 0.9587, Precision: 0.9353, Recall: 0.9384, F1: 0.9368
Validation Loss: 0.8341, Accuracy: 0.8316, Precision: 0.7893, Recall: 0.8028, F1: 0.7926
Testing Loss: 0.6794, Accuracy: 0.8684, Precision: 0.8361, Recall: 0.8301, F1: 0.8323
LM Predictions:  [4, 3, 4, 3, 2, 5, 4, 1, 4, 5, 0, 1, 4, 5, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 3, 0, 5, 4, 0, 2, 3, 4, 3, 5, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7297, Accuracy: 0.8333, Precision: 0.7889, Recall: 0.7076, F1: 0.7380
Epoch 46/70
Train Loss: 0.1020, Accuracy: 0.9611, Precision: 0.9392, Recall: 0.9415, F1: 0.9403
Validation Loss: 0.8509, Accuracy: 0.8230, Precision: 0.7787, Recall: 0.7835, F1: 0.7780
Testing Loss: 0.6943, Accuracy: 0.8671, Precision: 0.8360, Recall: 0.8148, F1: 0.8241
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 5, 1, 4, 5, 2, 0, 0, 0, 5, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 0, 5, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.2327, Accuracy: 0.5238, Precision: 0.7063, Recall: 0.4833, F1: 0.4778
Epoch 47/70
Train Loss: 0.1272, Accuracy: 0.9552, Precision: 0.9339, Recall: 0.9339, F1: 0.9338
Validation Loss: 0.9057, Accuracy: 0.8294, Precision: 0.7747, Recall: 0.7957, F1: 0.7838
Testing Loss: 0.7136, Accuracy: 0.8768, Precision: 0.8378, Recall: 0.8320, F1: 0.8346
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 2, 4, 5, 5, 2, 4, 3, 2, 3, 0, 0, 2, 1, 3, 0, 4, 4, 0, 5, 0, 2, 4, 0, 0, 4, 0, 0, 3, 4, 5, 5, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.3082, Accuracy: 0.5952, Precision: 0.6346, Recall: 0.4788, F1: 0.4801
Epoch 48/70
Train Loss: 0.1119, Accuracy: 0.9592, Precision: 0.9340, Recall: 0.9379, F1: 0.9358
Validation Loss: 1.1000, Accuracy: 0.8145, Precision: 0.7739, Recall: 0.7630, F1: 0.7641
Testing Loss: 0.8288, Accuracy: 0.8623, Precision: 0.8231, Recall: 0.7959, F1: 0.8068
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 2, 5, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 3, 4, 4, 3, 0, 3, 2, 3, 0, 2, 4, 0, 2, 3, 4, 3, 5, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6682, Accuracy: 0.8333, Precision: 0.7371, Recall: 0.7091, F1: 0.7114
Epoch 49/70
Train Loss: 0.1084, Accuracy: 0.9602, Precision: 0.9412, Recall: 0.9335, F1: 0.9372
Validation Loss: 0.8890, Accuracy: 0.8252, Precision: 0.7890, Recall: 0.7961, F1: 0.7894
Testing Loss: 0.7229, Accuracy: 0.8671, Precision: 0.8340, Recall: 0.8221, F1: 0.8273
LM Predictions:  [0, 3, 4, 3, 2, 5, 0, 1, 4, 5, 5, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 3, 4, 4, 3, 5, 3, 2, 3, 0, 5, 4, 0, 3, 3, 4, 3, 5, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7130, Accuracy: 0.7143, Precision: 0.7234, Recall: 0.6152, F1: 0.6380
Epoch 50/70
Train Loss: 0.1043, Accuracy: 0.9597, Precision: 0.9325, Recall: 0.9474, F1: 0.9393
Validation Loss: 1.0979, Accuracy: 0.8124, Precision: 0.7502, Recall: 0.7531, F1: 0.7460
Testing Loss: 0.7994, Accuracy: 0.8647, Precision: 0.8278, Recall: 0.7967, F1: 0.8072
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 0, 2, 2, 4, 3, 2, 3, 2, 0, 2, 1, 3, 0, 4, 4, 3, 0, 3, 2, 3, 0, 2, 4, 0, 0, 3, 4, 3, 5, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8742, Accuracy: 0.8095, Precision: 0.7071, Recall: 0.6576, F1: 0.6484
Epoch 51/70
Train Loss: 0.1057, Accuracy: 0.9623, Precision: 0.9419, Recall: 0.9380, F1: 0.9399
Validation Loss: 0.8808, Accuracy: 0.8358, Precision: 0.7920, Recall: 0.7923, F1: 0.7918
Testing Loss: 0.7432, Accuracy: 0.8720, Precision: 0.8434, Recall: 0.8234, F1: 0.8320
LM Predictions:  [4, 3, 4, 3, 2, 2, 0, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 5, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5043, Accuracy: 0.8333, Precision: 0.7315, Recall: 0.7061, F1: 0.7006
Epoch 52/70
Train Loss: 0.1134, Accuracy: 0.9583, Precision: 0.9382, Recall: 0.9339, F1: 0.9360
Validation Loss: 0.8758, Accuracy: 0.8316, Precision: 0.7870, Recall: 0.7992, F1: 0.7926
Testing Loss: 0.7477, Accuracy: 0.8671, Precision: 0.8337, Recall: 0.8214, F1: 0.8269
LM Predictions:  [4, 3, 4, 2, 2, 0, 4, 1, 4, 5, 5, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 5, 4, 4, 3, 0, 4, 2, 5, 0, 5, 4, 0, 5, 3, 4, 3, 5, 5, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.3354, Accuracy: 0.7381, Precision: 0.7571, Recall: 0.6455, F1: 0.6827
Epoch 53/70
Train Loss: 0.1038, Accuracy: 0.9628, Precision: 0.9410, Recall: 0.9424, F1: 0.9417
Validation Loss: 0.9895, Accuracy: 0.8252, Precision: 0.7690, Recall: 0.7737, F1: 0.7705
Testing Loss: 0.7986, Accuracy: 0.8659, Precision: 0.8256, Recall: 0.8084, F1: 0.8158
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 0, 0, 0, 4, 0, 0, 3, 4, 0, 5, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6649, Accuracy: 0.6667, Precision: 0.7130, Recall: 0.5970, F1: 0.5758
Epoch 54/70
Train Loss: 0.0925, Accuracy: 0.9613, Precision: 0.9378, Recall: 0.9439, F1: 0.9408
Validation Loss: 1.1004, Accuracy: 0.8230, Precision: 0.7754, Recall: 0.7850, F1: 0.7776
Testing Loss: 0.8286, Accuracy: 0.8696, Precision: 0.8335, Recall: 0.8220, F1: 0.8270
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 2, 5, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 5, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5982, Accuracy: 0.8571, Precision: 0.7554, Recall: 0.7212, F1: 0.7303
Epoch 55/70
Train Loss: 0.1229, Accuracy: 0.9576, Precision: 0.9405, Recall: 0.9301, F1: 0.9347
Validation Loss: 1.0244, Accuracy: 0.8145, Precision: 0.7604, Recall: 0.7535, F1: 0.7536
Testing Loss: 0.7744, Accuracy: 0.8514, Precision: 0.8175, Recall: 0.7815, F1: 0.7951
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 0, 2, 1, 4, 2, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 1, 4, 0, 0, 3, 4, 0, 0, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6911, Accuracy: 0.7381, Precision: 0.7925, Recall: 0.7691, F1: 0.7349
Epoch 56/70
Train Loss: 0.0967, Accuracy: 0.9640, Precision: 0.9481, Recall: 0.9398, F1: 0.9437
Validation Loss: 0.8220, Accuracy: 0.8380, Precision: 0.7880, Recall: 0.8095, F1: 0.7967
Testing Loss: 0.7453, Accuracy: 0.8720, Precision: 0.8328, Recall: 0.8334, F1: 0.8326
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 3, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5730, Accuracy: 0.7619, Precision: 0.8103, Recall: 0.7891, F1: 0.7671
Epoch 57/70
Train Loss: 0.0997, Accuracy: 0.9618, Precision: 0.9401, Recall: 0.9437, F1: 0.9419
Validation Loss: 0.9628, Accuracy: 0.8358, Precision: 0.7898, Recall: 0.7820, F1: 0.7845
Testing Loss: 0.7201, Accuracy: 0.8780, Precision: 0.8510, Recall: 0.8317, F1: 0.8405
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4492, Accuracy: 0.8333, Precision: 0.8667, Recall: 0.8436, F1: 0.8255
Epoch 58/70
Train Loss: 0.0976, Accuracy: 0.9609, Precision: 0.9373, Recall: 0.9462, F1: 0.9414
Validation Loss: 0.9499, Accuracy: 0.8230, Precision: 0.7700, Recall: 0.7904, F1: 0.7778
Testing Loss: 0.7526, Accuracy: 0.8623, Precision: 0.8254, Recall: 0.8222, F1: 0.8235
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 0, 3, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 3, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5840, Accuracy: 0.7143, Precision: 0.7881, Recall: 0.7527, F1: 0.7245
Epoch 59/70
Train Loss: 0.0912, Accuracy: 0.9651, Precision: 0.9448, Recall: 0.9473, F1: 0.9460
Validation Loss: 0.9702, Accuracy: 0.8337, Precision: 0.7804, Recall: 0.7648, F1: 0.7659
Testing Loss: 0.8227, Accuracy: 0.8551, Precision: 0.8146, Recall: 0.7830, F1: 0.7948
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5575, Accuracy: 0.7619, Precision: 0.8325, Recall: 0.7891, F1: 0.7694
Epoch 60/70
Train Loss: 0.0850, Accuracy: 0.9649, Precision: 0.9471, Recall: 0.9382, F1: 0.9425
Validation Loss: 0.9738, Accuracy: 0.8230, Precision: 0.7758, Recall: 0.7981, F1: 0.7848
Testing Loss: 0.7264, Accuracy: 0.8696, Precision: 0.8325, Recall: 0.8314, F1: 0.8314
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 5, 5, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 3, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6056, Accuracy: 0.8571, Precision: 0.7497, Recall: 0.7242, F1: 0.7273
Epoch 61/70
Train Loss: 0.1031, Accuracy: 0.9613, Precision: 0.9417, Recall: 0.9459, F1: 0.9437
Validation Loss: 0.9875, Accuracy: 0.8358, Precision: 0.7778, Recall: 0.7762, F1: 0.7752
Testing Loss: 0.8103, Accuracy: 0.8647, Precision: 0.8268, Recall: 0.7950, F1: 0.8068
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 0, 5, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 5, 0, 0, 4, 0, 5, 3, 4, 0, 2, 5, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6024, Accuracy: 0.7619, Precision: 0.7424, Recall: 0.6606, F1: 0.6634
Epoch 62/70
Train Loss: 0.0977, Accuracy: 0.9644, Precision: 0.9485, Recall: 0.9397, F1: 0.9439
Validation Loss: 1.0099, Accuracy: 0.8230, Precision: 0.7720, Recall: 0.7875, F1: 0.7790
Testing Loss: 0.7783, Accuracy: 0.8732, Precision: 0.8430, Recall: 0.8327, F1: 0.8373
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 5, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 5, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6145, Accuracy: 0.7619, Precision: 0.7308, Recall: 0.6576, F1: 0.6501
Epoch 63/70
Train Loss: 0.0929, Accuracy: 0.9630, Precision: 0.9381, Recall: 0.9495, F1: 0.9435
Validation Loss: 0.9887, Accuracy: 0.8166, Precision: 0.7642, Recall: 0.7707, F1: 0.7663
Testing Loss: 0.7606, Accuracy: 0.8659, Precision: 0.8330, Recall: 0.8124, F1: 0.8218
LM Predictions:  [0, 3, 4, 3, 2, 0, 4, 1, 4, 0, 3, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4547, Accuracy: 0.8095, Precision: 0.8477, Recall: 0.8327, F1: 0.8090
Epoch 64/70
Train Loss: 0.1117, Accuracy: 0.9602, Precision: 0.9353, Recall: 0.9398, F1: 0.9374
Validation Loss: 0.9896, Accuracy: 0.8252, Precision: 0.7824, Recall: 0.7699, F1: 0.7711
Testing Loss: 0.7485, Accuracy: 0.8575, Precision: 0.8226, Recall: 0.7868, F1: 0.7978
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 0, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6238, Accuracy: 0.6905, Precision: 0.8338, Recall: 0.7345, F1: 0.7216
Epoch 65/70
Train Loss: 0.0943, Accuracy: 0.9651, Precision: 0.9434, Recall: 0.9443, F1: 0.9438
Validation Loss: 1.0025, Accuracy: 0.8273, Precision: 0.7793, Recall: 0.7524, F1: 0.7569
Testing Loss: 0.8089, Accuracy: 0.8696, Precision: 0.8340, Recall: 0.7824, F1: 0.7965
LM Predictions:  [0, 3, 4, 3, 2, 0, 4, 1, 4, 3, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4509, Accuracy: 0.8333, Precision: 0.8596, Recall: 0.8509, F1: 0.8300
Epoch 66/70
Train Loss: 0.0946, Accuracy: 0.9606, Precision: 0.9418, Recall: 0.9311, F1: 0.9359
Validation Loss: 0.9943, Accuracy: 0.8358, Precision: 0.7735, Recall: 0.7642, F1: 0.7648
Testing Loss: 0.8129, Accuracy: 0.8659, Precision: 0.8374, Recall: 0.7960, F1: 0.8121
LM Predictions:  [0, 3, 4, 0, 2, 0, 4, 1, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 0, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5744, Accuracy: 0.6667, Precision: 0.8222, Recall: 0.7164, F1: 0.7008
Epoch 67/70
Train Loss: 0.0789, Accuracy: 0.9685, Precision: 0.9513, Recall: 0.9539, F1: 0.9525
Validation Loss: 1.0039, Accuracy: 0.8230, Precision: 0.7657, Recall: 0.7848, F1: 0.7715
Testing Loss: 0.7921, Accuracy: 0.8684, Precision: 0.8328, Recall: 0.8212, F1: 0.8265
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3774, Accuracy: 0.8571, Precision: 0.8711, Recall: 0.8673, F1: 0.8530
Epoch 68/70
Train Loss: 0.0939, Accuracy: 0.9590, Precision: 0.9365, Recall: 0.9412, F1: 0.9388
Validation Loss: 0.9466, Accuracy: 0.8401, Precision: 0.7992, Recall: 0.7801, F1: 0.7860
Testing Loss: 0.7613, Accuracy: 0.8732, Precision: 0.8439, Recall: 0.7989, F1: 0.8145
LM Predictions:  [4, 3, 5, 3, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5700, Accuracy: 0.9286, Precision: 0.8077, Recall: 0.7697, F1: 0.7843
Epoch 69/70
Train Loss: 0.0857, Accuracy: 0.9640, Precision: 0.9478, Recall: 0.9377, F1: 0.9425
Validation Loss: 0.9803, Accuracy: 0.8337, Precision: 0.7788, Recall: 0.7767, F1: 0.7747
Testing Loss: 0.7866, Accuracy: 0.8732, Precision: 0.8370, Recall: 0.8049, F1: 0.8153
LM Predictions:  [4, 3, 2, 3, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3401, Accuracy: 0.9286, Precision: 0.9571, Recall: 0.9236, F1: 0.9338
Epoch 70/70
Train Loss: 0.0763, Accuracy: 0.9647, Precision: 0.9452, Recall: 0.9434, F1: 0.9443
Validation Loss: 1.0024, Accuracy: 0.8316, Precision: 0.7837, Recall: 0.7852, F1: 0.7839
Testing Loss: 0.8093, Accuracy: 0.8732, Precision: 0.8428, Recall: 0.8250, F1: 0.8332
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 2, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 2, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4875, Accuracy: 0.7857, Precision: 0.8364, Recall: 0.8127, F1: 0.7933
For middle layers:  [4, 5, 6, 7]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.9796, Accuracy: 0.6507, Precision: 0.5681, Recall: 0.5096, F1: 0.5156
Validation Loss: 0.4344, Accuracy: 0.8614, Precision: 0.8938, Recall: 0.7712, F1: 0.7625
Testing Loss: 0.4395, Accuracy: 0.8575, Precision: 0.8934, Recall: 0.7482, F1: 0.7426
LM Predictions:  [0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 4, 3, 0, 3, 0, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6894, Accuracy: 0.1667, Precision: 0.1562, Recall: 0.2382, F1: 0.1084
Epoch 2/70
Train Loss: 0.4676, Accuracy: 0.8558, Precision: 0.7984, Recall: 0.7749, F1: 0.7823
Validation Loss: 0.4308, Accuracy: 0.8550, Precision: 0.7991, Recall: 0.8324, F1: 0.8126
Testing Loss: 0.3670, Accuracy: 0.8841, Precision: 0.8392, Recall: 0.8572, F1: 0.8461
LM Predictions:  [5, 5, 5, 5, 5, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 3, 5, 5, 5, 0, 5, 0, 5, 5, 5, 0, 4, 5, 4, 5, 5, 5, 0, 0, 5, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.9283, Accuracy: 0.0476, Precision: 0.2222, Recall: 0.0318, F1: 0.0541
Epoch 3/70
Train Loss: 0.3824, Accuracy: 0.8836, Precision: 0.8391, Recall: 0.8393, F1: 0.8387
Validation Loss: 0.4018, Accuracy: 0.8849, Precision: 0.8461, Recall: 0.8747, F1: 0.8579
Testing Loss: 0.3428, Accuracy: 0.8889, Precision: 0.8512, Recall: 0.8670, F1: 0.8579
LM Predictions:  [5, 3, 3, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 5, 3, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.8137, Accuracy: 0.1429, Precision: 0.3148, Recall: 0.1303, F1: 0.1502
Epoch 4/70
Train Loss: 0.3463, Accuracy: 0.8935, Precision: 0.8492, Recall: 0.8394, F1: 0.8441
Validation Loss: 0.4026, Accuracy: 0.8699, Precision: 0.8407, Recall: 0.8410, F1: 0.8376
Testing Loss: 0.3306, Accuracy: 0.8913, Precision: 0.8651, Recall: 0.8457, F1: 0.8542
LM Predictions:  [5, 3, 5, 0, 0, 0, 0, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 3, 0, 5, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 0, 5, 0, 5, 5, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0604, Accuracy: 0.0714, Precision: 0.1795, Recall: 0.0667, F1: 0.0741
Epoch 5/70
Train Loss: 0.3039, Accuracy: 0.9068, Precision: 0.8653, Recall: 0.8656, F1: 0.8653
Validation Loss: 0.4178, Accuracy: 0.8721, Precision: 0.8386, Recall: 0.8257, F1: 0.8232
Testing Loss: 0.3173, Accuracy: 0.8949, Precision: 0.8754, Recall: 0.8336, F1: 0.8440
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 2, 5, 0, 5, 5, 5, 5, 5, 0, 2, 0, 5, 1, 3, 0, 5, 5, 0, 5, 0, 2, 5, 0, 0, 5, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6274, Accuracy: 0.2619, Precision: 0.6694, Recall: 0.2606, F1: 0.2722
Epoch 6/70
Train Loss: 0.2615, Accuracy: 0.9239, Precision: 0.8864, Recall: 0.8863, F1: 0.8862
Validation Loss: 0.4371, Accuracy: 0.8614, Precision: 0.8323, Recall: 0.8001, F1: 0.8087
Testing Loss: 0.3731, Accuracy: 0.8804, Precision: 0.8535, Recall: 0.8136, F1: 0.8283
LM Predictions:  [0, 3, 5, 0, 2, 0, 0, 2, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 0, 5, 5, 0, 5, 0, 2, 5, 0, 0, 5, 0, 0, 5, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0283, Accuracy: 0.2381, Precision: 0.6462, Recall: 0.2455, F1: 0.2421
Epoch 7/70
Train Loss: 0.2438, Accuracy: 0.9241, Precision: 0.8859, Recall: 0.8915, F1: 0.8883
Validation Loss: 0.4344, Accuracy: 0.8635, Precision: 0.8284, Recall: 0.8437, F1: 0.8331
Testing Loss: 0.3819, Accuracy: 0.8853, Precision: 0.8604, Recall: 0.8604, F1: 0.8581
LM Predictions:  [5, 3, 5, 0, 2, 0, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 5, 5, 0, 5, 0, 2, 5, 5, 0, 5, 0, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.6216, Accuracy: 0.2381, Precision: 0.6750, Recall: 0.2273, F1: 0.2722
Epoch 8/70
Train Loss: 0.2221, Accuracy: 0.9315, Precision: 0.8985, Recall: 0.8970, F1: 0.8974
Validation Loss: 0.4266, Accuracy: 0.8550, Precision: 0.8206, Recall: 0.8319, F1: 0.8199
Testing Loss: 0.4123, Accuracy: 0.8732, Precision: 0.8447, Recall: 0.8505, F1: 0.8439
LM Predictions:  [5, 3, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 1, 5, 5, 5, 0, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0695, Accuracy: 0.1667, Precision: 0.6500, Recall: 0.1636, F1: 0.2254
Epoch 9/70
Train Loss: 0.2094, Accuracy: 0.9369, Precision: 0.9003, Recall: 0.9100, F1: 0.9046
Validation Loss: 0.3854, Accuracy: 0.8742, Precision: 0.8359, Recall: 0.8479, F1: 0.8403
Testing Loss: 0.3852, Accuracy: 0.8913, Precision: 0.8634, Recall: 0.8608, F1: 0.8595
LM Predictions:  [5, 3, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 2, 5, 5, 5, 5, 0, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1963, Accuracy: 0.2143, Precision: 0.7500, Recall: 0.2121, F1: 0.2811
Epoch 10/70
Train Loss: 0.1971, Accuracy: 0.9346, Precision: 0.8992, Recall: 0.8980, F1: 0.8986
Validation Loss: 0.4166, Accuracy: 0.8678, Precision: 0.8332, Recall: 0.8710, F1: 0.8418
Testing Loss: 0.3999, Accuracy: 0.8877, Precision: 0.8633, Recall: 0.8830, F1: 0.8627
LM Predictions:  [5, 3, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 5, 5, 1, 3, 5, 5, 5, 5, 5, 5, 2, 5, 5, 5, 4, 0, 5, 5, 4, 5, 0, 5, 0, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.2605, Accuracy: 0.2143, Precision: 0.7500, Recall: 0.1939, F1: 0.2877
Epoch 11/70
Train Loss: 0.1852, Accuracy: 0.9417, Precision: 0.9090, Recall: 0.9175, F1: 0.9123
Validation Loss: 0.5929, Accuracy: 0.8742, Precision: 0.8334, Recall: 0.8284, F1: 0.8299
Testing Loss: 0.4711, Accuracy: 0.8780, Precision: 0.8488, Recall: 0.8046, F1: 0.8212
LM Predictions:  [5, 3, 5, 0, 2, 0, 0, 2, 5, 0, 0, 5, 5, 5, 3, 5, 0, 0, 5, 1, 3, 0, 2, 5, 5, 0, 0, 2, 5, 0, 0, 4, 0, 5, 3, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2244, Accuracy: 0.3095, Precision: 0.5907, Recall: 0.3106, F1: 0.2985
Epoch 12/70
Train Loss: 0.1574, Accuracy: 0.9483, Precision: 0.9160, Recall: 0.9219, F1: 0.9185
Validation Loss: 0.5371, Accuracy: 0.8657, Precision: 0.8121, Recall: 0.8146, F1: 0.8119
Testing Loss: 0.4360, Accuracy: 0.8780, Precision: 0.8495, Recall: 0.8219, F1: 0.8343
LM Predictions:  [0, 3, 5, 0, 2, 0, 0, 2, 5, 5, 5, 2, 5, 5, 2, 5, 2, 0, 5, 1, 3, 0, 2, 5, 0, 0, 0, 2, 2, 0, 5, 4, 0, 2, 3, 4, 3, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0007, Accuracy: 0.4048, Precision: 0.6521, Recall: 0.3727, F1: 0.3731
Epoch 13/70
Train Loss: 0.1484, Accuracy: 0.9526, Precision: 0.9235, Recall: 0.9283, F1: 0.9255
Validation Loss: 0.4594, Accuracy: 0.8806, Precision: 0.8338, Recall: 0.8489, F1: 0.8405
Testing Loss: 0.4067, Accuracy: 0.8841, Precision: 0.8580, Recall: 0.8476, F1: 0.8511
LM Predictions:  [0, 3, 4, 0, 2, 0, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5, 0, 0, 5, 1, 3, 0, 2, 5, 0, 5, 0, 2, 5, 0, 5, 4, 0, 5, 3, 4, 3, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.2204, Accuracy: 0.3571, Precision: 0.6763, Recall: 0.3242, F1: 0.3630
Epoch 14/70
Train Loss: 0.1441, Accuracy: 0.9528, Precision: 0.9260, Recall: 0.9321, F1: 0.9283
Validation Loss: 0.5590, Accuracy: 0.8721, Precision: 0.8295, Recall: 0.8506, F1: 0.8375
Testing Loss: 0.4892, Accuracy: 0.8732, Precision: 0.8486, Recall: 0.8474, F1: 0.8460
LM Predictions:  [0, 3, 5, 0, 2, 0, 5, 1, 5, 5, 5, 1, 5, 5, 3, 5, 0, 0, 1, 1, 3, 5, 4, 5, 0, 5, 0, 2, 2, 5, 5, 4, 0, 5, 3, 4, 3, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.6235, Accuracy: 0.3810, Precision: 0.6232, Recall: 0.3758, F1: 0.4186
Epoch 15/70
Train Loss: 0.1571, Accuracy: 0.9450, Precision: 0.9150, Recall: 0.9206, F1: 0.9175
Validation Loss: 0.5010, Accuracy: 0.8721, Precision: 0.8270, Recall: 0.8073, F1: 0.8090
Testing Loss: 0.4496, Accuracy: 0.8696, Precision: 0.8388, Recall: 0.7980, F1: 0.8102
LM Predictions:  [4, 3, 5, 0, 2, 0, 0, 1, 5, 0, 0, 5, 5, 5, 2, 5, 0, 0, 5, 1, 3, 0, 5, 5, 0, 0, 0, 2, 5, 0, 0, 5, 0, 5, 3, 4, 0, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.6513, Accuracy: 0.3571, Precision: 0.7105, Recall: 0.3591, F1: 0.3643
Epoch 16/70
Train Loss: 0.1337, Accuracy: 0.9526, Precision: 0.9237, Recall: 0.9258, F1: 0.9245
Validation Loss: 0.5499, Accuracy: 0.8657, Precision: 0.8026, Recall: 0.7991, F1: 0.7939
Testing Loss: 0.4799, Accuracy: 0.8671, Precision: 0.8240, Recall: 0.7903, F1: 0.7992
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 2, 4, 3, 2, 0, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 0, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.2687, Accuracy: 0.5952, Precision: 0.7878, Recall: 0.6382, F1: 0.6233
Epoch 17/70
Train Loss: 0.1213, Accuracy: 0.9566, Precision: 0.9296, Recall: 0.9336, F1: 0.9314
Validation Loss: 0.7089, Accuracy: 0.8593, Precision: 0.8231, Recall: 0.7951, F1: 0.8038
Testing Loss: 0.5747, Accuracy: 0.8684, Precision: 0.8370, Recall: 0.7949, F1: 0.8118
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 5, 2, 0, 0, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 5, 0, 0, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.1096, Accuracy: 0.5952, Precision: 0.7105, Recall: 0.5470, F1: 0.5334
Epoch 18/70
Train Loss: 0.1196, Accuracy: 0.9542, Precision: 0.9276, Recall: 0.9334, F1: 0.9301
Validation Loss: 0.6294, Accuracy: 0.8699, Precision: 0.8237, Recall: 0.8068, F1: 0.8031
Testing Loss: 0.5661, Accuracy: 0.8587, Precision: 0.8100, Recall: 0.7767, F1: 0.7844
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 0, 0, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 1, 3, 4, 3, 0, 0, 0, 4, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8835, Accuracy: 0.7619, Precision: 0.8203, Recall: 0.7927, F1: 0.7469
Epoch 19/70
Train Loss: 0.1069, Accuracy: 0.9573, Precision: 0.9340, Recall: 0.9326, F1: 0.9332
Validation Loss: 0.6111, Accuracy: 0.8721, Precision: 0.8231, Recall: 0.8227, F1: 0.8201
Testing Loss: 0.5538, Accuracy: 0.8732, Precision: 0.8364, Recall: 0.8085, F1: 0.8180
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 0, 0, 1, 4, 5, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 3, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7214, Accuracy: 0.7619, Precision: 0.7153, Recall: 0.6576, F1: 0.6527
Epoch 20/70
Train Loss: 0.1169, Accuracy: 0.9547, Precision: 0.9331, Recall: 0.9314, F1: 0.9322
Validation Loss: 0.5739, Accuracy: 0.8657, Precision: 0.8256, Recall: 0.8112, F1: 0.8164
Testing Loss: 0.5067, Accuracy: 0.8708, Precision: 0.8430, Recall: 0.7996, F1: 0.8173
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 1, 2, 3, 2, 0, 1, 1, 3, 0, 4, 2, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.9564, Accuracy: 0.6429, Precision: 0.6437, Recall: 0.5788, F1: 0.5536
Epoch 21/70
Train Loss: 0.1062, Accuracy: 0.9564, Precision: 0.9320, Recall: 0.9347, F1: 0.9332
Validation Loss: 0.7100, Accuracy: 0.8699, Precision: 0.8308, Recall: 0.8348, F1: 0.8301
Testing Loss: 0.5302, Accuracy: 0.8768, Precision: 0.8327, Recall: 0.8230, F1: 0.8273
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 5, 0, 5, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 2, 3, 5, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 4]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.6752, Accuracy: 0.6905, Precision: 0.6852, Recall: 0.5758, F1: 0.5951
Epoch 22/70
Train Loss: 0.1014, Accuracy: 0.9599, Precision: 0.9335, Recall: 0.9478, F1: 0.9396
Validation Loss: 0.5548, Accuracy: 0.8721, Precision: 0.8362, Recall: 0.8451, F1: 0.8365
Testing Loss: 0.5097, Accuracy: 0.8684, Precision: 0.8443, Recall: 0.8295, F1: 0.8337
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 5, 0, 1, 1, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 1, 3, 5, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8478, Accuracy: 0.7143, Precision: 0.6667, Recall: 0.6152, F1: 0.6016
Epoch 23/70
Train Loss: 0.0917, Accuracy: 0.9611, Precision: 0.9406, Recall: 0.9402, F1: 0.9403
Validation Loss: 0.6274, Accuracy: 0.8763, Precision: 0.8354, Recall: 0.8213, F1: 0.8262
Testing Loss: 0.5955, Accuracy: 0.8708, Precision: 0.8464, Recall: 0.8147, F1: 0.8281
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 0, 0, 1, 1, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 5, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7077, Accuracy: 0.7143, Precision: 0.6810, Recall: 0.6091, F1: 0.6081
Epoch 24/70
Train Loss: 0.0960, Accuracy: 0.9594, Precision: 0.9336, Recall: 0.9404, F1: 0.9367
Validation Loss: 0.7174, Accuracy: 0.8529, Precision: 0.7977, Recall: 0.8020, F1: 0.7960
Testing Loss: 0.6650, Accuracy: 0.8623, Precision: 0.8183, Recall: 0.7993, F1: 0.8067
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 0, 3, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 0, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5401, Accuracy: 0.7619, Precision: 0.8302, Recall: 0.7964, F1: 0.7683
Epoch 25/70
Train Loss: 0.0901, Accuracy: 0.9604, Precision: 0.9388, Recall: 0.9367, F1: 0.9377
Validation Loss: 0.7065, Accuracy: 0.8699, Precision: 0.8263, Recall: 0.8121, F1: 0.8170
Testing Loss: 0.7029, Accuracy: 0.8744, Precision: 0.8458, Recall: 0.8092, F1: 0.8237
LM Predictions:  [0, 3, 4, 0, 2, 0, 4, 1, 4, 2, 0, 1, 5, 3, 2, 3, 0, 0, 1, 1, 3, 4, 4, 4, 0, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4747, Accuracy: 0.7381, Precision: 0.6914, Recall: 0.6439, F1: 0.6341
Epoch 26/70
Train Loss: 0.0940, Accuracy: 0.9594, Precision: 0.9359, Recall: 0.9411, F1: 0.9383
Validation Loss: 0.7446, Accuracy: 0.8614, Precision: 0.8095, Recall: 0.8025, F1: 0.8023
Testing Loss: 0.6964, Accuracy: 0.8599, Precision: 0.8252, Recall: 0.7900, F1: 0.8020
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 1, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 2, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4655, Accuracy: 0.7857, Precision: 0.8389, Recall: 0.7927, F1: 0.7761
Epoch 27/70
Train Loss: 0.0735, Accuracy: 0.9625, Precision: 0.9374, Recall: 0.9429, F1: 0.9400
Validation Loss: 0.7647, Accuracy: 0.8657, Precision: 0.8631, Recall: 0.7704, F1: 0.7739
Testing Loss: 0.8094, Accuracy: 0.8490, Precision: 0.8527, Recall: 0.7444, F1: 0.7547
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 0, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.9462, Accuracy: 0.6667, Precision: 0.8526, Recall: 0.7182, F1: 0.7064
Epoch 28/70
Train Loss: 0.0841, Accuracy: 0.9616, Precision: 0.9396, Recall: 0.9405, F1: 0.9400
Validation Loss: 0.8573, Accuracy: 0.8614, Precision: 0.8051, Recall: 0.8019, F1: 0.7952
Testing Loss: 0.7458, Accuracy: 0.8599, Precision: 0.7822, Recall: 0.7784, F1: 0.7719
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3109, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9000, F1: 0.8876
Epoch 29/70
Train Loss: 0.0820, Accuracy: 0.9642, Precision: 0.9431, Recall: 0.9443, F1: 0.9437
Validation Loss: 0.7196, Accuracy: 0.8657, Precision: 0.8189, Recall: 0.8257, F1: 0.8208
Testing Loss: 0.7257, Accuracy: 0.8635, Precision: 0.8294, Recall: 0.8078, F1: 0.8174
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4610, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7727, F1: 0.7617
Epoch 30/70
Train Loss: 0.0760, Accuracy: 0.9649, Precision: 0.9466, Recall: 0.9436, F1: 0.9451
Validation Loss: 0.6889, Accuracy: 0.8721, Precision: 0.8328, Recall: 0.8317, F1: 0.8313
Testing Loss: 0.7156, Accuracy: 0.8659, Precision: 0.8419, Recall: 0.8152, F1: 0.8266
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 5, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2968, Accuracy: 0.8810, Precision: 0.7480, Recall: 0.7212, F1: 0.7257
Epoch 31/70
Train Loss: 0.0660, Accuracy: 0.9694, Precision: 0.9513, Recall: 0.9558, F1: 0.9534
Validation Loss: 0.8144, Accuracy: 0.8593, Precision: 0.8094, Recall: 0.8032, F1: 0.8030
Testing Loss: 0.7907, Accuracy: 0.8611, Precision: 0.8271, Recall: 0.7924, F1: 0.8035
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 2, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2459, Accuracy: 0.9286, Precision: 0.9359, Recall: 0.9218, F1: 0.9229
Epoch 32/70
Train Loss: 0.0688, Accuracy: 0.9644, Precision: 0.9464, Recall: 0.9433, F1: 0.9448
Validation Loss: 0.8130, Accuracy: 0.8593, Precision: 0.8043, Recall: 0.7809, F1: 0.7823
Testing Loss: 0.8255, Accuracy: 0.8514, Precision: 0.7992, Recall: 0.7644, F1: 0.7744
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 0, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2984, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8691, F1: 0.8512
Epoch 33/70
Train Loss: 0.0690, Accuracy: 0.9651, Precision: 0.9463, Recall: 0.9445, F1: 0.9454
Validation Loss: 0.7975, Accuracy: 0.8699, Precision: 0.8342, Recall: 0.8229, F1: 0.8254
Testing Loss: 0.7624, Accuracy: 0.8684, Precision: 0.8322, Recall: 0.8040, F1: 0.8153
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2714, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9218, F1: 0.8980
Epoch 34/70
Train Loss: 0.0722, Accuracy: 0.9640, Precision: 0.9434, Recall: 0.9420, F1: 0.9426
Validation Loss: 0.9364, Accuracy: 0.8657, Precision: 0.8231, Recall: 0.8042, F1: 0.8087
Testing Loss: 0.7931, Accuracy: 0.8732, Precision: 0.8401, Recall: 0.8026, F1: 0.8162
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2403, Accuracy: 0.8810, Precision: 0.8929, Recall: 0.9036, F1: 0.8799
Epoch 35/70
Train Loss: 0.0628, Accuracy: 0.9659, Precision: 0.9438, Recall: 0.9475, F1: 0.9456
Validation Loss: 0.8629, Accuracy: 0.8721, Precision: 0.8297, Recall: 0.8332, F1: 0.8307
Testing Loss: 0.8106, Accuracy: 0.8659, Precision: 0.8385, Recall: 0.8178, F1: 0.8263
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 5, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3510, Accuracy: 0.8810, Precision: 0.7593, Recall: 0.7394, F1: 0.7302
Epoch 36/70
Train Loss: 0.0715, Accuracy: 0.9628, Precision: 0.9414, Recall: 0.9448, F1: 0.9431
Validation Loss: 0.8653, Accuracy: 0.8635, Precision: 0.8185, Recall: 0.8138, F1: 0.8129
Testing Loss: 0.7883, Accuracy: 0.8671, Precision: 0.8404, Recall: 0.8106, F1: 0.8234
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2920, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8127, F1: 0.7887
Epoch 37/70
Train Loss: 0.0589, Accuracy: 0.9668, Precision: 0.9483, Recall: 0.9433, F1: 0.9457
Validation Loss: 0.7953, Accuracy: 0.8657, Precision: 0.8173, Recall: 0.8013, F1: 0.8032
Testing Loss: 0.8602, Accuracy: 0.8551, Precision: 0.8265, Recall: 0.7766, F1: 0.7917
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 2, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2992, Accuracy: 0.8333, Precision: 0.8623, Recall: 0.8491, F1: 0.8256
Epoch 38/70
Train Loss: 0.0932, Accuracy: 0.9609, Precision: 0.9416, Recall: 0.9475, F1: 0.9445
Validation Loss: 0.8426, Accuracy: 0.8166, Precision: 0.7838, Recall: 0.7719, F1: 0.7705
Testing Loss: 0.7317, Accuracy: 0.8430, Precision: 0.8375, Recall: 0.7963, F1: 0.8092
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 4, 4, 0, 2, 1, 4, 3, 2, 3, 0, 0, 0, 1, 3, 0, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 0, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8273, Accuracy: 0.6905, Precision: 0.8422, Recall: 0.6909, F1: 0.6715
Epoch 39/70
Train Loss: 0.0880, Accuracy: 0.9609, Precision: 0.9399, Recall: 0.9447, F1: 0.9422
Validation Loss: 0.9461, Accuracy: 0.8507, Precision: 0.8082, Recall: 0.8062, F1: 0.8058
Testing Loss: 0.7415, Accuracy: 0.8732, Precision: 0.8407, Recall: 0.8203, F1: 0.8294
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3648, Accuracy: 0.7619, Precision: 0.8667, Recall: 0.8127, F1: 0.7887
Epoch 40/70
Train Loss: 0.0576, Accuracy: 0.9673, Precision: 0.9452, Recall: 0.9542, F1: 0.9495
Validation Loss: 0.8486, Accuracy: 0.8614, Precision: 0.8136, Recall: 0.8025, F1: 0.8043
Testing Loss: 0.7646, Accuracy: 0.8744, Precision: 0.8484, Recall: 0.8087, F1: 0.8243
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 5, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3247, Accuracy: 0.7857, Precision: 0.7308, Recall: 0.6955, F1: 0.6752
Epoch 41/70
Train Loss: 0.0566, Accuracy: 0.9699, Precision: 0.9549, Recall: 0.9480, F1: 0.9513
Validation Loss: 0.8962, Accuracy: 0.8507, Precision: 0.7928, Recall: 0.7752, F1: 0.7719
Testing Loss: 0.8035, Accuracy: 0.8623, Precision: 0.8187, Recall: 0.7824, F1: 0.7941
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1955, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8855, F1: 0.8581
Epoch 42/70
Train Loss: 0.0741, Accuracy: 0.9647, Precision: 0.9442, Recall: 0.9479, F1: 0.9459
Validation Loss: 0.9267, Accuracy: 0.8486, Precision: 0.7959, Recall: 0.8024, F1: 0.7968
Testing Loss: 0.7116, Accuracy: 0.8756, Precision: 0.8379, Recall: 0.8256, F1: 0.8311
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 1, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2750, Accuracy: 0.8810, Precision: 0.8778, Recall: 0.9018, F1: 0.8652
Epoch 43/70
Train Loss: 0.0656, Accuracy: 0.9663, Precision: 0.9482, Recall: 0.9450, F1: 0.9466
Validation Loss: 0.9036, Accuracy: 0.8550, Precision: 0.8036, Recall: 0.8032, F1: 0.8008
Testing Loss: 0.7447, Accuracy: 0.8768, Precision: 0.8464, Recall: 0.8139, F1: 0.8262
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1883, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9800, F1: 0.9713
Epoch 44/70
Train Loss: 0.0728, Accuracy: 0.9647, Precision: 0.9437, Recall: 0.9432, F1: 0.9435
Validation Loss: 0.9617, Accuracy: 0.8614, Precision: 0.7985, Recall: 0.8028, F1: 0.7969
Testing Loss: 0.8195, Accuracy: 0.8756, Precision: 0.8407, Recall: 0.8037, F1: 0.8148
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2776, Accuracy: 0.8571, Precision: 0.8909, Recall: 0.8909, F1: 0.8618
Epoch 45/70
Train Loss: 0.0645, Accuracy: 0.9656, Precision: 0.9446, Recall: 0.9469, F1: 0.9457
Validation Loss: 0.9239, Accuracy: 0.8529, Precision: 0.7913, Recall: 0.7954, F1: 0.7894
Testing Loss: 0.7635, Accuracy: 0.8804, Precision: 0.8472, Recall: 0.8243, F1: 0.8333
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 0, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2377, Accuracy: 0.8571, Precision: 0.8714, Recall: 0.8909, F1: 0.8571
Epoch 46/70
Train Loss: 0.0620, Accuracy: 0.9680, Precision: 0.9478, Recall: 0.9505, F1: 0.9492
Validation Loss: 0.9930, Accuracy: 0.8550, Precision: 0.7948, Recall: 0.8057, F1: 0.7970
Testing Loss: 0.7811, Accuracy: 0.8732, Precision: 0.8313, Recall: 0.8097, F1: 0.8166
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2731, Accuracy: 0.8095, Precision: 0.8611, Recall: 0.8473, F1: 0.8181
Epoch 47/70
Train Loss: 0.0586, Accuracy: 0.9692, Precision: 0.9509, Recall: 0.9491, F1: 0.9500
Validation Loss: 0.9703, Accuracy: 0.8486, Precision: 0.7964, Recall: 0.7886, F1: 0.7868
Testing Loss: 0.7972, Accuracy: 0.8527, Precision: 0.8038, Recall: 0.7861, F1: 0.7900
LM Predictions:  [0, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2059, Accuracy: 0.9048, Precision: 0.9050, Recall: 0.9273, F1: 0.9053
Epoch 48/70
Train Loss: 0.0531, Accuracy: 0.9689, Precision: 0.9543, Recall: 0.9481, F1: 0.9512
Validation Loss: 1.0278, Accuracy: 0.8614, Precision: 0.8044, Recall: 0.8106, F1: 0.8017
Testing Loss: 0.8815, Accuracy: 0.8768, Precision: 0.8427, Recall: 0.8183, F1: 0.8255
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2465, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9073, F1: 0.8784
Epoch 49/70
Train Loss: 0.0577, Accuracy: 0.9685, Precision: 0.9459, Recall: 0.9563, F1: 0.9508
Validation Loss: 0.8155, Accuracy: 0.8465, Precision: 0.7854, Recall: 0.7896, F1: 0.7836
Testing Loss: 0.7706, Accuracy: 0.8635, Precision: 0.8319, Recall: 0.7980, F1: 0.8100
LM Predictions:  [0, 3, 4, 0, 2, 1, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4047, Accuracy: 0.7857, Precision: 0.8436, Recall: 0.8291, F1: 0.7914
Epoch 50/70
Train Loss: 0.0721, Accuracy: 0.9642, Precision: 0.9506, Recall: 0.9357, F1: 0.9427
Validation Loss: 0.8712, Accuracy: 0.8635, Precision: 0.8141, Recall: 0.8380, F1: 0.8243
Testing Loss: 0.7295, Accuracy: 0.8792, Precision: 0.8436, Recall: 0.8294, F1: 0.8346
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2664, Accuracy: 0.8095, Precision: 0.8611, Recall: 0.8473, F1: 0.8181
Epoch 51/70
Train Loss: 0.0543, Accuracy: 0.9682, Precision: 0.9485, Recall: 0.9504, F1: 0.9494
Validation Loss: 0.8501, Accuracy: 0.8614, Precision: 0.8104, Recall: 0.8233, F1: 0.8145
Testing Loss: 0.8108, Accuracy: 0.8671, Precision: 0.8270, Recall: 0.8132, F1: 0.8188
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1783, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9218, F1: 0.9003
Epoch 52/70
Train Loss: 0.0528, Accuracy: 0.9687, Precision: 0.9459, Recall: 0.9541, F1: 0.9498
Validation Loss: 0.8309, Accuracy: 0.8465, Precision: 0.8071, Recall: 0.8314, F1: 0.8140
Testing Loss: 0.8130, Accuracy: 0.8659, Precision: 0.8306, Recall: 0.8359, F1: 0.8301
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 5, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2915, Accuracy: 0.8571, Precision: 0.7292, Recall: 0.7242, F1: 0.7174
Epoch 53/70
Train Loss: 0.0559, Accuracy: 0.9663, Precision: 0.9464, Recall: 0.9444, F1: 0.9454
Validation Loss: 0.8462, Accuracy: 0.8678, Precision: 0.8232, Recall: 0.8045, F1: 0.8054
Testing Loss: 0.8092, Accuracy: 0.8563, Precision: 0.8160, Recall: 0.7745, F1: 0.7857
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1772, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 54/70
Train Loss: 0.0524, Accuracy: 0.9699, Precision: 0.9504, Recall: 0.9540, F1: 0.9521
Validation Loss: 0.9106, Accuracy: 0.8699, Precision: 0.8165, Recall: 0.8250, F1: 0.8180
Testing Loss: 0.8841, Accuracy: 0.8671, Precision: 0.8302, Recall: 0.8098, F1: 0.8157
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 4, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1748, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 55/70
Train Loss: 0.0482, Accuracy: 0.9687, Precision: 0.9474, Recall: 0.9495, F1: 0.9484
Validation Loss: 0.9543, Accuracy: 0.8721, Precision: 0.8309, Recall: 0.8260, F1: 0.8258
Testing Loss: 0.9325, Accuracy: 0.8696, Precision: 0.8401, Recall: 0.8037, F1: 0.8174
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3804, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 56/70
Train Loss: 0.0613, Accuracy: 0.9659, Precision: 0.9449, Recall: 0.9474, F1: 0.9461
Validation Loss: 0.8576, Accuracy: 0.8721, Precision: 0.8280, Recall: 0.8261, F1: 0.8232
Testing Loss: 0.8087, Accuracy: 0.8659, Precision: 0.8310, Recall: 0.7878, F1: 0.8028
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2290, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8727, F1: 0.8416
Epoch 57/70
Train Loss: 0.0555, Accuracy: 0.9706, Precision: 0.9513, Recall: 0.9549, F1: 0.9531
Validation Loss: 1.0206, Accuracy: 0.8593, Precision: 0.7988, Recall: 0.8010, F1: 0.7930
Testing Loss: 0.8828, Accuracy: 0.8647, Precision: 0.8262, Recall: 0.7967, F1: 0.8061
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1372, Accuracy: 0.9762, Precision: 0.9833, Recall: 0.9818, F1: 0.9818
Epoch 58/70
Train Loss: 0.0583, Accuracy: 0.9704, Precision: 0.9554, Recall: 0.9459, F1: 0.9504
Validation Loss: 0.7644, Accuracy: 0.8401, Precision: 0.7926, Recall: 0.8217, F1: 0.8000
Testing Loss: 0.7091, Accuracy: 0.8490, Precision: 0.8080, Recall: 0.8119, F1: 0.8076
LM Predictions:  [4, 3, 3, 0, 2, 0, 4, 1, 3, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 5, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3852, Accuracy: 0.7619, Precision: 0.6856, Recall: 0.6576, F1: 0.6515
Epoch 59/70
Train Loss: 0.0799, Accuracy: 0.9621, Precision: 0.9368, Recall: 0.9472, F1: 0.9418
Validation Loss: 0.7349, Accuracy: 0.8507, Precision: 0.8003, Recall: 0.8272, F1: 0.8115
Testing Loss: 0.7172, Accuracy: 0.8659, Precision: 0.8312, Recall: 0.8316, F1: 0.8305
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1703, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 60/70
Train Loss: 0.0531, Accuracy: 0.9699, Precision: 0.9503, Recall: 0.9582, F1: 0.9541
Validation Loss: 0.8484, Accuracy: 0.8635, Precision: 0.8153, Recall: 0.8084, F1: 0.8072
Testing Loss: 0.8089, Accuracy: 0.8635, Precision: 0.8212, Recall: 0.7954, F1: 0.8037
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2505, Accuracy: 0.7381, Precision: 0.8417, Recall: 0.7945, F1: 0.7676
Epoch 61/70
Train Loss: 0.0485, Accuracy: 0.9670, Precision: 0.9507, Recall: 0.9490, F1: 0.9498
Validation Loss: 0.9270, Accuracy: 0.8593, Precision: 0.8105, Recall: 0.7991, F1: 0.8000
Testing Loss: 0.8814, Accuracy: 0.8659, Precision: 0.8298, Recall: 0.7897, F1: 0.8038
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1922, Accuracy: 0.9286, Precision: 0.9250, Recall: 0.9455, F1: 0.9223
Epoch 62/70
Train Loss: 0.0450, Accuracy: 0.9715, Precision: 0.9550, Recall: 0.9532, F1: 0.9540
Validation Loss: 0.9099, Accuracy: 0.8699, Precision: 0.8357, Recall: 0.8293, F1: 0.8314
Testing Loss: 0.9064, Accuracy: 0.8647, Precision: 0.8428, Recall: 0.8048, F1: 0.8209
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2090, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8309, F1: 0.8068
Epoch 63/70
Train Loss: 0.0466, Accuracy: 0.9708, Precision: 0.9513, Recall: 0.9585, F1: 0.9547
Validation Loss: 0.8627, Accuracy: 0.8614, Precision: 0.8185, Recall: 0.8021, F1: 0.8052
Testing Loss: 0.9130, Accuracy: 0.8502, Precision: 0.8158, Recall: 0.7685, F1: 0.7836
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2049, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8873, F1: 0.8580
Epoch 64/70
Train Loss: 0.0804, Accuracy: 0.9618, Precision: 0.9420, Recall: 0.9349, F1: 0.9383
Validation Loss: 0.9455, Accuracy: 0.8678, Precision: 0.8260, Recall: 0.8194, F1: 0.8198
Testing Loss: 0.8170, Accuracy: 0.8671, Precision: 0.8362, Recall: 0.8111, F1: 0.8215
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3124, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8527, F1: 0.8246
Epoch 65/70
Train Loss: 0.0608, Accuracy: 0.9696, Precision: 0.9500, Recall: 0.9513, F1: 0.9507
Validation Loss: 0.8788, Accuracy: 0.8550, Precision: 0.7872, Recall: 0.7802, F1: 0.7753
Testing Loss: 0.7977, Accuracy: 0.8575, Precision: 0.8309, Recall: 0.7727, F1: 0.7870
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2272, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 66/70
Train Loss: 0.0604, Accuracy: 0.9685, Precision: 0.9447, Recall: 0.9562, F1: 0.9501
Validation Loss: 0.9967, Accuracy: 0.8657, Precision: 0.8176, Recall: 0.7935, F1: 0.7982
Testing Loss: 0.9384, Accuracy: 0.8575, Precision: 0.8264, Recall: 0.7858, F1: 0.8003
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 5, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2348, Accuracy: 0.9762, Precision: 0.8333, Recall: 0.8182, F1: 0.8254
Epoch 67/70
Train Loss: 0.0551, Accuracy: 0.9706, Precision: 0.9530, Recall: 0.9516, F1: 0.9523
Validation Loss: 1.1047, Accuracy: 0.8465, Precision: 0.8007, Recall: 0.8184, F1: 0.8062
Testing Loss: 0.8923, Accuracy: 0.8563, Precision: 0.8179, Recall: 0.8179, F1: 0.8167
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 3, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2254, Accuracy: 0.8810, Precision: 0.8825, Recall: 0.9018, F1: 0.8745
Epoch 68/70
Train Loss: 0.0640, Accuracy: 0.9689, Precision: 0.9502, Recall: 0.9562, F1: 0.9529
Validation Loss: 1.0621, Accuracy: 0.8401, Precision: 0.7953, Recall: 0.7945, F1: 0.7911
Testing Loss: 0.8946, Accuracy: 0.8575, Precision: 0.8261, Recall: 0.7958, F1: 0.8043
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 4, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3462, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 69/70
Train Loss: 0.0891, Accuracy: 0.9649, Precision: 0.9574, Recall: 0.9311, F1: 0.9422
Validation Loss: 0.9688, Accuracy: 0.8422, Precision: 0.7770, Recall: 0.7784, F1: 0.7741
Testing Loss: 0.8374, Accuracy: 0.8635, Precision: 0.8286, Recall: 0.7974, F1: 0.8090
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1218, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 70/70
Train Loss: 0.0683, Accuracy: 0.9689, Precision: 0.9459, Recall: 0.9576, F1: 0.9514
Validation Loss: 1.0293, Accuracy: 0.8507, Precision: 0.7993, Recall: 0.7928, F1: 0.7905
Testing Loss: 0.8675, Accuracy: 0.8647, Precision: 0.8238, Recall: 0.7989, F1: 0.8067
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2506, Accuracy: 0.8333, Precision: 0.8659, Recall: 0.8691, F1: 0.8406
For later layers:  [8, 9, 10, 11]
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([50265, 768]), req grad: True
Layer: backbone.embeddings.token_type_embeddings.weight, Size: torch.Size([1, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([4098, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.0.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.0.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.0.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.1.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.1.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.1.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.2.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.2.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.2.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.3.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.3.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.3.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.4.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.4.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.4.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.5.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.5.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.5.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.6.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.6.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.6.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.attention.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.7.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.7.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.7.output.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.8.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.8.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.8.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.9.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.9.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.9.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.10.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.10.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.10.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.query_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.key_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.self.value_global.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.encoder.layer.11.attention.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.encoder.layer.11.intermediate.dense.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.encoder.layer.11.output.dense.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.pooler.dense.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.pooler.dense.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Epoch 1/70
Train Loss: 0.6087, Accuracy: 0.8093, Precision: 0.7823, Recall: 0.7253, F1: 0.7460
Validation Loss: 0.3507, Accuracy: 0.8870, Precision: 0.8607, Recall: 0.8174, F1: 0.8264
Testing Loss: 0.3214, Accuracy: 0.9034, Precision: 0.9055, Recall: 0.8375, F1: 0.8567
LM Predictions:  [5, 5, 0, 0, 0, 0, 0, 2, 5, 0, 0, 5, 5, 5, 5, 5, 0, 0, 5, 0, 3, 0, 2, 5, 0, 5, 0, 5, 5, 5, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 3.0764, Accuracy: 0.0952, Precision: 0.3529, Recall: 0.0985, F1: 0.0884
Epoch 2/70
Train Loss: 0.3392, Accuracy: 0.8985, Precision: 0.8566, Recall: 0.8504, F1: 0.8533
Validation Loss: 0.3475, Accuracy: 0.8699, Precision: 0.8267, Recall: 0.8296, F1: 0.8270
Testing Loss: 0.2977, Accuracy: 0.9010, Precision: 0.8748, Recall: 0.8620, F1: 0.8667
LM Predictions:  [5, 3, 3, 0, 5, 5, 5, 2, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 1, 5, 0, 5, 0, 2, 5, 5, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.5428, Accuracy: 0.1667, Precision: 0.4815, Recall: 0.1636, F1: 0.1999
Epoch 3/70
Train Loss: 0.2663, Accuracy: 0.9165, Precision: 0.8780, Recall: 0.8753, F1: 0.8765
Validation Loss: 0.3699, Accuracy: 0.8785, Precision: 0.8384, Recall: 0.8653, F1: 0.8491
Testing Loss: 0.3042, Accuracy: 0.9046, Precision: 0.8773, Recall: 0.8813, F1: 0.8765
LM Predictions:  [5, 5, 5, 3, 2, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 5, 5, 0, 5, 0, 5, 2, 5, 5, 5, 5, 5, 5, 4, 5, 0, 5, 2, 5, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.1590, Accuracy: 0.1905, Precision: 0.6250, Recall: 0.1606, F1: 0.2439
Epoch 4/70
Train Loss: 0.2177, Accuracy: 0.9329, Precision: 0.8991, Recall: 0.9057, F1: 0.9018
Validation Loss: 0.3656, Accuracy: 0.8827, Precision: 0.8469, Recall: 0.8461, F1: 0.8462
Testing Loss: 0.4151, Accuracy: 0.8829, Precision: 0.8589, Recall: 0.8431, F1: 0.8496
LM Predictions:  [5, 3, 5, 0, 2, 0, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 0, 5, 0, 5, 2, 5, 0, 5, 5, 5, 5, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.3736, Accuracy: 0.1905, Precision: 0.6167, Recall: 0.1788, F1: 0.2325
Epoch 5/70
Train Loss: 0.1830, Accuracy: 0.9383, Precision: 0.9039, Recall: 0.9070, F1: 0.9052
Validation Loss: 0.4203, Accuracy: 0.8763, Precision: 0.8381, Recall: 0.8560, F1: 0.8452
Testing Loss: 0.3642, Accuracy: 0.8986, Precision: 0.8743, Recall: 0.8777, F1: 0.8751
LM Predictions:  [5, 3, 4, 5, 2, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 5, 1, 3, 5, 5, 5, 5, 5, 0, 2, 5, 5, 0, 5, 5, 5, 3, 4, 5, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.7083, Accuracy: 0.2619, Precision: 0.7143, Recall: 0.2258, F1: 0.3107
Epoch 6/70
Train Loss: 0.1745, Accuracy: 0.9405, Precision: 0.9052, Recall: 0.9083, F1: 0.9067
Validation Loss: 0.4352, Accuracy: 0.8806, Precision: 0.8388, Recall: 0.8310, F1: 0.8329
Testing Loss: 0.3774, Accuracy: 0.8913, Precision: 0.8667, Recall: 0.8296, F1: 0.8447
LM Predictions:  [0, 5, 4, 0, 2, 0, 0, 5, 4, 0, 5, 1, 5, 5, 5, 5, 2, 0, 5, 1, 3, 5, 5, 5, 0, 0, 0, 2, 2, 0, 0, 5, 5, 5, 3, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 2.0919, Accuracy: 0.3333, Precision: 0.6694, Recall: 0.3242, F1: 0.3556
Epoch 7/70
Train Loss: 0.1396, Accuracy: 0.9512, Precision: 0.9192, Recall: 0.9266, F1: 0.9222
Validation Loss: 0.4776, Accuracy: 0.8742, Precision: 0.8344, Recall: 0.8256, F1: 0.8269
Testing Loss: 0.3834, Accuracy: 0.8901, Precision: 0.8659, Recall: 0.8242, F1: 0.8404
LM Predictions:  [0, 3, 4, 3, 2, 0, 3, 1, 4, 0, 2, 1, 4, 5, 1, 3, 2, 0, 1, 1, 3, 5, 5, 5, 0, 0, 0, 2, 2, 0, 0, 4, 5, 5, 3, 4, 0, 0, 5, 0, 0, 5]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 1.3458, Accuracy: 0.5238, Precision: 0.6235, Recall: 0.4864, F1: 0.4991
Epoch 8/70
Train Loss: 0.1353, Accuracy: 0.9545, Precision: 0.9273, Recall: 0.9311, F1: 0.9290
Validation Loss: 0.4826, Accuracy: 0.8614, Precision: 0.7987, Recall: 0.7909, F1: 0.7875
Testing Loss: 0.4304, Accuracy: 0.8780, Precision: 0.8455, Recall: 0.7974, F1: 0.8125
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 0, 2, 1, 4, 3, 1, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 1, 3, 4, 0, 0, 5, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.9928, Accuracy: 0.6190, Precision: 0.6268, Recall: 0.5667, F1: 0.5349
Epoch 9/70
Train Loss: 0.1239, Accuracy: 0.9557, Precision: 0.9287, Recall: 0.9308, F1: 0.9296
Validation Loss: 0.5622, Accuracy: 0.8742, Precision: 0.8269, Recall: 0.8113, F1: 0.8109
Testing Loss: 0.4901, Accuracy: 0.8792, Precision: 0.8514, Recall: 0.7995, F1: 0.8168
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 0, 1, 5, 3, 1, 3, 0, 0, 1, 1, 3, 0, 1, 4, 3, 0, 0, 2, 2, 0, 0, 4, 0, 0, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.8660, Accuracy: 0.5714, Precision: 0.6074, Recall: 0.5561, F1: 0.4974
Epoch 10/70
Train Loss: 0.1115, Accuracy: 0.9583, Precision: 0.9349, Recall: 0.9398, F1: 0.9371
Validation Loss: 0.4123, Accuracy: 0.8827, Precision: 0.8720, Recall: 0.8170, F1: 0.8306
Testing Loss: 0.4383, Accuracy: 0.8708, Precision: 0.8468, Recall: 0.7778, F1: 0.7960
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 0, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 0, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.7251, Accuracy: 0.6667, Precision: 0.8222, Recall: 0.7382, F1: 0.7102
Epoch 11/70
Train Loss: 0.0949, Accuracy: 0.9594, Precision: 0.9361, Recall: 0.9363, F1: 0.9361
Validation Loss: 0.6146, Accuracy: 0.8550, Precision: 0.8112, Recall: 0.8092, F1: 0.8057
Testing Loss: 0.4895, Accuracy: 0.8732, Precision: 0.8284, Recall: 0.8024, F1: 0.8106
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 0, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3930, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9055, F1: 0.8816
Epoch 12/70
Train Loss: 0.0905, Accuracy: 0.9604, Precision: 0.9371, Recall: 0.9393, F1: 0.9382
Validation Loss: 0.5928, Accuracy: 0.8678, Precision: 0.8271, Recall: 0.8140, F1: 0.8154
Testing Loss: 0.5676, Accuracy: 0.8756, Precision: 0.8444, Recall: 0.8053, F1: 0.8184
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 2, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.5070, Accuracy: 0.8095, Precision: 0.8400, Recall: 0.8273, F1: 0.8142
Epoch 13/70
Train Loss: 0.0821, Accuracy: 0.9644, Precision: 0.9433, Recall: 0.9446, F1: 0.9439
Validation Loss: 0.6667, Accuracy: 0.8465, Precision: 0.7970, Recall: 0.8059, F1: 0.7996
Testing Loss: 0.5429, Accuracy: 0.8756, Precision: 0.8401, Recall: 0.8275, F1: 0.8330
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 5, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2996, Accuracy: 0.9048, Precision: 0.7626, Recall: 0.7530, F1: 0.7560
Epoch 14/70
Train Loss: 0.0838, Accuracy: 0.9616, Precision: 0.9398, Recall: 0.9432, F1: 0.9415
Validation Loss: 0.6201, Accuracy: 0.8678, Precision: 0.8270, Recall: 0.8241, F1: 0.8249
Testing Loss: 0.5943, Accuracy: 0.8792, Precision: 0.8526, Recall: 0.8314, F1: 0.8407
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 5, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3987, Accuracy: 0.7857, Precision: 0.7222, Recall: 0.6727, F1: 0.6701
Epoch 15/70
Train Loss: 0.0709, Accuracy: 0.9654, Precision: 0.9421, Recall: 0.9506, F1: 0.9461
Validation Loss: 0.6259, Accuracy: 0.8657, Precision: 0.8251, Recall: 0.8191, F1: 0.8202
Testing Loss: 0.6834, Accuracy: 0.8671, Precision: 0.8391, Recall: 0.8172, F1: 0.8255
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3292, Accuracy: 0.8810, Precision: 0.9000, Recall: 0.9055, F1: 0.8795
Epoch 16/70
Train Loss: 0.0838, Accuracy: 0.9590, Precision: 0.9372, Recall: 0.9350, F1: 0.9361
Validation Loss: 0.6502, Accuracy: 0.8678, Precision: 0.8232, Recall: 0.8278, F1: 0.8246
Testing Loss: 0.5706, Accuracy: 0.8732, Precision: 0.8412, Recall: 0.8220, F1: 0.8304
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 5, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4290, Accuracy: 0.8333, Precision: 0.7424, Recall: 0.7197, F1: 0.6994
Epoch 17/70
Train Loss: 0.0823, Accuracy: 0.9602, Precision: 0.9405, Recall: 0.9372, F1: 0.9388
Validation Loss: 0.6846, Accuracy: 0.8721, Precision: 0.8273, Recall: 0.8187, F1: 0.8200
Testing Loss: 0.5678, Accuracy: 0.8865, Precision: 0.8551, Recall: 0.8209, F1: 0.8337
LM Predictions:  [4, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2934, Accuracy: 0.8333, Precision: 0.8833, Recall: 0.8691, F1: 0.8438
Epoch 18/70
Train Loss: 0.0679, Accuracy: 0.9654, Precision: 0.9421, Recall: 0.9433, F1: 0.9427
Validation Loss: 0.7302, Accuracy: 0.8614, Precision: 0.8126, Recall: 0.8071, F1: 0.8061
Testing Loss: 0.6269, Accuracy: 0.8671, Precision: 0.8262, Recall: 0.8023, F1: 0.8121
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2447, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9055, F1: 0.8804
Epoch 19/70
Train Loss: 0.0613, Accuracy: 0.9656, Precision: 0.9449, Recall: 0.9418, F1: 0.9433
Validation Loss: 0.7375, Accuracy: 0.8657, Precision: 0.8154, Recall: 0.8051, F1: 0.8067
Testing Loss: 0.6802, Accuracy: 0.8635, Precision: 0.8260, Recall: 0.7808, F1: 0.7941
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1495, Accuracy: 0.9762, Precision: 0.9667, Recall: 0.9818, F1: 0.9723
Epoch 20/70
Train Loss: 0.0625, Accuracy: 0.9673, Precision: 0.9491, Recall: 0.9451, F1: 0.9471
Validation Loss: 0.8229, Accuracy: 0.8571, Precision: 0.8094, Recall: 0.7991, F1: 0.8009
Testing Loss: 0.6905, Accuracy: 0.8671, Precision: 0.8334, Recall: 0.8030, F1: 0.8155
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1936, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9218, F1: 0.9003
Epoch 21/70
Train Loss: 0.0710, Accuracy: 0.9642, Precision: 0.9408, Recall: 0.9447, F1: 0.9427
Validation Loss: 0.8050, Accuracy: 0.8422, Precision: 0.7979, Recall: 0.7720, F1: 0.7758
Testing Loss: 0.6649, Accuracy: 0.8611, Precision: 0.8202, Recall: 0.7681, F1: 0.7833
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3390, Accuracy: 0.7381, Precision: 0.8417, Recall: 0.7945, F1: 0.7676
Epoch 22/70
Train Loss: 0.0685, Accuracy: 0.9637, Precision: 0.9456, Recall: 0.9338, F1: 0.9393
Validation Loss: 0.7845, Accuracy: 0.8721, Precision: 0.8116, Recall: 0.8188, F1: 0.8122
Testing Loss: 0.7292, Accuracy: 0.8696, Precision: 0.8169, Recall: 0.8042, F1: 0.8074
LM Predictions:  [4, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2510, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8855, F1: 0.8578
Epoch 23/70
Train Loss: 0.0662, Accuracy: 0.9654, Precision: 0.9460, Recall: 0.9423, F1: 0.9441
Validation Loss: 0.7121, Accuracy: 0.8593, Precision: 0.8013, Recall: 0.7960, F1: 0.7950
Testing Loss: 0.6460, Accuracy: 0.8744, Precision: 0.8287, Recall: 0.8000, F1: 0.8089
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2726, Accuracy: 0.8571, Precision: 0.8750, Recall: 0.8891, F1: 0.8607
Epoch 24/70
Train Loss: 0.0724, Accuracy: 0.9616, Precision: 0.9395, Recall: 0.9478, F1: 0.9434
Validation Loss: 0.8889, Accuracy: 0.8678, Precision: 0.8230, Recall: 0.7991, F1: 0.8040
Testing Loss: 0.7415, Accuracy: 0.8744, Precision: 0.8519, Recall: 0.8027, F1: 0.8189
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2284, Accuracy: 0.8333, Precision: 0.8742, Recall: 0.8673, F1: 0.8366
Epoch 25/70
Train Loss: 0.0660, Accuracy: 0.9682, Precision: 0.9524, Recall: 0.9450, F1: 0.9485
Validation Loss: 0.9565, Accuracy: 0.8571, Precision: 0.8147, Recall: 0.8068, F1: 0.8095
Testing Loss: 0.8269, Accuracy: 0.8732, Precision: 0.8444, Recall: 0.8221, F1: 0.8323
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2744, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 26/70
Train Loss: 0.0674, Accuracy: 0.9689, Precision: 0.9507, Recall: 0.9540, F1: 0.9523
Validation Loss: 0.7968, Accuracy: 0.8507, Precision: 0.7929, Recall: 0.7979, F1: 0.7929
Testing Loss: 0.6789, Accuracy: 0.8720, Precision: 0.8345, Recall: 0.8127, F1: 0.8220
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1826, Accuracy: 0.9286, Precision: 0.9262, Recall: 0.9418, F1: 0.9262
Epoch 27/70
Train Loss: 0.0536, Accuracy: 0.9689, Precision: 0.9508, Recall: 0.9534, F1: 0.9520
Validation Loss: 0.8695, Accuracy: 0.8742, Precision: 0.8350, Recall: 0.8150, F1: 0.8206
Testing Loss: 0.7838, Accuracy: 0.8720, Precision: 0.8424, Recall: 0.7990, F1: 0.8126
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2312, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9218, F1: 0.9003
Epoch 28/70
Train Loss: 0.0539, Accuracy: 0.9659, Precision: 0.9494, Recall: 0.9408, F1: 0.9449
Validation Loss: 0.8248, Accuracy: 0.8699, Precision: 0.8183, Recall: 0.8154, F1: 0.8154
Testing Loss: 0.7473, Accuracy: 0.8720, Precision: 0.8441, Recall: 0.8121, F1: 0.8255
LM Predictions:  [4, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2566, Accuracy: 0.8333, Precision: 0.8687, Recall: 0.8709, F1: 0.8429
Epoch 29/70
Train Loss: 0.0492, Accuracy: 0.9689, Precision: 0.9550, Recall: 0.9392, F1: 0.9464
Validation Loss: 0.8042, Accuracy: 0.8678, Precision: 0.8128, Recall: 0.8083, F1: 0.8090
Testing Loss: 0.7382, Accuracy: 0.8744, Precision: 0.8481, Recall: 0.8269, F1: 0.8362
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3268, Accuracy: 0.8571, Precision: 0.8818, Recall: 0.8836, F1: 0.8556
Epoch 30/70
Train Loss: 0.0460, Accuracy: 0.9718, Precision: 0.9518, Recall: 0.9604, F1: 0.9559
Validation Loss: 0.9005, Accuracy: 0.8614, Precision: 0.8088, Recall: 0.8102, F1: 0.8080
Testing Loss: 0.8125, Accuracy: 0.8732, Precision: 0.8451, Recall: 0.8212, F1: 0.8317
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2703, Accuracy: 0.8333, Precision: 0.8687, Recall: 0.8727, F1: 0.8406
Epoch 31/70
Train Loss: 0.0548, Accuracy: 0.9701, Precision: 0.9541, Recall: 0.9501, F1: 0.9521
Validation Loss: 0.9181, Accuracy: 0.8443, Precision: 0.7852, Recall: 0.7908, F1: 0.7860
Testing Loss: 0.7614, Accuracy: 0.8684, Precision: 0.8283, Recall: 0.8233, F1: 0.8256
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 2, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2876, Accuracy: 0.9524, Precision: 0.9692, Recall: 0.9418, F1: 0.9516
Epoch 32/70
Train Loss: 0.0587, Accuracy: 0.9644, Precision: 0.9418, Recall: 0.9428, F1: 0.9423
Validation Loss: 0.9007, Accuracy: 0.8593, Precision: 0.8053, Recall: 0.8055, F1: 0.8030
Testing Loss: 0.7994, Accuracy: 0.8732, Precision: 0.8394, Recall: 0.8098, F1: 0.8217
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2078, Accuracy: 0.9286, Precision: 0.9247, Recall: 0.9436, F1: 0.9284
Epoch 33/70
Train Loss: 0.0703, Accuracy: 0.9661, Precision: 0.9452, Recall: 0.9519, F1: 0.9484
Validation Loss: 0.8709, Accuracy: 0.8614, Precision: 0.8107, Recall: 0.8103, F1: 0.8082
Testing Loss: 0.7871, Accuracy: 0.8720, Precision: 0.8340, Recall: 0.8050, F1: 0.8155
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2369, Accuracy: 0.8571, Precision: 0.8833, Recall: 0.8873, F1: 0.8580
Epoch 34/70
Train Loss: 0.0504, Accuracy: 0.9668, Precision: 0.9477, Recall: 0.9462, F1: 0.9470
Validation Loss: 0.9325, Accuracy: 0.8614, Precision: 0.8130, Recall: 0.8054, F1: 0.8082
Testing Loss: 0.8429, Accuracy: 0.8708, Precision: 0.8378, Recall: 0.8089, F1: 0.8210
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1962, Accuracy: 0.9286, Precision: 0.9247, Recall: 0.9436, F1: 0.9284
Epoch 35/70
Train Loss: 0.0546, Accuracy: 0.9670, Precision: 0.9461, Recall: 0.9473, F1: 0.9467
Validation Loss: 0.9779, Accuracy: 0.8614, Precision: 0.8049, Recall: 0.8059, F1: 0.8012
Testing Loss: 0.8704, Accuracy: 0.8708, Precision: 0.8293, Recall: 0.7992, F1: 0.8089
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2210, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8527, F1: 0.8246
Epoch 36/70
Train Loss: 0.0479, Accuracy: 0.9680, Precision: 0.9486, Recall: 0.9466, F1: 0.9476
Validation Loss: 0.8772, Accuracy: 0.8763, Precision: 0.8271, Recall: 0.8125, F1: 0.8150
Testing Loss: 0.8484, Accuracy: 0.8684, Precision: 0.8300, Recall: 0.7863, F1: 0.7984
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2131, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 37/70
Train Loss: 0.0583, Accuracy: 0.9649, Precision: 0.9418, Recall: 0.9465, F1: 0.9441
Validation Loss: 0.8981, Accuracy: 0.8571, Precision: 0.8153, Recall: 0.7967, F1: 0.8005
Testing Loss: 0.8010, Accuracy: 0.8551, Precision: 0.8121, Recall: 0.7867, F1: 0.7964
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 3, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2984, Accuracy: 0.7619, Precision: 0.8319, Recall: 0.8164, F1: 0.7797
Epoch 38/70
Train Loss: 0.0716, Accuracy: 0.9647, Precision: 0.9445, Recall: 0.9443, F1: 0.9444
Validation Loss: 0.9136, Accuracy: 0.8529, Precision: 0.8054, Recall: 0.8042, F1: 0.8008
Testing Loss: 0.8586, Accuracy: 0.8539, Precision: 0.8220, Recall: 0.7994, F1: 0.8044
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2360, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 39/70
Train Loss: 0.0629, Accuracy: 0.9656, Precision: 0.9414, Recall: 0.9493, F1: 0.9452
Validation Loss: 0.9605, Accuracy: 0.8699, Precision: 0.8191, Recall: 0.8161, F1: 0.8156
Testing Loss: 0.8641, Accuracy: 0.8708, Precision: 0.8397, Recall: 0.8032, F1: 0.8160
LM Predictions:  [4, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1927, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9236, F1: 0.9006
Epoch 40/70
Train Loss: 0.0598, Accuracy: 0.9675, Precision: 0.9517, Recall: 0.9473, F1: 0.9493
Validation Loss: 0.9219, Accuracy: 0.8678, Precision: 0.8185, Recall: 0.8199, F1: 0.8186
Testing Loss: 0.8749, Accuracy: 0.8635, Precision: 0.8347, Recall: 0.8077, F1: 0.8184
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2777, Accuracy: 0.7381, Precision: 0.8625, Recall: 0.7927, F1: 0.7692
Epoch 41/70
Train Loss: 0.0540, Accuracy: 0.9677, Precision: 0.9489, Recall: 0.9488, F1: 0.9488
Validation Loss: 0.9512, Accuracy: 0.8657, Precision: 0.8221, Recall: 0.8100, F1: 0.8138
Testing Loss: 0.8173, Accuracy: 0.8732, Precision: 0.8403, Recall: 0.8042, F1: 0.8162
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2217, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8327, F1: 0.8070
Epoch 42/70
Train Loss: 0.0483, Accuracy: 0.9701, Precision: 0.9546, Recall: 0.9523, F1: 0.9535
Validation Loss: 0.9666, Accuracy: 0.8571, Precision: 0.8025, Recall: 0.8037, F1: 0.8013
Testing Loss: 0.8947, Accuracy: 0.8659, Precision: 0.8315, Recall: 0.8039, F1: 0.8151
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1842, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.9018
Epoch 43/70
Train Loss: 0.0516, Accuracy: 0.9715, Precision: 0.9558, Recall: 0.9535, F1: 0.9546
Validation Loss: 0.9008, Accuracy: 0.8593, Precision: 0.8045, Recall: 0.8079, F1: 0.8047
Testing Loss: 0.8665, Accuracy: 0.8696, Precision: 0.8345, Recall: 0.8109, F1: 0.8207
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1658, Accuracy: 0.9048, Precision: 0.9083, Recall: 0.9273, F1: 0.9007
Epoch 44/70
Train Loss: 0.0429, Accuracy: 0.9727, Precision: 0.9588, Recall: 0.9561, F1: 0.9574
Validation Loss: 0.9119, Accuracy: 0.8593, Precision: 0.8087, Recall: 0.8070, F1: 0.8068
Testing Loss: 0.8811, Accuracy: 0.8720, Precision: 0.8381, Recall: 0.8145, F1: 0.8249
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2087, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 45/70
Train Loss: 0.0411, Accuracy: 0.9720, Precision: 0.9530, Recall: 0.9597, F1: 0.9561
Validation Loss: 0.9529, Accuracy: 0.8657, Precision: 0.8189, Recall: 0.8162, F1: 0.8147
Testing Loss: 0.8969, Accuracy: 0.8732, Precision: 0.8380, Recall: 0.8130, F1: 0.8223
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1935, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 46/70
Train Loss: 0.0410, Accuracy: 0.9706, Precision: 0.9522, Recall: 0.9552, F1: 0.9537
Validation Loss: 0.9671, Accuracy: 0.8678, Precision: 0.8235, Recall: 0.8156, F1: 0.8172
Testing Loss: 0.9429, Accuracy: 0.8720, Precision: 0.8415, Recall: 0.8019, F1: 0.8160
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2008, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9200, F1: 0.8929
Epoch 47/70
Train Loss: 0.0584, Accuracy: 0.9656, Precision: 0.9505, Recall: 0.9414, F1: 0.9457
Validation Loss: 0.9043, Accuracy: 0.8678, Precision: 0.8142, Recall: 0.8100, F1: 0.8090
Testing Loss: 0.7686, Accuracy: 0.8756, Precision: 0.8376, Recall: 0.8081, F1: 0.8194
LM Predictions:  [0, 3, 4, 3, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2665, Accuracy: 0.8095, Precision: 0.8583, Recall: 0.8545, F1: 0.8206
Epoch 48/70
Train Loss: 0.0532, Accuracy: 0.9696, Precision: 0.9501, Recall: 0.9547, F1: 0.9523
Validation Loss: 0.9964, Accuracy: 0.8635, Precision: 0.8219, Recall: 0.8025, F1: 0.8075
Testing Loss: 0.9238, Accuracy: 0.8659, Precision: 0.8367, Recall: 0.7823, F1: 0.8013
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3177, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 49/70
Train Loss: 0.0518, Accuracy: 0.9663, Precision: 0.9431, Recall: 0.9495, F1: 0.9462
Validation Loss: 1.0012, Accuracy: 0.8721, Precision: 0.8201, Recall: 0.8146, F1: 0.8128
Testing Loss: 0.8739, Accuracy: 0.8720, Precision: 0.8384, Recall: 0.7880, F1: 0.8015
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1777, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.9018
Epoch 50/70
Train Loss: 0.0647, Accuracy: 0.9637, Precision: 0.9427, Recall: 0.9462, F1: 0.9444
Validation Loss: 1.0173, Accuracy: 0.8657, Precision: 0.8213, Recall: 0.8065, F1: 0.8098
Testing Loss: 0.8796, Accuracy: 0.8708, Precision: 0.8394, Recall: 0.8005, F1: 0.8144
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2290, Accuracy: 0.8333, Precision: 0.8687, Recall: 0.8673, F1: 0.8402
Epoch 51/70
Train Loss: 0.0458, Accuracy: 0.9701, Precision: 0.9549, Recall: 0.9496, F1: 0.9522
Validation Loss: 1.0029, Accuracy: 0.8699, Precision: 0.8228, Recall: 0.8177, F1: 0.8171
Testing Loss: 0.9092, Accuracy: 0.8756, Precision: 0.8432, Recall: 0.8064, F1: 0.8186
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2422, Accuracy: 0.7619, Precision: 0.8514, Recall: 0.8109, F1: 0.7822
Epoch 52/70
Train Loss: 0.0505, Accuracy: 0.9694, Precision: 0.9528, Recall: 0.9478, F1: 0.9502
Validation Loss: 0.8837, Accuracy: 0.8721, Precision: 0.8294, Recall: 0.8261, F1: 0.8249
Testing Loss: 0.8550, Accuracy: 0.8744, Precision: 0.8449, Recall: 0.8144, F1: 0.8275
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 5, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.4976, Accuracy: 0.6905, Precision: 0.6979, Recall: 0.6303, F1: 0.6115
Epoch 53/70
Train Loss: 0.0481, Accuracy: 0.9704, Precision: 0.9525, Recall: 0.9492, F1: 0.9508
Validation Loss: 1.0417, Accuracy: 0.8550, Precision: 0.8009, Recall: 0.8138, F1: 0.8055
Testing Loss: 0.8769, Accuracy: 0.8732, Precision: 0.8395, Recall: 0.8237, F1: 0.8309
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1725, Accuracy: 0.9048, Precision: 0.9028, Recall: 0.9273, F1: 0.9043
Epoch 54/70
Train Loss: 0.0485, Accuracy: 0.9682, Precision: 0.9500, Recall: 0.9535, F1: 0.9517
Validation Loss: 1.0285, Accuracy: 0.8678, Precision: 0.8246, Recall: 0.8173, F1: 0.8158
Testing Loss: 0.9436, Accuracy: 0.8732, Precision: 0.8399, Recall: 0.8018, F1: 0.8140
LM Predictions:  [0, 3, 4, 3, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1790, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.9018
Epoch 55/70
Train Loss: 0.0431, Accuracy: 0.9704, Precision: 0.9608, Recall: 0.9405, F1: 0.9494
Validation Loss: 1.0252, Accuracy: 0.8550, Precision: 0.8003, Recall: 0.8149, F1: 0.8058
Testing Loss: 0.9363, Accuracy: 0.8696, Precision: 0.8289, Recall: 0.8063, F1: 0.8135
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2485, Accuracy: 0.8095, Precision: 0.8769, Recall: 0.8473, F1: 0.8167
Epoch 56/70
Train Loss: 0.0456, Accuracy: 0.9713, Precision: 0.9490, Recall: 0.9662, F1: 0.9566
Validation Loss: 1.0111, Accuracy: 0.8657, Precision: 0.8230, Recall: 0.8174, F1: 0.8165
Testing Loss: 0.8950, Accuracy: 0.8792, Precision: 0.8434, Recall: 0.8146, F1: 0.8250
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2529, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8327, F1: 0.8070
Epoch 57/70
Train Loss: 0.0423, Accuracy: 0.9734, Precision: 0.9610, Recall: 0.9544, F1: 0.9576
Validation Loss: 1.0243, Accuracy: 0.8593, Precision: 0.8057, Recall: 0.8177, F1: 0.8104
Testing Loss: 0.9695, Accuracy: 0.8744, Precision: 0.8416, Recall: 0.8233, F1: 0.8312
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2088, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 58/70
Train Loss: 0.0471, Accuracy: 0.9692, Precision: 0.9532, Recall: 0.9433, F1: 0.9480
Validation Loss: 0.9809, Accuracy: 0.8678, Precision: 0.8225, Recall: 0.8054, F1: 0.8063
Testing Loss: 0.9508, Accuracy: 0.8647, Precision: 0.8159, Recall: 0.7775, F1: 0.7858
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2589, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 59/70
Train Loss: 0.0447, Accuracy: 0.9706, Precision: 0.9564, Recall: 0.9467, F1: 0.9513
Validation Loss: 0.9917, Accuracy: 0.8657, Precision: 0.8087, Recall: 0.8251, F1: 0.8154
Testing Loss: 0.9195, Accuracy: 0.8708, Precision: 0.8342, Recall: 0.8186, F1: 0.8255
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1904, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 60/70
Train Loss: 0.0446, Accuracy: 0.9692, Precision: 0.9481, Recall: 0.9547, F1: 0.9513
Validation Loss: 1.0439, Accuracy: 0.8678, Precision: 0.8183, Recall: 0.8112, F1: 0.8125
Testing Loss: 0.9401, Accuracy: 0.8756, Precision: 0.8466, Recall: 0.8127, F1: 0.8262
LM Predictions:  [4, 3, 4, 0, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2526, Accuracy: 0.7857, Precision: 0.8519, Recall: 0.8291, F1: 0.7990
Epoch 61/70
Train Loss: 0.0402, Accuracy: 0.9704, Precision: 0.9530, Recall: 0.9509, F1: 0.9519
Validation Loss: 1.0845, Accuracy: 0.8635, Precision: 0.8043, Recall: 0.8130, F1: 0.8058
Testing Loss: 0.9325, Accuracy: 0.8816, Precision: 0.8504, Recall: 0.8217, F1: 0.8321
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 4, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3315, Accuracy: 0.7143, Precision: 0.8403, Recall: 0.7527, F1: 0.7386
Epoch 62/70
Train Loss: 0.0851, Accuracy: 0.9618, Precision: 0.9405, Recall: 0.9379, F1: 0.9392
Validation Loss: 0.8982, Accuracy: 0.8465, Precision: 0.8042, Recall: 0.7912, F1: 0.7927
Testing Loss: 0.7061, Accuracy: 0.8720, Precision: 0.8263, Recall: 0.8020, F1: 0.8113
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 5, 1, 2, 1, 0, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 0, 5, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.3863, Accuracy: 0.8095, Precision: 0.7292, Recall: 0.7121, F1: 0.6969
Epoch 63/70
Train Loss: 0.0606, Accuracy: 0.9670, Precision: 0.9554, Recall: 0.9340, F1: 0.9433
Validation Loss: 0.9814, Accuracy: 0.8529, Precision: 0.8014, Recall: 0.7897, F1: 0.7914
Testing Loss: 0.8129, Accuracy: 0.8696, Precision: 0.8408, Recall: 0.8041, F1: 0.8183
LM Predictions:  [4, 3, 4, 0, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1915, Accuracy: 0.8810, Precision: 0.8944, Recall: 0.9018, F1: 0.8746
Epoch 64/70
Train Loss: 0.0486, Accuracy: 0.9682, Precision: 0.9450, Recall: 0.9532, F1: 0.9489
Validation Loss: 1.1073, Accuracy: 0.8593, Precision: 0.8133, Recall: 0.8011, F1: 0.8031
Testing Loss: 0.8881, Accuracy: 0.8768, Precision: 0.8489, Recall: 0.8114, F1: 0.8251
LM Predictions:  [0, 3, 4, 0, 2, 0, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 0, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2513, Accuracy: 0.7143, Precision: 0.8375, Recall: 0.7745, F1: 0.7482
Epoch 65/70
Train Loss: 0.0497, Accuracy: 0.9680, Precision: 0.9521, Recall: 0.9438, F1: 0.9478
Validation Loss: 0.9876, Accuracy: 0.8635, Precision: 0.8081, Recall: 0.8173, F1: 0.8112
Testing Loss: 0.8203, Accuracy: 0.8732, Precision: 0.8402, Recall: 0.8091, F1: 0.8211
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1773, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984
Epoch 66/70
Train Loss: 0.0509, Accuracy: 0.9701, Precision: 0.9527, Recall: 0.9523, F1: 0.9524
Validation Loss: 0.9559, Accuracy: 0.8678, Precision: 0.8264, Recall: 0.8245, F1: 0.8246
Testing Loss: 0.8567, Accuracy: 0.8768, Precision: 0.8461, Recall: 0.8197, F1: 0.8312
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 2, 0, 2, 4, 0, 2, 3, 4, 0, 2, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2353, Accuracy: 0.8095, Precision: 0.8667, Recall: 0.8473, F1: 0.8145
Epoch 67/70
Train Loss: 0.0432, Accuracy: 0.9711, Precision: 0.9556, Recall: 0.9512, F1: 0.9534
Validation Loss: 1.0362, Accuracy: 0.8657, Precision: 0.8228, Recall: 0.8142, F1: 0.8172
Testing Loss: 0.8559, Accuracy: 0.8768, Precision: 0.8481, Recall: 0.8133, F1: 0.8280
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 2, 0, 2, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1818, Accuracy: 0.9286, Precision: 0.9229, Recall: 0.9455, F1: 0.9286
Epoch 68/70
Train Loss: 0.0442, Accuracy: 0.9701, Precision: 0.9498, Recall: 0.9573, F1: 0.9534
Validation Loss: 0.9775, Accuracy: 0.8550, Precision: 0.7969, Recall: 0.8162, F1: 0.8048
Testing Loss: 0.8139, Accuracy: 0.8792, Precision: 0.8441, Recall: 0.8302, F1: 0.8356
LM Predictions:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1746, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Epoch 69/70
Train Loss: 0.0404, Accuracy: 0.9699, Precision: 0.9521, Recall: 0.9509, F1: 0.9515
Validation Loss: 1.0139, Accuracy: 0.8678, Precision: 0.8266, Recall: 0.8037, F1: 0.8094
Testing Loss: 0.8692, Accuracy: 0.8696, Precision: 0.8351, Recall: 0.7963, F1: 0.8078
LM Predictions:  [0, 3, 4, 0, 2, 2, 0, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 0, 0, 0, 2, 4, 0, 2, 4, 0, 2, 3, 4, 0, 0, 2, 0, 0, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.2170, Accuracy: 0.7857, Precision: 0.8714, Recall: 0.8291, F1: 0.8037
Epoch 70/70
Train Loss: 0.0420, Accuracy: 0.9723, Precision: 0.9592, Recall: 0.9468, F1: 0.9527
Validation Loss: 1.0841, Accuracy: 0.8593, Precision: 0.8060, Recall: 0.8161, F1: 0.8084
Testing Loss: 0.8858, Accuracy: 0.8804, Precision: 0.8417, Recall: 0.8230, F1: 0.8300
LM Predictions:  [4, 3, 4, 3, 2, 0, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 0, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 0, 4, 0, 2, 3, 4, 3, 0, 2, 0, 4, 3]
LM Labels:  [4, 3, 4, 3, 2, 2, 4, 1, 4, 1, 2, 1, 4, 3, 2, 3, 2, 0, 1, 1, 3, 2, 4, 4, 3, 0, 3, 2, 4, 0, 2, 4, 0, 2, 3, 4, 3, 2, 2, 0, 4, 3]
LM Loss: 0.1331, Accuracy: 0.9048, Precision: 0.9111, Recall: 0.9273, F1: 0.8984

