---------------------------------------------------------------------------
Results for seed:  28
Model: distilbert-base-uncased, Batch size: 16, Epochs: 70
Learning rate: 2e-05, Device: cuda:0
Noise: 1% with label 5
Label counts for Train:
  Label 0: 4666
  Label 3: 2159
  Label 2: 1304
  Label 5: 572
  Label 4: 1937
  Label 1: 5362
Label counts for Validation:
  Label 0: 550
  Label 2: 178
  Label 3: 275
  Label 1: 704
  Label 4: 212
  Label 5: 81
Label counts for Test:
  Label 0: 581
  Label 1: 695
  Label 4: 224
  Label 3: 275
  Label 2: 159
  Label 5: 66
160
Actual labels:  [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]
Label counts for Train:
  Label 0: 4693
  Label 3: 2195
  Label 2: 1341
  Label 5: 412
  Label 4: 1969
  Label 1: 5390
Layer: backbone.embeddings.word_embeddings.weight, Size: torch.Size([30522, 768]), req grad: True
Layer: backbone.embeddings.position_embeddings.weight, Size: torch.Size([512, 768]), req grad: True
Layer: backbone.embeddings.LayerNorm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.embeddings.LayerNorm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.0.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.0.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.0.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.0.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.0.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.0.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.0.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.0.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.1.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.1.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.1.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.1.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.1.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.1.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.1.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.1.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.2.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.2.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.2.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.2.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.2.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.2.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.2.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.2.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.3.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.3.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.3.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.3.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.3.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.3.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.3.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.3.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.4.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.4.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.4.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.4.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.4.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.4.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.4.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.4.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.attention.q_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.5.attention.q_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.attention.k_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.5.attention.k_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.attention.v_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.5.attention.v_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.attention.out_lin.weight, Size: torch.Size([768, 768]), req grad: True
Layer: backbone.transformer.layer.5.attention.out_lin.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.sa_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.sa_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.ffn.lin1.weight, Size: torch.Size([3072, 768]), req grad: True
Layer: backbone.transformer.layer.5.ffn.lin1.bias, Size: torch.Size([3072]), req grad: True
Layer: backbone.transformer.layer.5.ffn.lin2.weight, Size: torch.Size([768, 3072]), req grad: True
Layer: backbone.transformer.layer.5.ffn.lin2.bias, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.output_layer_norm.weight, Size: torch.Size([768]), req grad: True
Layer: backbone.transformer.layer.5.output_layer_norm.bias, Size: torch.Size([768]), req grad: True
Layer: classifier.weight, Size: torch.Size([6, 768]), req grad: True
Testing Loss: 0.4912, Accuracy: 0.9220, Precision: 0.8837, Recall: 0.8395, F1: 0.8541
LM Predictions:  [0, 1, 4, 2, 2, 3, 4, 1, 2, 3, 3, 4, 2, 0, 1, 3, 3, 1, 4, 2, 2, 3, 4, 1, 1, 2, 4, 2, 2, 3, 4, 1, 3, 4, 0, 2, 4, 4, 1, 1, 2, 3, 2, 4, 2, 3, 0, 1, 0, 2, 1, 4, 1, 0, 4, 3, 3, 3, 3, 0, 0, 2, 3, 2, 1, 3, 0, 4, 4, 0, 3, 0, 0, 4, 1, 1, 0, 0, 3, 2, 0, 2, 4, 3, 2, 4, 0, 3, 3, 1, 1, 2, 3, 1, 2, 4, 4, 0, 0, 0, 3, 1, 3, 4, 0, 1, 2, 2, 2, 4, 1, 2, 3, 3, 3, 1, 3, 1, 3, 2, 3, 1, 3, 0, 4, 2, 3, 3, 4, 4, 2, 0, 2, 1, 3, 4, 4, 2, 0, 1, 0, 0, 2, 2, 4, 4, 1, 2, 1, 0, 2, 4, 3, 4, 0, 4, 2, 3, 2, 2]
LM Labels:  [0, 1, 4, 2, 2, 3, 4, 1, 2, 3, 3, 4, 2, 0, 1, 3, 3, 1, 4, 2, 2, 3, 4, 1, 1, 2, 4, 2, 2, 3, 4, 1, 3, 4, 0, 2, 4, 4, 1, 1, 2, 3, 2, 4, 2, 3, 0, 1, 0, 2, 1, 4, 1, 0, 4, 3, 3, 3, 3, 0, 0, 2, 3, 2, 1, 3, 0, 4, 4, 0, 3, 0, 0, 4, 1, 1, 0, 0, 3, 2, 0, 2, 4, 3, 2, 4, 0, 3, 3, 1, 1, 2, 3, 1, 2, 4, 4, 0, 0, 0, 3, 1, 3, 4, 0, 1, 2, 2, 2, 4, 1, 2, 3, 3, 3, 1, 3, 1, 3, 2, 3, 1, 3, 0, 4, 2, 3, 3, 4, 4, 2, 0, 2, 1, 3, 4, 4, 2, 0, 1, 0, 0, 2, 2, 4, 4, 1, 2, 1, 0, 2, 4, 3, 4, 0, 4, 2, 3, 2, 2]
LM Loss: 0.0075, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000
Attention LayerNorm grads:  {5: 1.2407176654960494e-06, 4: 4.049741619382985e-06, 3: 7.47532294553821e-06, 2: 7.914652087492868e-06, 1: 8.937400707509369e-06, 0: 9.078130460693501e-06}
Output LayerNorm grads:  {5: 1.993711066461401e-06, 4: 4.612661541614216e-06, 3: 6.3151114773063455e-06, 2: 6.608424428122817e-06, 1: 7.860947334847879e-06, 0: 7.591520898131421e-06}

Attention LayerNorm grads:  {5: 2.8918744646944106e-05, 4: 0.00015172015991993248, 3: 0.0004992762114852667, 2: 0.000863031018525362, 1: 0.0011913367779925466, 0: 0.001438827021047473}
Output LayerNorm grads:  {5: 3.881143857142888e-05, 4: 0.00016303079610224813, 3: 0.000387582607800141, 2: 0.0006823150906711817, 1: 0.001020825351588428, 0: 0.001127638854086399}

---------------------------------------------------------------------------